/home/matheus/tcc_implementation/venv/bin/python /home/matheus/tcc_implementation/agents/dqn/keras_rl_train_dqn_agent.py
Using TensorFlow backend.
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2019-10-31 22:45:40.285625: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-31 22:45:40.307327: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2019-10-31 22:45:40.307624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a6e7a0 executing computations on platform Host. Devices:
2019-10-31 22:45:40.307647: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-31 22:45:40.329973: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Training for 1100000 steps ...
WARNING:tensorflow:From /home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

     160/1100000: episode: 1, duration: 0.193s, episode steps: 160, steps per second: 831, episode reward: -987.103, mean reward: -6.169 [-100.000, 2.138], mean action: 1.956 [0.000, 3.000], mean observation: 0.449 [-3.266, 5.195], loss: --, mae: --, mean_q: --
     322/1100000: episode: 2, duration: 0.132s, episode steps: 162, steps per second: 1232, episode reward: -686.288, mean reward: -4.236 [-100.000, 4.006], mean action: 1.963 [0.000, 3.000], mean observation: 0.260 [-0.635, 3.977], loss: --, mae: --, mean_q: --
     432/1100000: episode: 3, duration: 0.078s, episode steps: 110, steps per second: 1411, episode reward: -851.360, mean reward: -7.740 [-100.000, 0.593], mean action: 1.955 [0.000, 3.000], mean observation: 0.516 [-0.701, 5.157], loss: --, mae: --, mean_q: --
     515/1100000: episode: 4, duration: 0.062s, episode steps: 83, steps per second: 1330, episode reward: -622.633, mean reward: -7.502 [-100.000, -0.228], mean action: 1.976 [1.000, 3.000], mean observation: 0.352 [-0.688, 3.366], loss: --, mae: --, mean_q: --
     620/1100000: episode: 5, duration: 0.080s, episode steps: 105, steps per second: 1320, episode reward: -747.842, mean reward: -7.122 [-100.000, -0.940], mean action: 1.952 [0.000, 3.000], mean observation: 0.541 [-0.308, 4.932], loss: --, mae: --, mean_q: --
     736/1100000: episode: 6, duration: 0.085s, episode steps: 116, steps per second: 1366, episode reward: -511.829, mean reward: -4.412 [-100.000, 2.834], mean action: 1.983 [0.000, 3.000], mean observation: 0.355 [-0.382, 3.114], loss: --, mae: --, mean_q: --
     813/1100000: episode: 7, duration: 0.052s, episode steps: 77, steps per second: 1485, episode reward: -475.919, mean reward: -6.181 [-100.000, 0.535], mean action: 1.948 [0.000, 3.000], mean observation: 0.344 [-0.631, 2.457], loss: --, mae: --, mean_q: --
     929/1100000: episode: 8, duration: 0.090s, episode steps: 116, steps per second: 1284, episode reward: -455.365, mean reward: -3.926 [-100.000, 1.276], mean action: 1.957 [0.000, 3.000], mean observation: 0.418 [-0.350, 2.864], loss: --, mae: --, mean_q: --
2019-10-31 22:45:41.834326: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 22:45:41.835933: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 22:45:41.850939: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-10-31 22:45:41.852088: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-10-31 22:45:41.853203: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 22:45:41.854645: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
    1033/1100000: episode: 9, duration: 0.914s, episode steps: 104, steps per second: 114, episode reward: -392.695, mean reward: -3.776 [-100.000, 0.979], mean action: 1.404 [0.000, 3.000], mean observation: 0.316 [-0.728, 2.428], loss: 48.794688, mae: 1.365297, mean_q: 0.037595
    1117/1100000: episode: 10, duration: 0.530s, episode steps: 84, steps per second: 158, episode reward: -266.486, mean reward: -3.172 [-100.000, 6.494], mean action: 1.048 [0.000, 3.000], mean observation: 0.102 [-1.709, 5.209], loss: 51.455811, mae: 2.455516, mean_q: -1.481738
    1199/1100000: episode: 11, duration: 0.498s, episode steps: 82, steps per second: 165, episode reward: -402.137, mean reward: -4.904 [-100.000, 2.297], mean action: 1.841 [0.000, 3.000], mean observation: -0.107 [-2.867, 1.431], loss: 44.345673, mae: 3.073177, mean_q: -2.225611
    1336/1100000: episode: 12, duration: 0.781s, episode steps: 137, steps per second: 176, episode reward: -297.286, mean reward: -2.170 [-100.000, 3.849], mean action: 1.000 [0.000, 3.000], mean observation: 0.114 [-2.746, 2.152], loss: 36.455200, mae: 3.417913, mean_q: -2.681124
    1468/1100000: episode: 13, duration: 0.793s, episode steps: 132, steps per second: 166, episode reward: -98.672, mean reward: -0.748 [-100.000, 58.728], mean action: 2.167 [0.000, 3.000], mean observation: 0.026 [-1.582, 5.649], loss: 29.083071, mae: 4.196517, mean_q: -3.705768
    1536/1100000: episode: 14, duration: 0.394s, episode steps: 68, steps per second: 173, episode reward: -408.220, mean reward: -6.003 [-100.000, 3.054], mean action: 2.265 [0.000, 3.000], mean observation: -0.041 [-2.291, 1.620], loss: 25.101606, mae: 5.127925, mean_q: -4.260367
    1632/1100000: episode: 15, duration: 0.550s, episode steps: 96, steps per second: 175, episode reward: -265.523, mean reward: -2.766 [-100.000, 7.071], mean action: 0.896 [0.000, 3.000], mean observation: 0.023 [-1.666, 4.009], loss: 26.813116, mae: 5.801439, mean_q: -5.118563
    1921/1100000: episode: 16, duration: 1.703s, episode steps: 289, steps per second: 170, episode reward: -245.219, mean reward: -0.849 [-100.000, 11.032], mean action: 2.073 [0.000, 3.000], mean observation: 0.034 [-4.173, 1.406], loss: 21.908365, mae: 7.248973, mean_q: -6.933491
    2103/1100000: episode: 17, duration: 1.042s, episode steps: 182, steps per second: 175, episode reward: -147.982, mean reward: -0.813 [-100.000, 11.478], mean action: 1.527 [0.000, 3.000], mean observation: 0.043 [-0.887, 1.639], loss: 17.372990, mae: 8.189783, mean_q: -8.130484
    2216/1100000: episode: 18, duration: 0.647s, episode steps: 113, steps per second: 175, episode reward: -175.870, mean reward: -1.556 [-100.000, 47.010], mean action: 1.619 [0.000, 3.000], mean observation: 0.083 [-1.592, 3.368], loss: 15.926367, mae: 8.481153, mean_q: -8.348001
    2314/1100000: episode: 19, duration: 0.555s, episode steps: 98, steps per second: 176, episode reward: -26.206, mean reward: -0.267 [-100.000, 8.469], mean action: 1.765 [0.000, 3.000], mean observation: 0.088 [-1.199, 2.637], loss: 15.478989, mae: 9.164917, mean_q: -8.760227
    2544/1100000: episode: 20, duration: 1.326s, episode steps: 230, steps per second: 173, episode reward: -394.218, mean reward: -1.714 [-100.000, 7.360], mean action: 1.665 [0.000, 3.000], mean observation: -0.065 [-4.429, 1.558], loss: 16.066545, mae: 9.707530, mean_q: -8.908576
    2947/1100000: episode: 21, duration: 2.451s, episode steps: 403, steps per second: 164, episode reward: -65.317, mean reward: -0.162 [-100.000, 10.214], mean action: 1.901 [0.000, 3.000], mean observation: -0.008 [-1.159, 1.393], loss: 15.225115, mae: 10.979014, mean_q: -9.335352
    3396/1100000: episode: 22, duration: 2.843s, episode steps: 449, steps per second: 158, episode reward: -400.956, mean reward: -0.893 [-100.000, 48.769], mean action: 1.637 [0.000, 3.000], mean observation: 0.089 [-2.928, 1.417], loss: 14.550908, mae: 11.126218, mean_q: -7.306797
    4396/1100000: episode: 23, duration: 6.683s, episode steps: 1000, steps per second: 150, episode reward: -101.182, mean reward: -0.101 [-5.304, 6.261], mean action: 1.894 [0.000, 3.000], mean observation: 0.091 [-0.683, 1.545], loss: 14.822157, mae: 12.099158, mean_q: -4.182720
    4628/1100000: episode: 24, duration: 1.355s, episode steps: 232, steps per second: 171, episode reward: -80.708, mean reward: -0.348 [-100.000, 5.230], mean action: 1.970 [0.000, 3.000], mean observation: 0.134 [-0.968, 1.429], loss: 12.162365, mae: 12.403054, mean_q: 0.131407
    5026/1100000: episode: 25, duration: 2.437s, episode steps: 398, steps per second: 163, episode reward: -101.884, mean reward: -0.256 [-100.000, 9.160], mean action: 1.980 [0.000, 3.000], mean observation: -0.027 [-3.224, 1.400], loss: 13.045236, mae: 13.330743, mean_q: 2.681241
    6026/1100000: episode: 26, duration: 6.411s, episode steps: 1000, steps per second: 156, episode reward: -107.685, mean reward: -0.108 [-4.285, 4.590], mean action: 1.888 [0.000, 3.000], mean observation: 0.095 [-0.600, 1.531], loss: 13.854479, mae: 14.975101, mean_q: 7.692975
    7026/1100000: episode: 27, duration: 6.547s, episode steps: 1000, steps per second: 153, episode reward: -92.322, mean reward: -0.092 [-4.690, 4.685], mean action: 1.773 [0.000, 3.000], mean observation: 0.122 [-0.751, 1.443], loss: 12.319445, mae: 17.358442, mean_q: 13.176258
    7577/1100000: episode: 28, duration: 3.453s, episode steps: 551, steps per second: 160, episode reward: -84.414, mean reward: -0.153 [-100.000, 7.621], mean action: 1.639 [0.000, 3.000], mean observation: 0.058 [-1.613, 1.390], loss: 13.324559, mae: 18.788330, mean_q: 16.837553
    8389/1100000: episode: 29, duration: 5.909s, episode steps: 812, steps per second: 137, episode reward: -130.568, mean reward: -0.161 [-100.000, 13.191], mean action: 1.714 [0.000, 3.000], mean observation: 0.074 [-1.849, 1.592], loss: 11.354944, mae: 20.527378, mean_q: 19.549484
    8570/1100000: episode: 30, duration: 1.060s, episode steps: 181, steps per second: 171, episode reward: -128.980, mean reward: -0.713 [-100.000, 2.995], mean action: 1.669 [0.000, 3.000], mean observation: 0.006 [-1.002, 1.400], loss: 11.494079, mae: 21.307198, mean_q: 21.065554
    9276/1100000: episode: 31, duration: 4.848s, episode steps: 706, steps per second: 146, episode reward: -237.535, mean reward: -0.336 [-100.000, 4.217], mean action: 1.762 [0.000, 3.000], mean observation: 0.006 [-1.000, 1.399], loss: 10.964606, mae: 21.770681, mean_q: 22.276184
    9508/1100000: episode: 32, duration: 1.427s, episode steps: 232, steps per second: 163, episode reward: -161.395, mean reward: -0.696 [-100.000, 3.455], mean action: 1.746 [0.000, 3.000], mean observation: 0.107 [-1.001, 1.424], loss: 10.015947, mae: 22.036270, mean_q: 22.503675
   10508/1100000: episode: 33, duration: 6.798s, episode steps: 1000, steps per second: 147, episode reward: -184.486, mean reward: -0.184 [-5.674, 4.523], mean action: 1.876 [0.000, 3.000], mean observation: 0.185 [-0.868, 1.905], loss: 10.887652, mae: 21.842890, mean_q: 23.387417
   10677/1100000: episode: 34, duration: 0.991s, episode steps: 169, steps per second: 171, episode reward: -493.079, mean reward: -2.918 [-100.000, 3.653], mean action: 1.663 [0.000, 3.000], mean observation: 0.178 [-4.227, 3.338], loss: 10.870693, mae: 21.779263, mean_q: 24.299412
   11120/1100000: episode: 35, duration: 2.690s, episode steps: 443, steps per second: 165, episode reward: -267.891, mean reward: -0.605 [-100.000, 17.857], mean action: 1.684 [0.000, 3.000], mean observation: 0.158 [-1.204, 4.649], loss: 9.285583, mae: 21.735155, mean_q: 24.217390
   11509/1100000: episode: 36, duration: 2.322s, episode steps: 389, steps per second: 168, episode reward: -180.611, mean reward: -0.464 [-100.000, 11.514], mean action: 1.694 [0.000, 3.000], mean observation: 0.072 [-1.019, 1.411], loss: 10.041561, mae: 22.178978, mean_q: 24.161938
   12509/1100000: episode: 37, duration: 7.069s, episode steps: 1000, steps per second: 141, episode reward: -238.870, mean reward: -0.239 [-5.739, 6.127], mean action: 1.706 [0.000, 3.000], mean observation: 0.002 [-0.709, 1.400], loss: 7.027880, mae: 22.808382, mean_q: 24.662477
   12981/1100000: episode: 38, duration: 2.948s, episode steps: 472, steps per second: 160, episode reward: -239.454, mean reward: -0.507 [-100.000, 57.037], mean action: 1.725 [0.000, 3.000], mean observation: 0.148 [-0.912, 2.544], loss: 8.607070, mae: 22.779409, mean_q: 25.379047
   13981/1100000: episode: 39, duration: 6.601s, episode steps: 1000, steps per second: 151, episode reward: -147.479, mean reward: -0.147 [-5.318, 4.842], mean action: 1.827 [0.000, 3.000], mean observation: 0.166 [-0.598, 1.502], loss: 7.786804, mae: 22.979252, mean_q: 25.370831
   14981/1100000: episode: 40, duration: 6.613s, episode steps: 1000, steps per second: 151, episode reward: -133.973, mean reward: -0.134 [-4.865, 4.546], mean action: 1.750 [0.000, 3.000], mean observation: 0.238 [-0.267, 1.729], loss: 8.677227, mae: 23.680647, mean_q: 27.305225
   15981/1100000: episode: 41, duration: 6.954s, episode steps: 1000, steps per second: 144, episode reward: -102.185, mean reward: -0.102 [-6.198, 4.696], mean action: 1.872 [0.000, 3.000], mean observation: 0.039 [-0.600, 1.392], loss: 7.720907, mae: 25.201557, mean_q: 29.838865
   16981/1100000: episode: 42, duration: 6.494s, episode steps: 1000, steps per second: 154, episode reward: -211.171, mean reward: -0.211 [-4.717, 4.244], mean action: 1.864 [0.000, 3.000], mean observation: 0.247 [-0.159, 1.673], loss: 5.997678, mae: 26.599874, mean_q: 31.793907
   17981/1100000: episode: 43, duration: 7.605s, episode steps: 1000, steps per second: 131, episode reward: -104.179, mean reward: -0.104 [-4.274, 5.106], mean action: 1.726 [0.000, 3.000], mean observation: 0.113 [-0.334, 1.407], loss: 8.258292, mae: 28.046074, mean_q: 33.982250
   18981/1100000: episode: 44, duration: 7.357s, episode steps: 1000, steps per second: 136, episode reward: -138.677, mean reward: -0.139 [-4.953, 4.577], mean action: 1.953 [0.000, 3.000], mean observation: 0.226 [-0.323, 1.396], loss: 5.854063, mae: 28.873219, mean_q: 35.707100
   19981/1100000: episode: 45, duration: 6.581s, episode steps: 1000, steps per second: 152, episode reward: -97.669, mean reward: -0.098 [-5.720, 5.300], mean action: 1.840 [0.000, 3.000], mean observation: 0.195 [-0.488, 1.390], loss: 5.322731, mae: 29.512547, mean_q: 36.984928
   20981/1100000: episode: 46, duration: 6.798s, episode steps: 1000, steps per second: 147, episode reward: -94.858, mean reward: -0.095 [-5.859, 5.678], mean action: 1.873 [0.000, 3.000], mean observation: 0.172 [-1.096, 1.419], loss: 6.111601, mae: 30.233938, mean_q: 37.717823
   21232/1100000: episode: 47, duration: 1.531s, episode steps: 251, steps per second: 164, episode reward: -30.323, mean reward: -0.121 [-100.000, 9.002], mean action: 2.008 [0.000, 3.000], mean observation: 0.019 [-0.924, 2.056], loss: 5.855216, mae: 30.688059, mean_q: 38.556835
   22232/1100000: episode: 48, duration: 6.739s, episode steps: 1000, steps per second: 148, episode reward: -87.760, mean reward: -0.088 [-6.274, 6.637], mean action: 1.895 [0.000, 3.000], mean observation: 0.180 [-0.726, 1.442], loss: 6.846412, mae: 30.969559, mean_q: 38.881691
   22536/1100000: episode: 49, duration: 1.878s, episode steps: 304, steps per second: 162, episode reward: -98.501, mean reward: -0.324 [-100.000, 7.470], mean action: 1.928 [0.000, 3.000], mean observation: -0.016 [-1.032, 2.680], loss: 5.011282, mae: 31.044359, mean_q: 39.302162
   22720/1100000: episode: 50, duration: 1.112s, episode steps: 184, steps per second: 165, episode reward: -103.899, mean reward: -0.565 [-100.000, 17.576], mean action: 1.777 [0.000, 3.000], mean observation: -0.028 [-0.889, 1.397], loss: 9.720405, mae: 31.613039, mean_q: 39.428562
   23138/1100000: episode: 51, duration: 2.629s, episode steps: 418, steps per second: 159, episode reward: -305.429, mean reward: -0.731 [-100.000, 12.200], mean action: 1.921 [0.000, 3.000], mean observation: 0.210 [-1.046, 3.728], loss: 4.826431, mae: 31.698027, mean_q: 39.650307
   23601/1100000: episode: 52, duration: 2.935s, episode steps: 463, steps per second: 158, episode reward: -369.380, mean reward: -0.798 [-100.000, 35.057], mean action: 1.812 [0.000, 3.000], mean observation: 0.159 [-2.112, 1.423], loss: 4.427280, mae: 31.814192, mean_q: 39.974300
   23806/1100000: episode: 53, duration: 1.259s, episode steps: 205, steps per second: 163, episode reward: -49.989, mean reward: -0.244 [-100.000, 17.158], mean action: 1.795 [0.000, 3.000], mean observation: 0.084 [-1.354, 1.393], loss: 4.848420, mae: 31.693378, mean_q: 39.907494
   24806/1100000: episode: 54, duration: 6.567s, episode steps: 1000, steps per second: 152, episode reward: -120.138, mean reward: -0.120 [-5.479, 5.614], mean action: 1.876 [0.000, 3.000], mean observation: 0.150 [-0.587, 1.399], loss: 5.660841, mae: 32.320839, mean_q: 40.669697
   25806/1100000: episode: 55, duration: 7.118s, episode steps: 1000, steps per second: 140, episode reward: -143.789, mean reward: -0.144 [-5.698, 5.716], mean action: 1.743 [0.000, 3.000], mean observation: 0.059 [-0.622, 1.392], loss: 5.444389, mae: 32.659237, mean_q: 41.225155
   26335/1100000: episode: 56, duration: 3.439s, episode steps: 529, steps per second: 154, episode reward: -237.413, mean reward: -0.449 [-100.000, 8.764], mean action: 1.699 [0.000, 3.000], mean observation: 0.091 [-1.060, 1.413], loss: 3.860315, mae: 33.151939, mean_q: 41.718597
   27335/1100000: episode: 57, duration: 7.225s, episode steps: 1000, steps per second: 138, episode reward: -59.647, mean reward: -0.060 [-4.610, 4.682], mean action: 1.601 [0.000, 3.000], mean observation: 0.039 [-0.738, 1.397], loss: 7.910267, mae: 33.481014, mean_q: 42.572037
   28335/1100000: episode: 58, duration: 7.379s, episode steps: 1000, steps per second: 136, episode reward: -113.682, mean reward: -0.114 [-5.036, 4.496], mean action: 1.815 [0.000, 3.000], mean observation: 0.196 [-0.385, 1.488], loss: 4.424606, mae: 32.849586, mean_q: 41.655075
   29335/1100000: episode: 59, duration: 6.595s, episode steps: 1000, steps per second: 152, episode reward: -124.401, mean reward: -0.124 [-4.872, 4.696], mean action: 1.828 [0.000, 3.000], mean observation: 0.145 [-0.480, 1.422], loss: 4.117467, mae: 32.343884, mean_q: 40.996223
   30335/1100000: episode: 60, duration: 7.197s, episode steps: 1000, steps per second: 139, episode reward: -95.494, mean reward: -0.095 [-4.891, 5.521], mean action: 1.838 [0.000, 3.000], mean observation: 0.161 [-0.506, 1.408], loss: 6.621068, mae: 31.612606, mean_q: 40.180798
   30750/1100000: episode: 61, duration: 2.642s, episode steps: 415, steps per second: 157, episode reward: -264.166, mean reward: -0.637 [-100.000, 47.468], mean action: 1.776 [0.000, 3.000], mean observation: 0.017 [-1.666, 1.402], loss: 3.557092, mae: 31.141024, mean_q: 39.653809
   31750/1100000: episode: 62, duration: 6.735s, episode steps: 1000, steps per second: 148, episode reward: -74.944, mean reward: -0.075 [-5.487, 5.245], mean action: 1.829 [0.000, 3.000], mean observation: 0.153 [-0.897, 1.496], loss: 6.341209, mae: 30.736338, mean_q: 38.568947
   32750/1100000: episode: 63, duration: 7.371s, episode steps: 1000, steps per second: 136, episode reward: -116.373, mean reward: -0.116 [-5.332, 5.437], mean action: 1.752 [0.000, 3.000], mean observation: 0.100 [-0.460, 1.405], loss: 5.367491, mae: 30.810764, mean_q: 38.696358
   32903/1100000: episode: 64, duration: 0.930s, episode steps: 153, steps per second: 165, episode reward: -42.122, mean reward: -0.275 [-100.000, 18.798], mean action: 1.477 [0.000, 3.000], mean observation: -0.015 [-0.875, 1.597], loss: 3.505194, mae: 30.815229, mean_q: 39.094189
   33903/1100000: episode: 65, duration: 6.832s, episode steps: 1000, steps per second: 146, episode reward: -68.216, mean reward: -0.068 [-5.803, 6.106], mean action: 1.910 [0.000, 3.000], mean observation: 0.042 [-0.600, 1.401], loss: 3.792798, mae: 30.750082, mean_q: 38.781673
   34016/1100000: episode: 66, duration: 0.693s, episode steps: 113, steps per second: 163, episode reward: -167.700, mean reward: -1.484 [-100.000, 4.856], mean action: 1.752 [0.000, 3.000], mean observation: -0.144 [-2.182, 1.425], loss: 3.784400, mae: 30.600359, mean_q: 38.185215
   34616/1100000: episode: 67, duration: 3.958s, episode steps: 600, steps per second: 152, episode reward: -152.915, mean reward: -0.255 [-100.000, 3.827], mean action: 1.725 [0.000, 3.000], mean observation: 0.112 [-1.000, 1.418], loss: 4.086124, mae: 30.597347, mean_q: 38.809212
   35616/1100000: episode: 68, duration: 7.003s, episode steps: 1000, steps per second: 143, episode reward: -119.503, mean reward: -0.120 [-4.457, 3.947], mean action: 1.708 [0.000, 3.000], mean observation: 0.134 [-0.776, 1.404], loss: 7.146774, mae: 30.028883, mean_q: 37.875828
   36616/1100000: episode: 69, duration: 7.118s, episode steps: 1000, steps per second: 140, episode reward: -119.251, mean reward: -0.119 [-4.888, 4.290], mean action: 1.684 [0.000, 3.000], mean observation: 0.118 [-0.681, 1.416], loss: 5.377636, mae: 29.915501, mean_q: 38.074486
   37616/1100000: episode: 70, duration: 7.059s, episode steps: 1000, steps per second: 142, episode reward: -139.777, mean reward: -0.140 [-5.281, 3.999], mean action: 1.829 [0.000, 3.000], mean observation: 0.126 [-0.781, 1.422], loss: 5.040390, mae: 29.623922, mean_q: 37.358475
   38616/1100000: episode: 71, duration: 7.632s, episode steps: 1000, steps per second: 131, episode reward: -76.711, mean reward: -0.077 [-4.490, 4.718], mean action: 1.730 [0.000, 3.000], mean observation: 0.108 [-0.523, 1.414], loss: 5.629265, mae: 29.555149, mean_q: 37.491707
   39616/1100000: episode: 72, duration: 7.654s, episode steps: 1000, steps per second: 131, episode reward: -72.443, mean reward: -0.072 [-4.590, 5.221], mean action: 1.576 [0.000, 3.000], mean observation: 0.060 [-0.567, 1.406], loss: 4.109424, mae: 29.690027, mean_q: 37.775410
   39732/1100000: episode: 73, duration: 0.718s, episode steps: 116, steps per second: 162, episode reward: -121.085, mean reward: -1.044 [-100.000, 2.669], mean action: 1.534 [0.000, 3.000], mean observation: -0.178 [-1.003, 1.406], loss: 2.617630, mae: 29.562706, mean_q: 37.880802
   39941/1100000: episode: 74, duration: 1.319s, episode steps: 209, steps per second: 158, episode reward: -149.072, mean reward: -0.713 [-100.000, 3.946], mean action: 1.651 [0.000, 3.000], mean observation: -0.049 [-1.481, 1.409], loss: 1.671695, mae: 29.221054, mean_q: 37.683720
   40941/1100000: episode: 75, duration: 7.279s, episode steps: 1000, steps per second: 137, episode reward: -126.588, mean reward: -0.127 [-4.409, 4.453], mean action: 1.683 [0.000, 3.000], mean observation: 0.123 [-0.605, 1.427], loss: 5.938886, mae: 29.014862, mean_q: 37.034859
   41941/1100000: episode: 76, duration: 7.430s, episode steps: 1000, steps per second: 135, episode reward: -56.514, mean reward: -0.057 [-4.885, 4.398], mean action: 1.677 [0.000, 3.000], mean observation: 0.101 [-0.823, 1.471], loss: 2.414639, mae: 28.445732, mean_q: 36.366680
   42941/1100000: episode: 77, duration: 6.912s, episode steps: 1000, steps per second: 145, episode reward: -98.972, mean reward: -0.099 [-4.849, 5.616], mean action: 1.769 [0.000, 3.000], mean observation: 0.126 [-0.563, 1.456], loss: 4.791963, mae: 27.954735, mean_q: 35.730789
   43105/1100000: episode: 78, duration: 1.027s, episode steps: 164, steps per second: 160, episode reward: -143.797, mean reward: -0.877 [-100.000, 2.736], mean action: 1.561 [0.000, 3.000], mean observation: -0.106 [-1.002, 1.393], loss: 2.616968, mae: 28.219391, mean_q: 35.848736
   44105/1100000: episode: 79, duration: 6.780s, episode steps: 1000, steps per second: 147, episode reward: -81.513, mean reward: -0.082 [-4.983, 6.687], mean action: 1.750 [0.000, 3.000], mean observation: 0.109 [-0.933, 1.461], loss: 3.425121, mae: 27.786308, mean_q: 35.520462
   44253/1100000: episode: 80, duration: 0.945s, episode steps: 148, steps per second: 157, episode reward: -266.538, mean reward: -1.801 [-100.000, 3.605], mean action: 2.095 [0.000, 3.000], mean observation: -0.184 [-2.374, 1.393], loss: 7.099377, mae: 28.073612, mean_q: 35.672844
   44937/1100000: episode: 81, duration: 4.834s, episode steps: 684, steps per second: 141, episode reward: -192.715, mean reward: -0.282 [-100.000, 3.720], mean action: 1.656 [0.000, 3.000], mean observation: -0.021 [-1.002, 1.399], loss: 5.512447, mae: 27.917528, mean_q: 35.406364
   45937/1100000: episode: 82, duration: 7.713s, episode steps: 1000, steps per second: 130, episode reward: -135.251, mean reward: -0.135 [-4.413, 3.928], mean action: 1.803 [0.000, 3.000], mean observation: 0.101 [-0.897, 1.390], loss: 3.306264, mae: 28.028704, mean_q: 35.846733
   46937/1100000: episode: 83, duration: 6.994s, episode steps: 1000, steps per second: 143, episode reward: -122.727, mean reward: -0.123 [-4.660, 3.955], mean action: 1.555 [0.000, 3.000], mean observation: 0.017 [-0.735, 1.413], loss: 4.209615, mae: 28.133274, mean_q: 35.585655
   47741/1100000: episode: 84, duration: 5.800s, episode steps: 804, steps per second: 139, episode reward: -208.883, mean reward: -0.260 [-100.000, 4.204], mean action: 1.648 [0.000, 3.000], mean observation: 0.002 [-1.002, 1.387], loss: 2.731855, mae: 28.031492, mean_q: 35.426617
   48741/1100000: episode: 85, duration: 6.860s, episode steps: 1000, steps per second: 146, episode reward: -87.637, mean reward: -0.088 [-4.625, 4.292], mean action: 1.647 [0.000, 3.000], mean observation: 0.114 [-0.966, 1.522], loss: 2.689216, mae: 27.654165, mean_q: 35.076790
   49025/1100000: episode: 86, duration: 1.842s, episode steps: 284, steps per second: 154, episode reward: -128.525, mean reward: -0.453 [-100.000, 4.643], mean action: 1.884 [0.000, 3.000], mean observation: 0.119 [-1.002, 1.488], loss: 1.850451, mae: 27.734409, mean_q: 35.137508
   50025/1100000: episode: 87, duration: 7.479s, episode steps: 1000, steps per second: 134, episode reward: -79.486, mean reward: -0.079 [-5.720, 5.609], mean action: 1.840 [0.000, 3.000], mean observation: 0.019 [-0.609, 1.560], loss: 3.658071, mae: 27.480120, mean_q: 34.878918
   51025/1100000: episode: 88, duration: 7.201s, episode steps: 1000, steps per second: 139, episode reward: -108.464, mean reward: -0.108 [-5.925, 4.530], mean action: 1.821 [0.000, 3.000], mean observation: 0.037 [-0.600, 1.409], loss: 3.177222, mae: 26.889465, mean_q: 33.980133
   51158/1100000: episode: 89, duration: 0.852s, episode steps: 133, steps per second: 156, episode reward: -130.937, mean reward: -0.984 [-100.000, 2.864], mean action: 1.556 [0.000, 3.000], mean observation: 0.043 [-1.002, 1.397], loss: 4.134765, mae: 26.414946, mean_q: 33.277042
   51297/1100000: episode: 90, duration: 0.901s, episode steps: 139, steps per second: 154, episode reward: -104.413, mean reward: -0.751 [-100.000, 3.088], mean action: 1.734 [0.000, 3.000], mean observation: 0.040 [-1.000, 1.406], loss: 1.973221, mae: 26.930208, mean_q: 34.167072
   52297/1100000: episode: 91, duration: 7.469s, episode steps: 1000, steps per second: 134, episode reward: -86.197, mean reward: -0.086 [-5.292, 4.577], mean action: 1.766 [0.000, 3.000], mean observation: 0.017 [-0.600, 1.389], loss: 3.602846, mae: 26.320250, mean_q: 33.337395
   53297/1100000: episode: 92, duration: 7.412s, episode steps: 1000, steps per second: 135, episode reward: -99.708, mean reward: -0.100 [-5.777, 5.188], mean action: 1.816 [0.000, 3.000], mean observation: 0.021 [-0.600, 1.408], loss: 2.612811, mae: 25.516684, mean_q: 32.171455
   54297/1100000: episode: 93, duration: 7.070s, episode steps: 1000, steps per second: 141, episode reward: -56.908, mean reward: -0.057 [-4.117, 3.788], mean action: 1.668 [0.000, 3.000], mean observation: 0.113 [-0.990, 1.436], loss: 2.910799, mae: 24.960712, mean_q: 31.290155
   55297/1100000: episode: 94, duration: 7.208s, episode steps: 1000, steps per second: 139, episode reward: -48.725, mean reward: -0.049 [-5.704, 5.167], mean action: 1.676 [0.000, 3.000], mean observation: 0.178 [-0.383, 1.423], loss: 2.725623, mae: 24.402828, mean_q: 30.501633
   56297/1100000: episode: 95, duration: 6.996s, episode steps: 1000, steps per second: 143, episode reward: -101.387, mean reward: -0.101 [-5.462, 5.344], mean action: 1.768 [0.000, 3.000], mean observation: 0.022 [-0.600, 1.388], loss: 2.620845, mae: 23.801422, mean_q: 29.826855
   57297/1100000: episode: 96, duration: 7.119s, episode steps: 1000, steps per second: 140, episode reward: -100.712, mean reward: -0.101 [-4.840, 5.691], mean action: 1.769 [0.000, 3.000], mean observation: 0.164 [-0.404, 1.404], loss: 2.059800, mae: 23.635101, mean_q: 29.910032
   58297/1100000: episode: 97, duration: 7.514s, episode steps: 1000, steps per second: 133, episode reward: -77.870, mean reward: -0.078 [-5.503, 4.530], mean action: 1.747 [0.000, 3.000], mean observation: 0.152 [-0.604, 1.412], loss: 2.864615, mae: 23.025707, mean_q: 28.909224
   59297/1100000: episode: 98, duration: 7.135s, episode steps: 1000, steps per second: 140, episode reward: -72.950, mean reward: -0.073 [-4.863, 5.106], mean action: 1.785 [0.000, 3.000], mean observation: 0.168 [-0.463, 1.403], loss: 2.719436, mae: 22.535652, mean_q: 28.225750
   60297/1100000: episode: 99, duration: 7.593s, episode steps: 1000, steps per second: 132, episode reward: -77.559, mean reward: -0.078 [-4.539, 5.106], mean action: 1.874 [0.000, 3.000], mean observation: 0.177 [-0.488, 1.389], loss: 2.433134, mae: 22.017982, mean_q: 27.201374
   61297/1100000: episode: 100, duration: 7.453s, episode steps: 1000, steps per second: 134, episode reward: -113.974, mean reward: -0.114 [-6.554, 5.369], mean action: 1.792 [0.000, 3.000], mean observation: 0.018 [-0.600, 1.422], loss: 2.799824, mae: 21.543285, mean_q: 26.842293
   62297/1100000: episode: 101, duration: 7.921s, episode steps: 1000, steps per second: 126, episode reward: -87.116, mean reward: -0.087 [-4.922, 4.519], mean action: 1.659 [0.000, 3.000], mean observation: 0.026 [-0.600, 1.396], loss: 1.732590, mae: 20.869410, mean_q: 25.666723
   63297/1100000: episode: 102, duration: 7.760s, episode steps: 1000, steps per second: 129, episode reward: -60.083, mean reward: -0.060 [-5.549, 4.309], mean action: 1.689 [0.000, 3.000], mean observation: 0.051 [-0.634, 1.389], loss: 2.394014, mae: 20.728682, mean_q: 25.400946
   64297/1100000: episode: 103, duration: 7.595s, episode steps: 1000, steps per second: 132, episode reward: -81.089, mean reward: -0.081 [-5.041, 4.969], mean action: 1.666 [0.000, 3.000], mean observation: 0.071 [-0.809, 1.400], loss: 2.616254, mae: 20.382477, mean_q: 24.932980
   65297/1100000: episode: 104, duration: 8.288s, episode steps: 1000, steps per second: 121, episode reward: -89.944, mean reward: -0.090 [-4.981, 4.418], mean action: 1.872 [0.000, 3.000], mean observation: 0.161 [-0.463, 1.400], loss: 2.301951, mae: 20.098991, mean_q: 24.868097
   66297/1100000: episode: 105, duration: 7.640s, episode steps: 1000, steps per second: 131, episode reward: -96.076, mean reward: -0.096 [-6.032, 4.842], mean action: 1.706 [0.000, 3.000], mean observation: 0.075 [-0.341, 1.420], loss: 1.789129, mae: 19.834427, mean_q: 24.801239
   67297/1100000: episode: 106, duration: 7.875s, episode steps: 1000, steps per second: 127, episode reward: -46.740, mean reward: -0.047 [-4.933, 4.737], mean action: 1.691 [0.000, 3.000], mean observation: 0.152 [-0.532, 1.431], loss: 1.567325, mae: 19.261988, mean_q: 24.412527
   68297/1100000: episode: 107, duration: 7.233s, episode steps: 1000, steps per second: 138, episode reward: -35.161, mean reward: -0.035 [-5.101, 4.962], mean action: 1.824 [0.000, 3.000], mean observation: 0.156 [-0.653, 1.518], loss: 1.709955, mae: 18.906359, mean_q: 24.024969
   69297/1100000: episode: 108, duration: 7.496s, episode steps: 1000, steps per second: 133, episode reward: -110.755, mean reward: -0.111 [-4.550, 4.402], mean action: 1.629 [0.000, 3.000], mean observation: 0.155 [-0.388, 1.410], loss: 1.080823, mae: 18.203867, mean_q: 23.292768
   69692/1100000: episode: 109, duration: 2.730s, episode steps: 395, steps per second: 145, episode reward: -229.883, mean reward: -0.582 [-100.000, 4.635], mean action: 1.577 [0.000, 3.000], mean observation: 0.150 [-0.556, 1.453], loss: 1.452018, mae: 17.907091, mean_q: 23.073328
   70692/1100000: episode: 110, duration: 7.599s, episode steps: 1000, steps per second: 132, episode reward: -110.628, mean reward: -0.111 [-4.607, 4.918], mean action: 1.557 [0.000, 3.000], mean observation: 0.058 [-0.608, 1.409], loss: 1.863137, mae: 17.717073, mean_q: 22.601057
   71692/1100000: episode: 111, duration: 8.297s, episode steps: 1000, steps per second: 121, episode reward: -100.897, mean reward: -0.101 [-4.977, 5.366], mean action: 1.741 [0.000, 3.000], mean observation: 0.150 [-0.625, 1.421], loss: 1.844594, mae: 17.518417, mean_q: 22.295094
   72692/1100000: episode: 112, duration: 7.515s, episode steps: 1000, steps per second: 133, episode reward: -99.430, mean reward: -0.099 [-5.724, 5.467], mean action: 1.714 [0.000, 3.000], mean observation: 0.039 [-0.600, 1.403], loss: 1.142004, mae: 17.679041, mean_q: 22.281918
   73692/1100000: episode: 113, duration: 7.363s, episode steps: 1000, steps per second: 136, episode reward: -63.686, mean reward: -0.064 [-4.087, 4.368], mean action: 1.538 [0.000, 3.000], mean observation: 0.070 [-0.610, 1.393], loss: 1.123387, mae: 17.316574, mean_q: 21.786102
   74239/1100000: episode: 114, duration: 3.753s, episode steps: 547, steps per second: 146, episode reward: -208.332, mean reward: -0.381 [-100.000, 4.557], mean action: 1.527 [0.000, 3.000], mean observation: 0.123 [-0.630, 1.394], loss: 1.664684, mae: 17.530432, mean_q: 22.073997
   75239/1100000: episode: 115, duration: 7.538s, episode steps: 1000, steps per second: 133, episode reward: -50.678, mean reward: -0.051 [-5.514, 4.870], mean action: 1.553 [0.000, 3.000], mean observation: 0.057 [-0.707, 1.519], loss: 1.308653, mae: 17.102942, mean_q: 21.433077
   76239/1100000: episode: 116, duration: 7.113s, episode steps: 1000, steps per second: 141, episode reward: -17.215, mean reward: -0.017 [-4.833, 4.729], mean action: 1.726 [0.000, 3.000], mean observation: 0.096 [-0.779, 1.427], loss: 2.266255, mae: 16.721363, mean_q: 21.083309
   76367/1100000: episode: 117, duration: 0.852s, episode steps: 128, steps per second: 150, episode reward: -126.096, mean reward: -0.985 [-100.000, 2.251], mean action: 1.672 [0.000, 3.000], mean observation: 0.129 [-0.600, 1.446], loss: 0.902413, mae: 16.484690, mean_q: 20.819202
   77367/1100000: episode: 118, duration: 8.533s, episode steps: 1000, steps per second: 117, episode reward: -77.420, mean reward: -0.077 [-4.726, 4.550], mean action: 1.659 [0.000, 3.000], mean observation: 0.021 [-0.600, 1.420], loss: 2.624892, mae: 16.580816, mean_q: 21.070021
   78367/1100000: episode: 119, duration: 7.864s, episode steps: 1000, steps per second: 127, episode reward: -87.110, mean reward: -0.087 [-4.932, 4.398], mean action: 1.658 [0.000, 3.000], mean observation: 0.150 [-0.775, 1.398], loss: 1.012393, mae: 16.458679, mean_q: 21.011862
   79367/1100000: episode: 120, duration: 7.635s, episode steps: 1000, steps per second: 131, episode reward: -91.824, mean reward: -0.092 [-5.757, 5.076], mean action: 1.698 [0.000, 3.000], mean observation: 0.061 [-0.525, 1.404], loss: 2.466617, mae: 16.039272, mean_q: 20.301620
   80367/1100000: episode: 121, duration: 7.576s, episode steps: 1000, steps per second: 132, episode reward: -83.164, mean reward: -0.083 [-4.910, 4.287], mean action: 1.658 [0.000, 3.000], mean observation: 0.158 [-0.446, 1.462], loss: 2.268983, mae: 15.563805, mean_q: 19.674223
   81282/1100000: episode: 122, duration: 6.768s, episode steps: 915, steps per second: 135, episode reward: -397.943, mean reward: -0.435 [-100.000, 4.852], mean action: 1.741 [0.000, 3.000], mean observation: -0.039 [-1.000, 1.998], loss: 2.178918, mae: 15.287820, mean_q: 19.152441
   82282/1100000: episode: 123, duration: 7.344s, episode steps: 1000, steps per second: 136, episode reward: -58.299, mean reward: -0.058 [-4.896, 4.686], mean action: 1.772 [0.000, 3.000], mean observation: 0.103 [-0.507, 1.416], loss: 1.343288, mae: 15.232617, mean_q: 19.140074
   83282/1100000: episode: 124, duration: 7.769s, episode steps: 1000, steps per second: 129, episode reward: -166.616, mean reward: -0.167 [-6.259, 6.926], mean action: 1.861 [0.000, 3.000], mean observation: 0.029 [-0.769, 1.398], loss: 1.533556, mae: 15.455151, mean_q: 19.329933
   84282/1100000: episode: 125, duration: 7.457s, episode steps: 1000, steps per second: 134, episode reward: -42.500, mean reward: -0.043 [-4.970, 4.558], mean action: 1.666 [0.000, 3.000], mean observation: 0.111 [-0.850, 1.509], loss: 1.194930, mae: 15.848007, mean_q: 19.928415
   85282/1100000: episode: 126, duration: 7.783s, episode steps: 1000, steps per second: 128, episode reward: -37.755, mean reward: -0.038 [-20.492, 14.196], mean action: 1.795 [0.000, 3.000], mean observation: 0.145 [-0.665, 1.417], loss: 1.027199, mae: 16.186859, mean_q: 20.235411
   86282/1100000: episode: 127, duration: 7.750s, episode steps: 1000, steps per second: 129, episode reward: -101.684, mean reward: -0.102 [-4.068, 4.746], mean action: 1.630 [0.000, 3.000], mean observation: 0.172 [-0.656, 1.472], loss: 1.663695, mae: 16.512213, mean_q: 20.338621
   87282/1100000: episode: 128, duration: 7.187s, episode steps: 1000, steps per second: 139, episode reward: -90.834, mean reward: -0.091 [-5.771, 4.582], mean action: 1.607 [0.000, 3.000], mean observation: 0.174 [-0.520, 1.413], loss: 1.380738, mae: 16.442581, mean_q: 20.323559
   88282/1100000: episode: 129, duration: 8.287s, episode steps: 1000, steps per second: 121, episode reward: -64.844, mean reward: -0.065 [-4.243, 4.634], mean action: 1.655 [0.000, 3.000], mean observation: 0.173 [-0.507, 1.412], loss: 1.241447, mae: 16.199718, mean_q: 19.903316
   89083/1100000: episode: 130, duration: 5.848s, episode steps: 801, steps per second: 137, episode reward: 206.218, mean reward: 0.257 [-18.697, 100.000], mean action: 1.905 [0.000, 3.000], mean observation: 0.028 [-0.732, 1.397], loss: 1.707514, mae: 16.266134, mean_q: 20.073311
   90083/1100000: episode: 131, duration: 7.528s, episode steps: 1000, steps per second: 133, episode reward: -26.836, mean reward: -0.027 [-4.556, 6.149], mean action: 1.690 [0.000, 3.000], mean observation: 0.111 [-0.849, 1.430], loss: 1.700098, mae: 15.982560, mean_q: 19.423344
   90629/1100000: episode: 132, duration: 3.744s, episode steps: 546, steps per second: 146, episode reward: -60.558, mean reward: -0.111 [-100.000, 10.991], mean action: 1.808 [0.000, 3.000], mean observation: 0.094 [-0.489, 1.388], loss: 0.821369, mae: 15.486620, mean_q: 18.978064
   91629/1100000: episode: 133, duration: 7.453s, episode steps: 1000, steps per second: 134, episode reward: -126.662, mean reward: -0.127 [-5.088, 5.785], mean action: 1.756 [0.000, 3.000], mean observation: -0.068 [-0.705, 1.524], loss: 1.517948, mae: 15.493429, mean_q: 18.955828
   92629/1100000: episode: 134, duration: 8.066s, episode steps: 1000, steps per second: 124, episode reward: -108.621, mean reward: -0.109 [-4.998, 5.667], mean action: 1.747 [0.000, 3.000], mean observation: -0.063 [-0.658, 1.404], loss: 2.008171, mae: 15.326650, mean_q: 18.990835
   93455/1100000: episode: 135, duration: 5.929s, episode steps: 826, steps per second: 139, episode reward: 153.205, mean reward: 0.185 [-12.212, 100.000], mean action: 2.100 [0.000, 3.000], mean observation: 0.064 [-0.537, 1.406], loss: 1.165814, mae: 15.443311, mean_q: 19.152504
   93854/1100000: episode: 136, duration: 2.782s, episode steps: 399, steps per second: 143, episode reward: -84.520, mean reward: -0.212 [-100.000, 27.695], mean action: 1.907 [0.000, 3.000], mean observation: 0.015 [-1.095, 1.530], loss: 1.446819, mae: 15.574401, mean_q: 19.365812
   94854/1100000: episode: 137, duration: 7.244s, episode steps: 1000, steps per second: 138, episode reward: -122.192, mean reward: -0.122 [-5.044, 4.493], mean action: 1.721 [0.000, 3.000], mean observation: 0.141 [-0.403, 1.484], loss: 1.607241, mae: 15.752629, mean_q: 19.586966
   95202/1100000: episode: 138, duration: 2.433s, episode steps: 348, steps per second: 143, episode reward: -7.849, mean reward: -0.023 [-100.000, 9.455], mean action: 1.649 [0.000, 3.000], mean observation: 0.005 [-0.758, 1.390], loss: 1.519472, mae: 15.452378, mean_q: 19.383724
   96202/1100000: episode: 139, duration: 7.459s, episode steps: 1000, steps per second: 134, episode reward: -92.482, mean reward: -0.092 [-4.487, 4.783], mean action: 1.562 [0.000, 3.000], mean observation: -0.049 [-0.600, 1.402], loss: 1.073846, mae: 15.694681, mean_q: 19.562925
   97202/1100000: episode: 140, duration: 7.961s, episode steps: 1000, steps per second: 126, episode reward: -141.919, mean reward: -0.142 [-4.911, 4.285], mean action: 1.785 [0.000, 3.000], mean observation: -0.044 [-0.608, 1.409], loss: 2.089915, mae: 15.538856, mean_q: 19.574760
   98061/1100000: episode: 141, duration: 6.515s, episode steps: 859, steps per second: 132, episode reward: -188.602, mean reward: -0.220 [-100.000, 16.920], mean action: 1.997 [0.000, 3.000], mean observation: 0.076 [-1.004, 1.391], loss: 2.804633, mae: 15.484488, mean_q: 19.482752
   98434/1100000: episode: 142, duration: 2.580s, episode steps: 373, steps per second: 145, episode reward: -211.552, mean reward: -0.567 [-100.000, 20.578], mean action: 1.654 [0.000, 3.000], mean observation: 0.048 [-1.007, 1.408], loss: 1.608855, mae: 15.085083, mean_q: 18.856903
   98814/1100000: episode: 143, duration: 2.609s, episode steps: 380, steps per second: 146, episode reward: -46.771, mean reward: -0.123 [-100.000, 7.663], mean action: 1.547 [0.000, 3.000], mean observation: 0.183 [-2.484, 1.395], loss: 1.778426, mae: 15.634894, mean_q: 19.720650
   99814/1100000: episode: 144, duration: 7.825s, episode steps: 1000, steps per second: 128, episode reward: 82.704, mean reward: 0.083 [-23.115, 22.575], mean action: 1.405 [0.000, 3.000], mean observation: 0.068 [-0.753, 1.394], loss: 1.394402, mae: 15.832270, mean_q: 19.854538
   99962/1100000: episode: 145, duration: 0.989s, episode steps: 148, steps per second: 150, episode reward: -150.359, mean reward: -1.016 [-100.000, 3.401], mean action: 1.885 [0.000, 3.000], mean observation: -0.100 [-1.002, 1.536], loss: 1.909618, mae: 16.519215, mean_q: 20.755402
  100193/1100000: episode: 146, duration: 1.547s, episode steps: 231, steps per second: 149, episode reward: -28.577, mean reward: -0.124 [-100.000, 13.540], mean action: 1.619 [0.000, 3.000], mean observation: 0.145 [-1.976, 1.403], loss: 3.691287, mae: 16.858461, mean_q: 21.242851
  100332/1100000: episode: 147, duration: 0.965s, episode steps: 139, steps per second: 144, episode reward: -66.965, mean reward: -0.482 [-100.000, 10.867], mean action: 1.647 [0.000, 3.000], mean observation: 0.176 [-2.836, 1.409], loss: 2.131482, mae: 16.887753, mean_q: 21.106354
  100733/1100000: episode: 148, duration: 2.871s, episode steps: 401, steps per second: 140, episode reward: 144.517, mean reward: 0.360 [-9.973, 100.000], mean action: 2.165 [0.000, 3.000], mean observation: -0.002 [-0.672, 1.389], loss: 3.311566, mae: 16.865299, mean_q: 21.352604
  101003/1100000: episode: 149, duration: 1.826s, episode steps: 270, steps per second: 148, episode reward: 1.341, mean reward: 0.005 [-100.000, 13.200], mean action: 1.837 [0.000, 3.000], mean observation: 0.141 [-0.825, 1.410], loss: 3.089722, mae: 16.956835, mean_q: 21.408522
  102003/1100000: episode: 150, duration: 7.594s, episode steps: 1000, steps per second: 132, episode reward: -32.824, mean reward: -0.033 [-5.061, 5.860], mean action: 1.739 [0.000, 3.000], mean observation: -0.037 [-0.890, 1.461], loss: 1.882740, mae: 17.397661, mean_q: 22.310453
  102240/1100000: episode: 151, duration: 1.593s, episode steps: 237, steps per second: 149, episode reward: -72.919, mean reward: -0.308 [-100.000, 13.251], mean action: 1.764 [0.000, 3.000], mean observation: 0.161 [-0.621, 1.406], loss: 2.804909, mae: 17.740406, mean_q: 22.597208
  102583/1100000: episode: 152, duration: 2.348s, episode steps: 343, steps per second: 146, episode reward: -64.391, mean reward: -0.188 [-100.000, 12.478], mean action: 1.685 [0.000, 3.000], mean observation: 0.068 [-0.685, 1.402], loss: 3.404082, mae: 17.667345, mean_q: 22.466749
  103098/1100000: episode: 153, duration: 3.581s, episode steps: 515, steps per second: 144, episode reward: 207.694, mean reward: 0.403 [-18.491, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.042 [-0.530, 1.393], loss: 2.335638, mae: 18.093681, mean_q: 23.179619
  104098/1100000: episode: 154, duration: 7.404s, episode steps: 1000, steps per second: 135, episode reward: -53.349, mean reward: -0.053 [-6.109, 4.499], mean action: 1.799 [0.000, 3.000], mean observation: -0.033 [-0.600, 1.394], loss: 2.083144, mae: 18.336996, mean_q: 23.436838
  104232/1100000: episode: 155, duration: 0.881s, episode steps: 134, steps per second: 152, episode reward: -128.296, mean reward: -0.957 [-100.000, 36.678], mean action: 1.604 [0.000, 3.000], mean observation: -0.014 [-1.138, 1.421], loss: 1.308732, mae: 18.569260, mean_q: 23.899801
  104457/1100000: episode: 156, duration: 1.512s, episode steps: 225, steps per second: 149, episode reward: -18.175, mean reward: -0.081 [-100.000, 11.139], mean action: 1.929 [0.000, 3.000], mean observation: 0.073 [-0.652, 1.397], loss: 2.227533, mae: 18.681316, mean_q: 23.813396
  105127/1100000: episode: 157, duration: 4.905s, episode steps: 670, steps per second: 137, episode reward: -48.787, mean reward: -0.073 [-100.000, 16.923], mean action: 1.669 [0.000, 3.000], mean observation: 0.002 [-0.620, 1.510], loss: 2.931346, mae: 18.931425, mean_q: 24.318285
  106127/1100000: episode: 158, duration: 7.442s, episode steps: 1000, steps per second: 134, episode reward: 125.537, mean reward: 0.126 [-19.494, 12.887], mean action: 1.516 [0.000, 3.000], mean observation: 0.084 [-0.768, 1.385], loss: 1.887476, mae: 19.960356, mean_q: 26.002769
  106325/1100000: episode: 159, duration: 1.327s, episode steps: 198, steps per second: 149, episode reward: -199.659, mean reward: -1.008 [-100.000, 37.762], mean action: 1.631 [0.000, 3.000], mean observation: -0.039 [-1.670, 1.416], loss: 2.270293, mae: 20.812185, mean_q: 27.187174
  106916/1100000: episode: 160, duration: 4.324s, episode steps: 591, steps per second: 137, episode reward: -248.841, mean reward: -0.421 [-100.000, 13.971], mean action: 1.739 [0.000, 3.000], mean observation: -0.009 [-1.331, 1.412], loss: 2.151941, mae: 21.130579, mean_q: 27.626329
  107703/1100000: episode: 161, duration: 5.663s, episode steps: 787, steps per second: 139, episode reward: -218.119, mean reward: -0.277 [-100.000, 15.286], mean action: 1.854 [0.000, 3.000], mean observation: -0.012 [-0.938, 1.624], loss: 2.132980, mae: 22.265051, mean_q: 28.907084
  108703/1100000: episode: 162, duration: 7.653s, episode steps: 1000, steps per second: 131, episode reward: -70.516, mean reward: -0.071 [-4.526, 5.958], mean action: 1.667 [0.000, 3.000], mean observation: -0.029 [-0.600, 1.428], loss: 2.452040, mae: 23.072084, mean_q: 30.069004
  109703/1100000: episode: 163, duration: 7.296s, episode steps: 1000, steps per second: 137, episode reward: -85.344, mean reward: -0.085 [-4.479, 4.612], mean action: 1.587 [0.000, 3.000], mean observation: -0.038 [-0.600, 1.398], loss: 2.915957, mae: 23.512897, mean_q: 30.610062
  110703/1100000: episode: 164, duration: 8.318s, episode steps: 1000, steps per second: 120, episode reward: -40.084, mean reward: -0.040 [-8.657, 14.071], mean action: 1.748 [0.000, 3.000], mean observation: -0.065 [-0.936, 1.406], loss: 2.296454, mae: 24.221645, mean_q: 31.705122
  111703/1100000: episode: 165, duration: 7.471s, episode steps: 1000, steps per second: 134, episode reward: 92.195, mean reward: 0.092 [-19.364, 26.123], mean action: 1.441 [0.000, 3.000], mean observation: 0.058 [-0.904, 1.428], loss: 1.967396, mae: 24.969820, mean_q: 32.784328
  112120/1100000: episode: 166, duration: 2.898s, episode steps: 417, steps per second: 144, episode reward: -216.373, mean reward: -0.519 [-100.000, 5.202], mean action: 1.878 [0.000, 3.000], mean observation: -0.012 [-3.355, 1.414], loss: 2.450287, mae: 25.395313, mean_q: 33.059711
  112357/1100000: episode: 167, duration: 1.605s, episode steps: 237, steps per second: 148, episode reward: -27.774, mean reward: -0.117 [-100.000, 17.048], mean action: 1.464 [0.000, 3.000], mean observation: -0.015 [-0.853, 1.392], loss: 3.479769, mae: 25.407681, mean_q: 32.832279
  113357/1100000: episode: 168, duration: 8.053s, episode steps: 1000, steps per second: 124, episode reward: -97.056, mean reward: -0.097 [-4.877, 5.077], mean action: 1.726 [0.000, 3.000], mean observation: -0.035 [-0.798, 1.400], loss: 2.473061, mae: 25.295898, mean_q: 32.879864
  114357/1100000: episode: 169, duration: 7.881s, episode steps: 1000, steps per second: 127, episode reward: -19.622, mean reward: -0.020 [-12.721, 14.344], mean action: 1.851 [0.000, 3.000], mean observation: 0.171 [-0.662, 1.498], loss: 5.398859, mae: 25.386793, mean_q: 32.979546
  115357/1100000: episode: 170, duration: 7.295s, episode steps: 1000, steps per second: 137, episode reward: -62.550, mean reward: -0.063 [-4.481, 5.017], mean action: 1.815 [0.000, 3.000], mean observation: -0.019 [-0.738, 1.412], loss: 2.016192, mae: 25.659254, mean_q: 33.345455
  115815/1100000: episode: 171, duration: 3.157s, episode steps: 458, steps per second: 145, episode reward: -1.687, mean reward: -0.004 [-100.000, 18.570], mean action: 1.594 [0.000, 3.000], mean observation: 0.102 [-0.596, 1.391], loss: 2.347426, mae: 25.554741, mean_q: 33.424339
  116815/1100000: episode: 172, duration: 7.308s, episode steps: 1000, steps per second: 137, episode reward: 135.816, mean reward: 0.136 [-20.770, 20.990], mean action: 1.381 [0.000, 3.000], mean observation: 0.088 [-0.792, 1.387], loss: 2.854474, mae: 25.588617, mean_q: 33.461918
  117815/1100000: episode: 173, duration: 7.531s, episode steps: 1000, steps per second: 133, episode reward: -95.733, mean reward: -0.096 [-5.118, 4.666], mean action: 1.557 [0.000, 3.000], mean observation: -0.049 [-0.600, 1.402], loss: 2.420089, mae: 25.750814, mean_q: 33.605278
  118815/1100000: episode: 174, duration: 7.749s, episode steps: 1000, steps per second: 129, episode reward: -112.210, mean reward: -0.112 [-5.053, 5.120], mean action: 1.671 [0.000, 3.000], mean observation: 0.054 [-0.701, 1.412], loss: 2.551251, mae: 25.970606, mean_q: 33.887367
  119262/1100000: episode: 175, duration: 3.202s, episode steps: 447, steps per second: 140, episode reward: 158.706, mean reward: 0.355 [-15.454, 100.000], mean action: 1.935 [0.000, 3.000], mean observation: 0.013 [-0.612, 1.390], loss: 3.653852, mae: 26.371929, mean_q: 34.377068
  120262/1100000: episode: 176, duration: 7.565s, episode steps: 1000, steps per second: 132, episode reward: 3.837, mean reward: 0.004 [-13.713, 18.522], mean action: 1.924 [0.000, 3.000], mean observation: -0.036 [-0.756, 1.391], loss: 2.298891, mae: 26.286674, mean_q: 34.401020
  120717/1100000: episode: 177, duration: 3.279s, episode steps: 455, steps per second: 139, episode reward: -64.920, mean reward: -0.143 [-100.000, 14.770], mean action: 1.752 [0.000, 3.000], mean observation: 0.032 [-0.750, 1.410], loss: 2.493370, mae: 26.998451, mean_q: 35.223660
  121717/1100000: episode: 178, duration: 7.980s, episode steps: 1000, steps per second: 125, episode reward: 45.570, mean reward: 0.046 [-23.425, 14.024], mean action: 1.582 [0.000, 3.000], mean observation: 0.070 [-0.712, 1.410], loss: 2.705258, mae: 27.443449, mean_q: 35.841351
  122717/1100000: episode: 179, duration: 9.098s, episode steps: 1000, steps per second: 110, episode reward: -38.845, mean reward: -0.039 [-5.506, 4.500], mean action: 1.724 [0.000, 3.000], mean observation: -0.075 [-0.600, 1.386], loss: 2.876515, mae: 27.883673, mean_q: 36.502922
  123476/1100000: episode: 180, duration: 5.416s, episode steps: 759, steps per second: 140, episode reward: 166.252, mean reward: 0.219 [-19.596, 100.000], mean action: 2.619 [0.000, 3.000], mean observation: 0.198 [-0.550, 1.389], loss: 2.296123, mae: 27.939960, mean_q: 36.557678
  123793/1100000: episode: 181, duration: 2.145s, episode steps: 317, steps per second: 148, episode reward: -72.093, mean reward: -0.227 [-100.000, 12.044], mean action: 1.659 [0.000, 3.000], mean observation: 0.103 [-0.526, 1.415], loss: 2.539619, mae: 27.716492, mean_q: 36.147476
  124793/1100000: episode: 182, duration: 7.652s, episode steps: 1000, steps per second: 131, episode reward: -77.843, mean reward: -0.078 [-4.417, 4.704], mean action: 1.644 [0.000, 3.000], mean observation: 0.088 [-0.631, 1.507], loss: 2.037441, mae: 27.975552, mean_q: 36.663837
  125793/1100000: episode: 183, duration: 7.866s, episode steps: 1000, steps per second: 127, episode reward: -35.714, mean reward: -0.036 [-19.288, 16.746], mean action: 1.709 [0.000, 3.000], mean observation: 0.009 [-0.655, 1.415], loss: 3.512846, mae: 27.939045, mean_q: 36.594288
  126793/1100000: episode: 184, duration: 7.862s, episode steps: 1000, steps per second: 127, episode reward: -68.076, mean reward: -0.068 [-5.140, 5.605], mean action: 1.693 [0.000, 3.000], mean observation: -0.063 [-0.600, 1.414], loss: 2.888102, mae: 27.838427, mean_q: 36.735428
  127151/1100000: episode: 185, duration: 2.468s, episode steps: 358, steps per second: 145, episode reward: 14.846, mean reward: 0.041 [-100.000, 14.054], mean action: 1.701 [0.000, 3.000], mean observation: 0.030 [-0.944, 1.507], loss: 2.777777, mae: 27.554993, mean_q: 36.383247
  128151/1100000: episode: 186, duration: 7.920s, episode steps: 1000, steps per second: 126, episode reward: 4.441, mean reward: 0.004 [-4.662, 4.960], mean action: 1.813 [0.000, 3.000], mean observation: 0.010 [-0.420, 1.401], loss: 3.131491, mae: 27.384315, mean_q: 36.038033
  129151/1100000: episode: 187, duration: 8.116s, episode steps: 1000, steps per second: 123, episode reward: -23.223, mean reward: -0.023 [-7.841, 13.218], mean action: 1.730 [0.000, 3.000], mean observation: -0.012 [-0.741, 1.462], loss: 2.906928, mae: 27.239994, mean_q: 35.774277
  130151/1100000: episode: 188, duration: 7.765s, episode steps: 1000, steps per second: 129, episode reward: 2.693, mean reward: 0.003 [-4.595, 5.357], mean action: 1.790 [0.000, 3.000], mean observation: 0.021 [-0.602, 1.404], loss: 3.491324, mae: 27.165621, mean_q: 35.672840
  131151/1100000: episode: 189, duration: 7.485s, episode steps: 1000, steps per second: 134, episode reward: 5.471, mean reward: 0.005 [-5.106, 4.896], mean action: 1.780 [0.000, 3.000], mean observation: -0.013 [-0.607, 1.491], loss: 3.245615, mae: 27.077991, mean_q: 35.582859
  132151/1100000: episode: 190, duration: 8.027s, episode steps: 1000, steps per second: 125, episode reward: 6.844, mean reward: 0.007 [-18.055, 10.167], mean action: 1.581 [0.000, 3.000], mean observation: 0.012 [-0.622, 1.520], loss: 4.118986, mae: 26.530590, mean_q: 34.753532
  133151/1100000: episode: 191, duration: 7.807s, episode steps: 1000, steps per second: 128, episode reward: -72.401, mean reward: -0.072 [-4.937, 4.764], mean action: 1.795 [0.000, 3.000], mean observation: -0.059 [-0.600, 1.411], loss: 2.095914, mae: 26.215508, mean_q: 34.440254
  134151/1100000: episode: 192, duration: 7.415s, episode steps: 1000, steps per second: 135, episode reward: -132.274, mean reward: -0.132 [-13.552, 15.327], mean action: 1.722 [0.000, 3.000], mean observation: -0.052 [-1.071, 1.449], loss: 2.694070, mae: 26.104425, mean_q: 34.309071
  135050/1100000: episode: 193, duration: 6.672s, episode steps: 899, steps per second: 135, episode reward: -149.870, mean reward: -0.167 [-100.000, 10.588], mean action: 1.675 [0.000, 3.000], mean observation: 0.055 [-1.140, 1.415], loss: 2.941117, mae: 26.119291, mean_q: 34.392742
  136050/1100000: episode: 194, duration: 8.160s, episode steps: 1000, steps per second: 123, episode reward: -130.673, mean reward: -0.131 [-18.976, 17.039], mean action: 1.660 [0.000, 3.000], mean observation: -0.050 [-1.105, 1.392], loss: 2.958084, mae: 25.864187, mean_q: 34.234325
  137050/1100000: episode: 195, duration: 7.305s, episode steps: 1000, steps per second: 137, episode reward: -8.384, mean reward: -0.008 [-22.928, 21.266], mean action: 1.624 [0.000, 3.000], mean observation: 0.042 [-0.567, 1.402], loss: 3.494235, mae: 26.351398, mean_q: 34.845352
  137558/1100000: episode: 196, duration: 3.557s, episode steps: 508, steps per second: 143, episode reward: 197.227, mean reward: 0.388 [-17.959, 100.000], mean action: 1.687 [0.000, 3.000], mean observation: 0.025 [-0.653, 1.386], loss: 2.547737, mae: 26.252754, mean_q: 34.802387
  138558/1100000: episode: 197, duration: 7.381s, episode steps: 1000, steps per second: 135, episode reward: -49.544, mean reward: -0.050 [-5.496, 4.382], mean action: 1.843 [0.000, 3.000], mean observation: 0.057 [-0.624, 1.404], loss: 2.278822, mae: 26.631407, mean_q: 35.156353
  139558/1100000: episode: 198, duration: 7.846s, episode steps: 1000, steps per second: 127, episode reward: -105.320, mean reward: -0.105 [-6.094, 4.411], mean action: 1.794 [0.000, 3.000], mean observation: 0.076 [-0.686, 1.483], loss: 2.570124, mae: 26.507689, mean_q: 35.148781
  139706/1100000: episode: 199, duration: 0.983s, episode steps: 148, steps per second: 151, episode reward: 26.614, mean reward: 0.180 [-100.000, 12.187], mean action: 1.480 [0.000, 3.000], mean observation: -0.039 [-1.490, 1.424], loss: 2.751755, mae: 26.507637, mean_q: 35.085068
  140706/1100000: episode: 200, duration: 7.513s, episode steps: 1000, steps per second: 133, episode reward: 18.725, mean reward: 0.019 [-17.451, 14.045], mean action: 1.853 [0.000, 3.000], mean observation: -0.046 [-0.629, 1.497], loss: 2.691205, mae: 26.284990, mean_q: 34.745110
  140893/1100000: episode: 201, duration: 1.259s, episode steps: 187, steps per second: 149, episode reward: -21.564, mean reward: -0.115 [-100.000, 10.846], mean action: 1.866 [0.000, 3.000], mean observation: 0.059 [-0.862, 1.391], loss: 3.943654, mae: 26.628410, mean_q: 35.015247
  140950/1100000: episode: 202, duration: 0.376s, episode steps: 57, steps per second: 152, episode reward: -87.300, mean reward: -1.532 [-100.000, 11.882], mean action: 1.246 [0.000, 3.000], mean observation: 0.012 [-1.467, 4.944], loss: 1.498619, mae: 26.598215, mean_q: 34.780445
  141950/1100000: episode: 203, duration: 7.665s, episode steps: 1000, steps per second: 130, episode reward: 66.152, mean reward: 0.066 [-24.222, 24.014], mean action: 1.510 [0.000, 3.000], mean observation: 0.094 [-0.644, 1.392], loss: 3.195125, mae: 26.396044, mean_q: 34.788467
  142184/1100000: episode: 204, duration: 1.590s, episode steps: 234, steps per second: 147, episode reward: 28.328, mean reward: 0.121 [-100.000, 8.947], mean action: 1.739 [0.000, 3.000], mean observation: -0.068 [-0.945, 1.389], loss: 2.469513, mae: 26.684750, mean_q: 35.212448
  142308/1100000: episode: 205, duration: 0.827s, episode steps: 124, steps per second: 150, episode reward: -21.891, mean reward: -0.177 [-100.000, 16.998], mean action: 1.403 [0.000, 3.000], mean observation: 0.036 [-0.980, 1.437], loss: 7.225797, mae: 26.870201, mean_q: 35.540035
  142442/1100000: episode: 206, duration: 0.884s, episode steps: 134, steps per second: 152, episode reward: -110.224, mean reward: -0.823 [-100.000, 7.025], mean action: 1.075 [0.000, 3.000], mean observation: 0.025 [-4.722, 1.524], loss: 2.916304, mae: 26.954105, mean_q: 35.436600
  143442/1100000: episode: 207, duration: 7.641s, episode steps: 1000, steps per second: 131, episode reward: 48.468, mean reward: 0.048 [-18.653, 22.186], mean action: 1.830 [0.000, 3.000], mean observation: 0.067 [-0.673, 1.465], loss: 2.366610, mae: 26.646605, mean_q: 35.179810
  144442/1100000: episode: 208, duration: 7.945s, episode steps: 1000, steps per second: 126, episode reward: -38.471, mean reward: -0.038 [-6.519, 4.284], mean action: 1.750 [0.000, 3.000], mean observation: 0.074 [-0.818, 1.539], loss: 3.079773, mae: 26.728306, mean_q: 35.351303
  145442/1100000: episode: 209, duration: 8.047s, episode steps: 1000, steps per second: 124, episode reward: -59.202, mean reward: -0.059 [-24.900, 15.205], mean action: 1.793 [0.000, 3.000], mean observation: 0.072 [-0.629, 1.389], loss: 3.914319, mae: 26.704777, mean_q: 35.353371
  146442/1100000: episode: 210, duration: 7.476s, episode steps: 1000, steps per second: 134, episode reward: -51.020, mean reward: -0.051 [-7.809, 7.071], mean action: 1.814 [0.000, 3.000], mean observation: 0.080 [-0.817, 1.387], loss: 2.970776, mae: 26.934582, mean_q: 35.746052
  146556/1100000: episode: 211, duration: 0.752s, episode steps: 114, steps per second: 152, episode reward: -54.450, mean reward: -0.478 [-100.000, 12.815], mean action: 1.667 [0.000, 3.000], mean observation: 0.070 [-1.201, 1.431], loss: 2.484596, mae: 26.548775, mean_q: 35.342247
  147556/1100000: episode: 212, duration: 8.124s, episode steps: 1000, steps per second: 123, episode reward: -148.334, mean reward: -0.148 [-11.375, 39.894], mean action: 1.895 [0.000, 3.000], mean observation: 0.154 [-0.967, 1.425], loss: 3.341636, mae: 27.171156, mean_q: 36.157978
  147915/1100000: episode: 213, duration: 2.601s, episode steps: 359, steps per second: 138, episode reward: 195.217, mean reward: 0.544 [-17.013, 100.000], mean action: 2.178 [0.000, 3.000], mean observation: 0.054 [-0.600, 1.432], loss: 2.795495, mae: 27.186855, mean_q: 36.339622
  148211/1100000: episode: 214, duration: 2.030s, episode steps: 296, steps per second: 146, episode reward: -140.674, mean reward: -0.475 [-100.000, 4.603], mean action: 1.943 [0.000, 3.000], mean observation: 0.069 [-1.001, 1.462], loss: 4.605990, mae: 27.611631, mean_q: 36.801846
  149116/1100000: episode: 215, duration: 6.788s, episode steps: 905, steps per second: 133, episode reward: 121.888, mean reward: 0.135 [-19.803, 100.000], mean action: 2.148 [0.000, 3.000], mean observation: 0.013 [-0.756, 1.391], loss: 3.025994, mae: 27.525789, mean_q: 36.712021
  149450/1100000: episode: 216, duration: 2.332s, episode steps: 334, steps per second: 143, episode reward: 227.359, mean reward: 0.681 [-2.958, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: 0.001 [-0.662, 1.393], loss: 5.639804, mae: 27.729570, mean_q: 36.958622
  150378/1100000: episode: 217, duration: 7.284s, episode steps: 928, steps per second: 127, episode reward: 89.661, mean reward: 0.097 [-16.335, 100.000], mean action: 1.977 [0.000, 3.000], mean observation: 0.012 [-0.621, 1.527], loss: 3.013122, mae: 27.994612, mean_q: 37.235039
  150487/1100000: episode: 218, duration: 0.728s, episode steps: 109, steps per second: 150, episode reward: -60.963, mean reward: -0.559 [-100.000, 7.210], mean action: 1.578 [0.000, 3.000], mean observation: -0.101 [-2.684, 1.385], loss: 7.546620, mae: 28.338417, mean_q: 37.682106
  151487/1100000: episode: 219, duration: 7.654s, episode steps: 1000, steps per second: 131, episode reward: 23.141, mean reward: 0.023 [-21.084, 31.668], mean action: 1.672 [0.000, 3.000], mean observation: 0.088 [-0.648, 1.508], loss: 3.380966, mae: 28.346735, mean_q: 37.761616
  151783/1100000: episode: 220, duration: 2.036s, episode steps: 296, steps per second: 145, episode reward: 278.044, mean reward: 0.939 [-13.717, 100.000], mean action: 1.899 [0.000, 3.000], mean observation: -0.022 [-0.901, 1.388], loss: 3.381848, mae: 28.579365, mean_q: 38.087864
  151866/1100000: episode: 221, duration: 0.546s, episode steps: 83, steps per second: 152, episode reward: -179.420, mean reward: -2.162 [-100.000, 77.184], mean action: 1.494 [0.000, 3.000], mean observation: -0.071 [-1.577, 3.159], loss: 4.712543, mae: 28.215326, mean_q: 37.672714
  152007/1100000: episode: 222, duration: 0.944s, episode steps: 141, steps per second: 149, episode reward: -97.858, mean reward: -0.694 [-100.000, 14.941], mean action: 1.631 [0.000, 3.000], mean observation: 0.093 [-1.156, 2.616], loss: 2.776162, mae: 28.555227, mean_q: 37.928692
  152347/1100000: episode: 223, duration: 2.330s, episode steps: 340, steps per second: 146, episode reward: -30.419, mean reward: -0.089 [-100.000, 8.199], mean action: 1.676 [0.000, 3.000], mean observation: -0.027 [-0.727, 1.462], loss: 3.755740, mae: 29.019552, mean_q: 38.627937
  152595/1100000: episode: 224, duration: 1.711s, episode steps: 248, steps per second: 145, episode reward: 239.168, mean reward: 0.964 [-16.987, 100.000], mean action: 1.972 [0.000, 3.000], mean observation: 0.145 [-0.617, 1.388], loss: 3.788978, mae: 28.804605, mean_q: 38.329617
  152851/1100000: episode: 225, duration: 1.736s, episode steps: 256, steps per second: 147, episode reward: -36.167, mean reward: -0.141 [-100.000, 9.073], mean action: 1.914 [0.000, 3.000], mean observation: 0.097 [-0.676, 2.448], loss: 3.896655, mae: 28.921291, mean_q: 38.462006
  153633/1100000: episode: 226, duration: 5.537s, episode steps: 782, steps per second: 141, episode reward: 126.859, mean reward: 0.162 [-17.210, 100.000], mean action: 2.081 [0.000, 3.000], mean observation: 0.117 [-0.824, 1.466], loss: 3.350940, mae: 28.968498, mean_q: 38.456768
  154633/1100000: episode: 227, duration: 7.644s, episode steps: 1000, steps per second: 131, episode reward: 105.446, mean reward: 0.105 [-20.618, 23.278], mean action: 2.012 [0.000, 3.000], mean observation: 0.052 [-1.225, 1.457], loss: 4.485099, mae: 29.062475, mean_q: 38.739681
  154922/1100000: episode: 228, duration: 1.985s, episode steps: 289, steps per second: 146, episode reward: -217.537, mean reward: -0.753 [-100.000, 29.260], mean action: 1.689 [0.000, 3.000], mean observation: -0.044 [-0.704, 2.144], loss: 3.997943, mae: 29.240892, mean_q: 38.863289
  155593/1100000: episode: 229, duration: 4.761s, episode steps: 671, steps per second: 141, episode reward: -284.781, mean reward: -0.424 [-100.000, 4.674], mean action: 1.694 [0.000, 3.000], mean observation: 0.064 [-0.743, 1.410], loss: 3.254892, mae: 29.343407, mean_q: 39.110573
  156059/1100000: episode: 230, duration: 3.331s, episode steps: 466, steps per second: 140, episode reward: 190.769, mean reward: 0.409 [-8.978, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: -0.006 [-0.916, 1.525], loss: 4.363581, mae: 29.351742, mean_q: 39.148590
  156474/1100000: episode: 231, duration: 2.882s, episode steps: 415, steps per second: 144, episode reward: -169.953, mean reward: -0.410 [-100.000, 20.929], mean action: 1.916 [0.000, 3.000], mean observation: 0.164 [-0.720, 1.414], loss: 3.517359, mae: 29.528723, mean_q: 39.556797
  157474/1100000: episode: 232, duration: 8.483s, episode steps: 1000, steps per second: 118, episode reward: -28.370, mean reward: -0.028 [-4.727, 5.037], mean action: 1.893 [0.000, 3.000], mean observation: -0.100 [-0.777, 1.385], loss: 3.631077, mae: 29.415859, mean_q: 39.202251
  158474/1100000: episode: 233, duration: 7.841s, episode steps: 1000, steps per second: 128, episode reward: -92.297, mean reward: -0.092 [-4.481, 4.174], mean action: 1.712 [0.000, 3.000], mean observation: 0.040 [-0.826, 1.422], loss: 3.194540, mae: 29.491711, mean_q: 39.477367
  158737/1100000: episode: 234, duration: 1.798s, episode steps: 263, steps per second: 146, episode reward: 19.631, mean reward: 0.075 [-100.000, 13.378], mean action: 1.551 [0.000, 3.000], mean observation: -0.030 [-0.923, 1.459], loss: 4.235609, mae: 29.997530, mean_q: 39.937927
  159737/1100000: episode: 235, duration: 7.802s, episode steps: 1000, steps per second: 128, episode reward: -54.703, mean reward: -0.055 [-18.040, 11.888], mean action: 1.906 [0.000, 3.000], mean observation: -0.090 [-0.660, 1.436], loss: 3.808842, mae: 30.015593, mean_q: 40.056473
  160546/1100000: episode: 236, duration: 5.973s, episode steps: 809, steps per second: 135, episode reward: 104.819, mean reward: 0.130 [-26.670, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: -0.037 [-0.640, 1.408], loss: 3.808563, mae: 30.203136, mean_q: 40.209782
  160892/1100000: episode: 237, duration: 2.400s, episode steps: 346, steps per second: 144, episode reward: -380.368, mean reward: -1.099 [-100.000, 6.293], mean action: 1.951 [0.000, 3.000], mean observation: 0.055 [-3.958, 1.410], loss: 6.334390, mae: 30.538479, mean_q: 40.731987
  161757/1100000: episode: 238, duration: 6.449s, episode steps: 865, steps per second: 134, episode reward: 230.065, mean reward: 0.266 [-19.477, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.119 [-0.624, 1.556], loss: 4.584769, mae: 30.888046, mean_q: 41.085972
  162757/1100000: episode: 239, duration: 7.399s, episode steps: 1000, steps per second: 135, episode reward: 88.227, mean reward: 0.088 [-23.901, 21.382], mean action: 2.014 [0.000, 3.000], mean observation: 0.076 [-0.638, 1.418], loss: 4.726302, mae: 30.855965, mean_q: 41.317024
  163757/1100000: episode: 240, duration: 7.153s, episode steps: 1000, steps per second: 140, episode reward: -97.869, mean reward: -0.098 [-5.096, 5.068], mean action: 1.599 [0.000, 3.000], mean observation: -0.061 [-0.971, 1.392], loss: 3.292509, mae: 31.283285, mean_q: 41.815086
  163944/1100000: episode: 241, duration: 1.259s, episode steps: 187, steps per second: 149, episode reward: -81.370, mean reward: -0.435 [-100.000, 10.962], mean action: 1.631 [0.000, 3.000], mean observation: 0.060 [-1.049, 1.412], loss: 2.613927, mae: 32.106747, mean_q: 42.851425
  164325/1100000: episode: 242, duration: 2.684s, episode steps: 381, steps per second: 142, episode reward: -128.602, mean reward: -0.338 [-100.000, 4.515], mean action: 1.441 [0.000, 3.000], mean observation: 0.033 [-1.001, 1.405], loss: 5.164810, mae: 31.671875, mean_q: 42.042942
  164867/1100000: episode: 243, duration: 3.967s, episode steps: 542, steps per second: 137, episode reward: -225.676, mean reward: -0.416 [-100.000, 24.212], mean action: 1.852 [0.000, 3.000], mean observation: -0.054 [-1.363, 1.412], loss: 5.682894, mae: 31.725283, mean_q: 42.302246
  165308/1100000: episode: 244, duration: 3.123s, episode steps: 441, steps per second: 141, episode reward: -201.253, mean reward: -0.456 [-100.000, 11.396], mean action: 1.968 [0.000, 3.000], mean observation: -0.022 [-0.606, 1.640], loss: 3.962904, mae: 31.972240, mean_q: 42.717091
  165783/1100000: episode: 245, duration: 3.474s, episode steps: 475, steps per second: 137, episode reward: 213.551, mean reward: 0.450 [-19.497, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.062 [-0.505, 1.392], loss: 5.161410, mae: 31.796505, mean_q: 42.257111
  166653/1100000: episode: 246, duration: 6.476s, episode steps: 870, steps per second: 134, episode reward: 146.370, mean reward: 0.168 [-19.104, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.046 [-0.582, 1.397], loss: 4.275150, mae: 31.640459, mean_q: 42.186646
  167046/1100000: episode: 247, duration: 2.778s, episode steps: 393, steps per second: 141, episode reward: 143.101, mean reward: 0.364 [-12.333, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: -0.011 [-0.699, 1.412], loss: 3.366979, mae: 31.374598, mean_q: 42.066204
  167683/1100000: episode: 248, duration: 4.599s, episode steps: 637, steps per second: 139, episode reward: 263.501, mean reward: 0.414 [-9.341, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.075 [-0.753, 1.470], loss: 3.203529, mae: 31.637651, mean_q: 42.312218
  168146/1100000: episode: 249, duration: 3.304s, episode steps: 463, steps per second: 140, episode reward: 204.641, mean reward: 0.442 [-12.790, 100.000], mean action: 1.737 [0.000, 3.000], mean observation: -0.026 [-0.748, 1.390], loss: 5.813335, mae: 31.835052, mean_q: 42.541264
  168328/1100000: episode: 250, duration: 1.211s, episode steps: 182, steps per second: 150, episode reward: -2.747, mean reward: -0.015 [-100.000, 9.327], mean action: 1.527 [0.000, 3.000], mean observation: -0.044 [-0.600, 1.393], loss: 4.001915, mae: 32.052090, mean_q: 42.738888
  169328/1100000: episode: 251, duration: 7.180s, episode steps: 1000, steps per second: 139, episode reward: -86.040, mean reward: -0.086 [-4.865, 4.817], mean action: 1.672 [0.000, 3.000], mean observation: 0.047 [-0.671, 1.397], loss: 3.518280, mae: 32.140778, mean_q: 43.024677
  170328/1100000: episode: 252, duration: 7.555s, episode steps: 1000, steps per second: 132, episode reward: -23.009, mean reward: -0.023 [-17.610, 21.933], mean action: 1.805 [0.000, 3.000], mean observation: 0.086 [-0.907, 1.450], loss: 4.712220, mae: 32.572830, mean_q: 43.575829
  171328/1100000: episode: 253, duration: 7.817s, episode steps: 1000, steps per second: 128, episode reward: -26.331, mean reward: -0.026 [-22.751, 14.650], mean action: 1.692 [0.000, 3.000], mean observation: 0.102 [-0.468, 1.410], loss: 4.945493, mae: 32.399818, mean_q: 43.386509
  171491/1100000: episode: 254, duration: 1.077s, episode steps: 163, steps per second: 151, episode reward: -47.226, mean reward: -0.290 [-100.000, 13.166], mean action: 1.644 [0.000, 3.000], mean observation: 0.107 [-1.459, 1.463], loss: 8.947479, mae: 32.257141, mean_q: 43.147511
  171748/1100000: episode: 255, duration: 1.738s, episode steps: 257, steps per second: 148, episode reward: -32.864, mean reward: -0.128 [-100.000, 12.951], mean action: 1.790 [0.000, 3.000], mean observation: 0.150 [-1.040, 1.399], loss: 3.607320, mae: 32.867664, mean_q: 43.845341
  171920/1100000: episode: 256, duration: 1.150s, episode steps: 172, steps per second: 150, episode reward: 6.750, mean reward: 0.039 [-100.000, 24.628], mean action: 1.616 [0.000, 3.000], mean observation: -0.050 [-0.778, 1.410], loss: 5.615756, mae: 33.275185, mean_q: 44.654819
  172438/1100000: episode: 257, duration: 3.708s, episode steps: 518, steps per second: 140, episode reward: 231.006, mean reward: 0.446 [-17.521, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: 0.030 [-0.761, 1.410], loss: 4.527952, mae: 32.573776, mean_q: 43.626995
  173438/1100000: episode: 258, duration: 8.235s, episode steps: 1000, steps per second: 121, episode reward: -7.515, mean reward: -0.008 [-4.146, 5.936], mean action: 1.664 [0.000, 3.000], mean observation: 0.070 [-0.565, 1.397], loss: 3.878973, mae: 32.694294, mean_q: 43.808495
  174388/1100000: episode: 259, duration: 7.651s, episode steps: 950, steps per second: 124, episode reward: 150.427, mean reward: 0.158 [-23.511, 100.000], mean action: 1.591 [0.000, 3.000], mean observation: 0.156 [-0.937, 1.411], loss: 3.842653, mae: 33.149822, mean_q: 44.504745
  175169/1100000: episode: 260, duration: 5.737s, episode steps: 781, steps per second: 136, episode reward: 176.064, mean reward: 0.225 [-20.081, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.111 [-0.737, 1.405], loss: 4.129304, mae: 33.334381, mean_q: 44.800556
  176169/1100000: episode: 261, duration: 7.883s, episode steps: 1000, steps per second: 127, episode reward: -32.387, mean reward: -0.032 [-4.241, 4.494], mean action: 1.740 [0.000, 3.000], mean observation: 0.078 [-0.541, 1.456], loss: 3.982370, mae: 33.554077, mean_q: 44.988918
  176506/1100000: episode: 262, duration: 2.330s, episode steps: 337, steps per second: 145, episode reward: 238.925, mean reward: 0.709 [-10.999, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.053 [-0.435, 1.471], loss: 9.264237, mae: 33.360390, mean_q: 44.719131
  177506/1100000: episode: 263, duration: 7.945s, episode steps: 1000, steps per second: 126, episode reward: 84.937, mean reward: 0.085 [-17.291, 22.269], mean action: 1.877 [0.000, 3.000], mean observation: -0.012 [-0.728, 1.526], loss: 5.630912, mae: 33.659073, mean_q: 45.109459
  178506/1100000: episode: 264, duration: 7.470s, episode steps: 1000, steps per second: 134, episode reward: 151.165, mean reward: 0.151 [-20.024, 14.970], mean action: 1.712 [0.000, 3.000], mean observation: 0.054 [-0.732, 1.499], loss: 5.503572, mae: 33.985413, mean_q: 45.525486
  179506/1100000: episode: 265, duration: 7.592s, episode steps: 1000, steps per second: 132, episode reward: 142.418, mean reward: 0.142 [-23.437, 27.629], mean action: 1.551 [0.000, 3.000], mean observation: 0.012 [-0.869, 1.485], loss: 5.351083, mae: 34.015865, mean_q: 45.761856
  180070/1100000: episode: 266, duration: 4.022s, episode steps: 564, steps per second: 140, episode reward: 250.257, mean reward: 0.444 [-19.006, 100.000], mean action: 2.271 [0.000, 3.000], mean observation: 0.021 [-0.847, 1.416], loss: 4.742968, mae: 34.850227, mean_q: 46.820999
  180523/1100000: episode: 267, duration: 3.161s, episode steps: 453, steps per second: 143, episode reward: -169.818, mean reward: -0.375 [-100.000, 16.164], mean action: 1.790 [0.000, 3.000], mean observation: -0.051 [-1.643, 1.499], loss: 3.769434, mae: 35.161346, mean_q: 47.254993
  180721/1100000: episode: 268, duration: 1.338s, episode steps: 198, steps per second: 148, episode reward: -347.643, mean reward: -1.756 [-100.000, 4.454], mean action: 1.652 [0.000, 3.000], mean observation: -0.046 [-0.947, 2.407], loss: 3.550083, mae: 35.548607, mean_q: 47.805092
  181195/1100000: episode: 269, duration: 3.343s, episode steps: 474, steps per second: 142, episode reward: 263.266, mean reward: 0.555 [-19.897, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.218 [-0.589, 1.448], loss: 4.015575, mae: 35.942184, mean_q: 48.282799
  181698/1100000: episode: 270, duration: 3.592s, episode steps: 503, steps per second: 140, episode reward: 246.948, mean reward: 0.491 [-19.485, 100.000], mean action: 1.994 [0.000, 3.000], mean observation: 0.046 [-0.838, 1.403], loss: 5.394246, mae: 36.221287, mean_q: 48.468624
  182371/1100000: episode: 271, duration: 5.215s, episode steps: 673, steps per second: 129, episode reward: 174.598, mean reward: 0.259 [-12.601, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: -0.010 [-0.869, 1.412], loss: 6.352732, mae: 36.941425, mean_q: 49.355022
  182744/1100000: episode: 272, duration: 2.555s, episode steps: 373, steps per second: 146, episode reward: 281.487, mean reward: 0.755 [-9.897, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.095 [-0.718, 1.412], loss: 6.774878, mae: 37.337696, mean_q: 50.003021
  183744/1100000: episode: 273, duration: 7.327s, episode steps: 1000, steps per second: 136, episode reward: -77.879, mean reward: -0.078 [-5.400, 5.267], mean action: 1.672 [0.000, 3.000], mean observation: 0.057 [-0.862, 1.492], loss: 5.167361, mae: 37.951424, mean_q: 50.677555
  183929/1100000: episode: 274, duration: 1.239s, episode steps: 185, steps per second: 149, episode reward: -11.501, mean reward: -0.062 [-100.000, 16.314], mean action: 1.805 [0.000, 3.000], mean observation: 0.105 [-1.454, 1.463], loss: 3.896990, mae: 38.044231, mean_q: 51.322624
  184084/1100000: episode: 275, duration: 1.040s, episode steps: 155, steps per second: 149, episode reward: -254.676, mean reward: -1.643 [-100.000, 68.398], mean action: 1.516 [0.000, 3.000], mean observation: -0.194 [-2.835, 1.387], loss: 4.678020, mae: 38.219360, mean_q: 50.947845
  185084/1100000: episode: 276, duration: 7.737s, episode steps: 1000, steps per second: 129, episode reward: 109.914, mean reward: 0.110 [-20.663, 16.542], mean action: 1.884 [0.000, 3.000], mean observation: 0.073 [-1.513, 1.448], loss: 6.331751, mae: 38.876671, mean_q: 52.116772
  185617/1100000: episode: 277, duration: 3.846s, episode steps: 533, steps per second: 139, episode reward: 139.084, mean reward: 0.261 [-22.899, 100.000], mean action: 1.777 [0.000, 3.000], mean observation: 0.149 [-0.894, 1.417], loss: 5.494229, mae: 39.013317, mean_q: 52.376530
  186617/1100000: episode: 278, duration: 7.144s, episode steps: 1000, steps per second: 140, episode reward: 124.753, mean reward: 0.125 [-18.942, 17.949], mean action: 0.908 [0.000, 3.000], mean observation: 0.173 [-0.530, 1.405], loss: 5.886869, mae: 39.924759, mean_q: 53.445206
  186917/1100000: episode: 279, duration: 2.113s, episode steps: 300, steps per second: 142, episode reward: 251.633, mean reward: 0.839 [-11.923, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.101 [-0.739, 1.420], loss: 5.090483, mae: 40.124306, mean_q: 53.907509
  187758/1100000: episode: 280, duration: 6.752s, episode steps: 841, steps per second: 125, episode reward: 227.534, mean reward: 0.271 [-19.682, 100.000], mean action: 2.190 [0.000, 3.000], mean observation: 0.010 [-0.634, 1.385], loss: 4.744453, mae: 40.842297, mean_q: 54.793049
  187846/1100000: episode: 281, duration: 0.583s, episode steps: 88, steps per second: 151, episode reward: -277.935, mean reward: -3.158 [-100.000, 3.424], mean action: 1.739 [0.000, 3.000], mean observation: -0.022 [-1.756, 1.423], loss: 2.363240, mae: 40.653519, mean_q: 54.732056
  188340/1100000: episode: 282, duration: 3.682s, episode steps: 494, steps per second: 134, episode reward: 217.429, mean reward: 0.440 [-17.010, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: -0.049 [-0.672, 1.499], loss: 4.970382, mae: 41.280201, mean_q: 55.479164
  189340/1100000: episode: 283, duration: 7.328s, episode steps: 1000, steps per second: 136, episode reward: 140.423, mean reward: 0.140 [-20.604, 22.603], mean action: 2.021 [0.000, 3.000], mean observation: 0.087 [-0.752, 1.388], loss: 3.837015, mae: 41.743805, mean_q: 56.037312
  189669/1100000: episode: 284, duration: 2.271s, episode steps: 329, steps per second: 145, episode reward: 258.805, mean reward: 0.787 [-11.064, 100.000], mean action: 1.669 [0.000, 3.000], mean observation: 0.166 [-0.621, 1.400], loss: 5.639567, mae: 42.562347, mean_q: 57.238369
  190253/1100000: episode: 285, duration: 4.344s, episode steps: 584, steps per second: 134, episode reward: 234.268, mean reward: 0.401 [-18.866, 100.000], mean action: 1.483 [0.000, 3.000], mean observation: 0.141 [-0.898, 1.397], loss: 5.317517, mae: 42.770103, mean_q: 57.436249
  190868/1100000: episode: 286, duration: 4.443s, episode steps: 615, steps per second: 138, episode reward: 250.398, mean reward: 0.407 [-17.335, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.108 [-0.585, 1.427], loss: 5.345080, mae: 43.124249, mean_q: 57.804966
  191868/1100000: episode: 287, duration: 7.295s, episode steps: 1000, steps per second: 137, episode reward: 32.573, mean reward: 0.033 [-18.740, 12.782], mean action: 1.614 [0.000, 3.000], mean observation: 0.167 [-0.522, 1.409], loss: 6.308090, mae: 43.463528, mean_q: 58.361069
  191980/1100000: episode: 288, duration: 0.739s, episode steps: 112, steps per second: 152, episode reward: -408.710, mean reward: -3.649 [-100.000, 2.975], mean action: 1.857 [0.000, 3.000], mean observation: -0.140 [-3.021, 4.395], loss: 4.093475, mae: 44.538727, mean_q: 59.950237
  192830/1100000: episode: 289, duration: 6.139s, episode steps: 850, steps per second: 138, episode reward: 236.937, mean reward: 0.279 [-19.175, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: -0.010 [-0.832, 1.409], loss: 4.193873, mae: 44.124146, mean_q: 59.120884
  193332/1100000: episode: 290, duration: 3.519s, episode steps: 502, steps per second: 143, episode reward: 157.018, mean reward: 0.313 [-12.424, 100.000], mean action: 1.725 [0.000, 3.000], mean observation: 0.106 [-0.475, 1.406], loss: 5.090865, mae: 43.383606, mean_q: 58.209297
  193895/1100000: episode: 291, duration: 3.985s, episode steps: 563, steps per second: 141, episode reward: 262.438, mean reward: 0.466 [-19.904, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.135 [-0.691, 1.479], loss: 6.483162, mae: 43.504684, mean_q: 58.535633
  194347/1100000: episode: 292, duration: 3.130s, episode steps: 452, steps per second: 144, episode reward: 207.816, mean reward: 0.460 [-12.655, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.160 [-0.602, 1.409], loss: 3.903842, mae: 43.509125, mean_q: 58.498859
  194801/1100000: episode: 293, duration: 3.202s, episode steps: 454, steps per second: 142, episode reward: 220.359, mean reward: 0.485 [-3.217, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: -0.018 [-0.807, 1.456], loss: 7.602003, mae: 43.573402, mean_q: 58.474041
  195074/1100000: episode: 294, duration: 1.876s, episode steps: 273, steps per second: 145, episode reward: -104.037, mean reward: -0.381 [-100.000, 6.622], mean action: 1.582 [0.000, 3.000], mean observation: 0.055 [-2.641, 1.407], loss: 6.945234, mae: 43.695885, mean_q: 58.550461
  195671/1100000: episode: 295, duration: 4.158s, episode steps: 597, steps per second: 144, episode reward: 178.095, mean reward: 0.298 [-17.907, 100.000], mean action: 1.749 [0.000, 3.000], mean observation: 0.003 [-0.676, 1.397], loss: 5.512725, mae: 43.543163, mean_q: 58.505169
  196023/1100000: episode: 296, duration: 2.412s, episode steps: 352, steps per second: 146, episode reward: 270.747, mean reward: 0.769 [-18.028, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.098 [-0.530, 1.506], loss: 7.159015, mae: 43.282177, mean_q: 58.168457
  197023/1100000: episode: 297, duration: 7.766s, episode steps: 1000, steps per second: 129, episode reward: 123.685, mean reward: 0.124 [-20.675, 20.805], mean action: 1.588 [0.000, 3.000], mean observation: 0.064 [-0.600, 1.468], loss: 5.194685, mae: 43.140579, mean_q: 58.123398
  197490/1100000: episode: 298, duration: 3.335s, episode steps: 467, steps per second: 140, episode reward: 244.060, mean reward: 0.523 [-18.027, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: -0.007 [-0.792, 1.389], loss: 5.302977, mae: 42.876633, mean_q: 57.662472
  198490/1100000: episode: 299, duration: 7.591s, episode steps: 1000, steps per second: 132, episode reward: 116.593, mean reward: 0.117 [-21.367, 22.206], mean action: 1.489 [0.000, 3.000], mean observation: -0.003 [-0.744, 1.389], loss: 5.156471, mae: 42.973751, mean_q: 57.831772
  198644/1100000: episode: 300, duration: 1.038s, episode steps: 154, steps per second: 148, episode reward: -53.235, mean reward: -0.346 [-100.000, 6.317], mean action: 1.877 [0.000, 3.000], mean observation: 0.099 [-0.661, 1.424], loss: 7.103480, mae: 43.181198, mean_q: 57.800678
  199644/1100000: episode: 301, duration: 7.445s, episode steps: 1000, steps per second: 134, episode reward: 23.926, mean reward: 0.024 [-20.811, 23.050], mean action: 1.706 [0.000, 3.000], mean observation: 0.186 [-0.631, 1.407], loss: 5.374839, mae: 43.618813, mean_q: 58.659576
  199953/1100000: episode: 302, duration: 2.097s, episode steps: 309, steps per second: 147, episode reward: 218.910, mean reward: 0.708 [-14.047, 100.000], mean action: 1.735 [0.000, 3.000], mean observation: 0.162 [-0.498, 1.389], loss: 4.142890, mae: 44.136543, mean_q: 59.303379
  200392/1100000: episode: 303, duration: 3.115s, episode steps: 439, steps per second: 141, episode reward: 258.861, mean reward: 0.590 [-12.383, 100.000], mean action: 2.043 [0.000, 3.000], mean observation: -0.009 [-0.791, 1.451], loss: 4.331957, mae: 44.893551, mean_q: 60.302925
  200675/1100000: episode: 304, duration: 1.936s, episode steps: 283, steps per second: 146, episode reward: 256.609, mean reward: 0.907 [-20.898, 100.000], mean action: 2.049 [0.000, 3.000], mean observation: 0.123 [-1.024, 1.442], loss: 5.759043, mae: 44.535244, mean_q: 59.684643
  201034/1100000: episode: 305, duration: 2.495s, episode steps: 359, steps per second: 144, episode reward: -236.067, mean reward: -0.658 [-100.000, 15.034], mean action: 1.632 [0.000, 3.000], mean observation: -0.012 [-0.600, 1.699], loss: 6.480975, mae: 45.066631, mean_q: 60.467438
  201680/1100000: episode: 306, duration: 4.912s, episode steps: 646, steps per second: 132, episode reward: 194.460, mean reward: 0.301 [-21.718, 100.000], mean action: 2.128 [0.000, 3.000], mean observation: 0.013 [-0.622, 1.414], loss: 6.856599, mae: 45.065845, mean_q: 60.440548
  201817/1100000: episode: 307, duration: 0.920s, episode steps: 137, steps per second: 149, episode reward: 16.857, mean reward: 0.123 [-100.000, 17.996], mean action: 1.847 [0.000, 3.000], mean observation: 0.088 [-1.526, 1.395], loss: 4.950495, mae: 45.852318, mean_q: 61.149185
  202204/1100000: episode: 308, duration: 2.752s, episode steps: 387, steps per second: 141, episode reward: 203.870, mean reward: 0.527 [-19.868, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.158 [-0.482, 1.411], loss: 6.811070, mae: 45.702042, mean_q: 61.315880
  202678/1100000: episode: 309, duration: 3.357s, episode steps: 474, steps per second: 141, episode reward: 231.042, mean reward: 0.487 [-18.429, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.137 [-0.523, 1.488], loss: 5.474694, mae: 46.105057, mean_q: 61.927792
  203314/1100000: episode: 310, duration: 4.550s, episode steps: 636, steps per second: 140, episode reward: 184.217, mean reward: 0.290 [-22.031, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.036 [-0.860, 1.408], loss: 4.959795, mae: 46.663376, mean_q: 62.604885
  203614/1100000: episode: 311, duration: 2.057s, episode steps: 300, steps per second: 146, episode reward: -170.282, mean reward: -0.568 [-100.000, 12.101], mean action: 1.733 [0.000, 3.000], mean observation: 0.147 [-1.083, 1.407], loss: 3.213112, mae: 46.717083, mean_q: 62.516640
  203796/1100000: episode: 312, duration: 1.209s, episode steps: 182, steps per second: 151, episode reward: -45.798, mean reward: -0.252 [-100.000, 19.396], mean action: 1.599 [0.000, 3.000], mean observation: -0.034 [-0.684, 1.834], loss: 3.611457, mae: 47.491673, mean_q: 63.370567
  204520/1100000: episode: 313, duration: 5.175s, episode steps: 724, steps per second: 140, episode reward: 204.629, mean reward: 0.283 [-19.197, 100.000], mean action: 2.290 [0.000, 3.000], mean observation: 0.057 [-0.761, 1.565], loss: 7.255225, mae: 47.045235, mean_q: 62.806900
  205520/1100000: episode: 314, duration: 7.590s, episode steps: 1000, steps per second: 132, episode reward: 103.940, mean reward: 0.104 [-19.870, 23.378], mean action: 1.661 [0.000, 3.000], mean observation: 0.177 [-0.949, 1.399], loss: 7.336158, mae: 47.700600, mean_q: 63.715820
  205946/1100000: episode: 315, duration: 3.031s, episode steps: 426, steps per second: 141, episode reward: 162.974, mean reward: 0.383 [-13.733, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.109 [-0.754, 1.405], loss: 4.798725, mae: 48.116402, mean_q: 64.550346
  206246/1100000: episode: 316, duration: 2.059s, episode steps: 300, steps per second: 146, episode reward: 269.444, mean reward: 0.898 [-8.695, 100.000], mean action: 1.587 [0.000, 3.000], mean observation: 0.041 [-0.632, 1.399], loss: 11.571167, mae: 48.346588, mean_q: 64.517365
  206409/1100000: episode: 317, duration: 1.098s, episode steps: 163, steps per second: 148, episode reward: 14.895, mean reward: 0.091 [-100.000, 16.213], mean action: 1.834 [0.000, 3.000], mean observation: -0.076 [-0.839, 1.386], loss: 5.339128, mae: 48.024963, mean_q: 64.427116
  206564/1100000: episode: 318, duration: 1.034s, episode steps: 155, steps per second: 150, episode reward: -187.018, mean reward: -1.207 [-100.000, 7.287], mean action: 2.019 [0.000, 3.000], mean observation: 0.139 [-4.497, 1.430], loss: 11.619975, mae: 48.394943, mean_q: 64.831375
  206857/1100000: episode: 319, duration: 2.015s, episode steps: 293, steps per second: 145, episode reward: 264.156, mean reward: 0.902 [-2.583, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.097 [-0.584, 1.403], loss: 6.503686, mae: 48.407532, mean_q: 64.828552
  207755/1100000: episode: 320, duration: 6.636s, episode steps: 898, steps per second: 135, episode reward: 171.664, mean reward: 0.191 [-20.239, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.241 [-0.803, 1.435], loss: 4.981551, mae: 48.337795, mean_q: 64.824051
  208180/1100000: episode: 321, duration: 2.977s, episode steps: 425, steps per second: 143, episode reward: 259.283, mean reward: 0.610 [-8.461, 100.000], mean action: 1.659 [0.000, 3.000], mean observation: 0.101 [-0.460, 1.394], loss: 8.947914, mae: 48.629623, mean_q: 65.242722
  208892/1100000: episode: 322, duration: 5.273s, episode steps: 712, steps per second: 135, episode reward: 259.502, mean reward: 0.364 [-19.909, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: 0.262 [-0.723, 1.503], loss: 6.917736, mae: 48.505386, mean_q: 65.212013
  209445/1100000: episode: 323, duration: 4.046s, episode steps: 553, steps per second: 137, episode reward: -323.502, mean reward: -0.585 [-100.000, 18.555], mean action: 1.671 [0.000, 3.000], mean observation: -0.014 [-0.818, 2.264], loss: 5.533981, mae: 48.485935, mean_q: 65.015579
  209819/1100000: episode: 324, duration: 2.615s, episode steps: 374, steps per second: 143, episode reward: 256.296, mean reward: 0.685 [-15.706, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.114 [-0.648, 1.395], loss: 5.585030, mae: 48.353943, mean_q: 64.723373
  210303/1100000: episode: 325, duration: 3.513s, episode steps: 484, steps per second: 138, episode reward: 240.931, mean reward: 0.498 [-17.785, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.121 [-0.736, 1.394], loss: 5.619609, mae: 48.328175, mean_q: 64.759727
  210646/1100000: episode: 326, duration: 2.413s, episode steps: 343, steps per second: 142, episode reward: 232.719, mean reward: 0.678 [-20.387, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.113 [-0.562, 1.412], loss: 5.090600, mae: 47.881058, mean_q: 64.308739
  210953/1100000: episode: 327, duration: 2.117s, episode steps: 307, steps per second: 145, episode reward: 218.895, mean reward: 0.713 [-9.626, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.122 [-0.500, 1.399], loss: 4.495529, mae: 47.646553, mean_q: 63.938084
  211210/1100000: episode: 328, duration: 1.758s, episode steps: 257, steps per second: 146, episode reward: -22.455, mean reward: -0.087 [-100.000, 19.770], mean action: 1.665 [0.000, 3.000], mean observation: 0.097 [-1.123, 1.411], loss: 4.843382, mae: 47.403286, mean_q: 63.350964
  211810/1100000: episode: 329, duration: 4.222s, episode steps: 600, steps per second: 142, episode reward: 191.139, mean reward: 0.319 [-15.040, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.203 [-0.644, 1.436], loss: 6.938761, mae: 47.227551, mean_q: 63.165012
  212415/1100000: episode: 330, duration: 4.452s, episode steps: 605, steps per second: 136, episode reward: 256.877, mean reward: 0.425 [-18.172, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.121 [-0.613, 1.397], loss: 7.613729, mae: 46.798798, mean_q: 62.628021
  213415/1100000: episode: 331, duration: 7.722s, episode steps: 1000, steps per second: 130, episode reward: -46.891, mean reward: -0.047 [-6.404, 4.498], mean action: 1.709 [0.000, 3.000], mean observation: -0.053 [-0.638, 1.464], loss: 6.515176, mae: 46.242546, mean_q: 61.814320
  214116/1100000: episode: 332, duration: 5.116s, episode steps: 701, steps per second: 137, episode reward: 229.961, mean reward: 0.328 [-21.003, 100.000], mean action: 0.869 [0.000, 3.000], mean observation: 0.171 [-0.627, 1.394], loss: 6.105809, mae: 46.302254, mean_q: 61.858192
  214521/1100000: episode: 333, duration: 2.819s, episode steps: 405, steps per second: 144, episode reward: 280.068, mean reward: 0.692 [-9.741, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.085 [-0.970, 1.389], loss: 3.698192, mae: 46.231014, mean_q: 61.828888
  215521/1100000: episode: 334, duration: 7.993s, episode steps: 1000, steps per second: 125, episode reward: 15.431, mean reward: 0.015 [-18.902, 19.396], mean action: 1.832 [0.000, 3.000], mean observation: -0.042 [-1.219, 1.526], loss: 4.879594, mae: 46.304283, mean_q: 61.867176
  215882/1100000: episode: 335, duration: 2.542s, episode steps: 361, steps per second: 142, episode reward: 244.778, mean reward: 0.678 [-12.030, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.048 [-0.949, 1.403], loss: 4.125072, mae: 46.301636, mean_q: 62.040680
  216882/1100000: episode: 336, duration: 7.102s, episode steps: 1000, steps per second: 141, episode reward: 91.498, mean reward: 0.091 [-20.229, 21.629], mean action: 1.601 [0.000, 3.000], mean observation: 0.273 [-0.500, 1.408], loss: 5.584294, mae: 46.079967, mean_q: 61.745804
  217699/1100000: episode: 337, duration: 5.923s, episode steps: 817, steps per second: 138, episode reward: 199.031, mean reward: 0.244 [-18.837, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.142 [-0.743, 1.531], loss: 5.255429, mae: 45.790226, mean_q: 61.294102
  217934/1100000: episode: 338, duration: 1.599s, episode steps: 235, steps per second: 147, episode reward: 261.209, mean reward: 1.112 [-2.899, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.041 [-0.675, 1.394], loss: 5.051071, mae: 45.404594, mean_q: 60.688461
  218516/1100000: episode: 339, duration: 4.189s, episode steps: 582, steps per second: 139, episode reward: 216.884, mean reward: 0.373 [-17.695, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: -0.033 [-0.670, 1.387], loss: 4.805252, mae: 45.208267, mean_q: 60.500515
  218747/1100000: episode: 340, duration: 1.551s, episode steps: 231, steps per second: 149, episode reward: 240.194, mean reward: 1.040 [-8.684, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.180 [-0.495, 1.396], loss: 4.260674, mae: 45.182346, mean_q: 60.383717
  219596/1100000: episode: 341, duration: 6.587s, episode steps: 849, steps per second: 129, episode reward: -328.208, mean reward: -0.387 [-100.000, 29.552], mean action: 1.657 [0.000, 3.000], mean observation: -0.017 [-1.902, 1.405], loss: 5.701314, mae: 45.556591, mean_q: 60.918747
  219912/1100000: episode: 342, duration: 2.205s, episode steps: 316, steps per second: 143, episode reward: 234.812, mean reward: 0.743 [-10.505, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.089 [-0.773, 1.411], loss: 5.055877, mae: 45.367653, mean_q: 60.692944
  220260/1100000: episode: 343, duration: 2.384s, episode steps: 348, steps per second: 146, episode reward: 249.086, mean reward: 0.716 [-8.801, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: 0.153 [-0.776, 1.444], loss: 5.772794, mae: 45.293198, mean_q: 60.512650
  220531/1100000: episode: 344, duration: 1.839s, episode steps: 271, steps per second: 147, episode reward: 243.618, mean reward: 0.899 [-6.344, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.105 [-0.888, 1.429], loss: 4.859638, mae: 45.326271, mean_q: 60.667622
  221085/1100000: episode: 345, duration: 3.892s, episode steps: 554, steps per second: 142, episode reward: 158.300, mean reward: 0.286 [-19.770, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.018 [-1.011, 1.397], loss: 5.200276, mae: 45.383659, mean_q: 60.825592
  222085/1100000: episode: 346, duration: 7.875s, episode steps: 1000, steps per second: 127, episode reward: -19.201, mean reward: -0.019 [-23.106, 23.673], mean action: 1.873 [0.000, 3.000], mean observation: -0.020 [-0.664, 1.392], loss: 5.127534, mae: 44.907990, mean_q: 60.151035
  222415/1100000: episode: 347, duration: 2.284s, episode steps: 330, steps per second: 144, episode reward: 256.667, mean reward: 0.778 [-10.477, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.084 [-0.502, 1.410], loss: 5.534132, mae: 44.906658, mean_q: 59.911907
  223326/1100000: episode: 348, duration: 6.428s, episode steps: 911, steps per second: 142, episode reward: 277.272, mean reward: 0.304 [-18.187, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.058 [-0.870, 1.386], loss: 5.910690, mae: 44.768250, mean_q: 59.698227
  223532/1100000: episode: 349, duration: 1.396s, episode steps: 206, steps per second: 148, episode reward: -15.058, mean reward: -0.073 [-100.000, 16.333], mean action: 1.626 [0.000, 3.000], mean observation: 0.086 [-0.855, 1.403], loss: 4.719692, mae: 44.441307, mean_q: 58.883205
  223827/1100000: episode: 350, duration: 2.014s, episode steps: 295, steps per second: 147, episode reward: 209.929, mean reward: 0.712 [-12.210, 100.000], mean action: 1.800 [0.000, 3.000], mean observation: 0.173 [-0.509, 1.397], loss: 7.338303, mae: 44.763458, mean_q: 59.876892
  224256/1100000: episode: 351, duration: 2.940s, episode steps: 429, steps per second: 146, episode reward: 278.918, mean reward: 0.650 [-17.403, 100.000], mean action: 0.823 [0.000, 3.000], mean observation: 0.234 [-0.660, 1.509], loss: 7.196176, mae: 45.093319, mean_q: 59.752785
  224650/1100000: episode: 352, duration: 2.725s, episode steps: 394, steps per second: 145, episode reward: 260.734, mean reward: 0.662 [-11.358, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.141 [-0.775, 1.413], loss: 4.212461, mae: 45.674164, mean_q: 60.830654
  224837/1100000: episode: 353, duration: 1.258s, episode steps: 187, steps per second: 149, episode reward: -263.360, mean reward: -1.408 [-100.000, 4.263], mean action: 1.877 [0.000, 3.000], mean observation: -0.011 [-0.754, 1.873], loss: 8.214366, mae: 45.140343, mean_q: 59.770126
  225498/1100000: episode: 354, duration: 4.808s, episode steps: 661, steps per second: 137, episode reward: 233.428, mean reward: 0.353 [-13.762, 100.000], mean action: 1.716 [0.000, 3.000], mean observation: -0.023 [-0.776, 1.397], loss: 6.673670, mae: 45.740047, mean_q: 61.118984
  225781/1100000: episode: 355, duration: 1.959s, episode steps: 283, steps per second: 144, episode reward: 265.571, mean reward: 0.938 [-19.337, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.085 [-0.701, 1.388], loss: 5.846264, mae: 45.231510, mean_q: 60.387341
  226212/1100000: episode: 356, duration: 3.048s, episode steps: 431, steps per second: 141, episode reward: 172.902, mean reward: 0.401 [-16.863, 100.000], mean action: 2.132 [0.000, 3.000], mean observation: 0.110 [-0.685, 1.413], loss: 3.852001, mae: 45.768490, mean_q: 61.112518
  226783/1100000: episode: 357, duration: 3.984s, episode steps: 571, steps per second: 143, episode reward: 221.083, mean reward: 0.387 [-19.602, 100.000], mean action: 2.392 [0.000, 3.000], mean observation: 0.046 [-0.622, 1.490], loss: 5.488763, mae: 45.841316, mean_q: 61.366039
  226935/1100000: episode: 358, duration: 1.010s, episode steps: 152, steps per second: 150, episode reward: -14.201, mean reward: -0.093 [-100.000, 9.868], mean action: 1.362 [0.000, 3.000], mean observation: 0.129 [-0.612, 1.477], loss: 8.396733, mae: 46.156738, mean_q: 61.896061
  227288/1100000: episode: 359, duration: 2.416s, episode steps: 353, steps per second: 146, episode reward: 227.389, mean reward: 0.644 [-20.789, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.058 [-0.795, 1.427], loss: 5.286728, mae: 46.201805, mean_q: 61.922573
  227907/1100000: episode: 360, duration: 4.535s, episode steps: 619, steps per second: 136, episode reward: 146.444, mean reward: 0.237 [-19.372, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: -0.039 [-0.789, 1.426], loss: 5.244197, mae: 46.569588, mean_q: 62.396671
  228203/1100000: episode: 361, duration: 2.040s, episode steps: 296, steps per second: 145, episode reward: 195.571, mean reward: 0.661 [-19.610, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.202 [-0.862, 1.409], loss: 5.148784, mae: 46.551914, mean_q: 62.348354
  228815/1100000: episode: 362, duration: 4.392s, episode steps: 612, steps per second: 139, episode reward: 254.874, mean reward: 0.416 [-23.393, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.064 [-0.666, 1.390], loss: 4.781007, mae: 46.788956, mean_q: 62.738270
  229131/1100000: episode: 363, duration: 2.204s, episode steps: 316, steps per second: 143, episode reward: 260.551, mean reward: 0.825 [-7.292, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.010 [-0.699, 1.387], loss: 3.870312, mae: 47.137554, mean_q: 63.033970
  229392/1100000: episode: 364, duration: 1.745s, episode steps: 261, steps per second: 150, episode reward: 269.032, mean reward: 1.031 [-18.003, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.228 [-0.649, 1.389], loss: 7.565872, mae: 47.096375, mean_q: 63.196293
  230392/1100000: episode: 365, duration: 7.582s, episode steps: 1000, steps per second: 132, episode reward: 80.074, mean reward: 0.080 [-20.649, 19.426], mean action: 1.072 [0.000, 3.000], mean observation: 0.036 [-0.600, 1.461], loss: 4.031593, mae: 47.164040, mean_q: 63.131035
  231261/1100000: episode: 366, duration: 6.105s, episode steps: 869, steps per second: 142, episode reward: 199.898, mean reward: 0.230 [-20.069, 100.000], mean action: 0.867 [0.000, 3.000], mean observation: 0.083 [-0.634, 1.391], loss: 4.598126, mae: 46.782337, mean_q: 62.578587
  232094/1100000: episode: 367, duration: 6.674s, episode steps: 833, steps per second: 125, episode reward: 127.600, mean reward: 0.153 [-17.920, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: -0.040 [-0.685, 1.409], loss: 5.784430, mae: 46.961445, mean_q: 62.776379
  232227/1100000: episode: 368, duration: 0.883s, episode steps: 133, steps per second: 151, episode reward: 6.868, mean reward: 0.052 [-100.000, 18.639], mean action: 1.617 [0.000, 3.000], mean observation: 0.036 [-0.712, 1.726], loss: 11.198583, mae: 47.135452, mean_q: 63.072250
  232746/1100000: episode: 369, duration: 3.729s, episode steps: 519, steps per second: 139, episode reward: 277.023, mean reward: 0.534 [-17.780, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.124 [-0.614, 1.400], loss: 3.866028, mae: 47.185589, mean_q: 63.354130
  233117/1100000: episode: 370, duration: 2.554s, episode steps: 371, steps per second: 145, episode reward: 240.975, mean reward: 0.650 [-11.155, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.234 [-0.564, 1.425], loss: 3.347569, mae: 47.159477, mean_q: 63.187225
  233414/1100000: episode: 371, duration: 1.994s, episode steps: 297, steps per second: 149, episode reward: 251.773, mean reward: 0.848 [-15.388, 100.000], mean action: 1.828 [0.000, 3.000], mean observation: -0.010 [-0.947, 1.482], loss: 6.506090, mae: 47.318871, mean_q: 63.476379
  233673/1100000: episode: 372, duration: 1.762s, episode steps: 259, steps per second: 147, episode reward: -156.778, mean reward: -0.605 [-100.000, 38.677], mean action: 2.000 [0.000, 3.000], mean observation: -0.017 [-0.733, 1.657], loss: 8.429709, mae: 47.540661, mean_q: 63.813736
  233916/1100000: episode: 373, duration: 1.648s, episode steps: 243, steps per second: 147, episode reward: 256.855, mean reward: 1.057 [-6.535, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.059 [-0.741, 1.411], loss: 4.173304, mae: 47.437531, mean_q: 63.651428
  234237/1100000: episode: 374, duration: 2.216s, episode steps: 321, steps per second: 145, episode reward: 239.877, mean reward: 0.747 [-9.654, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.174 [-0.582, 1.453], loss: 4.854110, mae: 47.312057, mean_q: 63.365150
  234602/1100000: episode: 375, duration: 2.507s, episode steps: 365, steps per second: 146, episode reward: 263.912, mean reward: 0.723 [-10.454, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.122 [-0.622, 1.498], loss: 3.487171, mae: 47.434639, mean_q: 63.535458
  235178/1100000: episode: 376, duration: 4.183s, episode steps: 576, steps per second: 138, episode reward: 209.612, mean reward: 0.364 [-19.941, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.065 [-0.822, 1.405], loss: 4.432319, mae: 47.043762, mean_q: 62.956013
  235795/1100000: episode: 377, duration: 4.572s, episode steps: 617, steps per second: 135, episode reward: 248.198, mean reward: 0.402 [-18.489, 100.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.019 [-0.757, 1.391], loss: 6.105274, mae: 47.451023, mean_q: 63.672703
  236768/1100000: episode: 378, duration: 7.212s, episode steps: 973, steps per second: 135, episode reward: 193.072, mean reward: 0.198 [-19.031, 100.000], mean action: 1.786 [0.000, 3.000], mean observation: 0.066 [-0.794, 1.406], loss: 6.530993, mae: 46.919971, mean_q: 62.647957
  236993/1100000: episode: 379, duration: 1.513s, episode steps: 225, steps per second: 149, episode reward: 276.011, mean reward: 1.227 [-21.705, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.085 [-0.734, 1.392], loss: 4.608908, mae: 46.914894, mean_q: 62.736412
  237328/1100000: episode: 380, duration: 2.292s, episode steps: 335, steps per second: 146, episode reward: 228.712, mean reward: 0.683 [-19.390, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.022 [-0.773, 1.474], loss: 7.236785, mae: 46.799229, mean_q: 62.871899
  237553/1100000: episode: 381, duration: 1.500s, episode steps: 225, steps per second: 150, episode reward: -79.698, mean reward: -0.354 [-100.000, 15.844], mean action: 1.556 [0.000, 3.000], mean observation: 0.139 [-0.566, 1.517], loss: 6.251588, mae: 47.313862, mean_q: 63.162369
  237855/1100000: episode: 382, duration: 2.085s, episode steps: 302, steps per second: 145, episode reward: 240.282, mean reward: 0.796 [-9.854, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.209 [-0.728, 1.414], loss: 6.088723, mae: 46.832317, mean_q: 62.721458
  238467/1100000: episode: 383, duration: 4.354s, episode steps: 612, steps per second: 141, episode reward: 184.993, mean reward: 0.302 [-18.334, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.263 [-0.597, 1.507], loss: 5.633870, mae: 46.843315, mean_q: 62.587048
  238876/1100000: episode: 384, duration: 2.823s, episode steps: 409, steps per second: 145, episode reward: 201.088, mean reward: 0.492 [-17.896, 100.000], mean action: 1.638 [0.000, 3.000], mean observation: 0.163 [-0.484, 1.423], loss: 4.290500, mae: 46.799549, mean_q: 62.710594
  239435/1100000: episode: 385, duration: 3.947s, episode steps: 559, steps per second: 142, episode reward: 218.943, mean reward: 0.392 [-18.360, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: -0.011 [-0.619, 1.425], loss: 6.004380, mae: 46.415882, mean_q: 62.025715
  239637/1100000: episode: 386, duration: 1.358s, episode steps: 202, steps per second: 149, episode reward: -74.817, mean reward: -0.370 [-100.000, 16.614], mean action: 1.688 [0.000, 3.000], mean observation: -0.107 [-2.039, 1.402], loss: 3.998703, mae: 46.652161, mean_q: 62.613522
  239921/1100000: episode: 387, duration: 1.928s, episode steps: 284, steps per second: 147, episode reward: 284.869, mean reward: 1.003 [-20.185, 100.000], mean action: 1.521 [0.000, 3.000], mean observation: 0.136 [-0.763, 1.418], loss: 4.607244, mae: 46.607265, mean_q: 62.354561
  240351/1100000: episode: 388, duration: 3.043s, episode steps: 430, steps per second: 141, episode reward: 221.439, mean reward: 0.515 [-19.184, 100.000], mean action: 1.928 [0.000, 3.000], mean observation: 0.209 [-0.574, 1.402], loss: 3.984300, mae: 46.300682, mean_q: 62.138481
  240663/1100000: episode: 389, duration: 2.160s, episode steps: 312, steps per second: 144, episode reward: -193.382, mean reward: -0.620 [-100.000, 4.064], mean action: 1.833 [0.000, 3.000], mean observation: 0.158 [-0.803, 1.419], loss: 4.286807, mae: 46.865665, mean_q: 62.838055
  240876/1100000: episode: 390, duration: 1.427s, episode steps: 213, steps per second: 149, episode reward: 296.674, mean reward: 1.393 [-13.941, 100.000], mean action: 1.709 [0.000, 3.000], mean observation: 0.133 [-0.771, 1.406], loss: 6.940835, mae: 46.908958, mean_q: 62.833687
  241392/1100000: episode: 391, duration: 3.657s, episode steps: 516, steps per second: 141, episode reward: 264.737, mean reward: 0.513 [-20.110, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.136 [-0.701, 1.437], loss: 7.981386, mae: 47.154079, mean_q: 63.014599
  241484/1100000: episode: 392, duration: 0.616s, episode steps: 92, steps per second: 149, episode reward: -109.718, mean reward: -1.193 [-100.000, 7.837], mean action: 1.859 [0.000, 3.000], mean observation: -0.010 [-1.120, 3.999], loss: 3.851765, mae: 47.387951, mean_q: 63.138851
  241920/1100000: episode: 393, duration: 3.054s, episode steps: 436, steps per second: 143, episode reward: 253.912, mean reward: 0.582 [-17.479, 100.000], mean action: 1.021 [0.000, 3.000], mean observation: 0.258 [-0.631, 1.411], loss: 6.405643, mae: 46.980038, mean_q: 62.824619
  242465/1100000: episode: 394, duration: 3.986s, episode steps: 545, steps per second: 137, episode reward: 195.598, mean reward: 0.359 [-16.129, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: -0.029 [-0.790, 1.385], loss: 5.332405, mae: 47.493126, mean_q: 63.518250
  242919/1100000: episode: 395, duration: 3.181s, episode steps: 454, steps per second: 143, episode reward: 214.554, mean reward: 0.473 [-11.246, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.167 [-0.826, 1.420], loss: 4.996366, mae: 47.219101, mean_q: 63.223293
  243336/1100000: episode: 396, duration: 2.873s, episode steps: 417, steps per second: 145, episode reward: 236.604, mean reward: 0.567 [-9.867, 100.000], mean action: 0.847 [0.000, 3.000], mean observation: 0.131 [-0.594, 1.409], loss: 4.748333, mae: 47.172695, mean_q: 63.207279
  243985/1100000: episode: 397, duration: 4.516s, episode steps: 649, steps per second: 144, episode reward: 208.841, mean reward: 0.322 [-19.834, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.156 [-0.391, 1.401], loss: 5.822273, mae: 47.425255, mean_q: 63.364780
  244753/1100000: episode: 398, duration: 5.754s, episode steps: 768, steps per second: 133, episode reward: 180.459, mean reward: 0.235 [-22.637, 100.000], mean action: 1.724 [0.000, 3.000], mean observation: 0.180 [-0.796, 1.520], loss: 4.657473, mae: 47.200237, mean_q: 63.151764
  245312/1100000: episode: 399, duration: 3.962s, episode steps: 559, steps per second: 141, episode reward: 193.876, mean reward: 0.347 [-19.389, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.247 [-0.456, 1.405], loss: 4.690264, mae: 47.197109, mean_q: 62.937206
  245389/1100000: episode: 400, duration: 0.519s, episode steps: 77, steps per second: 148, episode reward: -23.875, mean reward: -0.310 [-100.000, 18.083], mean action: 1.714 [0.000, 3.000], mean observation: 0.016 [-1.740, 1.388], loss: 3.600904, mae: 46.969143, mean_q: 62.928196
  246214/1100000: episode: 401, duration: 6.584s, episode steps: 825, steps per second: 125, episode reward: 138.575, mean reward: 0.168 [-17.445, 100.000], mean action: 1.678 [0.000, 3.000], mean observation: -0.037 [-0.849, 1.419], loss: 7.997661, mae: 47.558521, mean_q: 63.580692
  246550/1100000: episode: 402, duration: 2.469s, episode steps: 336, steps per second: 136, episode reward: 228.543, mean reward: 0.680 [-19.956, 100.000], mean action: 1.795 [0.000, 3.000], mean observation: 0.006 [-0.707, 1.387], loss: 6.510178, mae: 47.499931, mean_q: 63.444683
  246747/1100000: episode: 403, duration: 1.334s, episode steps: 197, steps per second: 148, episode reward: 277.415, mean reward: 1.408 [-21.233, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.069 [-0.558, 1.389], loss: 5.324776, mae: 47.467674, mean_q: 63.582821
  247075/1100000: episode: 404, duration: 2.254s, episode steps: 328, steps per second: 146, episode reward: 268.511, mean reward: 0.819 [-9.649, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.091 [-0.559, 1.493], loss: 4.454121, mae: 47.276131, mean_q: 63.152706
  247323/1100000: episode: 405, duration: 1.676s, episode steps: 248, steps per second: 148, episode reward: 220.963, mean reward: 0.891 [-3.330, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.001 [-0.600, 1.413], loss: 4.209002, mae: 47.548908, mean_q: 63.619965
  248230/1100000: episode: 406, duration: 6.709s, episode steps: 907, steps per second: 135, episode reward: 237.829, mean reward: 0.262 [-22.846, 100.000], mean action: 0.853 [0.000, 3.000], mean observation: 0.177 [-1.047, 1.386], loss: 4.425502, mae: 47.322914, mean_q: 63.212662
  248492/1100000: episode: 407, duration: 1.784s, episode steps: 262, steps per second: 147, episode reward: 242.030, mean reward: 0.924 [-17.902, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.047 [-0.593, 1.413], loss: 5.763174, mae: 47.681255, mean_q: 63.735031
  248865/1100000: episode: 408, duration: 2.543s, episode steps: 373, steps per second: 147, episode reward: -28.947, mean reward: -0.078 [-100.000, 5.602], mean action: 1.743 [0.000, 3.000], mean observation: 0.000 [-0.607, 1.402], loss: 3.939773, mae: 47.830921, mean_q: 63.877708
  249143/1100000: episode: 409, duration: 1.871s, episode steps: 278, steps per second: 149, episode reward: 270.296, mean reward: 0.972 [-2.687, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.130 [-0.616, 1.470], loss: 6.185964, mae: 47.433849, mean_q: 63.606831
  249623/1100000: episode: 410, duration: 3.451s, episode steps: 480, steps per second: 139, episode reward: -207.869, mean reward: -0.433 [-100.000, 3.510], mean action: 1.727 [0.000, 3.000], mean observation: 0.141 [-1.000, 1.917], loss: 5.435411, mae: 47.541172, mean_q: 63.397713
  249977/1100000: episode: 411, duration: 2.545s, episode steps: 354, steps per second: 139, episode reward: 282.196, mean reward: 0.797 [-10.898, 100.000], mean action: 1.661 [0.000, 3.000], mean observation: 0.023 [-0.713, 1.391], loss: 4.679549, mae: 47.760826, mean_q: 63.463165
  250541/1100000: episode: 412, duration: 4.042s, episode steps: 564, steps per second: 140, episode reward: 184.216, mean reward: 0.327 [-19.623, 100.000], mean action: 2.449 [0.000, 3.000], mean observation: 0.081 [-0.694, 1.412], loss: 6.151477, mae: 47.773876, mean_q: 63.787033
  251199/1100000: episode: 413, duration: 4.471s, episode steps: 658, steps per second: 147, episode reward: 253.191, mean reward: 0.385 [-18.348, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.067 [-0.709, 1.399], loss: 4.598620, mae: 47.405853, mean_q: 63.278313
  252125/1100000: episode: 414, duration: 6.656s, episode steps: 926, steps per second: 139, episode reward: 197.681, mean reward: 0.213 [-21.058, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.216 [-0.634, 1.665], loss: 6.729570, mae: 47.567818, mean_q: 63.376869
  252423/1100000: episode: 415, duration: 2.057s, episode steps: 298, steps per second: 145, episode reward: 238.875, mean reward: 0.802 [-9.819, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: 0.031 [-0.766, 1.453], loss: 6.346649, mae: 47.632236, mean_q: 63.302963
  252948/1100000: episode: 416, duration: 3.693s, episode steps: 525, steps per second: 142, episode reward: -2.055, mean reward: -0.004 [-100.000, 22.275], mean action: 2.419 [0.000, 3.000], mean observation: 0.077 [-0.808, 1.413], loss: 6.931687, mae: 47.700001, mean_q: 63.564598
  253131/1100000: episode: 417, duration: 1.226s, episode steps: 183, steps per second: 149, episode reward: 20.973, mean reward: 0.115 [-100.000, 14.112], mean action: 1.607 [0.000, 3.000], mean observation: -0.053 [-0.600, 1.394], loss: 3.817528, mae: 47.610905, mean_q: 63.727257
  253405/1100000: episode: 418, duration: 1.892s, episode steps: 274, steps per second: 145, episode reward: 277.169, mean reward: 1.012 [-19.024, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.143 [-0.597, 1.411], loss: 3.974129, mae: 47.481205, mean_q: 63.588833
  253559/1100000: episode: 419, duration: 1.031s, episode steps: 154, steps per second: 149, episode reward: 264.650, mean reward: 1.719 [-15.018, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.005 [-0.822, 1.392], loss: 9.629169, mae: 48.127975, mean_q: 64.260818
  253910/1100000: episode: 420, duration: 2.379s, episode steps: 351, steps per second: 148, episode reward: 302.643, mean reward: 0.862 [-17.495, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.117 [-0.791, 1.473], loss: 5.932838, mae: 47.899895, mean_q: 63.972401
  254099/1100000: episode: 421, duration: 1.278s, episode steps: 189, steps per second: 148, episode reward: 267.929, mean reward: 1.418 [-11.237, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.019 [-0.685, 1.386], loss: 2.733478, mae: 48.028625, mean_q: 64.478233
  254340/1100000: episode: 422, duration: 1.642s, episode steps: 241, steps per second: 147, episode reward: 257.073, mean reward: 1.067 [-9.267, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.119 [-0.584, 1.398], loss: 4.270506, mae: 48.349075, mean_q: 64.741257
  254548/1100000: episode: 423, duration: 1.397s, episode steps: 208, steps per second: 149, episode reward: -103.072, mean reward: -0.496 [-100.000, 12.794], mean action: 1.697 [0.000, 3.000], mean observation: -0.019 [-0.982, 1.416], loss: 8.778848, mae: 48.333420, mean_q: 64.552437
  255079/1100000: episode: 424, duration: 3.788s, episode steps: 531, steps per second: 140, episode reward: 208.896, mean reward: 0.393 [-9.799, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.172 [-0.676, 1.397], loss: 6.294907, mae: 48.051289, mean_q: 64.329330
  256079/1100000: episode: 425, duration: 7.133s, episode steps: 1000, steps per second: 140, episode reward: 134.930, mean reward: 0.135 [-21.534, 21.408], mean action: 1.791 [0.000, 3.000], mean observation: 0.091 [-0.738, 1.398], loss: 8.099203, mae: 48.127094, mean_q: 64.425880
  256389/1100000: episode: 426, duration: 2.127s, episode steps: 310, steps per second: 146, episode reward: 235.319, mean reward: 0.759 [-10.067, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.067 [-0.602, 1.431], loss: 5.725220, mae: 48.410458, mean_q: 64.751915
  256635/1100000: episode: 427, duration: 1.678s, episode steps: 246, steps per second: 147, episode reward: 202.540, mean reward: 0.823 [-14.195, 100.000], mean action: 1.951 [0.000, 3.000], mean observation: 0.014 [-0.600, 1.406], loss: 7.713832, mae: 48.109772, mean_q: 64.514412
  257635/1100000: episode: 428, duration: 7.001s, episode steps: 1000, steps per second: 143, episode reward: 167.318, mean reward: 0.167 [-19.413, 13.688], mean action: 0.909 [0.000, 3.000], mean observation: 0.225 [-0.713, 1.390], loss: 6.493979, mae: 47.934208, mean_q: 64.341934
  258039/1100000: episode: 429, duration: 2.773s, episode steps: 404, steps per second: 146, episode reward: 226.532, mean reward: 0.561 [-17.490, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.038 [-0.786, 1.407], loss: 5.894820, mae: 48.153519, mean_q: 64.670219
  258622/1100000: episode: 430, duration: 4.114s, episode steps: 583, steps per second: 142, episode reward: 147.929, mean reward: 0.254 [-18.336, 100.000], mean action: 1.878 [0.000, 3.000], mean observation: 0.152 [-0.383, 1.398], loss: 5.171229, mae: 47.853374, mean_q: 64.282562
  259161/1100000: episode: 431, duration: 3.901s, episode steps: 539, steps per second: 138, episode reward: 151.112, mean reward: 0.280 [-24.707, 100.000], mean action: 2.078 [0.000, 3.000], mean observation: 0.155 [-0.577, 1.405], loss: 5.345552, mae: 47.606579, mean_q: 63.939705
  259352/1100000: episode: 432, duration: 1.293s, episode steps: 191, steps per second: 148, episode reward: 237.930, mean reward: 1.246 [-7.689, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.040 [-0.788, 1.401], loss: 7.043213, mae: 47.980373, mean_q: 64.533775
  259633/1100000: episode: 433, duration: 1.904s, episode steps: 281, steps per second: 148, episode reward: 267.547, mean reward: 0.952 [-10.265, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.094 [-0.617, 1.490], loss: 8.074692, mae: 47.712296, mean_q: 63.911777
  259761/1100000: episode: 434, duration: 0.854s, episode steps: 128, steps per second: 150, episode reward: 44.691, mean reward: 0.349 [-100.000, 9.788], mean action: 1.531 [0.000, 3.000], mean observation: 0.072 [-1.486, 1.389], loss: 4.351158, mae: 47.837944, mean_q: 64.528442
  259909/1100000: episode: 435, duration: 0.991s, episode steps: 148, steps per second: 149, episode reward: -4.805, mean reward: -0.032 [-100.000, 17.601], mean action: 1.588 [0.000, 3.000], mean observation: 0.021 [-0.796, 1.492], loss: 9.582422, mae: 47.689774, mean_q: 63.981945
  260037/1100000: episode: 436, duration: 0.860s, episode steps: 128, steps per second: 149, episode reward: 44.685, mean reward: 0.349 [-100.000, 38.903], mean action: 1.797 [0.000, 3.000], mean observation: -0.102 [-0.879, 1.673], loss: 12.689589, mae: 48.269917, mean_q: 64.733582
  260705/1100000: episode: 437, duration: 5.000s, episode steps: 668, steps per second: 134, episode reward: 161.501, mean reward: 0.242 [-19.652, 100.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.127 [-0.581, 1.408], loss: 6.646503, mae: 47.965725, mean_q: 64.454987
  260878/1100000: episode: 438, duration: 1.176s, episode steps: 173, steps per second: 147, episode reward: -191.501, mean reward: -1.107 [-100.000, 13.601], mean action: 1.728 [0.000, 3.000], mean observation: -0.130 [-2.061, 1.400], loss: 7.050715, mae: 47.814808, mean_q: 64.317001
  260968/1100000: episode: 439, duration: 0.598s, episode steps: 90, steps per second: 150, episode reward: -265.634, mean reward: -2.951 [-100.000, 2.615], mean action: 2.078 [0.000, 3.000], mean observation: 0.003 [-4.962, 1.553], loss: 14.292221, mae: 47.866646, mean_q: 64.451820
  261114/1100000: episode: 440, duration: 0.989s, episode steps: 146, steps per second: 148, episode reward: 43.101, mean reward: 0.295 [-100.000, 5.776], mean action: 1.747 [0.000, 3.000], mean observation: -0.064 [-0.858, 1.393], loss: 6.801868, mae: 48.132675, mean_q: 64.747314
  261715/1100000: episode: 441, duration: 4.384s, episode steps: 601, steps per second: 137, episode reward: 222.446, mean reward: 0.370 [-20.347, 100.000], mean action: 1.973 [0.000, 3.000], mean observation: 0.091 [-0.962, 1.501], loss: 8.540715, mae: 48.267361, mean_q: 64.895149
  262513/1100000: episode: 442, duration: 6.163s, episode steps: 798, steps per second: 129, episode reward: 208.348, mean reward: 0.261 [-17.614, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.012 [-0.740, 1.393], loss: 7.228337, mae: 48.211346, mean_q: 64.694954
  262769/1100000: episode: 443, duration: 1.724s, episode steps: 256, steps per second: 149, episode reward: 50.091, mean reward: 0.196 [-100.000, 13.188], mean action: 1.652 [0.000, 3.000], mean observation: 0.051 [-0.807, 1.436], loss: 9.303516, mae: 47.949493, mean_q: 64.332161
  263295/1100000: episode: 444, duration: 3.686s, episode steps: 526, steps per second: 143, episode reward: 267.362, mean reward: 0.508 [-20.363, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.134 [-0.658, 1.479], loss: 7.830941, mae: 48.253578, mean_q: 64.679207
  263924/1100000: episode: 445, duration: 4.552s, episode steps: 629, steps per second: 138, episode reward: 222.537, mean reward: 0.354 [-19.405, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.158 [-0.704, 1.499], loss: 6.445603, mae: 48.251335, mean_q: 64.827232
  264424/1100000: episode: 446, duration: 3.442s, episode steps: 500, steps per second: 145, episode reward: 220.324, mean reward: 0.441 [-12.422, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.151 [-0.648, 1.686], loss: 10.050415, mae: 48.253632, mean_q: 64.737671
  265002/1100000: episode: 447, duration: 4.186s, episode steps: 578, steps per second: 138, episode reward: 233.344, mean reward: 0.404 [-19.727, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.045 [-0.600, 1.401], loss: 6.418001, mae: 48.002621, mean_q: 64.462097
  265330/1100000: episode: 448, duration: 2.264s, episode steps: 328, steps per second: 145, episode reward: 242.462, mean reward: 0.739 [-18.491, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.092 [-0.602, 1.406], loss: 9.400996, mae: 48.006355, mean_q: 64.253311
  265807/1100000: episode: 449, duration: 3.419s, episode steps: 477, steps per second: 140, episode reward: 180.382, mean reward: 0.378 [-10.510, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: -0.008 [-0.600, 1.465], loss: 7.448826, mae: 47.973473, mean_q: 64.369705
  266310/1100000: episode: 450, duration: 3.472s, episode steps: 503, steps per second: 145, episode reward: 214.142, mean reward: 0.426 [-19.286, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.233 [-0.413, 1.468], loss: 6.525079, mae: 47.447849, mean_q: 63.634731
  266939/1100000: episode: 451, duration: 4.498s, episode steps: 629, steps per second: 140, episode reward: 285.182, mean reward: 0.453 [-19.287, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.190 [-0.717, 1.432], loss: 5.005638, mae: 47.762634, mean_q: 64.126190
  267939/1100000: episode: 452, duration: 7.478s, episode steps: 1000, steps per second: 134, episode reward: 119.625, mean reward: 0.120 [-21.789, 21.930], mean action: 1.417 [0.000, 3.000], mean observation: 0.090 [-0.694, 1.404], loss: 6.201067, mae: 47.779736, mean_q: 64.180588
  268334/1100000: episode: 453, duration: 2.736s, episode steps: 395, steps per second: 144, episode reward: 242.244, mean reward: 0.613 [-19.157, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.147 [-0.525, 1.405], loss: 8.257000, mae: 47.745914, mean_q: 63.982735
  268594/1100000: episode: 454, duration: 1.773s, episode steps: 260, steps per second: 147, episode reward: 286.890, mean reward: 1.103 [-11.455, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: -0.041 [-0.805, 1.386], loss: 5.480206, mae: 47.711727, mean_q: 63.903111
  268823/1100000: episode: 455, duration: 1.545s, episode steps: 229, steps per second: 148, episode reward: -19.705, mean reward: -0.086 [-100.000, 13.633], mean action: 1.419 [0.000, 3.000], mean observation: 0.198 [-0.459, 1.420], loss: 6.679310, mae: 47.586311, mean_q: 63.847507
  269197/1100000: episode: 456, duration: 2.633s, episode steps: 374, steps per second: 142, episode reward: 231.523, mean reward: 0.619 [-10.507, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.015 [-0.619, 1.431], loss: 4.254927, mae: 47.418785, mean_q: 63.403084
  269481/1100000: episode: 457, duration: 1.927s, episode steps: 284, steps per second: 147, episode reward: 249.579, mean reward: 0.879 [-9.934, 100.000], mean action: 1.785 [0.000, 3.000], mean observation: 0.076 [-0.666, 1.409], loss: 4.940153, mae: 47.097282, mean_q: 63.357937
  269744/1100000: episode: 458, duration: 1.799s, episode steps: 263, steps per second: 146, episode reward: 279.070, mean reward: 1.061 [-17.334, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.063 [-0.625, 1.388], loss: 8.444509, mae: 47.526157, mean_q: 63.800350
  270744/1100000: episode: 459, duration: 7.378s, episode steps: 1000, steps per second: 136, episode reward: 144.584, mean reward: 0.145 [-18.145, 22.259], mean action: 2.394 [0.000, 3.000], mean observation: 0.217 [-0.582, 1.516], loss: 7.079621, mae: 47.935562, mean_q: 64.348862
  271103/1100000: episode: 460, duration: 2.484s, episode steps: 359, steps per second: 145, episode reward: 220.667, mean reward: 0.615 [-17.339, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: -0.021 [-0.918, 1.398], loss: 5.605052, mae: 47.599300, mean_q: 63.960686
  271237/1100000: episode: 461, duration: 0.895s, episode steps: 134, steps per second: 150, episode reward: -9.050, mean reward: -0.068 [-100.000, 10.686], mean action: 1.657 [0.000, 3.000], mean observation: -0.010 [-0.645, 1.401], loss: 10.079497, mae: 48.074097, mean_q: 64.105675
  272024/1100000: episode: 462, duration: 5.719s, episode steps: 787, steps per second: 138, episode reward: 248.786, mean reward: 0.316 [-20.384, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.163 [-0.670, 1.407], loss: 5.215535, mae: 47.500393, mean_q: 63.821461
  273024/1100000: episode: 463, duration: 7.298s, episode steps: 1000, steps per second: 137, episode reward: 78.729, mean reward: 0.079 [-19.414, 22.450], mean action: 1.977 [0.000, 3.000], mean observation: 0.259 [-0.488, 1.406], loss: 5.178346, mae: 46.851360, mean_q: 62.753025
  273358/1100000: episode: 464, duration: 2.271s, episode steps: 334, steps per second: 147, episode reward: 274.378, mean reward: 0.821 [-10.017, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.146 [-0.807, 1.399], loss: 9.051591, mae: 46.751461, mean_q: 62.702843
  273707/1100000: episode: 465, duration: 2.383s, episode steps: 349, steps per second: 146, episode reward: 262.873, mean reward: 0.753 [-20.964, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.112 [-0.764, 1.410], loss: 9.442816, mae: 46.618576, mean_q: 62.613571
  274115/1100000: episode: 466, duration: 2.776s, episode steps: 408, steps per second: 147, episode reward: 311.982, mean reward: 0.765 [-11.760, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.130 [-0.661, 1.534], loss: 5.963165, mae: 46.766342, mean_q: 62.761318
  274349/1100000: episode: 467, duration: 1.570s, episode steps: 234, steps per second: 149, episode reward: -19.661, mean reward: -0.084 [-100.000, 57.962], mean action: 1.449 [0.000, 3.000], mean observation: 0.077 [-0.888, 1.389], loss: 7.192405, mae: 46.582905, mean_q: 62.719067
  274652/1100000: episode: 468, duration: 2.090s, episode steps: 303, steps per second: 145, episode reward: 267.468, mean reward: 0.883 [-17.533, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.116 [-0.705, 1.416], loss: 4.144375, mae: 47.174412, mean_q: 63.493217
  275122/1100000: episode: 469, duration: 3.357s, episode steps: 470, steps per second: 140, episode reward: 137.877, mean reward: 0.293 [-12.269, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: -0.051 [-0.739, 1.404], loss: 6.766192, mae: 46.747597, mean_q: 62.857418
  275490/1100000: episode: 470, duration: 2.595s, episode steps: 368, steps per second: 142, episode reward: 236.127, mean reward: 0.642 [-13.831, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.157 [-1.125, 1.396], loss: 9.117231, mae: 46.293404, mean_q: 62.265820
  275724/1100000: episode: 471, duration: 1.570s, episode steps: 234, steps per second: 149, episode reward: 229.771, mean reward: 0.982 [-10.790, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.007 [-0.850, 1.405], loss: 10.456355, mae: 46.333260, mean_q: 62.215126
  276081/1100000: episode: 472, duration: 2.458s, episode steps: 357, steps per second: 145, episode reward: 226.286, mean reward: 0.634 [-10.727, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: 0.199 [-0.617, 1.449], loss: 6.318883, mae: 46.683506, mean_q: 62.583923
  276487/1100000: episode: 473, duration: 2.799s, episode steps: 406, steps per second: 145, episode reward: 279.515, mean reward: 0.688 [-23.821, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.132 [-0.962, 1.519], loss: 5.032861, mae: 46.422817, mean_q: 62.424274
  276791/1100000: episode: 474, duration: 2.055s, episode steps: 304, steps per second: 148, episode reward: 272.061, mean reward: 0.895 [-3.566, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.218 [-0.584, 1.505], loss: 10.236122, mae: 46.318577, mean_q: 62.165771
  277054/1100000: episode: 475, duration: 1.766s, episode steps: 263, steps per second: 149, episode reward: 229.037, mean reward: 0.871 [-13.628, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.182 [-0.688, 1.408], loss: 5.491270, mae: 46.868744, mean_q: 62.938568
  277941/1100000: episode: 476, duration: 6.337s, episode steps: 887, steps per second: 140, episode reward: 210.916, mean reward: 0.238 [-18.941, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.063 [-0.600, 1.571], loss: 5.516024, mae: 46.469833, mean_q: 62.423477
  278581/1100000: episode: 477, duration: 4.528s, episode steps: 640, steps per second: 141, episode reward: 176.486, mean reward: 0.276 [-24.828, 100.000], mean action: 1.659 [0.000, 3.000], mean observation: -0.017 [-0.919, 1.399], loss: 6.427370, mae: 46.582817, mean_q: 62.639080
  278812/1100000: episode: 478, duration: 1.556s, episode steps: 231, steps per second: 148, episode reward: 212.087, mean reward: 0.918 [-8.941, 100.000], mean action: 0.983 [0.000, 3.000], mean observation: 0.162 [-0.814, 1.408], loss: 7.073998, mae: 46.985229, mean_q: 63.330872
  278946/1100000: episode: 479, duration: 0.918s, episode steps: 134, steps per second: 146, episode reward: 12.443, mean reward: 0.093 [-100.000, 11.738], mean action: 1.425 [0.000, 3.000], mean observation: 0.100 [-0.714, 1.710], loss: 7.339437, mae: 46.788372, mean_q: 62.868382
  279103/1100000: episode: 480, duration: 1.046s, episode steps: 157, steps per second: 150, episode reward: 19.993, mean reward: 0.127 [-100.000, 10.026], mean action: 1.573 [0.000, 3.000], mean observation: 0.103 [-1.284, 1.520], loss: 7.737875, mae: 46.966095, mean_q: 63.301323
  279219/1100000: episode: 481, duration: 0.783s, episode steps: 116, steps per second: 148, episode reward: -3.606, mean reward: -0.031 [-100.000, 11.634], mean action: 2.026 [1.000, 3.000], mean observation: 0.144 [-0.947, 1.413], loss: 3.654930, mae: 47.228016, mean_q: 63.668453
  279340/1100000: episode: 482, duration: 0.801s, episode steps: 121, steps per second: 151, episode reward: -19.228, mean reward: -0.159 [-100.000, 17.760], mean action: 1.545 [0.000, 3.000], mean observation: 0.056 [-0.789, 1.429], loss: 8.793571, mae: 47.020164, mean_q: 63.177406
  280340/1100000: episode: 483, duration: 7.218s, episode steps: 1000, steps per second: 139, episode reward: 97.271, mean reward: 0.097 [-19.742, 22.238], mean action: 1.516 [0.000, 3.000], mean observation: 0.035 [-0.742, 1.410], loss: 6.561237, mae: 47.378304, mean_q: 63.641026
  280820/1100000: episode: 484, duration: 3.535s, episode steps: 480, steps per second: 136, episode reward: 172.783, mean reward: 0.360 [-19.819, 100.000], mean action: 0.958 [0.000, 3.000], mean observation: 0.127 [-0.669, 1.504], loss: 5.863441, mae: 47.565475, mean_q: 63.963840
  280989/1100000: episode: 485, duration: 1.138s, episode steps: 169, steps per second: 149, episode reward: -19.973, mean reward: -0.118 [-100.000, 12.080], mean action: 2.000 [0.000, 3.000], mean observation: -0.085 [-0.628, 1.418], loss: 4.084502, mae: 47.816406, mean_q: 64.219620
  281284/1100000: episode: 486, duration: 2.026s, episode steps: 295, steps per second: 146, episode reward: 249.928, mean reward: 0.847 [-23.218, 100.000], mean action: 1.786 [0.000, 3.000], mean observation: 0.001 [-0.996, 1.395], loss: 9.963286, mae: 47.775906, mean_q: 64.219841
  281465/1100000: episode: 487, duration: 1.200s, episode steps: 181, steps per second: 151, episode reward: -213.055, mean reward: -1.177 [-100.000, 13.040], mean action: 1.354 [0.000, 3.000], mean observation: -0.104 [-1.850, 1.481], loss: 7.552541, mae: 47.628265, mean_q: 63.903511
  281563/1100000: episode: 488, duration: 0.654s, episode steps: 98, steps per second: 150, episode reward: -33.646, mean reward: -0.343 [-100.000, 15.845], mean action: 1.643 [0.000, 3.000], mean observation: -0.015 [-0.933, 1.622], loss: 12.422576, mae: 46.864292, mean_q: 62.867298
  281780/1100000: episode: 489, duration: 1.473s, episode steps: 217, steps per second: 147, episode reward: 250.796, mean reward: 1.156 [-10.052, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.070 [-0.651, 1.397], loss: 8.964770, mae: 47.366280, mean_q: 63.469158
  282065/1100000: episode: 490, duration: 1.933s, episode steps: 285, steps per second: 147, episode reward: 206.191, mean reward: 0.723 [-9.469, 100.000], mean action: 0.849 [0.000, 3.000], mean observation: 0.202 [-0.887, 1.449], loss: 10.457047, mae: 47.494736, mean_q: 63.837444
  282368/1100000: episode: 491, duration: 2.094s, episode steps: 303, steps per second: 145, episode reward: 252.625, mean reward: 0.834 [-9.382, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.118 [-0.616, 1.408], loss: 6.367878, mae: 47.376587, mean_q: 63.683231
  282941/1100000: episode: 492, duration: 3.988s, episode steps: 573, steps per second: 144, episode reward: 240.967, mean reward: 0.421 [-19.880, 100.000], mean action: 0.686 [0.000, 3.000], mean observation: 0.142 [-0.867, 1.394], loss: 7.033733, mae: 47.863762, mean_q: 64.224159
  283054/1100000: episode: 493, duration: 0.750s, episode steps: 113, steps per second: 151, episode reward: 3.540, mean reward: 0.031 [-100.000, 18.791], mean action: 1.354 [0.000, 3.000], mean observation: 0.094 [-0.813, 1.419], loss: 9.752903, mae: 47.843651, mean_q: 63.919373
  283190/1100000: episode: 494, duration: 0.915s, episode steps: 136, steps per second: 149, episode reward: -6.845, mean reward: -0.050 [-100.000, 10.007], mean action: 1.662 [0.000, 3.000], mean observation: 0.057 [-0.725, 2.482], loss: 8.767223, mae: 47.516624, mean_q: 63.552361
  283541/1100000: episode: 495, duration: 2.431s, episode steps: 351, steps per second: 144, episode reward: 246.886, mean reward: 0.703 [-17.645, 100.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.131 [-0.844, 1.439], loss: 11.047553, mae: 47.668369, mean_q: 63.833111
  284016/1100000: episode: 496, duration: 3.338s, episode steps: 475, steps per second: 142, episode reward: 213.103, mean reward: 0.449 [-10.960, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: -0.020 [-0.824, 1.394], loss: 9.445314, mae: 47.242699, mean_q: 63.406925
  284623/1100000: episode: 497, duration: 4.203s, episode steps: 607, steps per second: 144, episode reward: 276.712, mean reward: 0.456 [-19.427, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.133 [-0.933, 1.506], loss: 7.055687, mae: 46.863514, mean_q: 62.859451
  285067/1100000: episode: 498, duration: 3.032s, episode steps: 444, steps per second: 146, episode reward: 263.125, mean reward: 0.593 [-17.367, 100.000], mean action: 0.748 [0.000, 3.000], mean observation: 0.158 [-0.891, 1.393], loss: 11.577500, mae: 46.690567, mean_q: 62.643131
  285471/1100000: episode: 499, duration: 2.719s, episode steps: 404, steps per second: 149, episode reward: 274.351, mean reward: 0.679 [-18.002, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.072 [-0.786, 1.633], loss: 9.084315, mae: 46.340424, mean_q: 62.270130
  285611/1100000: episode: 500, duration: 0.932s, episode steps: 140, steps per second: 150, episode reward: -186.673, mean reward: -1.333 [-100.000, 46.780], mean action: 1.729 [0.000, 3.000], mean observation: -0.021 [-1.934, 1.464], loss: 8.799891, mae: 46.098835, mean_q: 61.966167
  285832/1100000: episode: 501, duration: 1.481s, episode steps: 221, steps per second: 149, episode reward: 271.172, mean reward: 1.227 [-17.407, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.094 [-0.739, 1.441], loss: 6.968681, mae: 47.133766, mean_q: 63.218521
  286832/1100000: episode: 502, duration: 7.203s, episode steps: 1000, steps per second: 139, episode reward: 120.347, mean reward: 0.120 [-20.113, 22.304], mean action: 1.694 [0.000, 3.000], mean observation: 0.061 [-0.662, 1.402], loss: 9.404539, mae: 46.433975, mean_q: 62.292866
  287832/1100000: episode: 503, duration: 7.738s, episode steps: 1000, steps per second: 129, episode reward: 51.253, mean reward: 0.051 [-20.972, 17.011], mean action: 1.863 [0.000, 3.000], mean observation: 0.163 [-0.866, 1.518], loss: 8.287776, mae: 46.559967, mean_q: 62.571297
  288263/1100000: episode: 504, duration: 2.977s, episode steps: 431, steps per second: 145, episode reward: 215.852, mean reward: 0.501 [-10.696, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.080 [-1.545, 1.410], loss: 6.542994, mae: 46.739532, mean_q: 62.858559
  288860/1100000: episode: 505, duration: 4.189s, episode steps: 597, steps per second: 143, episode reward: 169.152, mean reward: 0.283 [-21.253, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.147 [-0.997, 1.424], loss: 9.875662, mae: 47.099312, mean_q: 63.275101
  288940/1100000: episode: 506, duration: 0.547s, episode steps: 80, steps per second: 146, episode reward: -43.393, mean reward: -0.542 [-100.000, 26.728], mean action: 1.988 [0.000, 3.000], mean observation: 0.184 [-3.620, 1.400], loss: 7.208447, mae: 47.362190, mean_q: 63.859741
  289264/1100000: episode: 507, duration: 2.289s, episode steps: 324, steps per second: 142, episode reward: 237.392, mean reward: 0.733 [-18.501, 100.000], mean action: 1.994 [0.000, 3.000], mean observation: 0.048 [-0.600, 1.478], loss: 7.635829, mae: 46.940315, mean_q: 62.882969
  289466/1100000: episode: 508, duration: 1.355s, episode steps: 202, steps per second: 149, episode reward: 264.749, mean reward: 1.311 [-10.834, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.161 [-1.014, 1.403], loss: 9.385347, mae: 47.340210, mean_q: 63.521351
  289880/1100000: episode: 509, duration: 2.902s, episode steps: 414, steps per second: 143, episode reward: 180.557, mean reward: 0.436 [-9.574, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: -0.083 [-0.798, 1.432], loss: 8.960649, mae: 47.571754, mean_q: 63.990429
  290137/1100000: episode: 510, duration: 1.732s, episode steps: 257, steps per second: 148, episode reward: 233.518, mean reward: 0.909 [-9.630, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.073 [-0.846, 1.409], loss: 8.892360, mae: 47.086563, mean_q: 63.163383
  290227/1100000: episode: 511, duration: 0.605s, episode steps: 90, steps per second: 149, episode reward: -141.426, mean reward: -1.571 [-100.000, 7.179], mean action: 2.278 [0.000, 3.000], mean observation: -0.096 [-1.478, 3.910], loss: 5.432348, mae: 47.305431, mean_q: 63.191807
  290823/1100000: episode: 512, duration: 4.136s, episode steps: 596, steps per second: 144, episode reward: 255.100, mean reward: 0.428 [-18.054, 100.000], mean action: 0.641 [0.000, 3.000], mean observation: 0.254 [-0.806, 1.414], loss: 8.187682, mae: 47.581211, mean_q: 63.888916
  290911/1100000: episode: 513, duration: 0.584s, episode steps: 88, steps per second: 151, episode reward: -73.208, mean reward: -0.832 [-100.000, 5.646], mean action: 2.102 [0.000, 3.000], mean observation: -0.002 [-0.873, 2.308], loss: 5.607296, mae: 47.119030, mean_q: 63.360722
  291234/1100000: episode: 514, duration: 2.227s, episode steps: 323, steps per second: 145, episode reward: 240.990, mean reward: 0.746 [-18.400, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.004 [-0.904, 1.396], loss: 5.805335, mae: 47.193398, mean_q: 63.252193
  291396/1100000: episode: 515, duration: 1.084s, episode steps: 162, steps per second: 149, episode reward: 79.674, mean reward: 0.492 [-100.000, 13.027], mean action: 1.778 [0.000, 3.000], mean observation: 0.053 [-0.856, 1.389], loss: 7.876143, mae: 46.826900, mean_q: 63.105938
  291565/1100000: episode: 516, duration: 1.118s, episode steps: 169, steps per second: 151, episode reward: -24.062, mean reward: -0.142 [-100.000, 17.105], mean action: 1.408 [0.000, 3.000], mean observation: -0.100 [-0.811, 1.520], loss: 8.890281, mae: 46.843624, mean_q: 62.802193
  291937/1100000: episode: 517, duration: 2.528s, episode steps: 372, steps per second: 147, episode reward: 230.919, mean reward: 0.621 [-11.150, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.133 [-0.890, 1.415], loss: 7.749344, mae: 47.502529, mean_q: 63.924385
  292197/1100000: episode: 518, duration: 1.769s, episode steps: 260, steps per second: 147, episode reward: 285.631, mean reward: 1.099 [-17.729, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.099 [-0.747, 1.401], loss: 5.781451, mae: 47.333569, mean_q: 63.705528
  292326/1100000: episode: 519, duration: 0.862s, episode steps: 129, steps per second: 150, episode reward: -100.740, mean reward: -0.781 [-100.000, 2.909], mean action: 1.822 [0.000, 3.000], mean observation: 0.014 [-1.002, 1.424], loss: 8.302545, mae: 48.514034, mean_q: 65.117676
  292508/1100000: episode: 520, duration: 1.219s, episode steps: 182, steps per second: 149, episode reward: 293.430, mean reward: 1.612 [-9.163, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.091 [-0.769, 1.394], loss: 11.037712, mae: 48.043716, mean_q: 64.589630
  292647/1100000: episode: 521, duration: 0.926s, episode steps: 139, steps per second: 150, episode reward: -84.814, mean reward: -0.610 [-100.000, 7.503], mean action: 1.475 [0.000, 3.000], mean observation: 0.057 [-0.923, 1.520], loss: 13.498224, mae: 47.860828, mean_q: 64.115784
  293560/1100000: episode: 522, duration: 6.570s, episode steps: 913, steps per second: 139, episode reward: 117.216, mean reward: 0.128 [-20.307, 100.000], mean action: 1.714 [0.000, 3.000], mean observation: 0.073 [-0.792, 1.480], loss: 8.377365, mae: 48.201344, mean_q: 64.620987
  293836/1100000: episode: 523, duration: 1.844s, episode steps: 276, steps per second: 150, episode reward: 237.886, mean reward: 0.862 [-17.337, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: -0.008 [-0.613, 1.429], loss: 9.541354, mae: 48.471275, mean_q: 64.941238
  294278/1100000: episode: 524, duration: 3.163s, episode steps: 442, steps per second: 140, episode reward: 265.566, mean reward: 0.601 [-10.118, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.111 [-1.226, 1.395], loss: 8.803092, mae: 48.380070, mean_q: 64.998787
  294976/1100000: episode: 525, duration: 4.917s, episode steps: 698, steps per second: 142, episode reward: 26.377, mean reward: 0.038 [-100.000, 12.303], mean action: 1.566 [0.000, 3.000], mean observation: 0.071 [-0.725, 1.390], loss: 6.499191, mae: 48.371025, mean_q: 64.932732
  295293/1100000: episode: 526, duration: 2.144s, episode steps: 317, steps per second: 148, episode reward: 260.006, mean reward: 0.820 [-10.081, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.086 [-0.602, 1.409], loss: 7.801525, mae: 48.126240, mean_q: 64.459297
  295724/1100000: episode: 527, duration: 3.017s, episode steps: 431, steps per second: 143, episode reward: 191.342, mean reward: 0.444 [-13.532, 100.000], mean action: 1.903 [0.000, 3.000], mean observation: 0.050 [-0.768, 1.407], loss: 6.692672, mae: 47.859936, mean_q: 64.271538
  296042/1100000: episode: 528, duration: 2.208s, episode steps: 318, steps per second: 144, episode reward: 220.501, mean reward: 0.693 [-17.400, 100.000], mean action: 2.264 [0.000, 3.000], mean observation: 0.029 [-0.689, 1.408], loss: 5.320058, mae: 48.559559, mean_q: 65.072914
  297042/1100000: episode: 529, duration: 7.653s, episode steps: 1000, steps per second: 131, episode reward: 85.140, mean reward: 0.085 [-20.586, 23.512], mean action: 1.524 [0.000, 3.000], mean observation: 0.059 [-0.793, 1.390], loss: 6.694437, mae: 48.684189, mean_q: 65.324257
  297877/1100000: episode: 530, duration: 5.930s, episode steps: 835, steps per second: 141, episode reward: 265.810, mean reward: 0.318 [-20.101, 100.000], mean action: 0.999 [0.000, 3.000], mean observation: 0.102 [-0.845, 1.566], loss: 7.399772, mae: 48.865288, mean_q: 65.513535
  298028/1100000: episode: 531, duration: 1.007s, episode steps: 151, steps per second: 150, episode reward: 16.185, mean reward: 0.107 [-100.000, 15.649], mean action: 1.616 [0.000, 3.000], mean observation: -0.073 [-0.785, 1.415], loss: 7.997396, mae: 48.348793, mean_q: 65.089218
  298295/1100000: episode: 532, duration: 1.806s, episode steps: 267, steps per second: 148, episode reward: 205.452, mean reward: 0.769 [-6.423, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.188 [-0.869, 1.403], loss: 3.794582, mae: 48.768097, mean_q: 65.339897
  298436/1100000: episode: 533, duration: 0.935s, episode steps: 141, steps per second: 151, episode reward: -36.793, mean reward: -0.261 [-100.000, 27.403], mean action: 1.376 [0.000, 3.000], mean observation: -0.074 [-0.758, 2.010], loss: 11.481804, mae: 48.542538, mean_q: 65.153358
  298573/1100000: episode: 534, duration: 0.908s, episode steps: 137, steps per second: 151, episode reward: 29.136, mean reward: 0.213 [-100.000, 13.662], mean action: 1.394 [0.000, 3.000], mean observation: -0.008 [-0.735, 1.783], loss: 4.181526, mae: 47.782600, mean_q: 64.102821
  299111/1100000: episode: 535, duration: 3.735s, episode steps: 538, steps per second: 144, episode reward: 197.579, mean reward: 0.367 [-18.520, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.048 [-1.186, 1.402], loss: 8.758386, mae: 48.294216, mean_q: 65.016701
  299777/1100000: episode: 536, duration: 4.722s, episode steps: 666, steps per second: 141, episode reward: 176.619, mean reward: 0.265 [-18.653, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.157 [-0.569, 1.394], loss: 8.902597, mae: 48.108521, mean_q: 64.463844
  300020/1100000: episode: 537, duration: 1.637s, episode steps: 243, steps per second: 148, episode reward: -38.507, mean reward: -0.158 [-100.000, 20.548], mean action: 1.852 [0.000, 3.000], mean observation: 0.007 [-0.769, 1.599], loss: 9.501904, mae: 47.789333, mean_q: 64.033379
  300272/1100000: episode: 538, duration: 1.681s, episode steps: 252, steps per second: 150, episode reward: 249.902, mean reward: 0.992 [-17.363, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.007 [-0.999, 1.439], loss: 6.186804, mae: 48.042316, mean_q: 64.386284
  300733/1100000: episode: 539, duration: 3.195s, episode steps: 461, steps per second: 144, episode reward: 276.840, mean reward: 0.601 [-11.647, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.085 [-0.748, 1.431], loss: 6.407986, mae: 47.921555, mean_q: 64.372139
  301307/1100000: episode: 540, duration: 4.058s, episode steps: 574, steps per second: 141, episode reward: 224.834, mean reward: 0.392 [-22.768, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.220 [-0.857, 1.389], loss: 8.951878, mae: 48.506546, mean_q: 65.132149
  301428/1100000: episode: 541, duration: 0.807s, episode steps: 121, steps per second: 150, episode reward: -21.299, mean reward: -0.176 [-100.000, 16.025], mean action: 1.512 [0.000, 3.000], mean observation: 0.021 [-0.915, 1.407], loss: 15.928788, mae: 48.089771, mean_q: 64.443222
  301535/1100000: episode: 542, duration: 0.719s, episode steps: 107, steps per second: 149, episode reward: -3.540, mean reward: -0.033 [-100.000, 22.269], mean action: 1.533 [0.000, 3.000], mean observation: 0.011 [-0.903, 1.414], loss: 5.722244, mae: 48.952038, mean_q: 65.736443
  301660/1100000: episode: 543, duration: 0.837s, episode steps: 125, steps per second: 149, episode reward: 27.164, mean reward: 0.217 [-100.000, 7.513], mean action: 1.712 [0.000, 3.000], mean observation: -0.062 [-0.892, 1.394], loss: 13.812881, mae: 48.461624, mean_q: 64.748863
  301886/1100000: episode: 544, duration: 1.525s, episode steps: 226, steps per second: 148, episode reward: -73.208, mean reward: -0.324 [-100.000, 13.770], mean action: 1.748 [0.000, 3.000], mean observation: -0.035 [-0.762, 1.396], loss: 7.710502, mae: 48.797729, mean_q: 65.568802
  302357/1100000: episode: 545, duration: 3.341s, episode steps: 471, steps per second: 141, episode reward: 231.669, mean reward: 0.492 [-17.867, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.042 [-1.317, 1.429], loss: 7.495015, mae: 47.978436, mean_q: 64.451859
  302492/1100000: episode: 546, duration: 0.908s, episode steps: 135, steps per second: 149, episode reward: -30.431, mean reward: -0.225 [-100.000, 12.569], mean action: 1.726 [0.000, 3.000], mean observation: -0.016 [-0.719, 1.390], loss: 4.458166, mae: 48.199848, mean_q: 64.618896
  302729/1100000: episode: 547, duration: 1.626s, episode steps: 237, steps per second: 146, episode reward: 5.583, mean reward: 0.024 [-100.000, 20.600], mean action: 1.920 [0.000, 3.000], mean observation: 0.178 [-1.165, 1.392], loss: 10.835179, mae: 47.756355, mean_q: 64.004662
  303304/1100000: episode: 548, duration: 3.979s, episode steps: 575, steps per second: 145, episode reward: 216.629, mean reward: 0.377 [-20.003, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.219 [-0.789, 1.405], loss: 10.191813, mae: 48.099865, mean_q: 64.683189
  303665/1100000: episode: 549, duration: 2.528s, episode steps: 361, steps per second: 143, episode reward: 197.272, mean reward: 0.546 [-9.744, 100.000], mean action: 1.590 [0.000, 3.000], mean observation: 0.121 [-0.926, 1.421], loss: 7.643141, mae: 47.773037, mean_q: 64.003174
  303871/1100000: episode: 550, duration: 1.394s, episode steps: 206, steps per second: 148, episode reward: -25.733, mean reward: -0.125 [-100.000, 9.753], mean action: 1.806 [0.000, 3.000], mean observation: 0.073 [-0.637, 1.654], loss: 11.852836, mae: 47.523537, mean_q: 63.872620
  304102/1100000: episode: 551, duration: 1.555s, episode steps: 231, steps per second: 149, episode reward: 230.667, mean reward: 0.999 [-11.274, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.023 [-0.724, 1.411], loss: 10.396121, mae: 47.287117, mean_q: 63.558578
  304248/1100000: episode: 552, duration: 0.969s, episode steps: 146, steps per second: 151, episode reward: 58.288, mean reward: 0.399 [-100.000, 17.824], mean action: 1.849 [0.000, 3.000], mean observation: 0.070 [-0.860, 1.906], loss: 9.615866, mae: 47.379475, mean_q: 63.764061
  304378/1100000: episode: 553, duration: 0.865s, episode steps: 130, steps per second: 150, episode reward: -235.927, mean reward: -1.815 [-100.000, 39.151], mean action: 1.538 [0.000, 3.000], mean observation: -0.182 [-2.482, 1.406], loss: 7.289099, mae: 47.521091, mean_q: 64.018616
  304649/1100000: episode: 554, duration: 1.867s, episode steps: 271, steps per second: 145, episode reward: 255.376, mean reward: 0.942 [-12.510, 100.000], mean action: 2.037 [0.000, 3.000], mean observation: 0.177 [-0.680, 1.412], loss: 9.971431, mae: 47.975933, mean_q: 64.400429
  305358/1100000: episode: 555, duration: 5.032s, episode steps: 709, steps per second: 141, episode reward: 180.878, mean reward: 0.255 [-24.248, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: -0.013 [-0.878, 1.395], loss: 8.872638, mae: 47.329834, mean_q: 63.491161
  306153/1100000: episode: 556, duration: 5.718s, episode steps: 795, steps per second: 139, episode reward: 257.945, mean reward: 0.324 [-19.244, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.127 [-0.769, 1.423], loss: 10.643641, mae: 47.453686, mean_q: 63.457664
  306271/1100000: episode: 557, duration: 0.835s, episode steps: 118, steps per second: 141, episode reward: 18.087, mean reward: 0.153 [-100.000, 56.994], mean action: 1.712 [0.000, 3.000], mean observation: 0.013 [-1.039, 1.413], loss: 5.668873, mae: 47.270184, mean_q: 63.348576
  306564/1100000: episode: 558, duration: 2.039s, episode steps: 293, steps per second: 144, episode reward: 232.600, mean reward: 0.794 [-9.654, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.057 [-1.009, 1.420], loss: 8.843940, mae: 47.248466, mean_q: 63.317146
  306947/1100000: episode: 559, duration: 2.692s, episode steps: 383, steps per second: 142, episode reward: 194.950, mean reward: 0.509 [-10.543, 100.000], mean action: 1.984 [0.000, 3.000], mean observation: 0.181 [-0.631, 1.425], loss: 9.947944, mae: 47.546967, mean_q: 63.682220
  307394/1100000: episode: 560, duration: 3.291s, episode steps: 447, steps per second: 136, episode reward: 178.327, mean reward: 0.399 [-10.559, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.054 [-1.035, 1.385], loss: 7.895437, mae: 47.760406, mean_q: 64.047844
  308056/1100000: episode: 561, duration: 4.608s, episode steps: 662, steps per second: 144, episode reward: 198.942, mean reward: 0.301 [-20.535, 100.000], mean action: 0.770 [0.000, 3.000], mean observation: 0.227 [-0.958, 1.412], loss: 8.379734, mae: 47.747597, mean_q: 63.955803
  308309/1100000: episode: 562, duration: 1.726s, episode steps: 253, steps per second: 147, episode reward: -27.753, mean reward: -0.110 [-100.000, 18.681], mean action: 1.617 [0.000, 3.000], mean observation: -0.032 [-0.766, 1.946], loss: 7.120732, mae: 47.058121, mean_q: 63.255081
  308884/1100000: episode: 563, duration: 4.105s, episode steps: 575, steps per second: 140, episode reward: 34.979, mean reward: 0.061 [-100.000, 21.274], mean action: 1.306 [0.000, 3.000], mean observation: 0.087 [-1.161, 1.423], loss: 10.070425, mae: 47.883045, mean_q: 64.161613
  309304/1100000: episode: 564, duration: 2.944s, episode steps: 420, steps per second: 143, episode reward: 170.144, mean reward: 0.405 [-13.303, 100.000], mean action: 1.721 [0.000, 3.000], mean observation: -0.003 [-0.716, 1.412], loss: 9.306149, mae: 47.920116, mean_q: 64.448151
  309550/1100000: episode: 565, duration: 1.660s, episode steps: 246, steps per second: 148, episode reward: 235.647, mean reward: 0.958 [-19.724, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.161 [-0.730, 1.414], loss: 7.717868, mae: 48.079220, mean_q: 64.591660
  310055/1100000: episode: 566, duration: 3.713s, episode steps: 505, steps per second: 136, episode reward: 182.793, mean reward: 0.362 [-18.452, 100.000], mean action: 2.275 [0.000, 3.000], mean observation: 0.082 [-0.878, 1.395], loss: 8.892245, mae: 47.909496, mean_q: 64.494888
  310779/1100000: episode: 567, duration: 5.345s, episode steps: 724, steps per second: 135, episode reward: 184.305, mean reward: 0.255 [-18.446, 100.000], mean action: 1.640 [0.000, 3.000], mean observation: 0.098 [-1.054, 1.400], loss: 7.749344, mae: 48.399033, mean_q: 64.935890
  311412/1100000: episode: 568, duration: 4.812s, episode steps: 633, steps per second: 132, episode reward: 153.621, mean reward: 0.243 [-23.317, 100.000], mean action: 1.749 [0.000, 3.000], mean observation: 0.048 [-0.805, 1.393], loss: 11.321190, mae: 48.097500, mean_q: 64.571312
  311693/1100000: episode: 569, duration: 1.941s, episode steps: 281, steps per second: 145, episode reward: 271.806, mean reward: 0.967 [-19.173, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.126 [-0.817, 1.395], loss: 10.030032, mae: 48.096149, mean_q: 64.650642
  312105/1100000: episode: 570, duration: 2.878s, episode steps: 412, steps per second: 143, episode reward: 279.827, mean reward: 0.679 [-17.048, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.040 [-1.453, 1.423], loss: 7.483666, mae: 48.205471, mean_q: 64.643974
  312357/1100000: episode: 571, duration: 1.713s, episode steps: 252, steps per second: 147, episode reward: -54.960, mean reward: -0.218 [-100.000, 16.529], mean action: 1.460 [0.000, 3.000], mean observation: 0.000 [-1.084, 1.816], loss: 11.745744, mae: 48.421795, mean_q: 64.966614
  312890/1100000: episode: 572, duration: 3.790s, episode steps: 533, steps per second: 141, episode reward: 213.335, mean reward: 0.400 [-19.183, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.197 [-1.239, 1.403], loss: 11.078451, mae: 48.204567, mean_q: 64.551559
  313304/1100000: episode: 573, duration: 2.975s, episode steps: 414, steps per second: 139, episode reward: 186.441, mean reward: 0.450 [-20.189, 100.000], mean action: 2.159 [0.000, 3.000], mean observation: 0.143 [-0.812, 1.408], loss: 8.617194, mae: 48.318657, mean_q: 64.802620
  313552/1100000: episode: 574, duration: 1.678s, episode steps: 248, steps per second: 148, episode reward: 226.699, mean reward: 0.914 [-8.653, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.119 [-0.827, 1.423], loss: 10.134854, mae: 48.966557, mean_q: 65.655693
  313977/1100000: episode: 575, duration: 3.007s, episode steps: 425, steps per second: 141, episode reward: 223.972, mean reward: 0.527 [-9.117, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: -0.084 [-0.708, 1.388], loss: 9.509768, mae: 48.293568, mean_q: 64.882256
  314424/1100000: episode: 576, duration: 3.068s, episode steps: 447, steps per second: 146, episode reward: 223.642, mean reward: 0.500 [-19.504, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: -0.015 [-0.679, 1.421], loss: 9.884295, mae: 49.108372, mean_q: 65.731331
  314773/1100000: episode: 577, duration: 2.509s, episode steps: 349, steps per second: 139, episode reward: -149.694, mean reward: -0.429 [-100.000, 33.179], mean action: 1.799 [0.000, 3.000], mean observation: -0.030 [-0.624, 1.624], loss: 8.964771, mae: 48.558716, mean_q: 64.969940
  314897/1100000: episode: 578, duration: 0.895s, episode steps: 124, steps per second: 139, episode reward: -45.333, mean reward: -0.366 [-100.000, 17.556], mean action: 1.976 [0.000, 3.000], mean observation: 0.049 [-1.398, 1.520], loss: 7.064328, mae: 49.627785, mean_q: 66.420761
  315030/1100000: episode: 579, duration: 0.894s, episode steps: 133, steps per second: 149, episode reward: -8.293, mean reward: -0.062 [-100.000, 22.332], mean action: 1.797 [0.000, 3.000], mean observation: -0.018 [-2.053, 1.396], loss: 13.626745, mae: 48.564423, mean_q: 65.089622
  315174/1100000: episode: 580, duration: 0.967s, episode steps: 144, steps per second: 149, episode reward: 24.910, mean reward: 0.173 [-100.000, 12.313], mean action: 1.674 [0.000, 3.000], mean observation: 0.106 [-0.710, 1.753], loss: 6.589666, mae: 48.492809, mean_q: 65.351990
  315518/1100000: episode: 581, duration: 2.447s, episode steps: 344, steps per second: 141, episode reward: 29.666, mean reward: 0.086 [-100.000, 17.367], mean action: 1.895 [0.000, 3.000], mean observation: 0.008 [-0.798, 1.396], loss: 6.897862, mae: 48.845097, mean_q: 65.270569
  315934/1100000: episode: 582, duration: 2.865s, episode steps: 416, steps per second: 145, episode reward: 229.934, mean reward: 0.553 [-23.120, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.064 [-1.428, 1.399], loss: 10.662210, mae: 48.874222, mean_q: 65.467194
  316084/1100000: episode: 583, duration: 1.011s, episode steps: 150, steps per second: 148, episode reward: 31.066, mean reward: 0.207 [-100.000, 17.208], mean action: 1.500 [0.000, 3.000], mean observation: 0.153 [-0.838, 1.495], loss: 10.160419, mae: 48.586002, mean_q: 64.982147
  316489/1100000: episode: 584, duration: 2.789s, episode steps: 405, steps per second: 145, episode reward: 265.537, mean reward: 0.656 [-19.183, 100.000], mean action: 1.644 [0.000, 3.000], mean observation: 0.058 [-0.844, 1.389], loss: 10.562860, mae: 48.951817, mean_q: 65.785873
  317133/1100000: episode: 585, duration: 4.833s, episode steps: 644, steps per second: 133, episode reward: -236.452, mean reward: -0.367 [-100.000, 28.592], mean action: 1.834 [0.000, 3.000], mean observation: -0.021 [-1.450, 1.467], loss: 8.946490, mae: 49.193146, mean_q: 65.832672
  317421/1100000: episode: 586, duration: 1.959s, episode steps: 288, steps per second: 147, episode reward: 284.772, mean reward: 0.989 [-7.718, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.111 [-0.681, 1.407], loss: 13.531626, mae: 48.776409, mean_q: 65.304878
  317806/1100000: episode: 587, duration: 2.771s, episode steps: 385, steps per second: 139, episode reward: 217.409, mean reward: 0.565 [-3.227, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.061 [-0.652, 1.404], loss: 14.150189, mae: 49.019756, mean_q: 65.735359
  317960/1100000: episode: 588, duration: 1.027s, episode steps: 154, steps per second: 150, episode reward: 32.680, mean reward: 0.212 [-100.000, 9.601], mean action: 1.708 [0.000, 3.000], mean observation: 0.178 [-0.591, 1.488], loss: 8.869642, mae: 49.128506, mean_q: 66.082619
  318387/1100000: episode: 589, duration: 3.000s, episode steps: 427, steps per second: 142, episode reward: 279.341, mean reward: 0.654 [-7.006, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.078 [-1.120, 1.396], loss: 12.248193, mae: 49.228073, mean_q: 65.965584
  318523/1100000: episode: 590, duration: 0.913s, episode steps: 136, steps per second: 149, episode reward: -1.328, mean reward: -0.010 [-100.000, 12.538], mean action: 1.757 [0.000, 3.000], mean observation: 0.019 [-0.713, 1.419], loss: 9.729085, mae: 49.095119, mean_q: 65.627144
  318762/1100000: episode: 591, duration: 1.620s, episode steps: 239, steps per second: 148, episode reward: 233.706, mean reward: 0.978 [-7.799, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.191 [-0.940, 1.395], loss: 15.950953, mae: 48.504894, mean_q: 64.870468
  319157/1100000: episode: 592, duration: 2.734s, episode steps: 395, steps per second: 144, episode reward: 277.424, mean reward: 0.702 [-17.633, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: 0.063 [-0.600, 1.407], loss: 10.492949, mae: 49.212234, mean_q: 65.875648
  319267/1100000: episode: 593, duration: 0.733s, episode steps: 110, steps per second: 150, episode reward: 10.813, mean reward: 0.098 [-100.000, 13.176], mean action: 1.636 [0.000, 3.000], mean observation: 0.006 [-0.676, 1.771], loss: 9.281150, mae: 48.868793, mean_q: 65.279152
  319504/1100000: episode: 594, duration: 1.621s, episode steps: 237, steps per second: 146, episode reward: 249.415, mean reward: 1.052 [-6.170, 100.000], mean action: 1.751 [0.000, 3.000], mean observation: 0.172 [-0.852, 1.465], loss: 6.217290, mae: 48.747185, mean_q: 65.592056
  320504/1100000: episode: 595, duration: 7.271s, episode steps: 1000, steps per second: 138, episode reward: 103.590, mean reward: 0.104 [-18.834, 13.168], mean action: 2.244 [0.000, 3.000], mean observation: 0.240 [-0.621, 1.462], loss: 11.490458, mae: 48.993069, mean_q: 65.655220
  320713/1100000: episode: 596, duration: 1.402s, episode steps: 209, steps per second: 149, episode reward: -207.334, mean reward: -0.992 [-100.000, 18.730], mean action: 1.598 [0.000, 3.000], mean observation: -0.063 [-0.857, 2.018], loss: 12.040503, mae: 49.533840, mean_q: 66.234573
  321155/1100000: episode: 597, duration: 3.069s, episode steps: 442, steps per second: 144, episode reward: 205.893, mean reward: 0.466 [-7.906, 100.000], mean action: 1.853 [0.000, 3.000], mean observation: 0.052 [-0.985, 1.471], loss: 13.174120, mae: 49.707657, mean_q: 66.534508
  321453/1100000: episode: 598, duration: 2.060s, episode steps: 298, steps per second: 145, episode reward: 35.984, mean reward: 0.121 [-100.000, 14.955], mean action: 1.779 [0.000, 3.000], mean observation: -0.018 [-0.669, 1.409], loss: 12.116771, mae: 49.371498, mean_q: 66.227753
  321704/1100000: episode: 599, duration: 1.720s, episode steps: 251, steps per second: 146, episode reward: 283.628, mean reward: 1.130 [-6.381, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.106 [-0.801, 1.515], loss: 8.528476, mae: 49.566986, mean_q: 66.383263
  322635/1100000: episode: 600, duration: 7.032s, episode steps: 931, steps per second: 132, episode reward: 206.770, mean reward: 0.222 [-18.042, 100.000], mean action: 1.562 [0.000, 3.000], mean observation: 0.229 [-0.602, 1.405], loss: 10.268064, mae: 49.905739, mean_q: 66.859734
  322951/1100000: episode: 601, duration: 2.165s, episode steps: 316, steps per second: 146, episode reward: 16.413, mean reward: 0.052 [-100.000, 18.971], mean action: 1.617 [0.000, 3.000], mean observation: 0.030 [-1.038, 1.449], loss: 10.936377, mae: 50.055298, mean_q: 67.106300
  323348/1100000: episode: 602, duration: 2.828s, episode steps: 397, steps per second: 140, episode reward: 202.324, mean reward: 0.510 [-17.982, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.159 [-0.768, 1.409], loss: 10.409663, mae: 50.601967, mean_q: 67.628098
  323656/1100000: episode: 603, duration: 2.081s, episode steps: 308, steps per second: 148, episode reward: 301.281, mean reward: 0.978 [-21.679, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.113 [-0.815, 1.536], loss: 10.226457, mae: 50.573090, mean_q: 67.630051
  323974/1100000: episode: 604, duration: 2.217s, episode steps: 318, steps per second: 143, episode reward: 267.225, mean reward: 0.840 [-10.637, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.056 [-0.785, 1.392], loss: 8.435922, mae: 50.921429, mean_q: 68.204208
  324639/1100000: episode: 605, duration: 5.041s, episode steps: 665, steps per second: 132, episode reward: 179.552, mean reward: 0.270 [-14.030, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.059 [-0.747, 1.493], loss: 9.251765, mae: 50.689205, mean_q: 67.860176
  325030/1100000: episode: 606, duration: 2.792s, episode steps: 391, steps per second: 140, episode reward: 218.381, mean reward: 0.559 [-21.603, 100.000], mean action: 1.483 [0.000, 3.000], mean observation: -0.039 [-0.621, 1.399], loss: 7.942850, mae: 50.847576, mean_q: 68.001671
  325453/1100000: episode: 607, duration: 2.954s, episode steps: 423, steps per second: 143, episode reward: 235.986, mean reward: 0.558 [-11.923, 100.000], mean action: 0.778 [0.000, 3.000], mean observation: -0.013 [-0.653, 1.401], loss: 12.170892, mae: 50.715462, mean_q: 67.942284
  325970/1100000: episode: 608, duration: 3.507s, episode steps: 517, steps per second: 147, episode reward: 271.045, mean reward: 0.524 [-19.105, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: 0.037 [-0.647, 1.494], loss: 9.251265, mae: 50.474335, mean_q: 67.619896
  326270/1100000: episode: 609, duration: 2.092s, episode steps: 300, steps per second: 143, episode reward: 261.493, mean reward: 0.872 [-13.128, 100.000], mean action: 1.770 [0.000, 3.000], mean observation: -0.053 [-0.853, 1.425], loss: 8.128685, mae: 50.572617, mean_q: 67.877975
  326705/1100000: episode: 610, duration: 3.005s, episode steps: 435, steps per second: 145, episode reward: 230.840, mean reward: 0.531 [-18.623, 100.000], mean action: 0.887 [0.000, 3.000], mean observation: 0.174 [-0.619, 1.421], loss: 10.203378, mae: 50.543026, mean_q: 67.739883
  327192/1100000: episode: 611, duration: 3.416s, episode steps: 487, steps per second: 143, episode reward: -85.992, mean reward: -0.177 [-100.000, 21.777], mean action: 1.616 [0.000, 3.000], mean observation: -0.019 [-1.339, 1.449], loss: 10.062111, mae: 50.891388, mean_q: 68.170189
  327555/1100000: episode: 612, duration: 2.566s, episode steps: 363, steps per second: 141, episode reward: 259.145, mean reward: 0.714 [-9.924, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.077 [-0.685, 1.388], loss: 9.547257, mae: 51.504089, mean_q: 68.909500
  327942/1100000: episode: 613, duration: 2.633s, episode steps: 387, steps per second: 147, episode reward: 238.209, mean reward: 0.616 [-20.004, 100.000], mean action: 1.770 [0.000, 3.000], mean observation: 0.039 [-0.840, 1.456], loss: 10.707730, mae: 51.268578, mean_q: 68.651428
  328354/1100000: episode: 614, duration: 2.887s, episode steps: 412, steps per second: 143, episode reward: 278.626, mean reward: 0.676 [-17.182, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.045 [-0.728, 1.389], loss: 9.723207, mae: 51.365612, mean_q: 68.862236
  328510/1100000: episode: 615, duration: 1.042s, episode steps: 156, steps per second: 150, episode reward: 37.657, mean reward: 0.241 [-100.000, 10.368], mean action: 1.865 [0.000, 3.000], mean observation: 0.122 [-0.839, 1.474], loss: 11.535913, mae: 51.598400, mean_q: 69.347397
  328775/1100000: episode: 616, duration: 1.789s, episode steps: 265, steps per second: 148, episode reward: 244.972, mean reward: 0.924 [-13.962, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.182 [-1.083, 1.520], loss: 11.851984, mae: 51.453377, mean_q: 68.740318
  329460/1100000: episode: 617, duration: 5.085s, episode steps: 685, steps per second: 135, episode reward: 212.860, mean reward: 0.311 [-19.163, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.088 [-0.764, 1.406], loss: 7.538168, mae: 51.460766, mean_q: 68.895439
  329786/1100000: episode: 618, duration: 2.211s, episode steps: 326, steps per second: 147, episode reward: -17.637, mean reward: -0.054 [-100.000, 11.869], mean action: 1.531 [0.000, 3.000], mean observation: 0.100 [-1.862, 1.501], loss: 10.323144, mae: 51.142933, mean_q: 68.549683
  330204/1100000: episode: 619, duration: 2.930s, episode steps: 418, steps per second: 143, episode reward: 220.902, mean reward: 0.528 [-13.952, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.002 [-0.888, 1.455], loss: 7.088650, mae: 51.724094, mean_q: 69.128448
  330472/1100000: episode: 620, duration: 1.837s, episode steps: 268, steps per second: 146, episode reward: 27.060, mean reward: 0.101 [-100.000, 16.856], mean action: 1.690 [0.000, 3.000], mean observation: -0.022 [-0.701, 1.486], loss: 10.025725, mae: 51.588753, mean_q: 68.989502
  330730/1100000: episode: 621, duration: 1.726s, episode steps: 258, steps per second: 149, episode reward: 281.406, mean reward: 1.091 [-11.426, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.096 [-0.791, 1.522], loss: 10.549036, mae: 51.728344, mean_q: 69.401665
  331585/1100000: episode: 622, duration: 6.684s, episode steps: 855, steps per second: 128, episode reward: -192.490, mean reward: -0.225 [-100.000, 22.982], mean action: 1.394 [0.000, 3.000], mean observation: -0.016 [-1.002, 1.422], loss: 12.031603, mae: 51.657681, mean_q: 69.161766
  332470/1100000: episode: 623, duration: 6.709s, episode steps: 885, steps per second: 132, episode reward: 200.805, mean reward: 0.227 [-19.242, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: 0.033 [-0.652, 1.503], loss: 9.622978, mae: 51.555420, mean_q: 68.948662
  333083/1100000: episode: 624, duration: 4.495s, episode steps: 613, steps per second: 136, episode reward: 206.313, mean reward: 0.337 [-12.630, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.048 [-0.993, 1.465], loss: 8.259549, mae: 51.844734, mean_q: 69.319687
  333568/1100000: episode: 625, duration: 3.429s, episode steps: 485, steps per second: 141, episode reward: 269.618, mean reward: 0.556 [-22.079, 100.000], mean action: 1.581 [0.000, 3.000], mean observation: 0.016 [-1.109, 1.410], loss: 10.418706, mae: 51.820095, mean_q: 69.315735
  333880/1100000: episode: 626, duration: 2.220s, episode steps: 312, steps per second: 141, episode reward: 228.465, mean reward: 0.732 [-11.571, 100.000], mean action: 2.119 [0.000, 3.000], mean observation: 0.190 [-0.921, 1.395], loss: 8.771729, mae: 51.888031, mean_q: 69.532486
  334176/1100000: episode: 627, duration: 2.018s, episode steps: 296, steps per second: 147, episode reward: 306.737, mean reward: 1.036 [-5.346, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.124 [-0.672, 1.431], loss: 11.392350, mae: 51.830994, mean_q: 69.521431
  334701/1100000: episode: 628, duration: 3.646s, episode steps: 525, steps per second: 144, episode reward: 224.092, mean reward: 0.427 [-5.831, 100.000], mean action: 1.516 [0.000, 3.000], mean observation: 0.014 [-1.099, 1.450], loss: 8.851233, mae: 52.110966, mean_q: 69.829300
  335043/1100000: episode: 629, duration: 2.385s, episode steps: 342, steps per second: 143, episode reward: -250.160, mean reward: -0.731 [-100.000, 33.753], mean action: 1.646 [0.000, 3.000], mean observation: 0.104 [-2.531, 1.438], loss: 11.622644, mae: 52.196163, mean_q: 69.771454
  335330/1100000: episode: 630, duration: 1.990s, episode steps: 287, steps per second: 144, episode reward: 248.298, mean reward: 0.865 [-19.390, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.096 [-0.785, 1.404], loss: 9.362328, mae: 52.469635, mean_q: 69.905609
  335616/1100000: episode: 631, duration: 1.997s, episode steps: 286, steps per second: 143, episode reward: 261.224, mean reward: 0.913 [-18.955, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.055 [-0.619, 1.398], loss: 9.216749, mae: 52.460575, mean_q: 70.247047
  336158/1100000: episode: 632, duration: 3.879s, episode steps: 542, steps per second: 140, episode reward: 219.374, mean reward: 0.405 [-19.045, 100.000], mean action: 1.685 [0.000, 3.000], mean observation: 0.137 [-0.731, 1.411], loss: 10.215028, mae: 52.706284, mean_q: 70.348969
  336257/1100000: episode: 633, duration: 0.651s, episode steps: 99, steps per second: 152, episode reward: -139.982, mean reward: -1.414 [-100.000, 8.445], mean action: 0.929 [0.000, 3.000], mean observation: 0.028 [-3.634, 1.490], loss: 9.428692, mae: 53.480789, mean_q: 71.583183
  336584/1100000: episode: 634, duration: 2.235s, episode steps: 327, steps per second: 146, episode reward: 253.070, mean reward: 0.774 [-17.489, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.100 [-0.722, 1.413], loss: 6.786560, mae: 53.383709, mean_q: 71.394188
  336880/1100000: episode: 635, duration: 2.050s, episode steps: 296, steps per second: 144, episode reward: 202.526, mean reward: 0.684 [-18.235, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.079 [-1.738, 1.439], loss: 6.279657, mae: 53.050964, mean_q: 71.018898
  337266/1100000: episode: 636, duration: 2.739s, episode steps: 386, steps per second: 141, episode reward: 219.063, mean reward: 0.568 [-10.329, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.199 [-0.853, 1.398], loss: 8.391094, mae: 53.335018, mean_q: 71.322556
  337716/1100000: episode: 637, duration: 3.162s, episode steps: 450, steps per second: 142, episode reward: 268.794, mean reward: 0.597 [-19.486, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.094 [-0.731, 1.442], loss: 10.611685, mae: 53.583805, mean_q: 71.439049
  338091/1100000: episode: 638, duration: 2.605s, episode steps: 375, steps per second: 144, episode reward: 205.258, mean reward: 0.547 [-19.636, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.174 [-0.772, 1.406], loss: 9.758658, mae: 53.217606, mean_q: 71.114426
  338307/1100000: episode: 639, duration: 1.459s, episode steps: 216, steps per second: 148, episode reward: -136.895, mean reward: -0.634 [-100.000, 25.486], mean action: 1.616 [0.000, 3.000], mean observation: -0.059 [-1.688, 1.459], loss: 10.644158, mae: 53.773418, mean_q: 72.062172
  338887/1100000: episode: 640, duration: 4.154s, episode steps: 580, steps per second: 140, episode reward: -77.670, mean reward: -0.134 [-100.000, 17.697], mean action: 1.584 [0.000, 3.000], mean observation: 0.116 [-0.948, 1.411], loss: 7.196765, mae: 53.290749, mean_q: 71.204231
  339166/1100000: episode: 641, duration: 1.888s, episode steps: 279, steps per second: 148, episode reward: -31.693, mean reward: -0.114 [-100.000, 14.673], mean action: 1.606 [0.000, 3.000], mean observation: -0.074 [-0.971, 1.495], loss: 8.172595, mae: 53.832298, mean_q: 71.983734
  339396/1100000: episode: 642, duration: 1.579s, episode steps: 230, steps per second: 146, episode reward: 252.360, mean reward: 1.097 [-8.936, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: -0.013 [-0.716, 1.400], loss: 13.051458, mae: 53.267384, mean_q: 70.778145
  339821/1100000: episode: 643, duration: 2.969s, episode steps: 425, steps per second: 143, episode reward: -177.174, mean reward: -0.417 [-100.000, 15.401], mean action: 1.885 [0.000, 3.000], mean observation: -0.001 [-0.983, 1.415], loss: 11.466967, mae: 53.723419, mean_q: 71.511757
  340544/1100000: episode: 644, duration: 5.139s, episode steps: 723, steps per second: 141, episode reward: 155.979, mean reward: 0.216 [-20.212, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.198 [-0.802, 1.399], loss: 10.898978, mae: 53.708324, mean_q: 71.744568
  340650/1100000: episode: 645, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: 7.070, mean reward: 0.067 [-100.000, 12.194], mean action: 1.623 [0.000, 3.000], mean observation: -0.039 [-1.190, 1.385], loss: 4.529377, mae: 53.160553, mean_q: 71.225616
  340972/1100000: episode: 646, duration: 2.256s, episode steps: 322, steps per second: 143, episode reward: 283.566, mean reward: 0.881 [-19.960, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.104 [-0.741, 1.396], loss: 10.315155, mae: 54.017555, mean_q: 71.929291
  341055/1100000: episode: 647, duration: 0.555s, episode steps: 83, steps per second: 150, episode reward: -66.561, mean reward: -0.802 [-100.000, 22.289], mean action: 1.494 [0.000, 3.000], mean observation: 0.135 [-4.005, 1.410], loss: 4.863958, mae: 53.879143, mean_q: 71.341064
  341659/1100000: episode: 648, duration: 4.318s, episode steps: 604, steps per second: 140, episode reward: 221.620, mean reward: 0.367 [-18.100, 100.000], mean action: 2.285 [0.000, 3.000], mean observation: 0.043 [-0.752, 1.407], loss: 10.104780, mae: 53.861649, mean_q: 71.952705
  341987/1100000: episode: 649, duration: 2.275s, episode steps: 328, steps per second: 144, episode reward: 241.208, mean reward: 0.735 [-12.427, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: -0.074 [-0.759, 1.396], loss: 10.008819, mae: 53.398838, mean_q: 71.194862
  342267/1100000: episode: 650, duration: 1.906s, episode steps: 280, steps per second: 147, episode reward: 212.585, mean reward: 0.759 [-9.643, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.059 [-0.805, 1.404], loss: 9.480517, mae: 53.878124, mean_q: 71.849892
  342792/1100000: episode: 651, duration: 3.799s, episode steps: 525, steps per second: 138, episode reward: 247.404, mean reward: 0.471 [-18.721, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.009 [-0.711, 1.441], loss: 11.827914, mae: 53.859516, mean_q: 71.757126
  342878/1100000: episode: 652, duration: 0.566s, episode steps: 86, steps per second: 152, episode reward: -113.459, mean reward: -1.319 [-100.000, 41.532], mean action: 1.140 [0.000, 3.000], mean observation: 0.102 [-2.182, 1.420], loss: 11.774598, mae: 53.951832, mean_q: 72.407623
  343446/1100000: episode: 653, duration: 4.187s, episode steps: 568, steps per second: 136, episode reward: 260.118, mean reward: 0.458 [-19.930, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.037 [-0.786, 1.390], loss: 8.722273, mae: 53.642689, mean_q: 71.460907
  343688/1100000: episode: 654, duration: 1.627s, episode steps: 242, steps per second: 149, episode reward: -186.828, mean reward: -0.772 [-100.000, 19.665], mean action: 1.669 [0.000, 3.000], mean observation: 0.033 [-1.642, 1.399], loss: 7.244221, mae: 53.034019, mean_q: 70.514091
  343996/1100000: episode: 655, duration: 2.124s, episode steps: 308, steps per second: 145, episode reward: 208.125, mean reward: 0.676 [-9.908, 100.000], mean action: 1.964 [0.000, 3.000], mean observation: 0.047 [-0.726, 1.477], loss: 9.426150, mae: 53.722599, mean_q: 71.635681
  344524/1100000: episode: 656, duration: 3.683s, episode steps: 528, steps per second: 143, episode reward: 239.948, mean reward: 0.454 [-15.504, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: -0.032 [-0.779, 1.410], loss: 8.504849, mae: 53.267281, mean_q: 70.842018
  344853/1100000: episode: 657, duration: 2.273s, episode steps: 329, steps per second: 145, episode reward: -161.325, mean reward: -0.490 [-100.000, 19.260], mean action: 1.778 [0.000, 3.000], mean observation: -0.017 [-1.923, 1.508], loss: 6.486552, mae: 53.358818, mean_q: 71.027390
  344931/1100000: episode: 658, duration: 0.522s, episode steps: 78, steps per second: 149, episode reward: -72.492, mean reward: -0.929 [-100.000, 8.209], mean action: 1.397 [0.000, 3.000], mean observation: 0.152 [-3.001, 1.396], loss: 6.506524, mae: 53.195850, mean_q: 70.392197
  345373/1100000: episode: 659, duration: 3.094s, episode steps: 442, steps per second: 143, episode reward: 268.491, mean reward: 0.607 [-19.477, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.064 [-0.714, 1.501], loss: 11.565551, mae: 53.998096, mean_q: 71.838791
  345450/1100000: episode: 660, duration: 0.518s, episode steps: 77, steps per second: 149, episode reward: -122.896, mean reward: -1.596 [-100.000, 21.974], mean action: 1.286 [0.000, 3.000], mean observation: 0.200 [-2.979, 1.406], loss: 14.390427, mae: 53.293900, mean_q: 70.890266
  345938/1100000: episode: 661, duration: 3.480s, episode steps: 488, steps per second: 140, episode reward: 171.312, mean reward: 0.351 [-15.568, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.104 [-0.871, 1.420], loss: 12.497952, mae: 53.936310, mean_q: 71.909515
  346418/1100000: episode: 662, duration: 3.347s, episode steps: 480, steps per second: 143, episode reward: 236.923, mean reward: 0.494 [-10.899, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.170 [-0.627, 1.423], loss: 8.580540, mae: 53.568008, mean_q: 71.031883
  346725/1100000: episode: 663, duration: 2.121s, episode steps: 307, steps per second: 145, episode reward: -16.128, mean reward: -0.053 [-100.000, 16.505], mean action: 1.814 [0.000, 3.000], mean observation: 0.104 [-1.611, 1.418], loss: 12.392430, mae: 53.372391, mean_q: 71.189163
  347215/1100000: episode: 664, duration: 3.482s, episode steps: 490, steps per second: 141, episode reward: 186.648, mean reward: 0.381 [-19.769, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.158 [-0.678, 1.420], loss: 8.560662, mae: 53.298901, mean_q: 71.027069
  348215/1100000: episode: 665, duration: 7.351s, episode steps: 1000, steps per second: 136, episode reward: 49.040, mean reward: 0.049 [-19.128, 22.492], mean action: 1.163 [0.000, 3.000], mean observation: 0.265 [-0.774, 1.412], loss: 9.571584, mae: 52.780655, mean_q: 70.268517
  348531/1100000: episode: 666, duration: 2.212s, episode steps: 316, steps per second: 143, episode reward: 247.874, mean reward: 0.784 [-8.015, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.027 [-0.752, 1.398], loss: 9.220660, mae: 52.410614, mean_q: 69.948692
  348898/1100000: episode: 667, duration: 2.607s, episode steps: 367, steps per second: 141, episode reward: 251.455, mean reward: 0.685 [-14.780, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: -0.007 [-1.033, 1.403], loss: 9.377124, mae: 52.783432, mean_q: 70.351891
  349898/1100000: episode: 668, duration: 8.011s, episode steps: 1000, steps per second: 125, episode reward: 51.428, mean reward: 0.051 [-24.924, 22.333], mean action: 1.434 [0.000, 3.000], mean observation: 0.037 [-1.025, 1.392], loss: 9.498860, mae: 52.216908, mean_q: 69.542427
  350628/1100000: episode: 669, duration: 5.373s, episode steps: 730, steps per second: 136, episode reward: 171.677, mean reward: 0.235 [-23.032, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.019 [-0.812, 1.401], loss: 9.795268, mae: 52.273331, mean_q: 69.547165
  350827/1100000: episode: 670, duration: 1.330s, episode steps: 199, steps per second: 150, episode reward: 26.756, mean reward: 0.134 [-100.000, 13.616], mean action: 1.653 [0.000, 3.000], mean observation: 0.115 [-0.758, 1.448], loss: 8.460546, mae: 52.736099, mean_q: 70.457367
  351094/1100000: episode: 671, duration: 1.827s, episode steps: 267, steps per second: 146, episode reward: 260.411, mean reward: 0.975 [-9.528, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.015 [-0.749, 1.390], loss: 8.494755, mae: 52.662674, mean_q: 70.059235
  351750/1100000: episode: 672, duration: 4.805s, episode steps: 656, steps per second: 137, episode reward: -221.612, mean reward: -0.338 [-100.000, 17.890], mean action: 1.934 [0.000, 3.000], mean observation: 0.119 [-0.887, 2.122], loss: 10.268641, mae: 52.357101, mean_q: 69.801201
  352363/1100000: episode: 673, duration: 4.381s, episode steps: 613, steps per second: 140, episode reward: 185.549, mean reward: 0.303 [-20.294, 100.000], mean action: 1.641 [0.000, 3.000], mean observation: 0.244 [-0.783, 1.412], loss: 8.628573, mae: 52.523022, mean_q: 70.322189
  352722/1100000: episode: 674, duration: 2.497s, episode steps: 359, steps per second: 144, episode reward: 164.026, mean reward: 0.457 [-17.842, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.075 [-0.707, 1.390], loss: 8.696736, mae: 52.228256, mean_q: 70.008621
  353722/1100000: episode: 675, duration: 7.115s, episode steps: 1000, steps per second: 141, episode reward: 117.346, mean reward: 0.117 [-19.224, 44.899], mean action: 1.693 [0.000, 3.000], mean observation: 0.183 [-1.293, 1.536], loss: 9.742271, mae: 52.354202, mean_q: 70.004379
  354722/1100000: episode: 676, duration: 7.638s, episode steps: 1000, steps per second: 131, episode reward: 83.435, mean reward: 0.083 [-21.182, 21.954], mean action: 1.696 [0.000, 3.000], mean observation: 0.256 [-0.816, 1.389], loss: 10.358656, mae: 52.320030, mean_q: 69.797523
  355257/1100000: episode: 677, duration: 3.913s, episode steps: 535, steps per second: 137, episode reward: 190.736, mean reward: 0.357 [-19.843, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: -0.017 [-0.726, 1.402], loss: 8.612590, mae: 52.048977, mean_q: 69.368401
  355793/1100000: episode: 678, duration: 3.835s, episode steps: 536, steps per second: 140, episode reward: 247.810, mean reward: 0.462 [-19.365, 100.000], mean action: 2.050 [0.000, 3.000], mean observation: 0.089 [-0.837, 1.462], loss: 8.485309, mae: 52.356449, mean_q: 69.995903
  356129/1100000: episode: 679, duration: 2.334s, episode steps: 336, steps per second: 144, episode reward: 288.099, mean reward: 0.857 [-17.716, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.125 [-0.755, 1.394], loss: 7.485677, mae: 51.920166, mean_q: 69.480858
  356568/1100000: episode: 680, duration: 3.123s, episode steps: 439, steps per second: 141, episode reward: 215.313, mean reward: 0.490 [-13.583, 100.000], mean action: 1.941 [0.000, 3.000], mean observation: 0.011 [-0.814, 1.393], loss: 9.343717, mae: 52.441601, mean_q: 70.302483
  356833/1100000: episode: 681, duration: 1.870s, episode steps: 265, steps per second: 142, episode reward: 253.514, mean reward: 0.957 [-9.443, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.136 [-0.809, 1.390], loss: 6.444230, mae: 51.919678, mean_q: 69.373497
  356940/1100000: episode: 682, duration: 0.724s, episode steps: 107, steps per second: 148, episode reward: -109.365, mean reward: -1.022 [-100.000, 48.736], mean action: 1.636 [0.000, 3.000], mean observation: -0.181 [-1.547, 1.398], loss: 8.929018, mae: 51.930470, mean_q: 69.561943
  357283/1100000: episode: 683, duration: 2.345s, episode steps: 343, steps per second: 146, episode reward: 194.689, mean reward: 0.568 [-12.291, 100.000], mean action: 1.994 [0.000, 3.000], mean observation: 0.022 [-0.745, 1.492], loss: 8.707552, mae: 51.795578, mean_q: 69.267723
  357688/1100000: episode: 684, duration: 2.759s, episode steps: 405, steps per second: 147, episode reward: 280.502, mean reward: 0.693 [-20.691, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.177 [-0.938, 1.490], loss: 10.778803, mae: 52.276985, mean_q: 69.793579
  357943/1100000: episode: 685, duration: 1.744s, episode steps: 255, steps per second: 146, episode reward: 227.579, mean reward: 0.892 [-3.747, 100.000], mean action: 1.651 [0.000, 3.000], mean observation: -0.001 [-0.780, 1.416], loss: 5.616069, mae: 51.635426, mean_q: 69.175949
  358086/1100000: episode: 686, duration: 0.947s, episode steps: 143, steps per second: 151, episode reward: -208.505, mean reward: -1.458 [-100.000, 6.928], mean action: 1.322 [0.000, 3.000], mean observation: -0.185 [-3.185, 1.485], loss: 13.463132, mae: 51.928364, mean_q: 69.227303
  358688/1100000: episode: 687, duration: 4.372s, episode steps: 602, steps per second: 138, episode reward: 257.472, mean reward: 0.428 [-17.781, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.018 [-0.868, 1.387], loss: 10.093613, mae: 52.130432, mean_q: 69.504707
  359607/1100000: episode: 688, duration: 7.246s, episode steps: 919, steps per second: 127, episode reward: 175.838, mean reward: 0.191 [-19.360, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.209 [-0.831, 1.407], loss: 10.819201, mae: 52.395077, mean_q: 69.515884
  359746/1100000: episode: 689, duration: 0.920s, episode steps: 139, steps per second: 151, episode reward: -204.313, mean reward: -1.470 [-100.000, 4.892], mean action: 1.633 [0.000, 3.000], mean observation: -0.112 [-1.043, 1.399], loss: 5.628423, mae: 51.952663, mean_q: 69.008743
  359952/1100000: episode: 690, duration: 1.385s, episode steps: 206, steps per second: 149, episode reward: 263.006, mean reward: 1.277 [-9.628, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.034 [-0.838, 1.386], loss: 12.624775, mae: 52.015579, mean_q: 69.199623
  360799/1100000: episode: 691, duration: 5.787s, episode steps: 847, steps per second: 146, episode reward: 211.809, mean reward: 0.250 [-24.383, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.094 [-0.713, 1.406], loss: 9.263963, mae: 52.044975, mean_q: 69.205429
  361063/1100000: episode: 692, duration: 1.792s, episode steps: 264, steps per second: 147, episode reward: 211.461, mean reward: 0.801 [-9.292, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.007 [-0.736, 1.410], loss: 7.885400, mae: 51.812698, mean_q: 69.121910
  361391/1100000: episode: 693, duration: 2.238s, episode steps: 328, steps per second: 147, episode reward: 230.167, mean reward: 0.702 [-21.160, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.142 [-0.851, 1.439], loss: 9.538848, mae: 51.935772, mean_q: 69.221382
  361656/1100000: episode: 694, duration: 1.804s, episode steps: 265, steps per second: 147, episode reward: -78.135, mean reward: -0.295 [-100.000, 16.836], mean action: 1.755 [0.000, 3.000], mean observation: 0.097 [-0.787, 1.404], loss: 7.673780, mae: 51.427067, mean_q: 68.728928
  361746/1100000: episode: 695, duration: 0.601s, episode steps: 90, steps per second: 150, episode reward: 21.177, mean reward: 0.235 [-100.000, 10.441], mean action: 1.978 [0.000, 3.000], mean observation: 0.025 [-1.110, 1.438], loss: 4.783016, mae: 51.880615, mean_q: 69.224670
  361893/1100000: episode: 696, duration: 0.982s, episode steps: 147, steps per second: 150, episode reward: -130.821, mean reward: -0.890 [-100.000, 11.760], mean action: 1.565 [0.000, 3.000], mean observation: -0.071 [-1.003, 1.415], loss: 5.358989, mae: 52.001652, mean_q: 69.412285
  362176/1100000: episode: 697, duration: 1.933s, episode steps: 283, steps per second: 146, episode reward: 267.749, mean reward: 0.946 [-7.862, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.168 [-1.067, 1.395], loss: 9.964906, mae: 52.009773, mean_q: 69.447289
  362546/1100000: episode: 698, duration: 2.676s, episode steps: 370, steps per second: 138, episode reward: 279.805, mean reward: 0.756 [-9.547, 100.000], mean action: 0.908 [0.000, 3.000], mean observation: 0.087 [-0.635, 1.399], loss: 8.468855, mae: 52.140697, mean_q: 69.739960
  363014/1100000: episode: 699, duration: 3.285s, episode steps: 468, steps per second: 142, episode reward: 235.058, mean reward: 0.502 [-19.328, 100.000], mean action: 2.331 [0.000, 3.000], mean observation: 0.056 [-0.828, 1.398], loss: 10.528347, mae: 52.384064, mean_q: 69.698769
  363275/1100000: episode: 700, duration: 1.761s, episode steps: 261, steps per second: 148, episode reward: 250.739, mean reward: 0.961 [-19.603, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.167 [-0.761, 1.410], loss: 10.174112, mae: 51.919796, mean_q: 68.922546
  364032/1100000: episode: 701, duration: 5.778s, episode steps: 757, steps per second: 131, episode reward: 115.557, mean reward: 0.153 [-24.801, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.119 [-1.152, 1.412], loss: 7.916566, mae: 52.052650, mean_q: 69.221916
  364199/1100000: episode: 702, duration: 1.105s, episode steps: 167, steps per second: 151, episode reward: -61.283, mean reward: -0.367 [-100.000, 23.680], mean action: 1.311 [0.000, 3.000], mean observation: 0.004 [-0.882, 3.383], loss: 8.361163, mae: 52.182178, mean_q: 69.351059
  364709/1100000: episode: 703, duration: 3.619s, episode steps: 510, steps per second: 141, episode reward: 226.190, mean reward: 0.444 [-17.092, 100.000], mean action: 1.767 [0.000, 3.000], mean observation: 0.047 [-0.655, 1.420], loss: 8.148977, mae: 52.272556, mean_q: 69.399712
  364827/1100000: episode: 704, duration: 0.783s, episode steps: 118, steps per second: 151, episode reward: -74.184, mean reward: -0.629 [-100.000, 15.456], mean action: 1.737 [0.000, 3.000], mean observation: -0.142 [-0.792, 1.416], loss: 15.211980, mae: 52.376682, mean_q: 69.572426
  365439/1100000: episode: 705, duration: 4.617s, episode steps: 612, steps per second: 133, episode reward: 205.896, mean reward: 0.336 [-22.233, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.070 [-0.711, 1.396], loss: 8.597869, mae: 52.290051, mean_q: 69.350494
  366053/1100000: episode: 706, duration: 4.203s, episode steps: 614, steps per second: 146, episode reward: 213.801, mean reward: 0.348 [-20.042, 100.000], mean action: 0.686 [0.000, 3.000], mean observation: 0.077 [-0.890, 1.400], loss: 8.935863, mae: 52.339527, mean_q: 69.525909
  366310/1100000: episode: 707, duration: 1.744s, episode steps: 257, steps per second: 147, episode reward: 190.533, mean reward: 0.741 [-17.905, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.103 [-0.869, 1.408], loss: 5.721016, mae: 52.172066, mean_q: 68.787224
  366588/1100000: episode: 708, duration: 1.890s, episode steps: 278, steps per second: 147, episode reward: 250.037, mean reward: 0.899 [-8.006, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.051 [-0.911, 1.450], loss: 7.759336, mae: 52.324619, mean_q: 69.501984
  366830/1100000: episode: 709, duration: 1.646s, episode steps: 242, steps per second: 147, episode reward: 41.004, mean reward: 0.169 [-100.000, 15.798], mean action: 1.950 [0.000, 3.000], mean observation: 0.140 [-0.607, 1.443], loss: 11.959816, mae: 52.415409, mean_q: 69.687286
  366984/1100000: episode: 710, duration: 1.028s, episode steps: 154, steps per second: 150, episode reward: 2.483, mean reward: 0.016 [-100.000, 16.256], mean action: 1.740 [0.000, 3.000], mean observation: -0.070 [-1.276, 1.399], loss: 7.500473, mae: 51.616520, mean_q: 68.257126
  367529/1100000: episode: 711, duration: 3.887s, episode steps: 545, steps per second: 140, episode reward: 250.925, mean reward: 0.460 [-18.958, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: -0.033 [-0.792, 1.391], loss: 10.854657, mae: 52.010288, mean_q: 68.977402
  368424/1100000: episode: 712, duration: 7.463s, episode steps: 895, steps per second: 120, episode reward: 186.765, mean reward: 0.209 [-17.727, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: -0.009 [-0.966, 1.386], loss: 8.234463, mae: 51.904537, mean_q: 68.832642
  368907/1100000: episode: 713, duration: 3.543s, episode steps: 483, steps per second: 136, episode reward: 237.056, mean reward: 0.491 [-19.990, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.045 [-1.015, 1.385], loss: 9.332662, mae: 52.018215, mean_q: 69.077446
  369150/1100000: episode: 714, duration: 1.639s, episode steps: 243, steps per second: 148, episode reward: 206.024, mean reward: 0.848 [-15.074, 100.000], mean action: 2.119 [0.000, 3.000], mean observation: 0.039 [-1.101, 1.412], loss: 7.371682, mae: 51.431053, mean_q: 68.357750
  369707/1100000: episode: 715, duration: 4.001s, episode steps: 557, steps per second: 139, episode reward: 221.999, mean reward: 0.399 [-17.591, 100.000], mean action: 1.953 [0.000, 3.000], mean observation: 0.203 [-1.052, 1.510], loss: 7.195734, mae: 51.708103, mean_q: 69.071167
  370009/1100000: episode: 716, duration: 2.074s, episode steps: 302, steps per second: 146, episode reward: 189.287, mean reward: 0.627 [-18.765, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.103 [-1.040, 1.410], loss: 6.994093, mae: 51.737000, mean_q: 68.876961
  370322/1100000: episode: 717, duration: 2.165s, episode steps: 313, steps per second: 145, episode reward: 237.991, mean reward: 0.760 [-19.294, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.070 [-0.781, 1.401], loss: 7.955882, mae: 51.619884, mean_q: 68.837448
  370912/1100000: episode: 718, duration: 4.238s, episode steps: 590, steps per second: 139, episode reward: -176.550, mean reward: -0.299 [-100.000, 13.285], mean action: 2.051 [0.000, 3.000], mean observation: -0.094 [-1.003, 1.554], loss: 8.267997, mae: 51.734619, mean_q: 68.804291
  371297/1100000: episode: 719, duration: 2.679s, episode steps: 385, steps per second: 144, episode reward: 299.752, mean reward: 0.779 [-18.542, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.102 [-0.625, 1.466], loss: 9.820525, mae: 51.741497, mean_q: 69.120224
  371897/1100000: episode: 720, duration: 4.372s, episode steps: 600, steps per second: 137, episode reward: 215.916, mean reward: 0.360 [-11.151, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.030 [-0.722, 1.482], loss: 8.891395, mae: 51.668556, mean_q: 68.570419
  372201/1100000: episode: 721, duration: 2.087s, episode steps: 304, steps per second: 146, episode reward: 254.945, mean reward: 0.839 [-13.288, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.049 [-0.850, 1.421], loss: 7.402884, mae: 51.665470, mean_q: 69.044128
  372516/1100000: episode: 722, duration: 2.185s, episode steps: 315, steps per second: 144, episode reward: 258.183, mean reward: 0.820 [-12.608, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.068 [-0.562, 1.420], loss: 8.753348, mae: 51.323902, mean_q: 68.497223
  372940/1100000: episode: 723, duration: 2.992s, episode steps: 424, steps per second: 142, episode reward: 231.894, mean reward: 0.547 [-18.190, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.039 [-0.603, 1.412], loss: 6.708779, mae: 51.672443, mean_q: 68.802589
  373940/1100000: episode: 724, duration: 7.830s, episode steps: 1000, steps per second: 128, episode reward: 147.707, mean reward: 0.148 [-21.733, 20.921], mean action: 1.945 [0.000, 3.000], mean observation: 0.065 [-0.769, 1.518], loss: 8.808629, mae: 51.706287, mean_q: 68.708939
  374201/1100000: episode: 725, duration: 1.771s, episode steps: 261, steps per second: 147, episode reward: 257.031, mean reward: 0.985 [-10.131, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.137 [-0.772, 1.393], loss: 7.205030, mae: 51.683407, mean_q: 68.904602
  374732/1100000: episode: 726, duration: 3.758s, episode steps: 531, steps per second: 141, episode reward: 189.762, mean reward: 0.357 [-19.545, 100.000], mean action: 1.874 [0.000, 3.000], mean observation: 0.143 [-0.596, 1.511], loss: 12.717015, mae: 52.177914, mean_q: 69.291794
  374838/1100000: episode: 727, duration: 0.707s, episode steps: 106, steps per second: 150, episode reward: -18.190, mean reward: -0.172 [-100.000, 11.051], mean action: 1.972 [0.000, 3.000], mean observation: -0.012 [-1.023, 2.411], loss: 8.651646, mae: 52.194031, mean_q: 69.956459
  375167/1100000: episode: 728, duration: 2.259s, episode steps: 329, steps per second: 146, episode reward: 226.059, mean reward: 0.687 [-11.610, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.058 [-0.936, 1.428], loss: 7.322495, mae: 52.200085, mean_q: 69.535759
  375617/1100000: episode: 729, duration: 3.128s, episode steps: 450, steps per second: 144, episode reward: 219.007, mean reward: 0.487 [-17.369, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.014 [-0.617, 1.401], loss: 6.699211, mae: 52.121967, mean_q: 69.564781
  376079/1100000: episode: 730, duration: 3.185s, episode steps: 462, steps per second: 145, episode reward: 180.111, mean reward: 0.390 [-18.001, 100.000], mean action: 2.320 [0.000, 3.000], mean observation: 0.091 [-0.830, 1.408], loss: 9.095119, mae: 52.491215, mean_q: 70.035141
  376330/1100000: episode: 731, duration: 1.714s, episode steps: 251, steps per second: 146, episode reward: 236.813, mean reward: 0.943 [-17.411, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: -0.026 [-0.881, 1.402], loss: 7.061668, mae: 52.657619, mean_q: 70.345024
  376782/1100000: episode: 732, duration: 3.162s, episode steps: 452, steps per second: 143, episode reward: 225.872, mean reward: 0.500 [-19.472, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.125 [-0.794, 1.456], loss: 8.460443, mae: 52.906998, mean_q: 70.549446
  377109/1100000: episode: 733, duration: 2.248s, episode steps: 327, steps per second: 145, episode reward: 275.471, mean reward: 0.842 [-21.730, 100.000], mean action: 0.804 [0.000, 3.000], mean observation: 0.190 [-1.105, 1.552], loss: 7.989611, mae: 52.874199, mean_q: 70.259247
  377382/1100000: episode: 734, duration: 1.855s, episode steps: 273, steps per second: 147, episode reward: 221.161, mean reward: 0.810 [-11.671, 100.000], mean action: 1.505 [0.000, 3.000], mean observation: -0.018 [-0.938, 1.402], loss: 5.172454, mae: 52.974525, mean_q: 70.542366
  377692/1100000: episode: 735, duration: 2.146s, episode steps: 310, steps per second: 144, episode reward: 228.322, mean reward: 0.737 [-14.782, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.003 [-0.766, 1.411], loss: 6.890301, mae: 53.100479, mean_q: 70.608742
  378075/1100000: episode: 736, duration: 2.677s, episode steps: 383, steps per second: 143, episode reward: 250.340, mean reward: 0.654 [-11.607, 100.000], mean action: 1.744 [0.000, 3.000], mean observation: 0.075 [-1.313, 1.403], loss: 6.535360, mae: 53.266609, mean_q: 70.957817
  379075/1100000: episode: 737, duration: 7.607s, episode steps: 1000, steps per second: 131, episode reward: 73.299, mean reward: 0.073 [-24.849, 25.048], mean action: 2.027 [0.000, 3.000], mean observation: 0.077 [-1.347, 1.416], loss: 8.736293, mae: 53.043415, mean_q: 70.660797
  379185/1100000: episode: 738, duration: 0.738s, episode steps: 110, steps per second: 149, episode reward: 11.019, mean reward: 0.100 [-100.000, 11.638], mean action: 1.900 [0.000, 3.000], mean observation: 0.111 [-1.386, 1.393], loss: 5.113201, mae: 53.286144, mean_q: 70.811981
  379502/1100000: episode: 739, duration: 2.161s, episode steps: 317, steps per second: 147, episode reward: 300.971, mean reward: 0.949 [-8.646, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.110 [-0.679, 1.510], loss: 8.065212, mae: 53.152493, mean_q: 70.604027
  379662/1100000: episode: 740, duration: 1.067s, episode steps: 160, steps per second: 150, episode reward: 67.008, mean reward: 0.419 [-100.000, 17.366], mean action: 1.581 [0.000, 3.000], mean observation: 0.163 [-0.634, 1.450], loss: 9.982598, mae: 53.118690, mean_q: 70.198265
  379792/1100000: episode: 741, duration: 0.865s, episode steps: 130, steps per second: 150, episode reward: -8.897, mean reward: -0.068 [-100.000, 9.179], mean action: 1.508 [0.000, 3.000], mean observation: -0.022 [-1.067, 1.406], loss: 6.229021, mae: 53.088982, mean_q: 70.413895
  380198/1100000: episode: 742, duration: 2.773s, episode steps: 406, steps per second: 146, episode reward: 294.168, mean reward: 0.725 [-17.405, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.140 [-0.854, 1.451], loss: 4.728592, mae: 52.644146, mean_q: 70.213577
  380479/1100000: episode: 743, duration: 1.904s, episode steps: 281, steps per second: 148, episode reward: 247.609, mean reward: 0.881 [-10.990, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.134 [-0.853, 1.486], loss: 6.765896, mae: 53.009632, mean_q: 70.680031
  380681/1100000: episode: 744, duration: 1.355s, episode steps: 202, steps per second: 149, episode reward: 278.029, mean reward: 1.376 [-12.008, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.032 [-0.789, 1.396], loss: 9.378320, mae: 53.153938, mean_q: 70.635620
  380935/1100000: episode: 745, duration: 1.745s, episode steps: 254, steps per second: 146, episode reward: 255.243, mean reward: 1.005 [-9.722, 100.000], mean action: 1.827 [0.000, 3.000], mean observation: 0.063 [-0.763, 1.402], loss: 7.159021, mae: 53.047489, mean_q: 70.643486
  381318/1100000: episode: 746, duration: 2.705s, episode steps: 383, steps per second: 142, episode reward: 220.594, mean reward: 0.576 [-17.460, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.106 [-0.955, 1.409], loss: 5.991809, mae: 53.405144, mean_q: 71.394554
  381446/1100000: episode: 747, duration: 0.864s, episode steps: 128, steps per second: 148, episode reward: 89.449, mean reward: 0.699 [-100.000, 20.037], mean action: 1.906 [0.000, 3.000], mean observation: 0.027 [-0.763, 1.664], loss: 13.082835, mae: 52.794678, mean_q: 70.151566
  381842/1100000: episode: 748, duration: 2.761s, episode steps: 396, steps per second: 143, episode reward: 168.275, mean reward: 0.425 [-13.146, 100.000], mean action: 1.922 [0.000, 3.000], mean observation: -0.095 [-0.749, 1.524], loss: 7.324211, mae: 53.767017, mean_q: 71.837128
  382088/1100000: episode: 749, duration: 1.757s, episode steps: 246, steps per second: 140, episode reward: 298.151, mean reward: 1.212 [-4.085, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.095 [-0.730, 1.491], loss: 9.256667, mae: 53.761837, mean_q: 71.770737
  382796/1100000: episode: 750, duration: 5.232s, episode steps: 708, steps per second: 135, episode reward: 189.272, mean reward: 0.267 [-18.795, 100.000], mean action: 0.806 [0.000, 3.000], mean observation: 0.118 [-0.709, 1.413], loss: 7.308583, mae: 53.504005, mean_q: 71.628685
  382965/1100000: episode: 751, duration: 1.137s, episode steps: 169, steps per second: 149, episode reward: -160.073, mean reward: -0.947 [-100.000, 40.284], mean action: 1.580 [0.000, 3.000], mean observation: 0.005 [-2.076, 1.448], loss: 7.436387, mae: 53.858845, mean_q: 72.118645
  383064/1100000: episode: 752, duration: 0.662s, episode steps: 99, steps per second: 150, episode reward: -231.385, mean reward: -2.337 [-100.000, 11.396], mean action: 1.212 [0.000, 3.000], mean observation: 0.155 [-1.530, 4.226], loss: 10.633510, mae: 53.235165, mean_q: 71.120964
  383376/1100000: episode: 753, duration: 2.111s, episode steps: 312, steps per second: 148, episode reward: 66.251, mean reward: 0.212 [-100.000, 58.809], mean action: 1.532 [0.000, 3.000], mean observation: 0.214 [-1.184, 1.388], loss: 7.891179, mae: 53.897636, mean_q: 72.085785
  383763/1100000: episode: 754, duration: 2.616s, episode steps: 387, steps per second: 148, episode reward: 246.679, mean reward: 0.637 [-19.163, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: -0.004 [-0.780, 1.514], loss: 8.051090, mae: 53.931854, mean_q: 71.752991
  384370/1100000: episode: 755, duration: 4.596s, episode steps: 607, steps per second: 132, episode reward: 196.874, mean reward: 0.324 [-24.175, 100.000], mean action: 2.035 [0.000, 3.000], mean observation: 0.164 [-0.894, 1.391], loss: 8.243592, mae: 53.699951, mean_q: 71.775948
  385370/1100000: episode: 756, duration: 7.680s, episode steps: 1000, steps per second: 130, episode reward: 101.766, mean reward: 0.102 [-20.427, 22.513], mean action: 1.303 [0.000, 3.000], mean observation: 0.009 [-0.784, 1.417], loss: 8.519743, mae: 53.307552, mean_q: 71.036255
  385742/1100000: episode: 757, duration: 2.639s, episode steps: 372, steps per second: 141, episode reward: 252.790, mean reward: 0.680 [-17.386, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.075 [-0.667, 1.458], loss: 9.056539, mae: 53.378239, mean_q: 70.842300
  385981/1100000: episode: 758, duration: 1.621s, episode steps: 239, steps per second: 147, episode reward: 256.484, mean reward: 1.073 [-7.377, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.177 [-0.690, 1.458], loss: 4.702148, mae: 52.825207, mean_q: 70.462585
  386302/1100000: episode: 759, duration: 2.215s, episode steps: 321, steps per second: 145, episode reward: 267.201, mean reward: 0.832 [-18.735, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.095 [-0.727, 1.396], loss: 7.252693, mae: 53.285599, mean_q: 71.285866
  386625/1100000: episode: 760, duration: 2.240s, episode steps: 323, steps per second: 144, episode reward: -154.699, mean reward: -0.479 [-100.000, 11.630], mean action: 1.678 [0.000, 3.000], mean observation: -0.110 [-1.681, 1.395], loss: 10.309861, mae: 53.236816, mean_q: 71.112076
  386894/1100000: episode: 761, duration: 1.864s, episode steps: 269, steps per second: 144, episode reward: 269.529, mean reward: 1.002 [-10.014, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.046 [-0.680, 1.397], loss: 7.499997, mae: 52.841290, mean_q: 70.483185
  387248/1100000: episode: 762, duration: 2.463s, episode steps: 354, steps per second: 144, episode reward: 211.045, mean reward: 0.596 [-14.649, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: -0.013 [-0.773, 1.407], loss: 5.944371, mae: 53.045132, mean_q: 70.826210
  387485/1100000: episode: 763, duration: 1.635s, episode steps: 237, steps per second: 145, episode reward: 223.146, mean reward: 0.942 [-19.901, 100.000], mean action: 1.658 [0.000, 3.000], mean observation: -0.021 [-0.861, 1.386], loss: 11.611236, mae: 53.515045, mean_q: 71.339935
  387928/1100000: episode: 764, duration: 3.139s, episode steps: 443, steps per second: 141, episode reward: 220.308, mean reward: 0.497 [-17.794, 100.000], mean action: 1.989 [0.000, 3.000], mean observation: 0.042 [-0.834, 1.402], loss: 7.713199, mae: 53.235641, mean_q: 71.093620
  388749/1100000: episode: 765, duration: 5.772s, episode steps: 821, steps per second: 142, episode reward: 244.381, mean reward: 0.298 [-19.236, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.160 [-0.765, 1.414], loss: 6.262301, mae: 52.893917, mean_q: 70.549690
  389025/1100000: episode: 766, duration: 1.938s, episode steps: 276, steps per second: 142, episode reward: 272.316, mean reward: 0.987 [-4.613, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.098 [-1.215, 1.409], loss: 7.732767, mae: 52.699631, mean_q: 69.924194
  389513/1100000: episode: 767, duration: 3.416s, episode steps: 488, steps per second: 143, episode reward: 221.400, mean reward: 0.454 [-18.226, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: -0.056 [-0.826, 1.414], loss: 8.413280, mae: 52.441471, mean_q: 70.007347
  389654/1100000: episode: 768, duration: 0.955s, episode steps: 141, steps per second: 148, episode reward: 9.203, mean reward: 0.065 [-100.000, 12.479], mean action: 1.936 [0.000, 3.000], mean observation: -0.038 [-0.682, 1.394], loss: 9.067303, mae: 53.309013, mean_q: 70.904457
  390006/1100000: episode: 769, duration: 2.487s, episode steps: 352, steps per second: 142, episode reward: 216.211, mean reward: 0.614 [-23.841, 100.000], mean action: 2.074 [0.000, 3.000], mean observation: 0.252 [-0.858, 1.462], loss: 7.726576, mae: 52.879688, mean_q: 70.340469
  390400/1100000: episode: 770, duration: 2.797s, episode steps: 394, steps per second: 141, episode reward: -181.189, mean reward: -0.460 [-100.000, 24.469], mean action: 1.695 [0.000, 3.000], mean observation: 0.086 [-0.737, 1.404], loss: 9.715446, mae: 52.865383, mean_q: 70.326790
  391400/1100000: episode: 771, duration: 7.566s, episode steps: 1000, steps per second: 132, episode reward: 130.203, mean reward: 0.130 [-18.729, 22.752], mean action: 1.011 [0.000, 3.000], mean observation: 0.263 [-0.918, 1.391], loss: 7.557388, mae: 52.852718, mean_q: 70.344017
  391701/1100000: episode: 772, duration: 2.052s, episode steps: 301, steps per second: 147, episode reward: 276.037, mean reward: 0.917 [-8.296, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.100 [-0.730, 1.424], loss: 6.586401, mae: 52.492329, mean_q: 70.178490
  391861/1100000: episode: 773, duration: 1.073s, episode steps: 160, steps per second: 149, episode reward: -559.801, mean reward: -3.499 [-100.000, 34.017], mean action: 1.712 [0.000, 3.000], mean observation: 0.257 [-0.950, 4.656], loss: 7.988585, mae: 52.006824, mean_q: 69.066483
  392165/1100000: episode: 774, duration: 2.053s, episode steps: 304, steps per second: 148, episode reward: 288.408, mean reward: 0.949 [-14.042, 100.000], mean action: 1.832 [0.000, 3.000], mean observation: 0.026 [-0.795, 1.478], loss: 8.171493, mae: 52.643139, mean_q: 69.845978
  392373/1100000: episode: 775, duration: 1.396s, episode steps: 208, steps per second: 149, episode reward: 250.777, mean reward: 1.206 [-2.725, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.166 [-0.887, 1.390], loss: 11.201214, mae: 52.348648, mean_q: 69.458908
  392612/1100000: episode: 776, duration: 1.608s, episode steps: 239, steps per second: 149, episode reward: 293.816, mean reward: 1.229 [-9.483, 100.000], mean action: 1.515 [0.000, 3.000], mean observation: 0.085 [-0.772, 1.427], loss: 9.291260, mae: 52.643364, mean_q: 69.924782
  393196/1100000: episode: 777, duration: 4.402s, episode steps: 584, steps per second: 133, episode reward: 227.299, mean reward: 0.389 [-18.027, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.070 [-0.728, 1.519], loss: 6.443867, mae: 52.282104, mean_q: 69.634560
  393745/1100000: episode: 778, duration: 3.915s, episode steps: 549, steps per second: 140, episode reward: 216.154, mean reward: 0.394 [-19.270, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.056 [-0.635, 1.411], loss: 9.618196, mae: 52.384644, mean_q: 69.424919
  394065/1100000: episode: 779, duration: 2.240s, episode steps: 320, steps per second: 143, episode reward: 289.592, mean reward: 0.905 [-3.055, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.067 [-0.748, 1.391], loss: 12.605772, mae: 52.553539, mean_q: 69.715012
  394534/1100000: episode: 780, duration: 3.312s, episode steps: 469, steps per second: 142, episode reward: 272.045, mean reward: 0.580 [-19.773, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.012 [-0.780, 1.391], loss: 9.362973, mae: 52.435726, mean_q: 69.822838
  395167/1100000: episode: 781, duration: 4.678s, episode steps: 633, steps per second: 135, episode reward: 120.988, mean reward: 0.191 [-10.620, 100.000], mean action: 1.795 [0.000, 3.000], mean observation: 0.109 [-0.711, 1.425], loss: 8.536521, mae: 52.470657, mean_q: 69.822556
  395671/1100000: episode: 782, duration: 3.595s, episode steps: 504, steps per second: 140, episode reward: 273.452, mean reward: 0.543 [-18.180, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.130 [-0.571, 1.410], loss: 7.030147, mae: 51.608902, mean_q: 68.675858
  396037/1100000: episode: 783, duration: 2.559s, episode steps: 366, steps per second: 143, episode reward: 227.216, mean reward: 0.621 [-18.018, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.029 [-0.709, 1.495], loss: 10.547527, mae: 51.832798, mean_q: 69.024437
  396322/1100000: episode: 784, duration: 1.972s, episode steps: 285, steps per second: 145, episode reward: 293.269, mean reward: 1.029 [-19.580, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.104 [-0.561, 1.411], loss: 6.886779, mae: 51.727825, mean_q: 69.211472
  396553/1100000: episode: 785, duration: 1.571s, episode steps: 231, steps per second: 147, episode reward: 284.531, mean reward: 1.232 [-11.361, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.071 [-0.732, 1.390], loss: 6.626662, mae: 52.163273, mean_q: 68.949463
  396906/1100000: episode: 786, duration: 2.455s, episode steps: 353, steps per second: 144, episode reward: 238.219, mean reward: 0.675 [-18.651, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: -0.056 [-0.767, 1.391], loss: 8.640318, mae: 52.119179, mean_q: 69.423126
  397197/1100000: episode: 787, duration: 2.004s, episode steps: 291, steps per second: 145, episode reward: 192.636, mean reward: 0.662 [-12.240, 100.000], mean action: 1.921 [0.000, 3.000], mean observation: 0.032 [-0.712, 1.415], loss: 7.653771, mae: 51.767151, mean_q: 68.788673
  397864/1100000: episode: 788, duration: 4.863s, episode steps: 667, steps per second: 137, episode reward: 206.210, mean reward: 0.309 [-21.136, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.017 [-0.826, 1.408], loss: 11.482417, mae: 52.141064, mean_q: 69.212791
  398262/1100000: episode: 789, duration: 2.812s, episode steps: 398, steps per second: 142, episode reward: 261.956, mean reward: 0.658 [-18.685, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: -0.009 [-0.676, 1.432], loss: 7.417349, mae: 51.728065, mean_q: 69.084915
  398635/1100000: episode: 790, duration: 2.564s, episode steps: 373, steps per second: 145, episode reward: 268.357, mean reward: 0.719 [-17.310, 100.000], mean action: 1.654 [0.000, 3.000], mean observation: -0.010 [-0.775, 1.474], loss: 13.223707, mae: 51.814999, mean_q: 69.002449
  399033/1100000: episode: 791, duration: 2.769s, episode steps: 398, steps per second: 144, episode reward: 257.532, mean reward: 0.647 [-10.679, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.185 [-0.846, 1.554], loss: 7.628544, mae: 51.753151, mean_q: 69.080940
  399501/1100000: episode: 792, duration: 3.394s, episode steps: 468, steps per second: 138, episode reward: 235.285, mean reward: 0.503 [-15.880, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.039 [-0.658, 1.391], loss: 10.414798, mae: 51.599762, mean_q: 68.956711
  399675/1100000: episode: 793, duration: 1.165s, episode steps: 174, steps per second: 149, episode reward: -196.812, mean reward: -1.131 [-100.000, 5.695], mean action: 1.385 [0.000, 3.000], mean observation: 0.178 [-0.970, 1.412], loss: 7.516722, mae: 51.768772, mean_q: 69.275703
  399903/1100000: episode: 794, duration: 1.545s, episode steps: 228, steps per second: 148, episode reward: 250.833, mean reward: 1.100 [-9.481, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.192 [-0.823, 1.398], loss: 5.058197, mae: 51.695305, mean_q: 69.388573
  400264/1100000: episode: 795, duration: 2.463s, episode steps: 361, steps per second: 147, episode reward: 214.874, mean reward: 0.595 [-17.541, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.190 [-0.923, 1.408], loss: 7.621184, mae: 51.536182, mean_q: 68.843178
  400510/1100000: episode: 796, duration: 1.663s, episode steps: 246, steps per second: 148, episode reward: 236.085, mean reward: 0.960 [-11.162, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.167 [-0.916, 1.394], loss: 6.954396, mae: 51.529903, mean_q: 69.119392
  400852/1100000: episode: 797, duration: 2.326s, episode steps: 342, steps per second: 147, episode reward: 261.586, mean reward: 0.765 [-19.035, 100.000], mean action: 0.845 [0.000, 3.000], mean observation: 0.003 [-0.791, 1.441], loss: 9.349625, mae: 51.598377, mean_q: 69.053497
  401604/1100000: episode: 798, duration: 5.487s, episode steps: 752, steps per second: 137, episode reward: 260.853, mean reward: 0.347 [-19.752, 100.000], mean action: 0.690 [0.000, 3.000], mean observation: 0.099 [-0.757, 1.408], loss: 7.744217, mae: 51.687481, mean_q: 68.737541
  401904/1100000: episode: 799, duration: 2.071s, episode steps: 300, steps per second: 145, episode reward: 211.401, mean reward: 0.705 [-14.720, 100.000], mean action: 1.623 [0.000, 3.000], mean observation: -0.025 [-0.963, 1.424], loss: 8.973566, mae: 51.268372, mean_q: 68.384438
  402356/1100000: episode: 800, duration: 3.132s, episode steps: 452, steps per second: 144, episode reward: 230.386, mean reward: 0.510 [-10.800, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: -0.003 [-0.708, 1.398], loss: 8.246881, mae: 51.506855, mean_q: 68.411446
  403043/1100000: episode: 801, duration: 4.895s, episode steps: 687, steps per second: 140, episode reward: 169.057, mean reward: 0.246 [-19.788, 100.000], mean action: 1.933 [0.000, 3.000], mean observation: 0.190 [-0.885, 1.487], loss: 9.094576, mae: 51.542271, mean_q: 68.601822
  403539/1100000: episode: 802, duration: 3.540s, episode steps: 496, steps per second: 140, episode reward: 182.231, mean reward: 0.367 [-9.651, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.125 [-0.977, 1.452], loss: 8.332602, mae: 51.311344, mean_q: 68.287605
  404058/1100000: episode: 803, duration: 3.736s, episode steps: 519, steps per second: 139, episode reward: 170.647, mean reward: 0.329 [-18.993, 100.000], mean action: 1.597 [0.000, 3.000], mean observation: 0.141 [-0.934, 1.412], loss: 5.556352, mae: 51.460472, mean_q: 68.776505
  404119/1100000: episode: 804, duration: 0.408s, episode steps: 61, steps per second: 149, episode reward: -205.045, mean reward: -3.361 [-100.000, 2.548], mean action: 1.295 [0.000, 3.000], mean observation: 0.041 [-1.636, 2.524], loss: 4.463522, mae: 51.764702, mean_q: 69.081917
  404430/1100000: episode: 805, duration: 2.123s, episode steps: 311, steps per second: 146, episode reward: 278.138, mean reward: 0.894 [-17.525, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.210 [-0.726, 1.409], loss: 7.944297, mae: 51.381004, mean_q: 68.358147
  404853/1100000: episode: 806, duration: 2.931s, episode steps: 423, steps per second: 144, episode reward: 258.553, mean reward: 0.611 [-19.998, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.090 [-0.751, 1.514], loss: 6.423965, mae: 51.627235, mean_q: 68.722939
  405127/1100000: episode: 807, duration: 1.860s, episode steps: 274, steps per second: 147, episode reward: 224.153, mean reward: 0.818 [-8.773, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.040 [-0.657, 1.404], loss: 5.869319, mae: 51.474014, mean_q: 68.769806
  405609/1100000: episode: 808, duration: 3.364s, episode steps: 482, steps per second: 143, episode reward: 254.994, mean reward: 0.529 [-18.091, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: -0.001 [-0.837, 1.386], loss: 6.078135, mae: 51.257908, mean_q: 68.517097
  405858/1100000: episode: 809, duration: 1.700s, episode steps: 249, steps per second: 146, episode reward: 283.214, mean reward: 1.137 [-4.538, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.092 [-0.747, 1.413], loss: 7.121294, mae: 51.862537, mean_q: 68.934753
  406086/1100000: episode: 810, duration: 1.548s, episode steps: 228, steps per second: 147, episode reward: 5.883, mean reward: 0.026 [-100.000, 17.136], mean action: 1.768 [0.000, 3.000], mean observation: -0.023 [-1.092, 1.387], loss: 5.720882, mae: 51.601517, mean_q: 68.873215
  406602/1100000: episode: 811, duration: 3.659s, episode steps: 516, steps per second: 141, episode reward: 224.726, mean reward: 0.436 [-17.510, 100.000], mean action: 2.415 [0.000, 3.000], mean observation: 0.120 [-0.844, 1.386], loss: 13.503675, mae: 51.743027, mean_q: 68.758919
  406879/1100000: episode: 812, duration: 1.878s, episode steps: 277, steps per second: 147, episode reward: 226.398, mean reward: 0.817 [-18.344, 100.000], mean action: 2.307 [0.000, 3.000], mean observation: 0.041 [-0.783, 1.394], loss: 7.462315, mae: 51.677277, mean_q: 68.945938
  407182/1100000: episode: 813, duration: 2.099s, episode steps: 303, steps per second: 144, episode reward: 255.912, mean reward: 0.845 [-18.288, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.093 [-0.667, 1.404], loss: 7.593308, mae: 51.598476, mean_q: 68.736290
  407566/1100000: episode: 814, duration: 2.704s, episode steps: 384, steps per second: 142, episode reward: 221.686, mean reward: 0.577 [-18.932, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.203 [-1.004, 1.395], loss: 5.337269, mae: 51.575001, mean_q: 68.807076
  407798/1100000: episode: 815, duration: 1.576s, episode steps: 232, steps per second: 147, episode reward: 235.219, mean reward: 1.014 [-17.614, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.058 [-0.695, 1.422], loss: 6.931087, mae: 51.427395, mean_q: 68.781067
  408197/1100000: episode: 816, duration: 2.774s, episode steps: 399, steps per second: 144, episode reward: 185.833, mean reward: 0.466 [-10.066, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.008 [-0.827, 1.401], loss: 6.025476, mae: 51.284050, mean_q: 68.437614
  408519/1100000: episode: 817, duration: 2.219s, episode steps: 322, steps per second: 145, episode reward: 279.476, mean reward: 0.868 [-17.526, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.097 [-0.539, 1.486], loss: 4.861710, mae: 51.493565, mean_q: 68.774055
  408712/1100000: episode: 818, duration: 1.293s, episode steps: 193, steps per second: 149, episode reward: 3.703, mean reward: 0.019 [-100.000, 24.117], mean action: 1.850 [0.000, 3.000], mean observation: -0.036 [-0.723, 1.414], loss: 5.521307, mae: 51.775349, mean_q: 69.051857
  409256/1100000: episode: 819, duration: 3.879s, episode steps: 544, steps per second: 140, episode reward: -306.821, mean reward: -0.564 [-100.000, 16.604], mean action: 1.855 [0.000, 3.000], mean observation: 0.077 [-0.887, 1.877], loss: 8.677927, mae: 51.998428, mean_q: 69.376534
  409406/1100000: episode: 820, duration: 1.010s, episode steps: 150, steps per second: 149, episode reward: -8.535, mean reward: -0.057 [-100.000, 19.614], mean action: 1.827 [0.000, 3.000], mean observation: 0.010 [-0.741, 1.406], loss: 11.988816, mae: 51.768902, mean_q: 68.902176
  409729/1100000: episode: 821, duration: 2.220s, episode steps: 323, steps per second: 145, episode reward: 236.559, mean reward: 0.732 [-11.990, 100.000], mean action: 1.638 [0.000, 3.000], mean observation: -0.017 [-0.719, 1.489], loss: 6.690577, mae: 51.892704, mean_q: 69.305923
  410013/1100000: episode: 822, duration: 1.937s, episode steps: 284, steps per second: 147, episode reward: 224.936, mean reward: 0.792 [-10.219, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.093 [-1.045, 1.423], loss: 5.170123, mae: 51.611519, mean_q: 69.000130
  410265/1100000: episode: 823, duration: 1.707s, episode steps: 252, steps per second: 148, episode reward: 270.577, mean reward: 1.074 [-3.584, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.087 [-0.756, 1.402], loss: 7.711291, mae: 51.749184, mean_q: 69.284225
  410497/1100000: episode: 824, duration: 1.579s, episode steps: 232, steps per second: 147, episode reward: 208.965, mean reward: 0.901 [-9.549, 100.000], mean action: 2.103 [0.000, 3.000], mean observation: -0.042 [-0.632, 1.404], loss: 9.913062, mae: 52.003216, mean_q: 69.573944
  410702/1100000: episode: 825, duration: 1.399s, episode steps: 205, steps per second: 147, episode reward: 5.920, mean reward: 0.029 [-100.000, 13.188], mean action: 1.698 [0.000, 3.000], mean observation: -0.023 [-0.910, 1.492], loss: 5.786564, mae: 52.150982, mean_q: 69.539780
  411361/1100000: episode: 826, duration: 4.630s, episode steps: 659, steps per second: 142, episode reward: 227.826, mean reward: 0.346 [-20.072, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: 0.202 [-0.582, 1.432], loss: 8.582025, mae: 52.139011, mean_q: 69.353630
  411638/1100000: episode: 827, duration: 1.900s, episode steps: 277, steps per second: 146, episode reward: 229.961, mean reward: 0.830 [-17.726, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.059 [-0.798, 1.410], loss: 8.050039, mae: 52.188175, mean_q: 69.301674
  412034/1100000: episode: 828, duration: 2.781s, episode steps: 396, steps per second: 142, episode reward: 257.396, mean reward: 0.650 [-23.309, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.090 [-0.776, 1.525], loss: 10.542902, mae: 52.065544, mean_q: 69.355103
  412235/1100000: episode: 829, duration: 1.374s, episode steps: 201, steps per second: 146, episode reward: -138.685, mean reward: -0.690 [-100.000, 18.103], mean action: 1.761 [0.000, 3.000], mean observation: 0.126 [-0.784, 1.664], loss: 10.713972, mae: 52.198112, mean_q: 69.331673
  412450/1100000: episode: 830, duration: 1.455s, episode steps: 215, steps per second: 148, episode reward: 269.544, mean reward: 1.254 [-10.691, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.064 [-0.798, 1.392], loss: 7.600377, mae: 51.873856, mean_q: 69.004150
  412586/1100000: episode: 831, duration: 0.909s, episode steps: 136, steps per second: 150, episode reward: 40.045, mean reward: 0.294 [-100.000, 16.864], mean action: 1.941 [0.000, 3.000], mean observation: 0.175 [-1.465, 1.406], loss: 4.611553, mae: 52.778175, mean_q: 70.069138
  412760/1100000: episode: 832, duration: 1.163s, episode steps: 174, steps per second: 150, episode reward: 60.572, mean reward: 0.348 [-100.000, 13.634], mean action: 1.770 [0.000, 3.000], mean observation: 0.152 [-0.560, 1.416], loss: 10.039026, mae: 53.149178, mean_q: 70.207558
  412965/1100000: episode: 833, duration: 1.367s, episode steps: 205, steps per second: 150, episode reward: -142.759, mean reward: -0.696 [-100.000, 4.290], mean action: 1.663 [0.000, 3.000], mean observation: 0.186 [-0.861, 1.482], loss: 6.145192, mae: 52.475124, mean_q: 70.019005
  413236/1100000: episode: 834, duration: 1.852s, episode steps: 271, steps per second: 146, episode reward: 303.969, mean reward: 1.122 [-9.931, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.106 [-0.818, 1.398], loss: 13.100307, mae: 52.268600, mean_q: 69.960091
  413543/1100000: episode: 835, duration: 2.102s, episode steps: 307, steps per second: 146, episode reward: 258.810, mean reward: 0.843 [-3.226, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.121 [-0.702, 1.404], loss: 12.689566, mae: 52.694660, mean_q: 70.411369
  413978/1100000: episode: 836, duration: 3.035s, episode steps: 435, steps per second: 143, episode reward: 284.515, mean reward: 0.654 [-9.551, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.126 [-0.686, 1.403], loss: 10.088744, mae: 52.413013, mean_q: 70.056496
  414360/1100000: episode: 837, duration: 2.614s, episode steps: 382, steps per second: 146, episode reward: 244.478, mean reward: 0.640 [-18.059, 100.000], mean action: 2.196 [0.000, 3.000], mean observation: 0.037 [-0.732, 1.433], loss: 10.705290, mae: 52.453217, mean_q: 70.098648
  414708/1100000: episode: 838, duration: 2.439s, episode steps: 348, steps per second: 143, episode reward: 295.016, mean reward: 0.848 [-11.026, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.088 [-0.874, 1.393], loss: 5.731902, mae: 52.447803, mean_q: 70.301704
  414852/1100000: episode: 839, duration: 0.964s, episode steps: 144, steps per second: 149, episode reward: 15.293, mean reward: 0.106 [-100.000, 12.239], mean action: 1.819 [0.000, 3.000], mean observation: -0.037 [-0.927, 1.393], loss: 6.990485, mae: 53.051006, mean_q: 70.861572
  415179/1100000: episode: 840, duration: 2.286s, episode steps: 327, steps per second: 143, episode reward: 200.486, mean reward: 0.613 [-13.935, 100.000], mean action: 1.483 [0.000, 3.000], mean observation: -0.046 [-0.824, 1.410], loss: 11.304563, mae: 52.667072, mean_q: 70.297386
  415430/1100000: episode: 841, duration: 1.706s, episode steps: 251, steps per second: 147, episode reward: 276.866, mean reward: 1.103 [-9.816, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.174 [-0.877, 1.389], loss: 14.775154, mae: 52.558079, mean_q: 70.349075
  415833/1100000: episode: 842, duration: 2.830s, episode steps: 403, steps per second: 142, episode reward: 226.028, mean reward: 0.561 [-17.932, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.206 [-0.837, 1.406], loss: 6.880415, mae: 52.960724, mean_q: 70.889091
  416058/1100000: episode: 843, duration: 1.575s, episode steps: 225, steps per second: 143, episode reward: 237.345, mean reward: 1.055 [-8.567, 100.000], mean action: 1.631 [0.000, 3.000], mean observation: -0.078 [-0.823, 1.387], loss: 8.074494, mae: 53.094524, mean_q: 70.982323
  416478/1100000: episode: 844, duration: 3.029s, episode steps: 420, steps per second: 139, episode reward: 234.850, mean reward: 0.559 [-11.133, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: -0.006 [-0.765, 1.456], loss: 6.681925, mae: 52.875183, mean_q: 70.734665
  416863/1100000: episode: 845, duration: 2.673s, episode steps: 385, steps per second: 144, episode reward: 213.425, mean reward: 0.554 [-21.113, 100.000], mean action: 2.229 [0.000, 3.000], mean observation: 0.171 [-0.725, 1.399], loss: 9.565502, mae: 52.987926, mean_q: 70.747345
  416997/1100000: episode: 846, duration: 0.894s, episode steps: 134, steps per second: 150, episode reward: -57.928, mean reward: -0.432 [-100.000, 11.103], mean action: 1.739 [0.000, 3.000], mean observation: -0.055 [-1.570, 1.411], loss: 10.939785, mae: 53.449577, mean_q: 71.362755
  417379/1100000: episode: 847, duration: 2.649s, episode steps: 382, steps per second: 144, episode reward: 245.026, mean reward: 0.641 [-17.786, 100.000], mean action: 0.780 [0.000, 3.000], mean observation: 0.244 [-0.909, 1.390], loss: 8.155825, mae: 53.072193, mean_q: 70.798599
  417741/1100000: episode: 848, duration: 2.498s, episode steps: 362, steps per second: 145, episode reward: 279.192, mean reward: 0.771 [-19.873, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.135 [-0.863, 1.402], loss: 7.859935, mae: 52.793564, mean_q: 70.733429
  418125/1100000: episode: 849, duration: 2.716s, episode steps: 384, steps per second: 141, episode reward: 179.996, mean reward: 0.469 [-9.952, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: -0.048 [-0.658, 1.417], loss: 9.491157, mae: 52.981037, mean_q: 70.647179
  418686/1100000: episode: 850, duration: 3.872s, episode steps: 561, steps per second: 145, episode reward: 240.555, mean reward: 0.429 [-12.872, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.179 [-0.732, 1.408], loss: 8.192393, mae: 52.848228, mean_q: 70.435577
  418846/1100000: episode: 851, duration: 1.074s, episode steps: 160, steps per second: 149, episode reward: 4.033, mean reward: 0.025 [-100.000, 14.237], mean action: 1.637 [0.000, 3.000], mean observation: 0.089 [-0.943, 1.691], loss: 14.945631, mae: 52.865013, mean_q: 70.745316
  418960/1100000: episode: 852, duration: 0.758s, episode steps: 114, steps per second: 150, episode reward: -95.947, mean reward: -0.842 [-100.000, 31.785], mean action: 1.640 [0.000, 3.000], mean observation: -0.169 [-1.566, 1.391], loss: 5.139566, mae: 53.170761, mean_q: 70.748825
  419340/1100000: episode: 853, duration: 2.677s, episode steps: 380, steps per second: 142, episode reward: 225.718, mean reward: 0.594 [-17.221, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.083 [-0.762, 1.392], loss: 8.962773, mae: 53.491745, mean_q: 71.167282
  419572/1100000: episode: 854, duration: 1.580s, episode steps: 232, steps per second: 147, episode reward: 256.525, mean reward: 1.106 [-9.801, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.040 [-0.883, 1.398], loss: 4.550691, mae: 53.589771, mean_q: 71.451897
  419911/1100000: episode: 855, duration: 2.297s, episode steps: 339, steps per second: 148, episode reward: 241.732, mean reward: 0.713 [-18.010, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.123 [-0.911, 1.404], loss: 9.529325, mae: 53.767735, mean_q: 72.025490
  420911/1100000: episode: 856, duration: 7.612s, episode steps: 1000, steps per second: 131, episode reward: 100.143, mean reward: 0.100 [-20.091, 23.383], mean action: 2.015 [0.000, 3.000], mean observation: 0.006 [-1.540, 1.505], loss: 11.243004, mae: 53.878590, mean_q: 72.206612
  421343/1100000: episode: 857, duration: 2.981s, episode steps: 432, steps per second: 145, episode reward: 218.785, mean reward: 0.506 [-9.658, 100.000], mean action: 2.005 [0.000, 3.000], mean observation: 0.107 [-0.750, 1.398], loss: 8.469488, mae: 53.606590, mean_q: 71.995735
  421749/1100000: episode: 858, duration: 2.870s, episode steps: 406, steps per second: 141, episode reward: 249.349, mean reward: 0.614 [-9.894, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.189 [-0.731, 1.405], loss: 5.675387, mae: 53.457878, mean_q: 71.788300
  422249/1100000: episode: 859, duration: 3.499s, episode steps: 500, steps per second: 143, episode reward: 239.856, mean reward: 0.480 [-19.220, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: -0.020 [-0.653, 1.483], loss: 6.473245, mae: 53.493416, mean_q: 71.925606
  423249/1100000: episode: 860, duration: 7.269s, episode steps: 1000, steps per second: 138, episode reward: 88.809, mean reward: 0.089 [-18.431, 14.855], mean action: 2.646 [0.000, 3.000], mean observation: 0.115 [-0.753, 1.501], loss: 8.306697, mae: 53.089405, mean_q: 71.138855
  423663/1100000: episode: 861, duration: 2.855s, episode steps: 414, steps per second: 145, episode reward: 245.806, mean reward: 0.594 [-19.571, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.207 [-0.653, 1.405], loss: 7.508722, mae: 52.847862, mean_q: 70.663544
  423897/1100000: episode: 862, duration: 1.565s, episode steps: 234, steps per second: 149, episode reward: 236.707, mean reward: 1.012 [-7.151, 100.000], mean action: 2.064 [0.000, 3.000], mean observation: 0.125 [-0.823, 1.477], loss: 8.031081, mae: 52.885448, mean_q: 70.775505
  424048/1100000: episode: 863, duration: 0.999s, episode steps: 151, steps per second: 151, episode reward: 46.241, mean reward: 0.306 [-100.000, 17.333], mean action: 1.483 [0.000, 3.000], mean observation: 0.132 [-0.708, 1.505], loss: 7.181990, mae: 53.321766, mean_q: 71.247543
  424275/1100000: episode: 864, duration: 1.522s, episode steps: 227, steps per second: 149, episode reward: 250.024, mean reward: 1.101 [-11.380, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.137 [-0.794, 1.442], loss: 10.036284, mae: 53.149643, mean_q: 71.189178
  424448/1100000: episode: 865, duration: 1.159s, episode steps: 173, steps per second: 149, episode reward: -63.545, mean reward: -0.367 [-100.000, 5.424], mean action: 1.647 [0.000, 3.000], mean observation: 0.037 [-1.668, 1.486], loss: 10.295737, mae: 53.987648, mean_q: 72.377586
  424670/1100000: episode: 866, duration: 1.537s, episode steps: 222, steps per second: 144, episode reward: 260.047, mean reward: 1.171 [-8.267, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: -0.069 [-0.714, 1.390], loss: 9.012902, mae: 53.616577, mean_q: 71.488335
  424889/1100000: episode: 867, duration: 1.502s, episode steps: 219, steps per second: 146, episode reward: 237.852, mean reward: 1.086 [-15.081, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: -0.033 [-0.963, 1.394], loss: 6.726741, mae: 53.916576, mean_q: 72.331017
  425461/1100000: episode: 868, duration: 3.978s, episode steps: 572, steps per second: 144, episode reward: 198.452, mean reward: 0.347 [-19.066, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.028 [-0.999, 1.413], loss: 9.066609, mae: 54.553928, mean_q: 72.992508
  425518/1100000: episode: 869, duration: 0.377s, episode steps: 57, steps per second: 151, episode reward: -123.955, mean reward: -2.175 [-100.000, 2.082], mean action: 1.807 [0.000, 3.000], mean observation: -0.101 [-6.770, 1.389], loss: 4.792472, mae: 54.889366, mean_q: 73.139656
  425746/1100000: episode: 870, duration: 1.536s, episode steps: 228, steps per second: 148, episode reward: 238.779, mean reward: 1.047 [-3.106, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.061 [-0.769, 1.406], loss: 5.928232, mae: 54.504173, mean_q: 72.722229
  426077/1100000: episode: 871, duration: 2.291s, episode steps: 331, steps per second: 144, episode reward: 218.792, mean reward: 0.661 [-10.491, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.119 [-0.769, 1.408], loss: 5.351642, mae: 54.539429, mean_q: 72.915665
  426397/1100000: episode: 872, duration: 2.159s, episode steps: 320, steps per second: 148, episode reward: 250.700, mean reward: 0.783 [-17.143, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.026 [-0.861, 1.401], loss: 6.336319, mae: 54.496788, mean_q: 72.796814
  426628/1100000: episode: 873, duration: 1.569s, episode steps: 231, steps per second: 147, episode reward: 292.866, mean reward: 1.268 [-9.654, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.060 [-0.712, 1.388], loss: 8.589775, mae: 54.273834, mean_q: 72.422401
  427035/1100000: episode: 874, duration: 2.819s, episode steps: 407, steps per second: 144, episode reward: 267.354, mean reward: 0.657 [-13.125, 100.000], mean action: 1.717 [0.000, 3.000], mean observation: 0.185 [-0.740, 1.393], loss: 9.256308, mae: 55.065208, mean_q: 73.323448
  427144/1100000: episode: 875, duration: 0.719s, episode steps: 109, steps per second: 152, episode reward: -2.013, mean reward: -0.018 [-100.000, 5.678], mean action: 1.706 [0.000, 3.000], mean observation: 0.023 [-0.974, 1.405], loss: 9.029385, mae: 54.246189, mean_q: 72.152534
  427458/1100000: episode: 876, duration: 2.160s, episode steps: 314, steps per second: 145, episode reward: 231.565, mean reward: 0.737 [-10.657, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.048 [-0.726, 1.409], loss: 5.284816, mae: 54.504234, mean_q: 72.703362
  427602/1100000: episode: 877, duration: 0.962s, episode steps: 144, steps per second: 150, episode reward: -38.872, mean reward: -0.270 [-100.000, 9.661], mean action: 1.590 [0.000, 3.000], mean observation: -0.028 [-0.724, 2.045], loss: 3.566172, mae: 54.284176, mean_q: 72.600395
  427782/1100000: episode: 878, duration: 1.206s, episode steps: 180, steps per second: 149, episode reward: 248.291, mean reward: 1.379 [-9.417, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.167 [-0.760, 1.403], loss: 15.934079, mae: 54.660763, mean_q: 73.022125
  428544/1100000: episode: 879, duration: 5.904s, episode steps: 762, steps per second: 129, episode reward: 160.673, mean reward: 0.211 [-17.357, 100.000], mean action: 2.164 [0.000, 3.000], mean observation: 0.172 [-0.756, 1.494], loss: 12.723261, mae: 54.427658, mean_q: 72.488770
  428763/1100000: episode: 880, duration: 1.475s, episode steps: 219, steps per second: 148, episode reward: 266.004, mean reward: 1.215 [-4.055, 100.000], mean action: 1.612 [0.000, 3.000], mean observation: -0.045 [-1.595, 1.403], loss: 5.520403, mae: 54.072952, mean_q: 72.350601
  429019/1100000: episode: 881, duration: 1.743s, episode steps: 256, steps per second: 147, episode reward: 255.212, mean reward: 0.997 [-18.080, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.000 [-1.047, 1.391], loss: 9.737649, mae: 53.996517, mean_q: 72.382057
  429136/1100000: episode: 882, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -5.168, mean reward: -0.044 [-100.000, 12.385], mean action: 1.573 [0.000, 3.000], mean observation: 0.115 [-0.675, 1.407], loss: 10.395444, mae: 53.746944, mean_q: 71.995392
  430136/1100000: episode: 883, duration: 7.236s, episode steps: 1000, steps per second: 138, episode reward: 35.354, mean reward: 0.035 [-25.466, 24.924], mean action: 1.872 [0.000, 3.000], mean observation: 0.148 [-0.724, 1.398], loss: 9.183225, mae: 54.066612, mean_q: 72.356659
  430210/1100000: episode: 884, duration: 0.496s, episode steps: 74, steps per second: 149, episode reward: -148.387, mean reward: -2.005 [-100.000, 4.769], mean action: 1.459 [0.000, 3.000], mean observation: -0.034 [-4.646, 1.406], loss: 7.244978, mae: 53.055283, mean_q: 70.883034
  430458/1100000: episode: 885, duration: 1.693s, episode steps: 248, steps per second: 147, episode reward: 243.805, mean reward: 0.983 [-11.852, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.111 [-0.761, 1.391], loss: 8.321524, mae: 53.810101, mean_q: 72.310432
  430572/1100000: episode: 886, duration: 0.753s, episode steps: 114, steps per second: 151, episode reward: -3.542, mean reward: -0.031 [-100.000, 16.517], mean action: 1.439 [0.000, 3.000], mean observation: 0.107 [-0.894, 1.448], loss: 10.276983, mae: 53.988987, mean_q: 72.367233
  431079/1100000: episode: 887, duration: 3.495s, episode steps: 507, steps per second: 145, episode reward: 263.045, mean reward: 0.519 [-14.344, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.022 [-0.961, 1.387], loss: 8.476664, mae: 53.854858, mean_q: 71.981071
  431443/1100000: episode: 888, duration: 2.527s, episode steps: 364, steps per second: 144, episode reward: 280.344, mean reward: 0.770 [-9.589, 100.000], mean action: 1.516 [0.000, 3.000], mean observation: 0.095 [-0.475, 1.406], loss: 8.064260, mae: 53.982967, mean_q: 72.189278
  431497/1100000: episode: 889, duration: 0.359s, episode steps: 54, steps per second: 150, episode reward: -150.366, mean reward: -2.785 [-100.000, 2.332], mean action: 0.981 [0.000, 3.000], mean observation: -0.027 [-5.541, 1.386], loss: 3.946843, mae: 54.312263, mean_q: 73.050903
  431613/1100000: episode: 890, duration: 0.766s, episode steps: 116, steps per second: 151, episode reward: -107.260, mean reward: -0.925 [-100.000, 9.364], mean action: 1.267 [0.000, 3.000], mean observation: 0.021 [-1.670, 1.463], loss: 6.640364, mae: 54.370106, mean_q: 72.466644
  431702/1100000: episode: 891, duration: 0.615s, episode steps: 89, steps per second: 145, episode reward: -95.576, mean reward: -1.074 [-100.000, 45.116], mean action: 1.326 [0.000, 3.000], mean observation: 0.052 [-1.494, 1.627], loss: 8.526056, mae: 53.460724, mean_q: 71.655670
  432021/1100000: episode: 892, duration: 2.197s, episode steps: 319, steps per second: 145, episode reward: 256.485, mean reward: 0.804 [-12.191, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.089 [-0.852, 1.407], loss: 10.235916, mae: 54.587536, mean_q: 72.626549
  432457/1100000: episode: 893, duration: 3.044s, episode steps: 436, steps per second: 143, episode reward: 205.202, mean reward: 0.471 [-18.502, 100.000], mean action: 0.835 [0.000, 3.000], mean observation: -0.003 [-0.891, 1.410], loss: 17.062492, mae: 54.106041, mean_q: 72.193626
  432842/1100000: episode: 894, duration: 2.751s, episode steps: 385, steps per second: 140, episode reward: 260.669, mean reward: 0.677 [-10.535, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: 0.093 [-0.564, 1.405], loss: 8.010229, mae: 54.504986, mean_q: 72.589249
  433248/1100000: episode: 895, duration: 2.907s, episode steps: 406, steps per second: 140, episode reward: 248.832, mean reward: 0.613 [-11.761, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.088 [-0.697, 1.479], loss: 10.280177, mae: 54.428234, mean_q: 72.610504
  433680/1100000: episode: 896, duration: 2.989s, episode steps: 432, steps per second: 145, episode reward: 230.517, mean reward: 0.534 [-15.422, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.118 [-0.695, 1.405], loss: 8.112602, mae: 54.519753, mean_q: 73.011177
  434186/1100000: episode: 897, duration: 3.545s, episode steps: 506, steps per second: 143, episode reward: 244.405, mean reward: 0.483 [-20.033, 100.000], mean action: 0.605 [0.000, 3.000], mean observation: 0.140 [-0.737, 1.411], loss: 8.101873, mae: 54.112755, mean_q: 72.076035
  434374/1100000: episode: 898, duration: 1.255s, episode steps: 188, steps per second: 150, episode reward: -171.211, mean reward: -0.911 [-100.000, 10.277], mean action: 1.378 [0.000, 3.000], mean observation: 0.075 [-1.241, 1.508], loss: 11.514580, mae: 54.241272, mean_q: 72.337006
  434940/1100000: episode: 899, duration: 4.133s, episode steps: 566, steps per second: 137, episode reward: 227.678, mean reward: 0.402 [-19.069, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.027 [-0.600, 1.416], loss: 8.041142, mae: 54.200619, mean_q: 72.080765
  435161/1100000: episode: 900, duration: 1.482s, episode steps: 221, steps per second: 149, episode reward: -355.190, mean reward: -1.607 [-100.000, 51.642], mean action: 1.452 [0.000, 3.000], mean observation: 0.147 [-2.657, 1.527], loss: 9.327657, mae: 53.815327, mean_q: 71.731911
  435519/1100000: episode: 901, duration: 2.457s, episode steps: 358, steps per second: 146, episode reward: 248.813, mean reward: 0.695 [-9.313, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.091 [-0.583, 1.461], loss: 7.930222, mae: 53.620358, mean_q: 71.411552
  435700/1100000: episode: 902, duration: 1.220s, episode steps: 181, steps per second: 148, episode reward: -28.432, mean reward: -0.157 [-100.000, 19.113], mean action: 1.740 [0.000, 3.000], mean observation: -0.051 [-0.649, 1.399], loss: 6.146291, mae: 54.195076, mean_q: 71.917969
  436050/1100000: episode: 903, duration: 2.430s, episode steps: 350, steps per second: 144, episode reward: -17.109, mean reward: -0.049 [-100.000, 13.167], mean action: 1.669 [0.000, 3.000], mean observation: -0.021 [-0.609, 1.404], loss: 7.794845, mae: 54.182354, mean_q: 72.318680
  436744/1100000: episode: 904, duration: 5.047s, episode steps: 694, steps per second: 138, episode reward: 209.175, mean reward: 0.301 [-17.355, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.078 [-0.644, 1.387], loss: 7.934857, mae: 54.276684, mean_q: 72.347412
  437744/1100000: episode: 905, duration: 7.325s, episode steps: 1000, steps per second: 137, episode reward: 107.070, mean reward: 0.107 [-22.112, 22.566], mean action: 0.965 [0.000, 3.000], mean observation: 0.056 [-1.028, 1.404], loss: 8.371418, mae: 54.040245, mean_q: 72.021309
  438122/1100000: episode: 906, duration: 2.605s, episode steps: 378, steps per second: 145, episode reward: 274.011, mean reward: 0.725 [-20.783, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.089 [-0.668, 1.390], loss: 5.681123, mae: 54.288761, mean_q: 72.391998
  438340/1100000: episode: 907, duration: 1.479s, episode steps: 218, steps per second: 147, episode reward: 288.268, mean reward: 1.322 [-12.101, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.078 [-0.590, 1.395], loss: 6.416629, mae: 53.978622, mean_q: 71.800835
  438598/1100000: episode: 908, duration: 1.752s, episode steps: 258, steps per second: 147, episode reward: 239.994, mean reward: 0.930 [-19.508, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.004 [-0.739, 1.397], loss: 7.921084, mae: 54.156834, mean_q: 72.240822
  438749/1100000: episode: 909, duration: 1.010s, episode steps: 151, steps per second: 150, episode reward: -8.131, mean reward: -0.054 [-100.000, 14.910], mean action: 1.536 [0.000, 3.000], mean observation: -0.093 [-0.929, 1.393], loss: 5.776291, mae: 55.198288, mean_q: 73.887634
  439399/1100000: episode: 910, duration: 4.508s, episode steps: 650, steps per second: 144, episode reward: 226.752, mean reward: 0.349 [-17.710, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.143 [-0.917, 1.475], loss: 8.820861, mae: 54.622898, mean_q: 72.877136
  439519/1100000: episode: 911, duration: 0.812s, episode steps: 120, steps per second: 148, episode reward: -201.028, mean reward: -1.675 [-100.000, 26.661], mean action: 1.800 [0.000, 3.000], mean observation: -0.002 [-2.180, 1.406], loss: 8.584723, mae: 54.026783, mean_q: 72.118568
  439890/1100000: episode: 912, duration: 2.620s, episode steps: 371, steps per second: 142, episode reward: 187.203, mean reward: 0.505 [-21.240, 100.000], mean action: 1.873 [0.000, 3.000], mean observation: 0.076 [-0.777, 1.454], loss: 8.877718, mae: 54.664074, mean_q: 72.941521
  440327/1100000: episode: 913, duration: 3.025s, episode steps: 437, steps per second: 144, episode reward: 165.039, mean reward: 0.378 [-18.613, 100.000], mean action: 1.950 [0.000, 3.000], mean observation: 0.209 [-0.865, 1.407], loss: 16.011663, mae: 54.605381, mean_q: 72.339348
  441184/1100000: episode: 914, duration: 6.327s, episode steps: 857, steps per second: 135, episode reward: 126.664, mean reward: 0.148 [-25.311, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.149 [-0.983, 1.526], loss: 7.198532, mae: 54.848648, mean_q: 72.857086
  441595/1100000: episode: 915, duration: 2.827s, episode steps: 411, steps per second: 145, episode reward: 260.386, mean reward: 0.634 [-20.186, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.114 [-0.622, 1.427], loss: 7.510963, mae: 54.631802, mean_q: 72.539268
  442019/1100000: episode: 916, duration: 3.098s, episode steps: 424, steps per second: 137, episode reward: 252.205, mean reward: 0.595 [-17.892, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.093 [-0.853, 1.394], loss: 11.716154, mae: 54.135586, mean_q: 71.917229
  442350/1100000: episode: 917, duration: 2.251s, episode steps: 331, steps per second: 147, episode reward: 263.368, mean reward: 0.796 [-18.772, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.224 [-0.615, 1.505], loss: 8.844513, mae: 54.404900, mean_q: 72.508286
  442673/1100000: episode: 918, duration: 2.186s, episode steps: 323, steps per second: 148, episode reward: 297.709, mean reward: 0.922 [-17.840, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.130 [-0.583, 1.447], loss: 11.834964, mae: 54.583363, mean_q: 72.291061
  443050/1100000: episode: 919, duration: 2.558s, episode steps: 377, steps per second: 147, episode reward: 245.974, mean reward: 0.652 [-18.682, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.242 [-0.814, 1.407], loss: 9.161368, mae: 54.592331, mean_q: 72.404457
  443321/1100000: episode: 920, duration: 1.818s, episode steps: 271, steps per second: 149, episode reward: 259.329, mean reward: 0.957 [-17.443, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.207 [-0.743, 1.471], loss: 7.587631, mae: 54.336456, mean_q: 72.518578
  443620/1100000: episode: 921, duration: 2.037s, episode steps: 299, steps per second: 147, episode reward: 305.143, mean reward: 1.021 [-19.407, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.121 [-0.656, 1.401], loss: 10.120010, mae: 55.005695, mean_q: 73.479897
  443810/1100000: episode: 922, duration: 1.265s, episode steps: 190, steps per second: 150, episode reward: 240.557, mean reward: 1.266 [-14.552, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: -0.038 [-0.746, 1.447], loss: 5.988123, mae: 54.952477, mean_q: 73.025185
  444117/1100000: episode: 923, duration: 2.137s, episode steps: 307, steps per second: 144, episode reward: 274.827, mean reward: 0.895 [-11.294, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.108 [-1.031, 1.385], loss: 9.059722, mae: 54.550087, mean_q: 72.351562
  444360/1100000: episode: 924, duration: 1.665s, episode steps: 243, steps per second: 146, episode reward: 266.006, mean reward: 1.095 [-19.650, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.079 [-0.861, 1.391], loss: 9.341265, mae: 54.642048, mean_q: 73.022736
  444718/1100000: episode: 925, duration: 2.494s, episode steps: 358, steps per second: 144, episode reward: 267.348, mean reward: 0.747 [-9.377, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.105 [-0.828, 1.401], loss: 5.745622, mae: 55.125504, mean_q: 73.087502
  445040/1100000: episode: 926, duration: 2.213s, episode steps: 322, steps per second: 146, episode reward: 250.561, mean reward: 0.778 [-17.343, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.112 [-0.917, 1.485], loss: 5.995414, mae: 54.984726, mean_q: 72.940041
  445198/1100000: episode: 927, duration: 1.066s, episode steps: 158, steps per second: 148, episode reward: 25.027, mean reward: 0.158 [-100.000, 12.414], mean action: 1.892 [0.000, 3.000], mean observation: 0.033 [-0.673, 1.412], loss: 5.897265, mae: 55.196358, mean_q: 73.404755
  446013/1100000: episode: 928, duration: 5.763s, episode steps: 815, steps per second: 141, episode reward: 218.421, mean reward: 0.268 [-18.911, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.088 [-0.600, 1.419], loss: 6.581031, mae: 54.991699, mean_q: 73.173538
  446247/1100000: episode: 929, duration: 1.590s, episode steps: 234, steps per second: 147, episode reward: 289.296, mean reward: 1.236 [-10.421, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.095 [-0.921, 1.388], loss: 9.185537, mae: 55.018696, mean_q: 73.104904
  446440/1100000: episode: 930, duration: 1.283s, episode steps: 193, steps per second: 150, episode reward: -22.237, mean reward: -0.115 [-100.000, 9.068], mean action: 1.368 [0.000, 3.000], mean observation: -0.120 [-0.810, 1.531], loss: 6.340332, mae: 55.046284, mean_q: 72.947296
  446721/1100000: episode: 931, duration: 1.910s, episode steps: 281, steps per second: 147, episode reward: 274.832, mean reward: 0.978 [-23.855, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.097 [-0.853, 1.508], loss: 5.427581, mae: 54.714260, mean_q: 72.538231
  446915/1100000: episode: 932, duration: 1.304s, episode steps: 194, steps per second: 149, episode reward: 269.066, mean reward: 1.387 [-9.462, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.083 [-0.860, 1.398], loss: 8.144912, mae: 55.310909, mean_q: 73.941475
  447015/1100000: episode: 933, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: -53.345, mean reward: -0.533 [-100.000, 15.623], mean action: 1.730 [0.000, 3.000], mean observation: 0.168 [-0.967, 1.400], loss: 10.559404, mae: 55.159931, mean_q: 73.243767
  447188/1100000: episode: 934, duration: 1.164s, episode steps: 173, steps per second: 149, episode reward: 285.320, mean reward: 1.649 [-10.316, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.043 [-0.727, 1.388], loss: 10.283077, mae: 54.696548, mean_q: 72.935623
  447565/1100000: episode: 935, duration: 2.621s, episode steps: 377, steps per second: 144, episode reward: 203.052, mean reward: 0.539 [-24.303, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.149 [-0.824, 1.409], loss: 11.845229, mae: 55.190289, mean_q: 73.197701
  447675/1100000: episode: 936, duration: 0.739s, episode steps: 110, steps per second: 149, episode reward: -141.745, mean reward: -1.289 [-100.000, 2.590], mean action: 1.791 [0.000, 3.000], mean observation: -0.058 [-1.158, 1.407], loss: 6.970014, mae: 54.837799, mean_q: 73.349976
  448116/1100000: episode: 937, duration: 3.060s, episode steps: 441, steps per second: 144, episode reward: 238.069, mean reward: 0.540 [-17.695, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.139 [-0.864, 1.427], loss: 10.405032, mae: 54.171982, mean_q: 72.209267
  448501/1100000: episode: 938, duration: 2.688s, episode steps: 385, steps per second: 143, episode reward: 254.861, mean reward: 0.662 [-17.771, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.132 [-0.873, 1.397], loss: 4.842995, mae: 54.815361, mean_q: 73.020714
  448898/1100000: episode: 939, duration: 2.738s, episode steps: 397, steps per second: 145, episode reward: 291.947, mean reward: 0.735 [-3.191, 100.000], mean action: 0.680 [0.000, 3.000], mean observation: 0.151 [-0.855, 1.486], loss: 5.737479, mae: 54.611279, mean_q: 72.974243
  449253/1100000: episode: 940, duration: 2.434s, episode steps: 355, steps per second: 146, episode reward: 225.811, mean reward: 0.636 [-20.120, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.073 [-1.464, 1.499], loss: 10.102530, mae: 54.657566, mean_q: 72.475441
  449519/1100000: episode: 941, duration: 1.788s, episode steps: 266, steps per second: 149, episode reward: 273.654, mean reward: 1.029 [-17.751, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.120 [-0.814, 1.404], loss: 11.034335, mae: 54.692589, mean_q: 73.025764
  449908/1100000: episode: 942, duration: 2.688s, episode steps: 389, steps per second: 145, episode reward: -155.404, mean reward: -0.399 [-100.000, 15.346], mean action: 1.643 [0.000, 3.000], mean observation: 0.245 [-0.855, 1.415], loss: 12.143361, mae: 54.923824, mean_q: 73.078056
  450220/1100000: episode: 943, duration: 2.134s, episode steps: 312, steps per second: 146, episode reward: 240.748, mean reward: 0.772 [-18.322, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: -0.011 [-0.685, 1.391], loss: 9.366870, mae: 54.974396, mean_q: 73.625069
  450666/1100000: episode: 944, duration: 3.265s, episode steps: 446, steps per second: 137, episode reward: 227.529, mean reward: 0.510 [-11.306, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: -0.021 [-0.768, 1.496], loss: 10.726541, mae: 54.997730, mean_q: 73.287895
  451104/1100000: episode: 945, duration: 3.120s, episode steps: 438, steps per second: 140, episode reward: 193.426, mean reward: 0.442 [-16.170, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.113 [-0.898, 1.470], loss: 8.504346, mae: 54.758476, mean_q: 72.881607
  451365/1100000: episode: 946, duration: 1.785s, episode steps: 261, steps per second: 146, episode reward: 254.259, mean reward: 0.974 [-8.548, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.068 [-0.752, 1.470], loss: 6.158860, mae: 54.722439, mean_q: 72.610916
  451709/1100000: episode: 947, duration: 2.385s, episode steps: 344, steps per second: 144, episode reward: 239.099, mean reward: 0.695 [-17.815, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: -0.003 [-0.822, 1.397], loss: 8.304923, mae: 54.538296, mean_q: 72.618904
  451950/1100000: episode: 948, duration: 1.627s, episode steps: 241, steps per second: 148, episode reward: 263.924, mean reward: 1.095 [-4.164, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.216 [-0.694, 1.396], loss: 5.000460, mae: 54.774979, mean_q: 72.925339
  452105/1100000: episode: 949, duration: 1.035s, episode steps: 155, steps per second: 150, episode reward: -73.682, mean reward: -0.475 [-100.000, 4.215], mean action: 1.890 [0.000, 3.000], mean observation: 0.001 [-1.000, 1.510], loss: 7.165832, mae: 55.270092, mean_q: 74.159538
  452453/1100000: episode: 950, duration: 2.421s, episode steps: 348, steps per second: 144, episode reward: 205.866, mean reward: 0.592 [-21.068, 100.000], mean action: 2.138 [0.000, 3.000], mean observation: 0.261 [-0.805, 1.425], loss: 6.618366, mae: 55.186020, mean_q: 73.511360
  452759/1100000: episode: 951, duration: 2.105s, episode steps: 306, steps per second: 145, episode reward: -148.214, mean reward: -0.484 [-100.000, 21.531], mean action: 2.098 [0.000, 3.000], mean observation: 0.258 [-0.819, 1.392], loss: 7.302545, mae: 54.902138, mean_q: 73.371475
  452916/1100000: episode: 952, duration: 1.058s, episode steps: 157, steps per second: 148, episode reward: -96.142, mean reward: -0.612 [-100.000, 14.000], mean action: 1.815 [0.000, 3.000], mean observation: 0.009 [-1.001, 1.466], loss: 8.716085, mae: 54.945312, mean_q: 73.387627
  453197/1100000: episode: 953, duration: 1.945s, episode steps: 281, steps per second: 144, episode reward: -61.977, mean reward: -0.221 [-100.000, 12.314], mean action: 1.669 [0.000, 3.000], mean observation: 0.124 [-1.100, 1.388], loss: 7.710136, mae: 55.065132, mean_q: 73.365746
  453474/1100000: episode: 954, duration: 1.935s, episode steps: 277, steps per second: 143, episode reward: 263.785, mean reward: 0.952 [-9.723, 100.000], mean action: 2.032 [0.000, 3.000], mean observation: -0.021 [-0.757, 1.399], loss: 10.942425, mae: 55.048725, mean_q: 73.411438
  453746/1100000: episode: 955, duration: 1.880s, episode steps: 272, steps per second: 145, episode reward: 231.457, mean reward: 0.851 [-17.594, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: -0.028 [-0.785, 1.399], loss: 5.228434, mae: 55.726013, mean_q: 74.430115
  454210/1100000: episode: 956, duration: 3.278s, episode steps: 464, steps per second: 142, episode reward: 243.902, mean reward: 0.526 [-19.183, 100.000], mean action: 0.761 [0.000, 3.000], mean observation: 0.128 [-0.818, 1.408], loss: 4.842569, mae: 55.317509, mean_q: 73.696335
  454684/1100000: episode: 957, duration: 3.195s, episode steps: 474, steps per second: 148, episode reward: 247.117, mean reward: 0.521 [-20.321, 100.000], mean action: 0.719 [0.000, 3.000], mean observation: 0.232 [-0.852, 1.400], loss: 8.808951, mae: 55.801640, mean_q: 74.576759
  454929/1100000: episode: 958, duration: 1.658s, episode steps: 245, steps per second: 148, episode reward: 281.432, mean reward: 1.149 [-2.334, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.116 [-0.817, 1.401], loss: 5.763631, mae: 55.960636, mean_q: 74.305580
  455176/1100000: episode: 959, duration: 1.655s, episode steps: 247, steps per second: 149, episode reward: 272.166, mean reward: 1.102 [-7.962, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.097 [-0.778, 1.441], loss: 14.558848, mae: 55.794193, mean_q: 74.083359
  455521/1100000: episode: 960, duration: 2.400s, episode steps: 345, steps per second: 144, episode reward: 262.248, mean reward: 0.760 [-20.013, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.202 [-0.822, 1.404], loss: 6.847796, mae: 55.867699, mean_q: 74.440735
  455848/1100000: episode: 961, duration: 2.236s, episode steps: 327, steps per second: 146, episode reward: 246.858, mean reward: 0.755 [-10.443, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.093 [-0.787, 1.430], loss: 8.934421, mae: 55.677776, mean_q: 73.897514
  456157/1100000: episode: 962, duration: 2.123s, episode steps: 309, steps per second: 146, episode reward: 247.422, mean reward: 0.801 [-17.111, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: -0.034 [-1.215, 1.395], loss: 7.140809, mae: 55.930748, mean_q: 74.564064
  456380/1100000: episode: 963, duration: 1.549s, episode steps: 223, steps per second: 144, episode reward: -183.929, mean reward: -0.825 [-100.000, 25.307], mean action: 1.991 [0.000, 3.000], mean observation: 0.066 [-0.881, 1.403], loss: 5.495381, mae: 56.589954, mean_q: 75.221024
  457036/1100000: episode: 964, duration: 4.695s, episode steps: 656, steps per second: 140, episode reward: -74.369, mean reward: -0.113 [-100.000, 19.368], mean action: 1.566 [0.000, 3.000], mean observation: 0.271 [-0.800, 1.395], loss: 9.275321, mae: 56.467678, mean_q: 75.127312
  457331/1100000: episode: 965, duration: 1.998s, episode steps: 295, steps per second: 148, episode reward: 252.849, mean reward: 0.857 [-17.740, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.058 [-0.902, 1.446], loss: 10.671348, mae: 56.249863, mean_q: 74.947258
  458049/1100000: episode: 966, duration: 5.264s, episode steps: 718, steps per second: 136, episode reward: 255.255, mean reward: 0.356 [-19.761, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.074 [-1.211, 1.394], loss: 8.540706, mae: 55.768780, mean_q: 74.514763
  458462/1100000: episode: 967, duration: 2.867s, episode steps: 413, steps per second: 144, episode reward: 286.314, mean reward: 0.693 [-10.891, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: -0.004 [-0.791, 1.396], loss: 4.945695, mae: 55.828720, mean_q: 74.620117
  458887/1100000: episode: 968, duration: 2.884s, episode steps: 425, steps per second: 147, episode reward: 236.273, mean reward: 0.556 [-18.148, 100.000], mean action: 0.845 [0.000, 3.000], mean observation: 0.176 [-1.325, 1.516], loss: 7.401867, mae: 56.130383, mean_q: 74.910233
  459409/1100000: episode: 969, duration: 3.757s, episode steps: 522, steps per second: 139, episode reward: 233.214, mean reward: 0.447 [-19.099, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.192 [-0.747, 1.466], loss: 8.779755, mae: 55.703747, mean_q: 73.988388
  460409/1100000: episode: 970, duration: 6.882s, episode steps: 1000, steps per second: 145, episode reward: 108.625, mean reward: 0.109 [-20.694, 21.189], mean action: 1.136 [0.000, 3.000], mean observation: -0.001 [-0.817, 1.406], loss: 8.755392, mae: 55.437851, mean_q: 73.966377
  460740/1100000: episode: 971, duration: 2.317s, episode steps: 331, steps per second: 143, episode reward: 230.380, mean reward: 0.696 [-13.834, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.126 [-0.613, 1.414], loss: 9.036527, mae: 55.295578, mean_q: 73.883240
  461290/1100000: episode: 972, duration: 3.788s, episode steps: 550, steps per second: 145, episode reward: 201.017, mean reward: 0.365 [-17.684, 100.000], mean action: 1.685 [0.000, 3.000], mean observation: 0.009 [-0.855, 1.407], loss: 8.077761, mae: 55.110970, mean_q: 73.423241
  461537/1100000: episode: 973, duration: 1.670s, episode steps: 247, steps per second: 148, episode reward: 254.324, mean reward: 1.030 [-18.241, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.087 [-0.764, 1.402], loss: 11.937842, mae: 55.048428, mean_q: 73.396347
  462537/1100000: episode: 974, duration: 7.335s, episode steps: 1000, steps per second: 136, episode reward: 59.774, mean reward: 0.060 [-25.548, 24.229], mean action: 2.043 [0.000, 3.000], mean observation: 0.150 [-0.925, 1.476], loss: 6.947221, mae: 55.462139, mean_q: 73.842125
  462791/1100000: episode: 975, duration: 1.696s, episode steps: 254, steps per second: 150, episode reward: 259.139, mean reward: 1.020 [-7.459, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.067 [-0.672, 1.470], loss: 8.129952, mae: 55.861244, mean_q: 74.483757
  462931/1100000: episode: 976, duration: 0.944s, episode steps: 140, steps per second: 148, episode reward: -129.903, mean reward: -0.928 [-100.000, 6.639], mean action: 1.643 [0.000, 3.000], mean observation: -0.015 [-3.226, 1.408], loss: 8.543541, mae: 55.618687, mean_q: 74.307083
  463176/1100000: episode: 977, duration: 1.652s, episode steps: 245, steps per second: 148, episode reward: 239.175, mean reward: 0.976 [-15.217, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: -0.017 [-0.774, 1.411], loss: 19.110601, mae: 56.048229, mean_q: 74.633835
  463465/1100000: episode: 978, duration: 1.974s, episode steps: 289, steps per second: 146, episode reward: 272.738, mean reward: 0.944 [-13.384, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.053 [-0.925, 1.394], loss: 9.705028, mae: 55.923965, mean_q: 74.592651
  463807/1100000: episode: 979, duration: 2.362s, episode steps: 342, steps per second: 145, episode reward: 264.873, mean reward: 0.774 [-17.455, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.104 [-0.962, 1.396], loss: 7.407428, mae: 55.896152, mean_q: 74.529213
  464363/1100000: episode: 980, duration: 3.894s, episode steps: 556, steps per second: 143, episode reward: 259.986, mean reward: 0.468 [-18.685, 100.000], mean action: 0.725 [0.000, 3.000], mean observation: 0.133 [-0.932, 1.486], loss: 5.515825, mae: 55.545467, mean_q: 74.023849
  464599/1100000: episode: 981, duration: 1.594s, episode steps: 236, steps per second: 148, episode reward: 237.026, mean reward: 1.004 [-16.369, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: -0.028 [-0.976, 1.492], loss: 8.548453, mae: 55.736523, mean_q: 74.280983
  465115/1100000: episode: 982, duration: 3.667s, episode steps: 516, steps per second: 141, episode reward: 223.428, mean reward: 0.433 [-19.388, 100.000], mean action: 2.498 [0.000, 3.000], mean observation: 0.038 [-0.773, 1.527], loss: 7.254172, mae: 55.581207, mean_q: 73.959412
  465270/1100000: episode: 983, duration: 1.034s, episode steps: 155, steps per second: 150, episode reward: 7.941, mean reward: 0.051 [-100.000, 8.537], mean action: 1.626 [0.000, 3.000], mean observation: -0.074 [-0.672, 1.481], loss: 6.076141, mae: 55.360382, mean_q: 73.724548
  465396/1100000: episode: 984, duration: 0.849s, episode steps: 126, steps per second: 148, episode reward: 9.496, mean reward: 0.075 [-100.000, 11.485], mean action: 1.683 [0.000, 3.000], mean observation: 0.133 [-0.764, 1.402], loss: 9.821145, mae: 55.330162, mean_q: 73.005539
  465731/1100000: episode: 985, duration: 2.280s, episode steps: 335, steps per second: 147, episode reward: 231.851, mean reward: 0.692 [-20.206, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: -0.022 [-0.803, 1.409], loss: 6.771702, mae: 56.560986, mean_q: 75.061378
  466007/1100000: episode: 986, duration: 1.872s, episode steps: 276, steps per second: 147, episode reward: 266.152, mean reward: 0.964 [-10.277, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.034 [-0.751, 1.467], loss: 8.020657, mae: 55.937489, mean_q: 74.124855
  466524/1100000: episode: 987, duration: 3.560s, episode steps: 517, steps per second: 145, episode reward: 269.824, mean reward: 0.522 [-18.701, 100.000], mean action: 0.795 [0.000, 3.000], mean observation: 0.236 [-1.225, 1.387], loss: 8.000023, mae: 55.585548, mean_q: 73.856316
  466785/1100000: episode: 988, duration: 1.787s, episode steps: 261, steps per second: 146, episode reward: 220.481, mean reward: 0.845 [-9.211, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: -0.045 [-1.130, 1.397], loss: 7.766268, mae: 56.186905, mean_q: 74.463928
  466879/1100000: episode: 989, duration: 0.623s, episode steps: 94, steps per second: 151, episode reward: -126.285, mean reward: -1.343 [-100.000, 9.659], mean action: 1.628 [0.000, 3.000], mean observation: 0.101 [-1.187, 3.133], loss: 13.665887, mae: 55.827053, mean_q: 74.443245
  467485/1100000: episode: 990, duration: 4.261s, episode steps: 606, steps per second: 142, episode reward: -174.762, mean reward: -0.288 [-100.000, 12.092], mean action: 1.919 [0.000, 3.000], mean observation: 0.207 [-0.608, 1.739], loss: 7.102077, mae: 55.977154, mean_q: 74.265709
  468346/1100000: episode: 991, duration: 6.317s, episode steps: 861, steps per second: 136, episode reward: 216.063, mean reward: 0.251 [-19.604, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: 0.070 [-0.966, 1.443], loss: 7.683824, mae: 55.934124, mean_q: 74.363312
  468741/1100000: episode: 992, duration: 2.739s, episode steps: 395, steps per second: 144, episode reward: 221.502, mean reward: 0.561 [-16.160, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: -0.024 [-0.756, 1.410], loss: 9.069546, mae: 55.886913, mean_q: 74.140099
  468890/1100000: episode: 993, duration: 0.999s, episode steps: 149, steps per second: 149, episode reward: 50.764, mean reward: 0.341 [-100.000, 18.628], mean action: 1.591 [0.000, 3.000], mean observation: 0.052 [-1.004, 1.394], loss: 9.744905, mae: 55.870892, mean_q: 74.493477
  469225/1100000: episode: 994, duration: 2.316s, episode steps: 335, steps per second: 145, episode reward: 267.354, mean reward: 0.798 [-16.962, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.024 [-0.871, 1.403], loss: 7.925390, mae: 55.769115, mean_q: 74.401596
  469502/1100000: episode: 995, duration: 1.874s, episode steps: 277, steps per second: 148, episode reward: 257.775, mean reward: 0.931 [-9.051, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.061 [-1.142, 1.468], loss: 6.789439, mae: 55.684582, mean_q: 74.245361
  470249/1100000: episode: 996, duration: 5.427s, episode steps: 747, steps per second: 138, episode reward: 184.415, mean reward: 0.247 [-24.693, 100.000], mean action: 1.855 [0.000, 3.000], mean observation: 0.074 [-0.762, 1.396], loss: 9.150499, mae: 55.847473, mean_q: 74.154266
  470425/1100000: episode: 997, duration: 1.179s, episode steps: 176, steps per second: 149, episode reward: -101.868, mean reward: -0.579 [-100.000, 24.218], mean action: 1.540 [0.000, 3.000], mean observation: 0.091 [-1.565, 1.413], loss: 6.393352, mae: 56.553673, mean_q: 75.234489
  470831/1100000: episode: 998, duration: 2.821s, episode steps: 406, steps per second: 144, episode reward: -165.948, mean reward: -0.409 [-100.000, 24.810], mean action: 1.505 [0.000, 3.000], mean observation: 0.163 [-1.239, 1.420], loss: 8.197778, mae: 56.097713, mean_q: 74.675194
  471399/1100000: episode: 999, duration: 3.975s, episode steps: 568, steps per second: 143, episode reward: 237.989, mean reward: 0.419 [-17.293, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: -0.033 [-0.734, 1.389], loss: 9.802762, mae: 56.250401, mean_q: 74.611519
  471654/1100000: episode: 1000, duration: 1.738s, episode steps: 255, steps per second: 147, episode reward: 272.729, mean reward: 1.070 [-10.754, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.177 [-0.759, 1.388], loss: 9.048935, mae: 56.448521, mean_q: 74.814697
  472192/1100000: episode: 1001, duration: 3.707s, episode steps: 538, steps per second: 145, episode reward: 223.568, mean reward: 0.416 [-21.740, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: -0.006 [-0.675, 1.400], loss: 7.185370, mae: 56.560730, mean_q: 75.460457
  472591/1100000: episode: 1002, duration: 2.737s, episode steps: 399, steps per second: 146, episode reward: 242.790, mean reward: 0.608 [-18.309, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.189 [-0.982, 1.388], loss: 8.326062, mae: 56.029140, mean_q: 74.164001
  472957/1100000: episode: 1003, duration: 2.509s, episode steps: 366, steps per second: 146, episode reward: 251.564, mean reward: 0.687 [-17.342, 100.000], mean action: 0.749 [0.000, 3.000], mean observation: 0.143 [-1.111, 1.403], loss: 10.234563, mae: 55.872395, mean_q: 74.238556
  473297/1100000: episode: 1004, duration: 2.300s, episode steps: 340, steps per second: 148, episode reward: 226.916, mean reward: 0.667 [-19.481, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.206 [-0.813, 1.430], loss: 10.286763, mae: 56.049816, mean_q: 74.188278
  474297/1100000: episode: 1005, duration: 7.422s, episode steps: 1000, steps per second: 135, episode reward: 121.331, mean reward: 0.121 [-20.216, 21.892], mean action: 1.012 [0.000, 3.000], mean observation: 0.055 [-0.724, 1.429], loss: 8.016151, mae: 56.399986, mean_q: 74.970261
  474559/1100000: episode: 1006, duration: 1.796s, episode steps: 262, steps per second: 146, episode reward: 255.941, mean reward: 0.977 [-3.715, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.127 [-1.142, 1.415], loss: 5.923675, mae: 55.892799, mean_q: 74.689125
  474797/1100000: episode: 1007, duration: 1.619s, episode steps: 238, steps per second: 147, episode reward: -47.163, mean reward: -0.198 [-100.000, 7.870], mean action: 1.689 [0.000, 3.000], mean observation: 0.046 [-1.353, 1.396], loss: 7.767739, mae: 56.342575, mean_q: 75.270424
  475371/1100000: episode: 1008, duration: 3.946s, episode steps: 574, steps per second: 145, episode reward: 185.989, mean reward: 0.324 [-19.708, 100.000], mean action: 0.899 [0.000, 3.000], mean observation: 0.145 [-1.009, 1.409], loss: 11.024796, mae: 55.780876, mean_q: 74.274261
  475769/1100000: episode: 1009, duration: 2.721s, episode steps: 398, steps per second: 146, episode reward: 270.288, mean reward: 0.679 [-20.123, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: -0.003 [-0.733, 1.392], loss: 9.575278, mae: 55.756145, mean_q: 74.093102
  476235/1100000: episode: 1010, duration: 3.415s, episode steps: 466, steps per second: 136, episode reward: 215.240, mean reward: 0.462 [-20.355, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.067 [-0.775, 1.403], loss: 8.123395, mae: 56.066170, mean_q: 74.608513
  476662/1100000: episode: 1011, duration: 2.925s, episode steps: 427, steps per second: 146, episode reward: 224.801, mean reward: 0.526 [-14.685, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.217 [-1.042, 1.397], loss: 10.343307, mae: 56.133846, mean_q: 75.031700
  477662/1100000: episode: 1012, duration: 7.392s, episode steps: 1000, steps per second: 135, episode reward: 92.550, mean reward: 0.093 [-25.782, 21.771], mean action: 1.249 [0.000, 3.000], mean observation: 0.195 [-0.878, 1.441], loss: 6.838520, mae: 55.684765, mean_q: 74.006409
  478115/1100000: episode: 1013, duration: 3.132s, episode steps: 453, steps per second: 145, episode reward: 259.854, mean reward: 0.574 [-10.099, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.118 [-0.649, 1.494], loss: 8.118299, mae: 54.976002, mean_q: 73.078415
  478572/1100000: episode: 1014, duration: 3.202s, episode steps: 457, steps per second: 143, episode reward: 276.366, mean reward: 0.605 [-20.351, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.110 [-0.800, 1.397], loss: 7.507052, mae: 55.316471, mean_q: 73.961678
  478966/1100000: episode: 1015, duration: 2.780s, episode steps: 394, steps per second: 142, episode reward: 234.011, mean reward: 0.594 [-17.923, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.077 [-0.860, 1.444], loss: 7.564646, mae: 54.919510, mean_q: 73.442917
  479340/1100000: episode: 1016, duration: 2.601s, episode steps: 374, steps per second: 144, episode reward: 259.863, mean reward: 0.695 [-20.429, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.079 [-0.798, 1.396], loss: 8.535139, mae: 55.274120, mean_q: 73.756989
  479545/1100000: episode: 1017, duration: 1.373s, episode steps: 205, steps per second: 149, episode reward: 283.179, mean reward: 1.381 [-24.416, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.081 [-0.895, 1.387], loss: 8.844101, mae: 55.023022, mean_q: 73.262726
  479883/1100000: episode: 1018, duration: 2.321s, episode steps: 338, steps per second: 146, episode reward: 259.143, mean reward: 0.767 [-17.916, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: -0.028 [-0.772, 1.444], loss: 6.663274, mae: 55.414413, mean_q: 74.031151
  480261/1100000: episode: 1019, duration: 2.655s, episode steps: 378, steps per second: 142, episode reward: 203.003, mean reward: 0.537 [-10.819, 100.000], mean action: 1.788 [0.000, 3.000], mean observation: 0.005 [-0.787, 1.424], loss: 7.856463, mae: 55.321964, mean_q: 73.879684
  480883/1100000: episode: 1020, duration: 4.689s, episode steps: 622, steps per second: 133, episode reward: 205.493, mean reward: 0.330 [-18.102, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.033 [-1.077, 1.522], loss: 5.044663, mae: 54.951153, mean_q: 73.487740
  481200/1100000: episode: 1021, duration: 2.181s, episode steps: 317, steps per second: 145, episode reward: 263.493, mean reward: 0.831 [-17.670, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.132 [-0.642, 1.397], loss: 9.883527, mae: 54.845795, mean_q: 73.073112
  481551/1100000: episode: 1022, duration: 2.537s, episode steps: 351, steps per second: 138, episode reward: 252.382, mean reward: 0.719 [-9.924, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: 0.082 [-0.738, 1.403], loss: 5.540479, mae: 55.211750, mean_q: 73.675873
  482085/1100000: episode: 1023, duration: 3.782s, episode steps: 534, steps per second: 141, episode reward: 207.187, mean reward: 0.388 [-22.919, 100.000], mean action: 2.094 [0.000, 3.000], mean observation: 0.112 [-1.121, 1.522], loss: 6.148376, mae: 54.647072, mean_q: 72.945267
  482443/1100000: episode: 1024, duration: 2.462s, episode steps: 358, steps per second: 145, episode reward: 266.190, mean reward: 0.744 [-17.537, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.130 [-0.878, 1.451], loss: 10.807918, mae: 54.534492, mean_q: 72.699745
  482842/1100000: episode: 1025, duration: 2.847s, episode steps: 399, steps per second: 140, episode reward: 266.386, mean reward: 0.668 [-19.313, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: -0.069 [-0.830, 1.430], loss: 7.582126, mae: 54.618950, mean_q: 73.066658
  483842/1100000: episode: 1026, duration: 7.289s, episode steps: 1000, steps per second: 137, episode reward: 105.630, mean reward: 0.106 [-20.086, 23.234], mean action: 1.323 [0.000, 3.000], mean observation: 0.247 [-1.039, 1.479], loss: 6.024825, mae: 54.282467, mean_q: 72.445686
  484173/1100000: episode: 1027, duration: 2.236s, episode steps: 331, steps per second: 148, episode reward: 218.967, mean reward: 0.662 [-10.546, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.188 [-0.619, 1.433], loss: 6.168972, mae: 54.279491, mean_q: 72.595634
  484937/1100000: episode: 1028, duration: 5.831s, episode steps: 764, steps per second: 131, episode reward: 188.327, mean reward: 0.247 [-19.391, 100.000], mean action: 2.395 [0.000, 3.000], mean observation: 0.217 [-0.653, 1.393], loss: 5.515766, mae: 53.896439, mean_q: 72.028336
  485081/1100000: episode: 1029, duration: 0.948s, episode steps: 144, steps per second: 152, episode reward: 13.129, mean reward: 0.091 [-100.000, 14.719], mean action: 1.354 [0.000, 3.000], mean observation: -0.030 [-0.773, 1.482], loss: 5.259272, mae: 53.223434, mean_q: 70.975098
  485256/1100000: episode: 1030, duration: 1.171s, episode steps: 175, steps per second: 149, episode reward: -178.158, mean reward: -1.018 [-100.000, 6.792], mean action: 1.857 [0.000, 3.000], mean observation: 0.047 [-2.806, 1.428], loss: 9.317408, mae: 53.983517, mean_q: 71.973747
  485625/1100000: episode: 1031, duration: 2.519s, episode steps: 369, steps per second: 146, episode reward: -210.507, mean reward: -0.570 [-100.000, 11.678], mean action: 1.474 [0.000, 3.000], mean observation: -0.002 [-1.412, 1.419], loss: 5.114978, mae: 53.862392, mean_q: 71.864075
  486010/1100000: episode: 1032, duration: 2.649s, episode steps: 385, steps per second: 145, episode reward: 238.188, mean reward: 0.619 [-17.288, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.029 [-0.767, 1.431], loss: 8.260026, mae: 53.966949, mean_q: 71.931091
  486307/1100000: episode: 1033, duration: 2.023s, episode steps: 297, steps per second: 147, episode reward: 223.987, mean reward: 0.754 [-14.091, 100.000], mean action: 1.970 [0.000, 3.000], mean observation: 0.193 [-0.751, 1.399], loss: 7.331728, mae: 53.509010, mean_q: 71.504181
  486860/1100000: episode: 1034, duration: 3.861s, episode steps: 553, steps per second: 143, episode reward: 203.942, mean reward: 0.369 [-16.465, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.151 [-0.971, 1.402], loss: 6.864939, mae: 53.559704, mean_q: 71.441422
  487104/1100000: episode: 1035, duration: 1.638s, episode steps: 244, steps per second: 149, episode reward: 209.999, mean reward: 0.861 [-14.284, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.188 [-0.756, 1.407], loss: 5.559732, mae: 53.911102, mean_q: 71.891472
  487279/1100000: episode: 1036, duration: 1.161s, episode steps: 175, steps per second: 151, episode reward: 37.686, mean reward: 0.215 [-100.000, 17.676], mean action: 1.446 [0.000, 3.000], mean observation: 0.001 [-0.810, 1.554], loss: 8.396121, mae: 53.543339, mean_q: 71.567986
  487499/1100000: episode: 1037, duration: 1.479s, episode steps: 220, steps per second: 149, episode reward: 237.362, mean reward: 1.079 [-2.879, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.065 [-0.753, 1.419], loss: 6.518955, mae: 53.638100, mean_q: 71.419937
  487829/1100000: episode: 1038, duration: 2.238s, episode steps: 330, steps per second: 147, episode reward: 207.469, mean reward: 0.629 [-13.728, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.123 [-1.139, 1.402], loss: 4.766837, mae: 53.647141, mean_q: 71.348000
  488133/1100000: episode: 1039, duration: 2.059s, episode steps: 304, steps per second: 148, episode reward: -114.526, mean reward: -0.377 [-100.000, 14.144], mean action: 1.724 [0.000, 3.000], mean observation: 0.178 [-0.931, 1.390], loss: 6.716904, mae: 54.025925, mean_q: 72.006645
  488399/1100000: episode: 1040, duration: 1.799s, episode steps: 266, steps per second: 148, episode reward: 269.939, mean reward: 1.015 [-9.940, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.081 [-0.912, 1.512], loss: 6.344398, mae: 53.496101, mean_q: 71.218758
  488695/1100000: episode: 1041, duration: 1.994s, episode steps: 296, steps per second: 148, episode reward: 204.847, mean reward: 0.692 [-14.340, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: 0.065 [-0.828, 1.420], loss: 8.840663, mae: 53.997936, mean_q: 72.001671
  488991/1100000: episode: 1042, duration: 1.995s, episode steps: 296, steps per second: 148, episode reward: 198.700, mean reward: 0.671 [-10.164, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.068 [-0.776, 1.449], loss: 8.179352, mae: 53.954735, mean_q: 71.593216
  489220/1100000: episode: 1043, duration: 1.527s, episode steps: 229, steps per second: 150, episode reward: -74.896, mean reward: -0.327 [-100.000, 13.699], mean action: 1.498 [0.000, 3.000], mean observation: 0.037 [-1.911, 1.503], loss: 6.739878, mae: 53.717693, mean_q: 71.585617
  489780/1100000: episode: 1044, duration: 3.995s, episode steps: 560, steps per second: 140, episode reward: 93.689, mean reward: 0.167 [-18.311, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.215 [-1.527, 1.419], loss: 6.546511, mae: 53.756996, mean_q: 71.494331
  489940/1100000: episode: 1045, duration: 1.079s, episode steps: 160, steps per second: 148, episode reward: -105.019, mean reward: -0.656 [-100.000, 13.664], mean action: 1.913 [0.000, 3.000], mean observation: 0.148 [-2.961, 1.456], loss: 4.770439, mae: 52.486641, mean_q: 69.917168
  490285/1100000: episode: 1046, duration: 2.379s, episode steps: 345, steps per second: 145, episode reward: 261.552, mean reward: 0.758 [-2.863, 100.000], mean action: 2.110 [0.000, 3.000], mean observation: 0.013 [-0.709, 1.395], loss: 6.050670, mae: 53.126556, mean_q: 70.964844
  490705/1100000: episode: 1047, duration: 2.956s, episode steps: 420, steps per second: 142, episode reward: 266.569, mean reward: 0.635 [-4.437, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.047 [-0.695, 1.412], loss: 6.769156, mae: 53.439327, mean_q: 71.261070
  491172/1100000: episode: 1048, duration: 3.220s, episode steps: 467, steps per second: 145, episode reward: 288.872, mean reward: 0.619 [-17.996, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.058 [-0.810, 1.491], loss: 7.329848, mae: 53.573273, mean_q: 71.304329
  491439/1100000: episode: 1049, duration: 1.841s, episode steps: 267, steps per second: 145, episode reward: 287.319, mean reward: 1.076 [-11.023, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.067 [-0.763, 1.399], loss: 7.925787, mae: 53.482437, mean_q: 71.001167
  491587/1100000: episode: 1050, duration: 0.978s, episode steps: 148, steps per second: 151, episode reward: 4.619, mean reward: 0.031 [-100.000, 13.761], mean action: 1.196 [0.000, 3.000], mean observation: 0.025 [-1.490, 1.430], loss: 10.444806, mae: 53.289783, mean_q: 70.636719
  491903/1100000: episode: 1051, duration: 2.181s, episode steps: 316, steps per second: 145, episode reward: 282.774, mean reward: 0.895 [-17.341, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.058 [-0.681, 1.388], loss: 6.531570, mae: 53.345989, mean_q: 70.950035
  492178/1100000: episode: 1052, duration: 1.853s, episode steps: 275, steps per second: 148, episode reward: 254.430, mean reward: 0.925 [-14.530, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.194 [-0.838, 1.392], loss: 6.158147, mae: 52.920151, mean_q: 70.574265
  492327/1100000: episode: 1053, duration: 0.996s, episode steps: 149, steps per second: 150, episode reward: 2.493, mean reward: 0.017 [-100.000, 19.080], mean action: 1.651 [0.000, 3.000], mean observation: 0.117 [-0.844, 1.410], loss: 7.518613, mae: 53.206097, mean_q: 70.990349
  492569/1100000: episode: 1054, duration: 1.648s, episode steps: 242, steps per second: 147, episode reward: 231.743, mean reward: 0.958 [-9.194, 100.000], mean action: 2.236 [0.000, 3.000], mean observation: 0.028 [-0.660, 1.477], loss: 8.082325, mae: 52.450340, mean_q: 69.651329
  493026/1100000: episode: 1055, duration: 3.177s, episode steps: 457, steps per second: 144, episode reward: -198.008, mean reward: -0.433 [-100.000, 36.860], mean action: 1.451 [0.000, 3.000], mean observation: 0.016 [-1.583, 1.446], loss: 7.840878, mae: 53.070469, mean_q: 70.681122
  493423/1100000: episode: 1056, duration: 2.862s, episode steps: 397, steps per second: 139, episode reward: 245.900, mean reward: 0.619 [-19.567, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.074 [-0.786, 1.463], loss: 7.323799, mae: 53.033733, mean_q: 70.435242
  493637/1100000: episode: 1057, duration: 1.417s, episode steps: 214, steps per second: 151, episode reward: 266.626, mean reward: 1.246 [-9.822, 100.000], mean action: 0.963 [0.000, 3.000], mean observation: 0.048 [-0.895, 1.519], loss: 5.257210, mae: 52.718399, mean_q: 70.042595
  494125/1100000: episode: 1058, duration: 3.395s, episode steps: 488, steps per second: 144, episode reward: 167.361, mean reward: 0.343 [-22.755, 100.000], mean action: 2.172 [0.000, 3.000], mean observation: 0.155 [-0.806, 1.400], loss: 6.146078, mae: 53.198147, mean_q: 70.874557
  494825/1100000: episode: 1059, duration: 4.973s, episode steps: 700, steps per second: 141, episode reward: 226.307, mean reward: 0.323 [-23.875, 100.000], mean action: 0.629 [0.000, 3.000], mean observation: 0.131 [-0.841, 1.388], loss: 8.014024, mae: 53.496223, mean_q: 70.857643
  494957/1100000: episode: 1060, duration: 0.883s, episode steps: 132, steps per second: 149, episode reward: -59.496, mean reward: -0.451 [-100.000, 6.736], mean action: 1.864 [0.000, 3.000], mean observation: -0.047 [-0.839, 1.413], loss: 6.915361, mae: 52.903141, mean_q: 69.761864
  495318/1100000: episode: 1061, duration: 2.504s, episode steps: 361, steps per second: 144, episode reward: 246.274, mean reward: 0.682 [-20.819, 100.000], mean action: 1.884 [0.000, 3.000], mean observation: 0.009 [-0.826, 1.537], loss: 7.125595, mae: 53.582291, mean_q: 71.170609
  495631/1100000: episode: 1062, duration: 2.164s, episode steps: 313, steps per second: 145, episode reward: 228.394, mean reward: 0.730 [-16.472, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.015 [-1.147, 1.397], loss: 7.027587, mae: 53.506046, mean_q: 70.935280
  496131/1100000: episode: 1063, duration: 3.520s, episode steps: 500, steps per second: 142, episode reward: 232.848, mean reward: 0.466 [-18.677, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.177 [-0.633, 1.455], loss: 5.097178, mae: 53.730171, mean_q: 71.436386
  496436/1100000: episode: 1064, duration: 2.088s, episode steps: 305, steps per second: 146, episode reward: 252.282, mean reward: 0.827 [-18.597, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: -0.007 [-1.020, 1.507], loss: 8.924558, mae: 53.648746, mean_q: 71.096619
  496664/1100000: episode: 1065, duration: 1.536s, episode steps: 228, steps per second: 148, episode reward: 277.689, mean reward: 1.218 [-9.623, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.080 [-0.802, 1.398], loss: 7.258513, mae: 53.783554, mean_q: 71.315994
  497171/1100000: episode: 1066, duration: 3.517s, episode steps: 507, steps per second: 144, episode reward: 232.225, mean reward: 0.458 [-18.104, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.245 [-0.726, 1.401], loss: 5.933508, mae: 53.549786, mean_q: 71.089928
  497316/1100000: episode: 1067, duration: 0.965s, episode steps: 145, steps per second: 150, episode reward: 12.713, mean reward: 0.088 [-100.000, 14.499], mean action: 1.600 [0.000, 3.000], mean observation: 0.115 [-0.731, 1.409], loss: 5.200831, mae: 53.347382, mean_q: 70.936035
  497920/1100000: episode: 1068, duration: 4.272s, episode steps: 604, steps per second: 141, episode reward: 282.262, mean reward: 0.467 [-22.191, 100.000], mean action: 0.733 [0.000, 3.000], mean observation: 0.036 [-0.715, 1.412], loss: 8.311044, mae: 53.389637, mean_q: 70.747154
  498122/1100000: episode: 1069, duration: 1.352s, episode steps: 202, steps per second: 149, episode reward: 273.274, mean reward: 1.353 [-17.123, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.162 [-0.812, 1.396], loss: 7.177879, mae: 53.254089, mean_q: 70.790878
  498316/1100000: episode: 1070, duration: 1.303s, episode steps: 194, steps per second: 149, episode reward: 291.081, mean reward: 1.500 [-2.461, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.068 [-0.845, 1.385], loss: 4.009500, mae: 53.410507, mean_q: 70.867432
  498534/1100000: episode: 1071, duration: 1.481s, episode steps: 218, steps per second: 147, episode reward: 22.213, mean reward: 0.102 [-100.000, 13.987], mean action: 1.505 [0.000, 3.000], mean observation: 0.120 [-0.933, 1.391], loss: 7.317183, mae: 53.043999, mean_q: 70.297066
  499204/1100000: episode: 1072, duration: 4.857s, episode steps: 670, steps per second: 138, episode reward: 228.062, mean reward: 0.340 [-17.765, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.025 [-1.337, 1.435], loss: 6.403414, mae: 53.164463, mean_q: 70.576805
  499422/1100000: episode: 1073, duration: 1.484s, episode steps: 218, steps per second: 147, episode reward: 248.360, mean reward: 1.139 [-5.207, 100.000], mean action: 1.986 [0.000, 3.000], mean observation: -0.029 [-0.664, 1.388], loss: 5.436662, mae: 53.102314, mean_q: 70.604691
  499942/1100000: episode: 1074, duration: 3.717s, episode steps: 520, steps per second: 140, episode reward: 212.020, mean reward: 0.408 [-17.811, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: 0.132 [-0.661, 1.412], loss: 7.843866, mae: 52.814556, mean_q: 70.248695
  500267/1100000: episode: 1075, duration: 2.202s, episode steps: 325, steps per second: 148, episode reward: 233.055, mean reward: 0.717 [-10.749, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: -0.038 [-0.624, 1.393], loss: 8.108991, mae: 53.410931, mean_q: 71.321846
  500455/1100000: episode: 1076, duration: 1.275s, episode steps: 188, steps per second: 147, episode reward: -101.972, mean reward: -0.542 [-100.000, 8.139], mean action: 1.596 [0.000, 3.000], mean observation: -0.002 [-0.773, 1.501], loss: 8.285899, mae: 53.122318, mean_q: 71.092682
  500684/1100000: episode: 1077, duration: 1.562s, episode steps: 229, steps per second: 147, episode reward: 18.786, mean reward: 0.082 [-100.000, 20.012], mean action: 1.590 [0.000, 3.000], mean observation: -0.018 [-0.737, 1.394], loss: 5.945703, mae: 53.406673, mean_q: 70.996246
  500891/1100000: episode: 1078, duration: 1.394s, episode steps: 207, steps per second: 149, episode reward: 294.745, mean reward: 1.424 [-7.448, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.092 [-0.834, 1.394], loss: 9.406833, mae: 53.467541, mean_q: 70.916168
  501581/1100000: episode: 1079, duration: 4.768s, episode steps: 690, steps per second: 145, episode reward: 219.108, mean reward: 0.318 [-19.820, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.037 [-0.811, 1.396], loss: 6.080292, mae: 53.873177, mean_q: 71.676476
  502051/1100000: episode: 1080, duration: 3.357s, episode steps: 470, steps per second: 140, episode reward: 217.628, mean reward: 0.463 [-17.838, 100.000], mean action: 1.681 [0.000, 3.000], mean observation: 0.225 [-0.993, 1.460], loss: 7.379429, mae: 53.754154, mean_q: 71.714111
  502467/1100000: episode: 1081, duration: 2.928s, episode steps: 416, steps per second: 142, episode reward: 201.278, mean reward: 0.484 [-21.713, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: -0.024 [-1.207, 1.399], loss: 9.282285, mae: 54.214054, mean_q: 72.232956
  502590/1100000: episode: 1082, duration: 0.824s, episode steps: 123, steps per second: 149, episode reward: 68.902, mean reward: 0.560 [-100.000, 13.421], mean action: 1.496 [0.000, 3.000], mean observation: 0.114 [-0.735, 1.391], loss: 7.508042, mae: 53.536533, mean_q: 71.303711
  502731/1100000: episode: 1083, duration: 0.951s, episode steps: 141, steps per second: 148, episode reward: 13.910, mean reward: 0.099 [-100.000, 8.392], mean action: 1.837 [0.000, 3.000], mean observation: 0.092 [-0.846, 1.402], loss: 5.749024, mae: 54.491322, mean_q: 72.830727
  503200/1100000: episode: 1084, duration: 3.340s, episode steps: 469, steps per second: 140, episode reward: 212.478, mean reward: 0.453 [-23.292, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.024 [-0.760, 1.401], loss: 10.020902, mae: 54.255329, mean_q: 72.546341
  503560/1100000: episode: 1085, duration: 2.440s, episode steps: 360, steps per second: 148, episode reward: 270.689, mean reward: 0.752 [-19.076, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.074 [-1.106, 1.479], loss: 7.130404, mae: 54.040562, mean_q: 72.081184
  504145/1100000: episode: 1086, duration: 3.998s, episode steps: 585, steps per second: 146, episode reward: 260.054, mean reward: 0.445 [-18.371, 100.000], mean action: 0.544 [0.000, 3.000], mean observation: 0.248 [-1.323, 1.404], loss: 6.569280, mae: 53.881966, mean_q: 71.558411
  504481/1100000: episode: 1087, duration: 2.274s, episode steps: 336, steps per second: 148, episode reward: 276.803, mean reward: 0.824 [-10.913, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.234 [-0.701, 1.432], loss: 8.566106, mae: 53.982258, mean_q: 71.881462
  504811/1100000: episode: 1088, duration: 2.247s, episode steps: 330, steps per second: 147, episode reward: 279.840, mean reward: 0.848 [-17.888, 100.000], mean action: 0.903 [0.000, 3.000], mean observation: 0.112 [-1.234, 1.502], loss: 5.405178, mae: 54.633347, mean_q: 72.995659
  505203/1100000: episode: 1089, duration: 2.721s, episode steps: 392, steps per second: 144, episode reward: 197.628, mean reward: 0.504 [-18.222, 100.000], mean action: 2.013 [0.000, 3.000], mean observation: 0.188 [-0.561, 1.409], loss: 7.044698, mae: 54.337921, mean_q: 72.512306
  505644/1100000: episode: 1090, duration: 3.100s, episode steps: 441, steps per second: 142, episode reward: 250.790, mean reward: 0.569 [-11.422, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.002 [-0.750, 1.418], loss: 6.968960, mae: 54.570389, mean_q: 72.685150
  505974/1100000: episode: 1091, duration: 2.266s, episode steps: 330, steps per second: 146, episode reward: 259.208, mean reward: 0.785 [-11.265, 100.000], mean action: 1.976 [0.000, 3.000], mean observation: -0.002 [-0.681, 1.453], loss: 6.570911, mae: 54.218620, mean_q: 72.450653
  506362/1100000: episode: 1092, duration: 2.674s, episode steps: 388, steps per second: 145, episode reward: 277.433, mean reward: 0.715 [-19.770, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.259 [-0.721, 1.402], loss: 6.665289, mae: 54.264912, mean_q: 72.172569
  506680/1100000: episode: 1093, duration: 2.174s, episode steps: 318, steps per second: 146, episode reward: 244.721, mean reward: 0.770 [-13.344, 100.000], mean action: 2.003 [0.000, 3.000], mean observation: 0.048 [-0.717, 1.509], loss: 7.348710, mae: 54.188740, mean_q: 72.337845
  506834/1100000: episode: 1094, duration: 1.029s, episode steps: 154, steps per second: 150, episode reward: 10.479, mean reward: 0.068 [-100.000, 17.074], mean action: 1.578 [0.000, 3.000], mean observation: -0.012 [-0.750, 1.402], loss: 5.388260, mae: 54.324821, mean_q: 72.495461
  507834/1100000: episode: 1095, duration: 7.306s, episode steps: 1000, steps per second: 137, episode reward: 123.483, mean reward: 0.123 [-19.838, 21.819], mean action: 0.949 [0.000, 3.000], mean observation: 0.047 [-0.953, 1.465], loss: 7.631160, mae: 54.336102, mean_q: 72.349579
  508132/1100000: episode: 1096, duration: 2.029s, episode steps: 298, steps per second: 147, episode reward: 241.329, mean reward: 0.810 [-9.510, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: -0.023 [-0.790, 1.393], loss: 5.864131, mae: 54.573761, mean_q: 72.695206
  508525/1100000: episode: 1097, duration: 2.746s, episode steps: 393, steps per second: 143, episode reward: 255.903, mean reward: 0.651 [-2.701, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.075 [-0.832, 1.411], loss: 7.826025, mae: 54.017300, mean_q: 72.080360
  508744/1100000: episode: 1098, duration: 1.471s, episode steps: 219, steps per second: 149, episode reward: 260.777, mean reward: 1.191 [-2.643, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: -0.090 [-0.713, 1.400], loss: 10.166504, mae: 54.313442, mean_q: 72.635498
  508974/1100000: episode: 1099, duration: 1.551s, episode steps: 230, steps per second: 148, episode reward: 226.511, mean reward: 0.985 [-8.939, 100.000], mean action: 1.504 [0.000, 3.000], mean observation: 0.015 [-0.894, 1.400], loss: 6.207232, mae: 53.835270, mean_q: 71.743546
  509211/1100000: episode: 1100, duration: 1.636s, episode steps: 237, steps per second: 145, episode reward: 235.438, mean reward: 0.993 [-9.975, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.041 [-0.920, 1.419], loss: 5.044083, mae: 53.772320, mean_q: 71.815689
  509704/1100000: episode: 1101, duration: 3.375s, episode steps: 493, steps per second: 146, episode reward: 291.418, mean reward: 0.591 [-10.196, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.214 [-1.182, 1.509], loss: 9.003716, mae: 53.920403, mean_q: 71.887283
  509813/1100000: episode: 1102, duration: 0.732s, episode steps: 109, steps per second: 149, episode reward: 21.351, mean reward: 0.196 [-100.000, 13.613], mean action: 1.734 [0.000, 3.000], mean observation: 0.116 [-1.319, 1.391], loss: 5.138010, mae: 53.859276, mean_q: 72.030296
  510043/1100000: episode: 1103, duration: 1.547s, episode steps: 230, steps per second: 149, episode reward: -230.424, mean reward: -1.002 [-100.000, 20.831], mean action: 1.587 [0.000, 3.000], mean observation: 0.014 [-0.926, 1.650], loss: 6.109285, mae: 54.273438, mean_q: 72.355499
  510252/1100000: episode: 1104, duration: 1.412s, episode steps: 209, steps per second: 148, episode reward: 287.933, mean reward: 1.378 [-11.030, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.104 [-0.903, 1.388], loss: 6.582624, mae: 53.925350, mean_q: 71.938232
  510540/1100000: episode: 1105, duration: 1.992s, episode steps: 288, steps per second: 145, episode reward: 250.160, mean reward: 0.869 [-11.519, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.088 [-0.773, 1.412], loss: 7.564036, mae: 53.959812, mean_q: 71.797508
  510633/1100000: episode: 1106, duration: 0.660s, episode steps: 93, steps per second: 141, episode reward: 32.754, mean reward: 0.352 [-100.000, 11.657], mean action: 1.720 [0.000, 3.000], mean observation: 0.108 [-1.572, 1.388], loss: 5.690494, mae: 53.918903, mean_q: 72.009071
  510810/1100000: episode: 1107, duration: 1.216s, episode steps: 177, steps per second: 146, episode reward: 211.472, mean reward: 1.195 [-17.290, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.126 [-1.669, 1.401], loss: 10.616069, mae: 53.701889, mean_q: 71.459335
  511259/1100000: episode: 1108, duration: 3.092s, episode steps: 449, steps per second: 145, episode reward: 264.920, mean reward: 0.590 [-17.984, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.003 [-1.087, 1.399], loss: 7.701620, mae: 54.146519, mean_q: 72.165901
  511726/1100000: episode: 1109, duration: 3.253s, episode steps: 467, steps per second: 144, episode reward: 206.669, mean reward: 0.443 [-17.203, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.006 [-0.902, 1.410], loss: 7.419388, mae: 54.236961, mean_q: 72.387901
  511976/1100000: episode: 1110, duration: 1.665s, episode steps: 250, steps per second: 150, episode reward: 270.476, mean reward: 1.082 [-6.778, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: -0.033 [-1.007, 1.503], loss: 7.990682, mae: 53.698006, mean_q: 71.613304
  512101/1100000: episode: 1111, duration: 0.844s, episode steps: 125, steps per second: 148, episode reward: 2.952, mean reward: 0.024 [-100.000, 12.667], mean action: 1.768 [0.000, 3.000], mean observation: -0.054 [-0.826, 1.391], loss: 9.243485, mae: 54.033024, mean_q: 72.073303
  512224/1100000: episode: 1112, duration: 0.820s, episode steps: 123, steps per second: 150, episode reward: 57.185, mean reward: 0.465 [-100.000, 18.549], mean action: 1.732 [0.000, 3.000], mean observation: 0.166 [-1.127, 1.397], loss: 10.427979, mae: 53.519749, mean_q: 71.221642
  512519/1100000: episode: 1113, duration: 2.023s, episode steps: 295, steps per second: 146, episode reward: 252.736, mean reward: 0.857 [-17.425, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.015 [-1.241, 1.391], loss: 9.707548, mae: 53.676071, mean_q: 71.464882
  513106/1100000: episode: 1114, duration: 4.262s, episode steps: 587, steps per second: 138, episode reward: 261.122, mean reward: 0.445 [-20.133, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.131 [-0.925, 1.385], loss: 9.763918, mae: 53.662086, mean_q: 71.448570
  513441/1100000: episode: 1115, duration: 2.291s, episode steps: 335, steps per second: 146, episode reward: 301.959, mean reward: 0.901 [-11.849, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.037 [-0.865, 1.386], loss: 7.455051, mae: 53.446678, mean_q: 71.231621
  513681/1100000: episode: 1116, duration: 1.617s, episode steps: 240, steps per second: 148, episode reward: 229.476, mean reward: 0.956 [-15.941, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.131 [-0.900, 1.494], loss: 5.411268, mae: 53.977573, mean_q: 72.034828
  514029/1100000: episode: 1117, duration: 2.387s, episode steps: 348, steps per second: 146, episode reward: 251.761, mean reward: 0.723 [-19.038, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.133 [-0.737, 1.431], loss: 7.157311, mae: 53.369881, mean_q: 71.336983
  514522/1100000: episode: 1118, duration: 3.561s, episode steps: 493, steps per second: 138, episode reward: 235.207, mean reward: 0.477 [-19.571, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.140 [-1.341, 1.416], loss: 8.934570, mae: 53.551205, mean_q: 71.331818
  514815/1100000: episode: 1119, duration: 2.005s, episode steps: 293, steps per second: 146, episode reward: 254.959, mean reward: 0.870 [-20.239, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.058 [-0.894, 1.415], loss: 8.756584, mae: 53.320107, mean_q: 70.992371
  515172/1100000: episode: 1120, duration: 2.436s, episode steps: 357, steps per second: 147, episode reward: 282.769, mean reward: 0.792 [-17.728, 100.000], mean action: 0.910 [0.000, 3.000], mean observation: -0.016 [-0.776, 1.392], loss: 6.656743, mae: 53.843060, mean_q: 71.831039
  515491/1100000: episode: 1121, duration: 2.207s, episode steps: 319, steps per second: 145, episode reward: 250.595, mean reward: 0.786 [-18.409, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.070 [-0.854, 1.458], loss: 5.938245, mae: 54.110840, mean_q: 72.047356
  515793/1100000: episode: 1122, duration: 2.074s, episode steps: 302, steps per second: 146, episode reward: 241.502, mean reward: 0.800 [-17.577, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.175 [-0.763, 1.445], loss: 10.311666, mae: 53.610401, mean_q: 71.641029
  515851/1100000: episode: 1123, duration: 0.390s, episode steps: 58, steps per second: 149, episode reward: -219.637, mean reward: -3.787 [-100.000, 81.466], mean action: 2.155 [0.000, 3.000], mean observation: -0.087 [-3.225, 1.392], loss: 6.306491, mae: 53.604851, mean_q: 71.165108
  516043/1100000: episode: 1124, duration: 1.286s, episode steps: 192, steps per second: 149, episode reward: -97.101, mean reward: -0.506 [-100.000, 7.598], mean action: 1.057 [0.000, 3.000], mean observation: 0.020 [-1.078, 1.528], loss: 9.890014, mae: 53.659363, mean_q: 71.391899
  516428/1100000: episode: 1125, duration: 2.700s, episode steps: 385, steps per second: 143, episode reward: 216.472, mean reward: 0.562 [-20.285, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.113 [-0.759, 1.412], loss: 9.215450, mae: 53.722530, mean_q: 71.633408
  516724/1100000: episode: 1126, duration: 2.017s, episode steps: 296, steps per second: 147, episode reward: 203.224, mean reward: 0.687 [-14.974, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: -0.052 [-0.749, 1.474], loss: 8.798239, mae: 53.708801, mean_q: 71.737671
  516828/1100000: episode: 1127, duration: 0.691s, episode steps: 104, steps per second: 150, episode reward: -76.827, mean reward: -0.739 [-100.000, 10.482], mean action: 1.317 [0.000, 3.000], mean observation: 0.076 [-2.611, 1.389], loss: 6.140762, mae: 52.952133, mean_q: 70.293480
  517175/1100000: episode: 1128, duration: 2.383s, episode steps: 347, steps per second: 146, episode reward: 254.389, mean reward: 0.733 [-2.915, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.005 [-0.600, 1.411], loss: 7.232168, mae: 53.306309, mean_q: 70.990532
  517363/1100000: episode: 1129, duration: 1.252s, episode steps: 188, steps per second: 150, episode reward: -80.529, mean reward: -0.428 [-100.000, 12.010], mean action: 1.138 [0.000, 3.000], mean observation: 0.026 [-0.958, 1.513], loss: 8.089796, mae: 53.349552, mean_q: 71.012634
  517703/1100000: episode: 1130, duration: 2.352s, episode steps: 340, steps per second: 145, episode reward: 256.515, mean reward: 0.754 [-17.813, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.069 [-0.886, 1.453], loss: 7.634377, mae: 52.845566, mean_q: 70.481018
  517984/1100000: episode: 1131, duration: 1.977s, episode steps: 281, steps per second: 142, episode reward: 193.756, mean reward: 0.690 [-17.526, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: -0.038 [-0.701, 1.401], loss: 6.338443, mae: 53.441677, mean_q: 71.135117
  518548/1100000: episode: 1132, duration: 3.998s, episode steps: 564, steps per second: 141, episode reward: 250.814, mean reward: 0.445 [-9.507, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.007 [-0.870, 1.400], loss: 8.015343, mae: 52.727646, mean_q: 70.122635
  519166/1100000: episode: 1133, duration: 4.445s, episode steps: 618, steps per second: 139, episode reward: 237.047, mean reward: 0.384 [-15.086, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.169 [-0.666, 1.526], loss: 5.537433, mae: 52.820114, mean_q: 70.375977
  519432/1100000: episode: 1134, duration: 1.931s, episode steps: 266, steps per second: 138, episode reward: 269.958, mean reward: 1.015 [-11.111, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.028 [-0.884, 1.400], loss: 9.550413, mae: 52.479877, mean_q: 69.782501
  519615/1100000: episode: 1135, duration: 1.242s, episode steps: 183, steps per second: 147, episode reward: -0.928, mean reward: -0.005 [-100.000, 11.878], mean action: 1.945 [0.000, 3.000], mean observation: -0.102 [-1.551, 1.409], loss: 8.011149, mae: 52.331615, mean_q: 69.912003
  519821/1100000: episode: 1136, duration: 1.398s, episode steps: 206, steps per second: 147, episode reward: -157.280, mean reward: -0.763 [-100.000, 47.197], mean action: 1.534 [0.000, 3.000], mean observation: -0.059 [-1.754, 1.406], loss: 5.210975, mae: 52.486633, mean_q: 70.051514
  520060/1100000: episode: 1137, duration: 1.616s, episode steps: 239, steps per second: 148, episode reward: 255.579, mean reward: 1.069 [-17.378, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.172 [-0.628, 1.396], loss: 6.023401, mae: 52.824734, mean_q: 70.427254
  520381/1100000: episode: 1138, duration: 2.197s, episode steps: 321, steps per second: 146, episode reward: 232.568, mean reward: 0.725 [-7.467, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.026 [-0.600, 1.449], loss: 8.802650, mae: 52.538063, mean_q: 70.174919
  520884/1100000: episode: 1139, duration: 3.585s, episode steps: 503, steps per second: 140, episode reward: 198.929, mean reward: 0.395 [-20.429, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.007 [-0.710, 1.410], loss: 8.736954, mae: 52.513676, mean_q: 70.026024
  521460/1100000: episode: 1140, duration: 4.191s, episode steps: 576, steps per second: 137, episode reward: 177.207, mean reward: 0.308 [-10.105, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.106 [-0.650, 1.413], loss: 7.287641, mae: 52.339119, mean_q: 69.757172
  521700/1100000: episode: 1141, duration: 1.644s, episode steps: 240, steps per second: 146, episode reward: 19.243, mean reward: 0.080 [-100.000, 12.385], mean action: 1.746 [0.000, 3.000], mean observation: -0.013 [-0.730, 1.397], loss: 9.416876, mae: 52.030949, mean_q: 69.331917
  522062/1100000: episode: 1142, duration: 2.479s, episode steps: 362, steps per second: 146, episode reward: -214.094, mean reward: -0.591 [-100.000, 13.841], mean action: 1.425 [0.000, 3.000], mean observation: 0.048 [-0.911, 1.828], loss: 7.067564, mae: 52.133865, mean_q: 69.110359
  522463/1100000: episode: 1143, duration: 2.778s, episode steps: 401, steps per second: 144, episode reward: 233.117, mean reward: 0.581 [-19.424, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.095 [-0.796, 1.440], loss: 10.636608, mae: 52.139259, mean_q: 69.319321
  522697/1100000: episode: 1144, duration: 1.582s, episode steps: 234, steps per second: 148, episode reward: 243.218, mean reward: 1.039 [-11.165, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.158 [-0.636, 1.428], loss: 9.855214, mae: 51.963142, mean_q: 69.248795
  523343/1100000: episode: 1145, duration: 4.890s, episode steps: 646, steps per second: 132, episode reward: 224.839, mean reward: 0.348 [-17.876, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.140 [-1.152, 1.427], loss: 6.944732, mae: 51.758495, mean_q: 68.945038
  524343/1100000: episode: 1146, duration: 7.569s, episode steps: 1000, steps per second: 132, episode reward: 73.341, mean reward: 0.073 [-19.922, 36.517], mean action: 1.567 [0.000, 3.000], mean observation: 0.014 [-0.690, 1.398], loss: 7.794576, mae: 51.630257, mean_q: 68.717468
  524711/1100000: episode: 1147, duration: 2.570s, episode steps: 368, steps per second: 143, episode reward: 216.537, mean reward: 0.588 [-11.111, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.020 [-0.939, 1.407], loss: 7.804329, mae: 51.429283, mean_q: 68.717148
  525149/1100000: episode: 1148, duration: 3.045s, episode steps: 438, steps per second: 144, episode reward: 267.172, mean reward: 0.610 [-17.721, 100.000], mean action: 0.703 [0.000, 3.000], mean observation: 0.134 [-1.174, 1.501], loss: 8.287261, mae: 51.143398, mean_q: 68.029243
  526149/1100000: episode: 1149, duration: 7.178s, episode steps: 1000, steps per second: 139, episode reward: 149.191, mean reward: 0.149 [-20.122, 13.676], mean action: 1.318 [0.000, 3.000], mean observation: 0.086 [-1.036, 1.487], loss: 7.662322, mae: 51.401142, mean_q: 68.414284
  526411/1100000: episode: 1150, duration: 1.798s, episode steps: 262, steps per second: 146, episode reward: 276.882, mean reward: 1.057 [-9.776, 100.000], mean action: 0.950 [0.000, 3.000], mean observation: 0.062 [-0.896, 1.391], loss: 9.757119, mae: 51.161572, mean_q: 67.969070
  526897/1100000: episode: 1151, duration: 3.479s, episode steps: 486, steps per second: 140, episode reward: 230.391, mean reward: 0.474 [-18.940, 100.000], mean action: 0.858 [0.000, 3.000], mean observation: -0.027 [-0.890, 1.549], loss: 7.895599, mae: 51.065514, mean_q: 68.051987
  527124/1100000: episode: 1152, duration: 1.530s, episode steps: 227, steps per second: 148, episode reward: 9.022, mean reward: 0.040 [-100.000, 5.873], mean action: 1.762 [0.000, 3.000], mean observation: 0.006 [-0.600, 1.520], loss: 9.604919, mae: 51.505592, mean_q: 68.676903
  527582/1100000: episode: 1153, duration: 3.273s, episode steps: 458, steps per second: 140, episode reward: 208.760, mean reward: 0.456 [-18.950, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: -0.005 [-0.829, 1.392], loss: 6.904636, mae: 51.495457, mean_q: 68.394424
  527906/1100000: episode: 1154, duration: 2.325s, episode steps: 324, steps per second: 139, episode reward: 236.033, mean reward: 0.728 [-17.034, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: -0.018 [-1.037, 1.489], loss: 8.783941, mae: 51.005421, mean_q: 67.753525
  528000/1100000: episode: 1155, duration: 0.631s, episode steps: 94, steps per second: 149, episode reward: -69.536, mean reward: -0.740 [-100.000, 9.623], mean action: 1.723 [0.000, 3.000], mean observation: -0.156 [-1.019, 3.395], loss: 9.047473, mae: 51.038780, mean_q: 67.873482
  528443/1100000: episode: 1156, duration: 3.064s, episode steps: 443, steps per second: 145, episode reward: -17.728, mean reward: -0.040 [-100.000, 16.140], mean action: 1.045 [0.000, 3.000], mean observation: 0.183 [-0.839, 1.399], loss: 8.738022, mae: 51.170166, mean_q: 68.386353
  529052/1100000: episode: 1157, duration: 4.494s, episode steps: 609, steps per second: 136, episode reward: 300.144, mean reward: 0.493 [-21.098, 100.000], mean action: 0.814 [0.000, 3.000], mean observation: 0.140 [-1.110, 1.394], loss: 8.772491, mae: 51.013618, mean_q: 68.007660
  529369/1100000: episode: 1158, duration: 2.207s, episode steps: 317, steps per second: 144, episode reward: 296.905, mean reward: 0.937 [-18.356, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.100 [-0.804, 1.385], loss: 7.625823, mae: 51.297821, mean_q: 68.629410
  529972/1100000: episode: 1159, duration: 4.545s, episode steps: 603, steps per second: 133, episode reward: 190.730, mean reward: 0.316 [-18.495, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: -0.022 [-1.115, 1.469], loss: 9.921578, mae: 51.523624, mean_q: 68.503448
  530324/1100000: episode: 1160, duration: 2.478s, episode steps: 352, steps per second: 142, episode reward: 179.949, mean reward: 0.511 [-20.678, 100.000], mean action: 1.963 [0.000, 3.000], mean observation: 0.086 [-0.900, 1.407], loss: 9.081727, mae: 51.381405, mean_q: 68.506294
  531324/1100000: episode: 1161, duration: 7.314s, episode steps: 1000, steps per second: 137, episode reward: 55.521, mean reward: 0.056 [-20.617, 20.924], mean action: 1.161 [0.000, 3.000], mean observation: 0.010 [-1.053, 1.396], loss: 7.348691, mae: 51.743717, mean_q: 68.835625
  531619/1100000: episode: 1162, duration: 2.000s, episode steps: 295, steps per second: 147, episode reward: 295.912, mean reward: 1.003 [-10.032, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.074 [-0.859, 1.460], loss: 6.402237, mae: 51.912567, mean_q: 69.153564
  531987/1100000: episode: 1163, duration: 2.535s, episode steps: 368, steps per second: 145, episode reward: 260.545, mean reward: 0.708 [-17.124, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.031 [-1.145, 1.496], loss: 7.924313, mae: 52.182198, mean_q: 69.427780
  532702/1100000: episode: 1164, duration: 5.168s, episode steps: 715, steps per second: 138, episode reward: 272.868, mean reward: 0.382 [-17.672, 100.000], mean action: 0.848 [0.000, 3.000], mean observation: 0.161 [-0.665, 1.390], loss: 7.152747, mae: 51.748089, mean_q: 69.107452
  533057/1100000: episode: 1165, duration: 2.433s, episode steps: 355, steps per second: 146, episode reward: 276.974, mean reward: 0.780 [-9.392, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: 0.103 [-0.755, 1.395], loss: 9.872934, mae: 51.814388, mean_q: 69.109856
  533447/1100000: episode: 1166, duration: 2.698s, episode steps: 390, steps per second: 145, episode reward: 267.658, mean reward: 0.686 [-10.213, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: -0.052 [-0.797, 1.444], loss: 8.994337, mae: 51.966946, mean_q: 69.393623
  533974/1100000: episode: 1167, duration: 3.652s, episode steps: 527, steps per second: 144, episode reward: 217.361, mean reward: 0.412 [-19.241, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: -0.031 [-0.698, 1.417], loss: 9.163319, mae: 52.317131, mean_q: 69.874611
  534276/1100000: episode: 1168, duration: 2.053s, episode steps: 302, steps per second: 147, episode reward: 250.470, mean reward: 0.829 [-2.843, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: -0.027 [-0.600, 1.474], loss: 6.284189, mae: 52.114822, mean_q: 69.539154
  534531/1100000: episode: 1169, duration: 1.713s, episode steps: 255, steps per second: 149, episode reward: 284.399, mean reward: 1.115 [-11.065, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.078 [-1.287, 1.398], loss: 6.652999, mae: 52.036777, mean_q: 69.512314
  534641/1100000: episode: 1170, duration: 0.761s, episode steps: 110, steps per second: 145, episode reward: -20.955, mean reward: -0.190 [-100.000, 11.378], mean action: 1.491 [0.000, 3.000], mean observation: 0.183 [-2.220, 1.412], loss: 6.693220, mae: 52.711605, mean_q: 70.326530
  534768/1100000: episode: 1171, duration: 0.851s, episode steps: 127, steps per second: 149, episode reward: -3.076, mean reward: -0.024 [-100.000, 11.441], mean action: 1.693 [0.000, 3.000], mean observation: -0.039 [-0.681, 1.400], loss: 8.393038, mae: 52.659184, mean_q: 70.333229
  535087/1100000: episode: 1172, duration: 2.208s, episode steps: 319, steps per second: 144, episode reward: 264.127, mean reward: 0.828 [-18.033, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.126 [-0.833, 1.398], loss: 6.049332, mae: 52.192345, mean_q: 69.685600
  535315/1100000: episode: 1173, duration: 1.537s, episode steps: 228, steps per second: 148, episode reward: -34.530, mean reward: -0.151 [-100.000, 14.561], mean action: 1.658 [0.000, 3.000], mean observation: 0.033 [-0.900, 1.456], loss: 8.013916, mae: 52.667397, mean_q: 70.562088
  535401/1100000: episode: 1174, duration: 0.576s, episode steps: 86, steps per second: 149, episode reward: -100.256, mean reward: -1.166 [-100.000, 18.673], mean action: 1.581 [0.000, 3.000], mean observation: 0.004 [-3.437, 1.408], loss: 3.652145, mae: 52.723354, mean_q: 70.217354
  536361/1100000: episode: 1175, duration: 6.907s, episode steps: 960, steps per second: 139, episode reward: 222.939, mean reward: 0.232 [-19.569, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.214 [-0.747, 1.407], loss: 11.261371, mae: 52.378246, mean_q: 70.159836
  536510/1100000: episode: 1176, duration: 0.992s, episode steps: 149, steps per second: 150, episode reward: -61.365, mean reward: -0.412 [-100.000, 14.578], mean action: 1.544 [0.000, 3.000], mean observation: -0.055 [-1.013, 1.416], loss: 8.540915, mae: 51.739933, mean_q: 69.371704
  536845/1100000: episode: 1177, duration: 2.297s, episode steps: 335, steps per second: 146, episode reward: 245.534, mean reward: 0.733 [-18.654, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.124 [-1.313, 1.407], loss: 7.026915, mae: 52.679443, mean_q: 70.657722
  537600/1100000: episode: 1178, duration: 5.659s, episode steps: 755, steps per second: 133, episode reward: -207.272, mean reward: -0.275 [-100.000, 21.817], mean action: 1.521 [0.000, 3.000], mean observation: 0.133 [-0.986, 1.522], loss: 8.484024, mae: 52.901905, mean_q: 70.906525
  537979/1100000: episode: 1179, duration: 2.603s, episode steps: 379, steps per second: 146, episode reward: 198.545, mean reward: 0.524 [-11.743, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.151 [-0.816, 1.403], loss: 9.396567, mae: 52.600079, mean_q: 70.400940
  538280/1100000: episode: 1180, duration: 2.067s, episode steps: 301, steps per second: 146, episode reward: 258.735, mean reward: 0.860 [-8.745, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: -0.063 [-0.927, 1.403], loss: 7.336298, mae: 52.284504, mean_q: 69.990425
  538608/1100000: episode: 1181, duration: 2.270s, episode steps: 328, steps per second: 144, episode reward: 277.555, mean reward: 0.846 [-9.002, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.098 [-0.860, 1.443], loss: 9.048128, mae: 52.780399, mean_q: 70.484985
  539227/1100000: episode: 1182, duration: 4.389s, episode steps: 619, steps per second: 141, episode reward: 201.932, mean reward: 0.326 [-17.512, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: 0.050 [-0.822, 1.509], loss: 6.981018, mae: 53.070137, mean_q: 71.140388
  539613/1100000: episode: 1183, duration: 2.725s, episode steps: 386, steps per second: 142, episode reward: 225.182, mean reward: 0.583 [-7.843, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.005 [-0.807, 1.398], loss: 7.687126, mae: 52.615093, mean_q: 70.520401
  539955/1100000: episode: 1184, duration: 2.312s, episode steps: 342, steps per second: 148, episode reward: 245.325, mean reward: 0.717 [-21.432, 100.000], mean action: 0.839 [0.000, 3.000], mean observation: 0.199 [-1.235, 1.396], loss: 6.946075, mae: 53.001835, mean_q: 70.996849
  540571/1100000: episode: 1185, duration: 4.347s, episode steps: 616, steps per second: 142, episode reward: 202.681, mean reward: 0.329 [-20.290, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.036 [-0.719, 1.411], loss: 6.513923, mae: 53.045746, mean_q: 71.051567
  540777/1100000: episode: 1186, duration: 1.380s, episode steps: 206, steps per second: 149, episode reward: -9.264, mean reward: -0.045 [-100.000, 8.911], mean action: 1.592 [0.000, 3.000], mean observation: 0.163 [-1.715, 1.498], loss: 6.826845, mae: 52.628304, mean_q: 70.628464
  541177/1100000: episode: 1187, duration: 2.834s, episode steps: 400, steps per second: 141, episode reward: 281.140, mean reward: 0.703 [-19.765, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.126 [-0.681, 1.394], loss: 10.032543, mae: 52.682961, mean_q: 70.540054
  541522/1100000: episode: 1188, duration: 2.367s, episode steps: 345, steps per second: 146, episode reward: 271.515, mean reward: 0.787 [-8.358, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.085 [-0.687, 1.470], loss: 7.227430, mae: 52.721909, mean_q: 70.825195
  541907/1100000: episode: 1189, duration: 2.731s, episode steps: 385, steps per second: 141, episode reward: 269.857, mean reward: 0.701 [-18.502, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.109 [-0.470, 1.407], loss: 10.472156, mae: 52.795010, mean_q: 70.700737
  542155/1100000: episode: 1190, duration: 1.667s, episode steps: 248, steps per second: 149, episode reward: 238.587, mean reward: 0.962 [-8.051, 100.000], mean action: 1.641 [0.000, 3.000], mean observation: 0.073 [-0.775, 1.429], loss: 9.683076, mae: 52.608635, mean_q: 70.675072
  542368/1100000: episode: 1191, duration: 1.440s, episode steps: 213, steps per second: 148, episode reward: 286.725, mean reward: 1.346 [-7.649, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.100 [-0.928, 1.390], loss: 9.175187, mae: 52.740276, mean_q: 70.668282
  542645/1100000: episode: 1192, duration: 1.904s, episode steps: 277, steps per second: 145, episode reward: 262.336, mean reward: 0.947 [-16.780, 100.000], mean action: 1.884 [0.000, 3.000], mean observation: 0.116 [-0.734, 1.465], loss: 7.752196, mae: 52.560455, mean_q: 70.684067
  542745/1100000: episode: 1193, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: -105.744, mean reward: -1.057 [-100.000, 11.376], mean action: 1.400 [0.000, 3.000], mean observation: 0.189 [-0.870, 2.033], loss: 8.935828, mae: 52.947777, mean_q: 70.711060
  542880/1100000: episode: 1194, duration: 0.908s, episode steps: 135, steps per second: 149, episode reward: 16.917, mean reward: 0.125 [-100.000, 77.242], mean action: 1.889 [0.000, 3.000], mean observation: 0.038 [-1.447, 1.392], loss: 8.661956, mae: 53.646019, mean_q: 72.039513
  543592/1100000: episode: 1195, duration: 5.117s, episode steps: 712, steps per second: 139, episode reward: 193.138, mean reward: 0.271 [-17.459, 114.351], mean action: 1.237 [0.000, 3.000], mean observation: 0.256 [-2.295, 1.400], loss: 8.880541, mae: 53.653057, mean_q: 72.028526
  543753/1100000: episode: 1196, duration: 1.075s, episode steps: 161, steps per second: 150, episode reward: -202.707, mean reward: -1.259 [-100.000, 38.119], mean action: 1.559 [0.000, 3.000], mean observation: 0.274 [-0.818, 1.767], loss: 5.527383, mae: 53.457607, mean_q: 71.907242
  543853/1100000: episode: 1197, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -157.928, mean reward: -1.579 [-100.000, 4.150], mean action: 1.430 [0.000, 3.000], mean observation: 0.062 [-1.561, 5.173], loss: 7.812127, mae: 53.787987, mean_q: 72.112465
  544164/1100000: episode: 1198, duration: 2.117s, episode steps: 311, steps per second: 147, episode reward: -231.705, mean reward: -0.745 [-100.000, 29.126], mean action: 1.791 [0.000, 3.000], mean observation: 0.185 [-0.726, 1.998], loss: 12.312459, mae: 53.332829, mean_q: 71.673828
  544775/1100000: episode: 1199, duration: 4.483s, episode steps: 611, steps per second: 136, episode reward: 153.004, mean reward: 0.250 [-20.554, 100.000], mean action: 2.288 [0.000, 3.000], mean observation: 0.109 [-0.712, 1.428], loss: 11.074231, mae: 53.280689, mean_q: 71.422035
  544924/1100000: episode: 1200, duration: 1.010s, episode steps: 149, steps per second: 148, episode reward: -29.519, mean reward: -0.198 [-100.000, 11.822], mean action: 1.779 [0.000, 3.000], mean observation: -0.009 [-0.600, 1.392], loss: 6.232612, mae: 52.997074, mean_q: 71.247459
  545342/1100000: episode: 1201, duration: 3.029s, episode steps: 418, steps per second: 138, episode reward: 214.631, mean reward: 0.513 [-19.292, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: 0.107 [-0.757, 1.391], loss: 7.217865, mae: 52.844284, mean_q: 70.728172
  545909/1100000: episode: 1202, duration: 4.085s, episode steps: 567, steps per second: 139, episode reward: 234.385, mean reward: 0.413 [-19.493, 100.000], mean action: 1.734 [0.000, 3.000], mean observation: 0.104 [-0.674, 1.396], loss: 9.177312, mae: 53.038074, mean_q: 71.007057
  546236/1100000: episode: 1203, duration: 2.249s, episode steps: 327, steps per second: 145, episode reward: 258.144, mean reward: 0.789 [-19.750, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.203 [-1.302, 1.517], loss: 9.745464, mae: 52.968369, mean_q: 70.784897
  546680/1100000: episode: 1204, duration: 3.073s, episode steps: 444, steps per second: 144, episode reward: 240.322, mean reward: 0.541 [-17.799, 100.000], mean action: 1.793 [0.000, 3.000], mean observation: 0.181 [-0.681, 1.394], loss: 9.209939, mae: 53.250858, mean_q: 71.470764
  547058/1100000: episode: 1205, duration: 2.596s, episode steps: 378, steps per second: 146, episode reward: 268.830, mean reward: 0.711 [-8.745, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.102 [-0.771, 1.450], loss: 15.654498, mae: 52.593910, mean_q: 70.550568
  547121/1100000: episode: 1206, duration: 0.421s, episode steps: 63, steps per second: 150, episode reward: -164.491, mean reward: -2.611 [-100.000, 18.600], mean action: 1.413 [0.000, 3.000], mean observation: -0.086 [-1.843, 5.037], loss: 3.506062, mae: 52.649910, mean_q: 70.940697
  547508/1100000: episode: 1207, duration: 2.782s, episode steps: 387, steps per second: 139, episode reward: 266.260, mean reward: 0.688 [-10.119, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.084 [-0.713, 1.389], loss: 11.065301, mae: 52.906704, mean_q: 71.055145
  547766/1100000: episode: 1208, duration: 1.761s, episode steps: 258, steps per second: 147, episode reward: 249.754, mean reward: 0.968 [-8.874, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.058 [-0.804, 1.395], loss: 9.938021, mae: 52.741402, mean_q: 70.942986
  547851/1100000: episode: 1209, duration: 0.561s, episode steps: 85, steps per second: 152, episode reward: -190.901, mean reward: -2.246 [-100.000, 7.491], mean action: 1.294 [0.000, 3.000], mean observation: 0.073 [-1.817, 4.981], loss: 10.287930, mae: 53.435841, mean_q: 71.742493
  547957/1100000: episode: 1210, duration: 0.713s, episode steps: 106, steps per second: 149, episode reward: -79.765, mean reward: -0.753 [-100.000, 12.920], mean action: 1.623 [0.000, 3.000], mean observation: 0.050 [-0.936, 2.908], loss: 6.365947, mae: 53.224468, mean_q: 71.489243
  548037/1100000: episode: 1211, duration: 0.536s, episode steps: 80, steps per second: 149, episode reward: -293.687, mean reward: -3.671 [-100.000, 2.323], mean action: 1.738 [0.000, 3.000], mean observation: -0.060 [-2.566, 1.422], loss: 19.300453, mae: 52.503197, mean_q: 70.764717
  548112/1100000: episode: 1212, duration: 0.502s, episode steps: 75, steps per second: 150, episode reward: -336.411, mean reward: -4.485 [-100.000, 47.515], mean action: 1.240 [0.000, 3.000], mean observation: 0.037 [-3.265, 1.387], loss: 15.598702, mae: 52.317993, mean_q: 70.196289
  548401/1100000: episode: 1213, duration: 1.962s, episode steps: 289, steps per second: 147, episode reward: 228.634, mean reward: 0.791 [-13.383, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: -0.035 [-1.020, 1.492], loss: 12.963130, mae: 53.336609, mean_q: 71.668350
  548606/1100000: episode: 1214, duration: 1.386s, episode steps: 205, steps per second: 148, episode reward: 243.143, mean reward: 1.186 [-9.714, 100.000], mean action: 1.805 [0.000, 3.000], mean observation: 0.097 [-0.837, 1.408], loss: 8.870861, mae: 53.608673, mean_q: 72.034904
  548925/1100000: episode: 1215, duration: 2.180s, episode steps: 319, steps per second: 146, episode reward: 268.198, mean reward: 0.841 [-2.523, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.082 [-0.815, 1.500], loss: 12.309266, mae: 53.405216, mean_q: 71.667206
  549264/1100000: episode: 1216, duration: 2.290s, episode steps: 339, steps per second: 148, episode reward: 244.828, mean reward: 0.722 [-19.510, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: 0.094 [-0.731, 1.407], loss: 9.041570, mae: 53.989899, mean_q: 72.429764
  549308/1100000: episode: 1217, duration: 0.298s, episode steps: 44, steps per second: 147, episode reward: -314.469, mean reward: -7.147 [-100.000, -2.139], mean action: 2.432 [1.000, 3.000], mean observation: -0.164 [-1.984, 2.753], loss: 7.015793, mae: 55.065609, mean_q: 74.091415
  549616/1100000: episode: 1218, duration: 2.105s, episode steps: 308, steps per second: 146, episode reward: -37.274, mean reward: -0.121 [-100.000, 10.000], mean action: 1.734 [0.000, 3.000], mean observation: 0.058 [-1.720, 1.399], loss: 9.880403, mae: 54.335670, mean_q: 72.707993
  549702/1100000: episode: 1219, duration: 0.570s, episode steps: 86, steps per second: 151, episode reward: -253.036, mean reward: -2.942 [-100.000, 4.566], mean action: 1.651 [0.000, 3.000], mean observation: -0.055 [-2.755, 1.437], loss: 9.938950, mae: 54.660343, mean_q: 73.385246
  550702/1100000: episode: 1220, duration: 7.544s, episode steps: 1000, steps per second: 133, episode reward: 89.904, mean reward: 0.090 [-21.398, 23.500], mean action: 2.498 [0.000, 3.000], mean observation: 0.272 [-0.917, 1.405], loss: 12.881330, mae: 54.472786, mean_q: 72.796425
  551015/1100000: episode: 1221, duration: 2.165s, episode steps: 313, steps per second: 145, episode reward: 258.395, mean reward: 0.826 [-14.534, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.011 [-0.910, 1.425], loss: 7.848001, mae: 54.450478, mean_q: 72.996422
  551177/1100000: episode: 1222, duration: 1.088s, episode steps: 162, steps per second: 149, episode reward: -211.445, mean reward: -1.305 [-100.000, 35.556], mean action: 1.901 [0.000, 3.000], mean observation: -0.100 [-2.366, 1.477], loss: 6.285007, mae: 54.050564, mean_q: 72.507317
  552163/1100000: episode: 1223, duration: 7.353s, episode steps: 986, steps per second: 134, episode reward: 263.553, mean reward: 0.267 [-20.709, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.082 [-0.781, 1.399], loss: 12.010044, mae: 54.624683, mean_q: 73.086418
  552952/1100000: episode: 1224, duration: 5.393s, episode steps: 789, steps per second: 146, episode reward: 237.258, mean reward: 0.301 [-19.713, 100.000], mean action: 0.534 [0.000, 3.000], mean observation: 0.237 [-0.727, 1.391], loss: 9.847733, mae: 54.304600, mean_q: 72.626297
  553193/1100000: episode: 1225, duration: 1.755s, episode steps: 241, steps per second: 137, episode reward: 297.448, mean reward: 1.234 [-9.376, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.087 [-0.859, 1.393], loss: 10.719481, mae: 54.247822, mean_q: 72.581924
  553403/1100000: episode: 1226, duration: 1.408s, episode steps: 210, steps per second: 149, episode reward: 276.369, mean reward: 1.316 [-8.984, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.062 [-0.686, 1.403], loss: 15.338252, mae: 54.067959, mean_q: 72.584267
  553630/1100000: episode: 1227, duration: 1.522s, episode steps: 227, steps per second: 149, episode reward: 243.484, mean reward: 1.073 [-8.099, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.016 [-0.820, 1.411], loss: 10.514575, mae: 54.432026, mean_q: 72.602715
  553715/1100000: episode: 1228, duration: 0.555s, episode steps: 85, steps per second: 153, episode reward: -147.919, mean reward: -1.740 [-100.000, 3.656], mean action: 1.129 [0.000, 3.000], mean observation: 0.017 [-1.246, 1.895], loss: 12.110213, mae: 54.587128, mean_q: 72.976128
  553948/1100000: episode: 1229, duration: 1.577s, episode steps: 233, steps per second: 148, episode reward: 259.252, mean reward: 1.113 [-9.313, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.018 [-0.794, 1.409], loss: 10.651056, mae: 54.132660, mean_q: 72.295570
  554490/1100000: episode: 1230, duration: 3.866s, episode steps: 542, steps per second: 140, episode reward: 203.966, mean reward: 0.376 [-17.895, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: -0.004 [-0.691, 1.403], loss: 8.462240, mae: 54.155212, mean_q: 72.182442
  554860/1100000: episode: 1231, duration: 2.540s, episode steps: 370, steps per second: 146, episode reward: 225.245, mean reward: 0.609 [-17.888, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.119 [-0.732, 1.400], loss: 8.677868, mae: 54.085281, mean_q: 72.093346
  554942/1100000: episode: 1232, duration: 0.543s, episode steps: 82, steps per second: 151, episode reward: -168.321, mean reward: -2.053 [-100.000, 4.387], mean action: 1.232 [0.000, 3.000], mean observation: 0.024 [-1.407, 4.402], loss: 6.140679, mae: 54.818443, mean_q: 72.840561
  555768/1100000: episode: 1233, duration: 5.943s, episode steps: 826, steps per second: 139, episode reward: 225.945, mean reward: 0.274 [-19.136, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.095 [-0.724, 1.454], loss: 11.304753, mae: 54.006996, mean_q: 72.149559
  555953/1100000: episode: 1234, duration: 1.232s, episode steps: 185, steps per second: 150, episode reward: 259.390, mean reward: 1.402 [-2.894, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.073 [-0.773, 1.404], loss: 5.798962, mae: 53.761055, mean_q: 71.510254
  556359/1100000: episode: 1235, duration: 2.775s, episode steps: 406, steps per second: 146, episode reward: 259.777, mean reward: 0.640 [-20.816, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.116 [-0.628, 1.407], loss: 12.544484, mae: 53.854515, mean_q: 71.700722
  556753/1100000: episode: 1236, duration: 2.777s, episode steps: 394, steps per second: 142, episode reward: 204.066, mean reward: 0.518 [-18.725, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.097 [-0.789, 1.398], loss: 11.152371, mae: 53.503052, mean_q: 71.137634
  556831/1100000: episode: 1237, duration: 0.509s, episode steps: 78, steps per second: 153, episode reward: -91.494, mean reward: -1.173 [-100.000, 12.484], mean action: 0.667 [0.000, 3.000], mean observation: 0.049 [-1.230, 4.473], loss: 7.516743, mae: 53.422356, mean_q: 71.221054
  557113/1100000: episode: 1238, duration: 1.915s, episode steps: 282, steps per second: 147, episode reward: 269.732, mean reward: 0.956 [-18.125, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.052 [-0.807, 1.400], loss: 9.697200, mae: 53.304298, mean_q: 70.992699
  557523/1100000: episode: 1239, duration: 2.921s, episode steps: 410, steps per second: 140, episode reward: -146.818, mean reward: -0.358 [-100.000, 21.538], mean action: 1.139 [0.000, 3.000], mean observation: 0.008 [-1.314, 1.397], loss: 9.143169, mae: 53.199837, mean_q: 70.804741
  558082/1100000: episode: 1240, duration: 4.098s, episode steps: 559, steps per second: 136, episode reward: 225.507, mean reward: 0.403 [-10.458, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.136 [-0.635, 1.400], loss: 8.750283, mae: 53.133015, mean_q: 70.891579
  558317/1100000: episode: 1241, duration: 1.584s, episode steps: 235, steps per second: 148, episode reward: 259.146, mean reward: 1.103 [-10.398, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.218 [-0.723, 1.426], loss: 7.671308, mae: 53.332432, mean_q: 71.200249
  558663/1100000: episode: 1242, duration: 2.414s, episode steps: 346, steps per second: 143, episode reward: 275.482, mean reward: 0.796 [-20.649, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.129 [-0.672, 1.409], loss: 7.797660, mae: 52.990261, mean_q: 70.703735
  559152/1100000: episode: 1243, duration: 3.450s, episode steps: 489, steps per second: 142, episode reward: 278.200, mean reward: 0.569 [-17.327, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.117 [-0.775, 1.422], loss: 7.441962, mae: 52.942390, mean_q: 70.642570
  559425/1100000: episode: 1244, duration: 1.885s, episode steps: 273, steps per second: 145, episode reward: 268.718, mean reward: 0.984 [-8.991, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.049 [-0.713, 1.385], loss: 8.140319, mae: 53.332294, mean_q: 71.171402
  559930/1100000: episode: 1245, duration: 3.552s, episode steps: 505, steps per second: 142, episode reward: 171.020, mean reward: 0.339 [-23.565, 100.000], mean action: 1.747 [0.000, 3.000], mean observation: 0.011 [-0.780, 1.467], loss: 8.750426, mae: 52.602692, mean_q: 70.134178
  560129/1100000: episode: 1246, duration: 1.339s, episode steps: 199, steps per second: 149, episode reward: 250.302, mean reward: 1.258 [-6.358, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.066 [-0.752, 1.402], loss: 8.710752, mae: 53.093613, mean_q: 70.750900
  560298/1100000: episode: 1247, duration: 1.144s, episode steps: 169, steps per second: 148, episode reward: 242.614, mean reward: 1.436 [-7.889, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.059 [-0.703, 1.410], loss: 8.322228, mae: 53.175335, mean_q: 70.426811
  561161/1100000: episode: 1248, duration: 6.439s, episode steps: 863, steps per second: 134, episode reward: 240.315, mean reward: 0.278 [-20.170, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.100 [-0.745, 1.409], loss: 9.568706, mae: 53.092091, mean_q: 70.768532
  561244/1100000: episode: 1249, duration: 0.555s, episode steps: 83, steps per second: 150, episode reward: -207.313, mean reward: -2.498 [-100.000, 5.505], mean action: 1.651 [0.000, 3.000], mean observation: -0.105 [-1.775, 1.392], loss: 7.701413, mae: 52.465183, mean_q: 69.996834
  561628/1100000: episode: 1250, duration: 2.659s, episode steps: 384, steps per second: 144, episode reward: 226.330, mean reward: 0.589 [-10.567, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.012 [-0.640, 1.407], loss: 8.328725, mae: 52.720509, mean_q: 70.052460
  562553/1100000: episode: 1251, duration: 6.935s, episode steps: 925, steps per second: 133, episode reward: 161.063, mean reward: 0.174 [-20.286, 100.000], mean action: 1.699 [0.000, 3.000], mean observation: 0.132 [-0.690, 1.505], loss: 8.275714, mae: 52.885281, mean_q: 70.229797
  562718/1100000: episode: 1252, duration: 1.113s, episode steps: 165, steps per second: 148, episode reward: -0.203, mean reward: -0.001 [-100.000, 7.619], mean action: 1.885 [0.000, 3.000], mean observation: 0.001 [-0.739, 2.862], loss: 12.097641, mae: 52.549225, mean_q: 70.029892
  563173/1100000: episode: 1253, duration: 3.215s, episode steps: 455, steps per second: 142, episode reward: 230.839, mean reward: 0.507 [-20.818, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: -0.042 [-0.989, 1.387], loss: 6.588829, mae: 53.089378, mean_q: 70.765984
  563391/1100000: episode: 1254, duration: 1.484s, episode steps: 218, steps per second: 147, episode reward: 192.083, mean reward: 0.881 [-17.199, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: -0.098 [-0.779, 1.392], loss: 7.241571, mae: 53.028610, mean_q: 70.735237
  563764/1100000: episode: 1255, duration: 2.563s, episode steps: 373, steps per second: 146, episode reward: 278.487, mean reward: 0.747 [-10.871, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.093 [-0.644, 1.406], loss: 8.283369, mae: 52.877983, mean_q: 70.182632
  564064/1100000: episode: 1256, duration: 2.060s, episode steps: 300, steps per second: 146, episode reward: -273.762, mean reward: -0.913 [-100.000, 13.147], mean action: 1.767 [0.000, 3.000], mean observation: 0.084 [-0.960, 1.762], loss: 8.255273, mae: 53.001507, mean_q: 70.736053
  565064/1100000: episode: 1257, duration: 7.839s, episode steps: 1000, steps per second: 128, episode reward: -63.645, mean reward: -0.064 [-23.318, 25.410], mean action: 2.125 [0.000, 3.000], mean observation: 0.075 [-0.709, 1.408], loss: 8.050301, mae: 52.871414, mean_q: 70.537300
  565244/1100000: episode: 1258, duration: 1.217s, episode steps: 180, steps per second: 148, episode reward: 245.969, mean reward: 1.366 [-3.201, 100.000], mean action: 1.844 [0.000, 3.000], mean observation: 0.173 [-1.105, 1.387], loss: 10.412962, mae: 53.064030, mean_q: 70.733200
  565436/1100000: episode: 1259, duration: 1.281s, episode steps: 192, steps per second: 150, episode reward: 236.151, mean reward: 1.230 [-10.242, 100.000], mean action: 1.969 [0.000, 3.000], mean observation: -0.030 [-0.821, 1.391], loss: 8.447808, mae: 52.711273, mean_q: 70.320320
  565693/1100000: episode: 1260, duration: 1.733s, episode steps: 257, steps per second: 148, episode reward: 258.965, mean reward: 1.008 [-3.063, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.069 [-0.698, 1.407], loss: 8.999284, mae: 52.709473, mean_q: 70.264656
  566693/1100000: episode: 1261, duration: 7.120s, episode steps: 1000, steps per second: 140, episode reward: 101.732, mean reward: 0.102 [-24.192, 12.478], mean action: 0.698 [0.000, 3.000], mean observation: 0.058 [-1.012, 1.436], loss: 8.316878, mae: 52.476940, mean_q: 69.873497
  566948/1100000: episode: 1262, duration: 1.749s, episode steps: 255, steps per second: 146, episode reward: 272.148, mean reward: 1.067 [-2.904, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.044 [-0.765, 1.395], loss: 4.708656, mae: 52.390751, mean_q: 69.662575
  567410/1100000: episode: 1263, duration: 3.312s, episode steps: 462, steps per second: 139, episode reward: 214.090, mean reward: 0.463 [-20.799, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.120 [-1.352, 1.392], loss: 6.371057, mae: 52.376862, mean_q: 69.783798
  567838/1100000: episode: 1264, duration: 3.073s, episode steps: 428, steps per second: 139, episode reward: 267.833, mean reward: 0.626 [-5.181, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.061 [-0.652, 1.400], loss: 7.822996, mae: 52.647160, mean_q: 69.969353
  568523/1100000: episode: 1265, duration: 4.969s, episode steps: 685, steps per second: 138, episode reward: 268.523, mean reward: 0.392 [-23.575, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.130 [-0.835, 1.520], loss: 6.600049, mae: 52.658188, mean_q: 70.210480
  568717/1100000: episode: 1266, duration: 1.322s, episode steps: 194, steps per second: 147, episode reward: -117.479, mean reward: -0.606 [-100.000, 6.066], mean action: 1.490 [0.000, 3.000], mean observation: 0.030 [-2.694, 1.445], loss: 5.560614, mae: 52.786678, mean_q: 70.343636
  569717/1100000: episode: 1267, duration: 7.007s, episode steps: 1000, steps per second: 143, episode reward: 131.616, mean reward: 0.132 [-21.241, 22.155], mean action: 0.791 [0.000, 3.000], mean observation: 0.176 [-0.658, 1.432], loss: 7.986512, mae: 53.003246, mean_q: 70.688553
  570301/1100000: episode: 1268, duration: 4.140s, episode steps: 584, steps per second: 141, episode reward: 276.412, mean reward: 0.473 [-18.743, 100.000], mean action: 0.740 [0.000, 3.000], mean observation: 0.117 [-0.794, 1.395], loss: 7.894027, mae: 52.888985, mean_q: 70.396881
  570765/1100000: episode: 1269, duration: 3.206s, episode steps: 464, steps per second: 145, episode reward: 303.767, mean reward: 0.655 [-17.933, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.121 [-0.841, 1.497], loss: 7.659628, mae: 53.105972, mean_q: 70.790871
  571430/1100000: episode: 1270, duration: 4.649s, episode steps: 665, steps per second: 143, episode reward: 218.785, mean reward: 0.329 [-18.031, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.052 [-0.718, 1.438], loss: 9.007140, mae: 52.589085, mean_q: 69.883095
  571613/1100000: episode: 1271, duration: 1.221s, episode steps: 183, steps per second: 150, episode reward: 255.929, mean reward: 1.399 [-2.817, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: -0.069 [-0.823, 1.387], loss: 6.229288, mae: 52.180347, mean_q: 69.261963
  571907/1100000: episode: 1272, duration: 1.984s, episode steps: 294, steps per second: 148, episode reward: 228.598, mean reward: 0.778 [-19.698, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: -0.007 [-0.731, 1.418], loss: 6.848254, mae: 52.325806, mean_q: 69.600754
  572683/1100000: episode: 1273, duration: 5.476s, episode steps: 776, steps per second: 142, episode reward: 205.332, mean reward: 0.265 [-18.040, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.024 [-0.686, 1.525], loss: 8.727168, mae: 52.346981, mean_q: 69.688118
  572975/1100000: episode: 1274, duration: 2.000s, episode steps: 292, steps per second: 146, episode reward: 273.145, mean reward: 0.935 [-19.591, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.228 [-0.764, 1.410], loss: 5.431702, mae: 52.545010, mean_q: 69.793266
  573327/1100000: episode: 1275, duration: 2.428s, episode steps: 352, steps per second: 145, episode reward: 232.101, mean reward: 0.659 [-18.418, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: -0.000 [-0.856, 1.505], loss: 6.117232, mae: 52.702625, mean_q: 70.176155
  573511/1100000: episode: 1276, duration: 1.254s, episode steps: 184, steps per second: 147, episode reward: 5.458, mean reward: 0.030 [-100.000, 12.681], mean action: 1.859 [0.000, 3.000], mean observation: -0.018 [-1.304, 1.422], loss: 6.224662, mae: 52.134212, mean_q: 69.291183
  573654/1100000: episode: 1277, duration: 0.959s, episode steps: 143, steps per second: 149, episode reward: -45.239, mean reward: -0.316 [-100.000, 9.713], mean action: 1.531 [0.000, 3.000], mean observation: 0.026 [-1.625, 1.517], loss: 9.838154, mae: 52.633224, mean_q: 70.390526
  573832/1100000: episode: 1278, duration: 1.204s, episode steps: 178, steps per second: 148, episode reward: 14.306, mean reward: 0.080 [-100.000, 10.036], mean action: 1.551 [0.000, 3.000], mean observation: -0.009 [-0.600, 1.401], loss: 8.253268, mae: 53.039585, mean_q: 71.093018
  574043/1100000: episode: 1279, duration: 1.417s, episode steps: 211, steps per second: 149, episode reward: 284.769, mean reward: 1.350 [-3.684, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.091 [-0.800, 1.391], loss: 10.400344, mae: 52.385033, mean_q: 69.948235
  574273/1100000: episode: 1280, duration: 1.558s, episode steps: 230, steps per second: 148, episode reward: 261.180, mean reward: 1.136 [-17.337, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.118 [-0.697, 1.406], loss: 10.626521, mae: 53.112431, mean_q: 70.782433
  574603/1100000: episode: 1281, duration: 2.293s, episode steps: 330, steps per second: 144, episode reward: 249.203, mean reward: 0.755 [-19.312, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: -0.033 [-0.603, 1.432], loss: 9.414228, mae: 52.922146, mean_q: 70.642990
  575022/1100000: episode: 1282, duration: 2.913s, episode steps: 419, steps per second: 144, episode reward: 236.240, mean reward: 0.564 [-19.951, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.003 [-0.854, 1.400], loss: 6.743897, mae: 52.620667, mean_q: 70.304260
  575239/1100000: episode: 1283, duration: 1.456s, episode steps: 217, steps per second: 149, episode reward: -49.284, mean reward: -0.227 [-100.000, 12.404], mean action: 1.507 [0.000, 3.000], mean observation: 0.141 [-1.061, 1.549], loss: 9.862883, mae: 53.106079, mean_q: 70.794197
  575546/1100000: episode: 1284, duration: 2.128s, episode steps: 307, steps per second: 144, episode reward: 221.671, mean reward: 0.722 [-21.058, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: -0.006 [-0.720, 1.395], loss: 8.829063, mae: 52.978195, mean_q: 70.483398
  575814/1100000: episode: 1285, duration: 1.822s, episode steps: 268, steps per second: 147, episode reward: 240.164, mean reward: 0.896 [-9.371, 100.000], mean action: 1.806 [0.000, 3.000], mean observation: 0.012 [-0.932, 1.452], loss: 7.058023, mae: 52.883030, mean_q: 70.184044
  576017/1100000: episode: 1286, duration: 1.356s, episode steps: 203, steps per second: 150, episode reward: 241.622, mean reward: 1.190 [-18.167, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.089 [-0.632, 1.410], loss: 6.551408, mae: 52.708725, mean_q: 70.312859
  576279/1100000: episode: 1287, duration: 1.784s, episode steps: 262, steps per second: 147, episode reward: 240.123, mean reward: 0.917 [-2.840, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: -0.079 [-0.627, 1.393], loss: 8.969361, mae: 52.673512, mean_q: 69.686653
  576508/1100000: episode: 1288, duration: 1.540s, episode steps: 229, steps per second: 149, episode reward: 255.013, mean reward: 1.114 [-18.874, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.092 [-1.187, 1.447], loss: 6.044147, mae: 52.983795, mean_q: 70.720047
  576729/1100000: episode: 1289, duration: 1.489s, episode steps: 221, steps per second: 148, episode reward: 295.272, mean reward: 1.336 [-7.124, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.112 [-0.666, 1.416], loss: 8.715974, mae: 53.134720, mean_q: 70.865898
  576961/1100000: episode: 1290, duration: 1.584s, episode steps: 232, steps per second: 146, episode reward: 207.882, mean reward: 0.896 [-14.348, 100.000], mean action: 2.013 [0.000, 3.000], mean observation: 0.173 [-0.638, 1.408], loss: 7.160858, mae: 53.095131, mean_q: 70.628151
  577961/1100000: episode: 1291, duration: 7.294s, episode steps: 1000, steps per second: 137, episode reward: -57.885, mean reward: -0.058 [-20.985, 22.964], mean action: 1.911 [0.000, 3.000], mean observation: 0.269 [-0.810, 1.395], loss: 8.652663, mae: 53.053780, mean_q: 70.686394
  578667/1100000: episode: 1292, duration: 5.249s, episode steps: 706, steps per second: 134, episode reward: 281.401, mean reward: 0.399 [-19.127, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.081 [-1.314, 1.394], loss: 6.952319, mae: 52.864300, mean_q: 70.478348
  579581/1100000: episode: 1293, duration: 6.878s, episode steps: 914, steps per second: 133, episode reward: 238.097, mean reward: 0.260 [-21.194, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.102 [-0.743, 1.444], loss: 7.017435, mae: 52.879177, mean_q: 70.636345
  580143/1100000: episode: 1294, duration: 3.974s, episode steps: 562, steps per second: 141, episode reward: 244.786, mean reward: 0.436 [-18.664, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.171 [-1.115, 1.410], loss: 7.917130, mae: 52.495113, mean_q: 70.106758
  580715/1100000: episode: 1295, duration: 4.324s, episode steps: 572, steps per second: 132, episode reward: 189.530, mean reward: 0.331 [-17.977, 100.000], mean action: 1.715 [0.000, 3.000], mean observation: 0.133 [-0.634, 1.401], loss: 6.479490, mae: 53.105610, mean_q: 70.472450
  580919/1100000: episode: 1296, duration: 1.370s, episode steps: 204, steps per second: 149, episode reward: -75.122, mean reward: -0.368 [-100.000, 3.858], mean action: 1.412 [0.000, 3.000], mean observation: -0.135 [-1.004, 1.413], loss: 7.100920, mae: 53.124290, mean_q: 70.853340
  581658/1100000: episode: 1297, duration: 5.400s, episode steps: 739, steps per second: 137, episode reward: 269.770, mean reward: 0.365 [-19.266, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.193 [-0.949, 1.423], loss: 5.850829, mae: 52.956467, mean_q: 70.520782
  582062/1100000: episode: 1298, duration: 2.750s, episode steps: 404, steps per second: 147, episode reward: 226.330, mean reward: 0.560 [-17.648, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.219 [-1.079, 1.397], loss: 7.166928, mae: 52.937866, mean_q: 70.360680
  582622/1100000: episode: 1299, duration: 3.966s, episode steps: 560, steps per second: 141, episode reward: 150.244, mean reward: 0.268 [-20.924, 100.000], mean action: 2.291 [0.000, 3.000], mean observation: 0.109 [-0.898, 1.403], loss: 6.149449, mae: 53.162868, mean_q: 70.581848
  582860/1100000: episode: 1300, duration: 1.630s, episode steps: 238, steps per second: 146, episode reward: 258.111, mean reward: 1.085 [-19.090, 100.000], mean action: 2.366 [0.000, 3.000], mean observation: 0.173 [-0.886, 1.407], loss: 8.370278, mae: 52.949097, mean_q: 70.474052
  583388/1100000: episode: 1301, duration: 3.658s, episode steps: 528, steps per second: 144, episode reward: 254.681, mean reward: 0.482 [-19.740, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.177 [-1.308, 1.387], loss: 7.489464, mae: 53.023254, mean_q: 70.643265
  583659/1100000: episode: 1302, duration: 1.850s, episode steps: 271, steps per second: 147, episode reward: 294.661, mean reward: 1.087 [-9.095, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.059 [-0.753, 1.409], loss: 7.018054, mae: 52.928596, mean_q: 70.553177
  583887/1100000: episode: 1303, duration: 1.534s, episode steps: 228, steps per second: 149, episode reward: 302.653, mean reward: 1.327 [-2.948, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.116 [-0.746, 1.386], loss: 5.554025, mae: 52.860565, mean_q: 70.185707
  584621/1100000: episode: 1304, duration: 5.715s, episode steps: 734, steps per second: 128, episode reward: -213.587, mean reward: -0.291 [-100.000, 30.828], mean action: 2.106 [0.000, 3.000], mean observation: 0.037 [-0.695, 1.719], loss: 7.565899, mae: 53.026108, mean_q: 70.472298
  585621/1100000: episode: 1305, duration: 7.741s, episode steps: 1000, steps per second: 129, episode reward: 124.751, mean reward: 0.125 [-18.083, 17.033], mean action: 1.196 [0.000, 3.000], mean observation: 0.147 [-0.792, 1.401], loss: 7.258815, mae: 53.081940, mean_q: 70.563423
  586039/1100000: episode: 1306, duration: 2.842s, episode steps: 418, steps per second: 147, episode reward: 213.464, mean reward: 0.511 [-20.542, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.203 [-0.992, 1.403], loss: 7.234595, mae: 52.914726, mean_q: 70.476143
  586679/1100000: episode: 1307, duration: 4.497s, episode steps: 640, steps per second: 142, episode reward: 215.456, mean reward: 0.337 [-22.520, 100.000], mean action: 1.842 [0.000, 3.000], mean observation: -0.015 [-0.770, 1.425], loss: 6.209001, mae: 53.022095, mean_q: 70.592361
  586970/1100000: episode: 1308, duration: 2.118s, episode steps: 291, steps per second: 137, episode reward: 260.292, mean reward: 0.894 [-18.620, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.123 [-0.692, 1.396], loss: 10.925728, mae: 52.825649, mean_q: 70.386681
  587256/1100000: episode: 1309, duration: 1.968s, episode steps: 286, steps per second: 145, episode reward: 289.764, mean reward: 1.013 [-19.573, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.096 [-1.369, 1.453], loss: 6.966962, mae: 53.581676, mean_q: 71.080856
  587368/1100000: episode: 1310, duration: 0.751s, episode steps: 112, steps per second: 149, episode reward: 35.736, mean reward: 0.319 [-100.000, 18.222], mean action: 1.884 [0.000, 3.000], mean observation: 0.025 [-0.876, 1.407], loss: 7.524586, mae: 54.052029, mean_q: 72.301254
  587912/1100000: episode: 1311, duration: 3.796s, episode steps: 544, steps per second: 143, episode reward: 226.703, mean reward: 0.417 [-10.180, 100.000], mean action: 1.526 [0.000, 3.000], mean observation: 0.175 [-0.619, 1.418], loss: 8.636035, mae: 53.650299, mean_q: 71.445908
  588369/1100000: episode: 1312, duration: 3.183s, episode steps: 457, steps per second: 144, episode reward: 239.182, mean reward: 0.523 [-20.026, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.112 [-0.589, 1.404], loss: 10.277267, mae: 53.429771, mean_q: 70.984406
  588674/1100000: episode: 1313, duration: 2.090s, episode steps: 305, steps per second: 146, episode reward: 302.078, mean reward: 0.990 [-17.407, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.099 [-0.648, 1.390], loss: 5.949050, mae: 53.487770, mean_q: 71.301414
  589162/1100000: episode: 1314, duration: 3.364s, episode steps: 488, steps per second: 145, episode reward: 189.306, mean reward: 0.388 [-10.455, 100.000], mean action: 1.670 [0.000, 3.000], mean observation: 0.064 [-1.019, 1.583], loss: 6.961924, mae: 53.600746, mean_q: 71.061890
  589653/1100000: episode: 1315, duration: 3.403s, episode steps: 491, steps per second: 144, episode reward: 195.551, mean reward: 0.398 [-14.988, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.149 [-0.760, 1.450], loss: 7.328868, mae: 53.669029, mean_q: 71.320396
  590069/1100000: episode: 1316, duration: 2.831s, episode steps: 416, steps per second: 147, episode reward: 206.203, mean reward: 0.496 [-20.083, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.015 [-0.671, 1.431], loss: 8.812489, mae: 53.420967, mean_q: 71.338120
  590509/1100000: episode: 1317, duration: 3.098s, episode steps: 440, steps per second: 142, episode reward: 270.173, mean reward: 0.614 [-21.644, 100.000], mean action: 2.020 [0.000, 3.000], mean observation: 0.234 [-0.674, 1.393], loss: 7.863264, mae: 53.276031, mean_q: 71.081627
  590945/1100000: episode: 1318, duration: 3.061s, episode steps: 436, steps per second: 142, episode reward: 263.830, mean reward: 0.605 [-23.050, 100.000], mean action: 0.844 [0.000, 3.000], mean observation: 0.249 [-0.837, 1.462], loss: 7.063972, mae: 53.350914, mean_q: 71.301262
  591226/1100000: episode: 1319, duration: 1.928s, episode steps: 281, steps per second: 146, episode reward: 264.699, mean reward: 0.942 [-8.934, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: -0.037 [-0.736, 1.399], loss: 8.413268, mae: 53.210854, mean_q: 71.033096
  591729/1100000: episode: 1320, duration: 3.534s, episode steps: 503, steps per second: 142, episode reward: 237.272, mean reward: 0.472 [-18.829, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.026 [-1.084, 1.485], loss: 6.373724, mae: 53.379932, mean_q: 71.230148
  592129/1100000: episode: 1321, duration: 2.775s, episode steps: 400, steps per second: 144, episode reward: 181.773, mean reward: 0.454 [-21.204, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: -0.059 [-0.802, 1.409], loss: 6.251437, mae: 53.689529, mean_q: 71.393890
  592785/1100000: episode: 1322, duration: 4.910s, episode steps: 656, steps per second: 134, episode reward: 223.494, mean reward: 0.341 [-17.186, 100.000], mean action: 2.494 [0.000, 3.000], mean observation: 0.207 [-0.787, 1.389], loss: 5.995443, mae: 53.436810, mean_q: 71.390427
  593148/1100000: episode: 1323, duration: 2.495s, episode steps: 363, steps per second: 146, episode reward: 281.537, mean reward: 0.776 [-17.576, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.144 [-0.656, 1.535], loss: 5.476425, mae: 53.683891, mean_q: 71.650406
  593302/1100000: episode: 1324, duration: 1.034s, episode steps: 154, steps per second: 149, episode reward: -14.638, mean reward: -0.095 [-100.000, 11.914], mean action: 1.831 [0.000, 3.000], mean observation: 0.016 [-0.600, 1.423], loss: 5.700283, mae: 54.395180, mean_q: 72.539047
  593630/1100000: episode: 1325, duration: 2.245s, episode steps: 328, steps per second: 146, episode reward: 262.223, mean reward: 0.799 [-17.513, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.226 [-0.549, 1.409], loss: 7.308827, mae: 53.531357, mean_q: 71.565659
  594146/1100000: episode: 1326, duration: 3.577s, episode steps: 516, steps per second: 144, episode reward: 275.276, mean reward: 0.533 [-18.536, 100.000], mean action: 0.835 [0.000, 3.000], mean observation: 0.146 [-0.959, 1.386], loss: 6.960315, mae: 53.892433, mean_q: 71.877533
  594370/1100000: episode: 1327, duration: 1.529s, episode steps: 224, steps per second: 146, episode reward: 210.431, mean reward: 0.939 [-18.553, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: -0.037 [-1.132, 1.429], loss: 4.909863, mae: 53.343079, mean_q: 71.276100
  594605/1100000: episode: 1328, duration: 1.593s, episode steps: 235, steps per second: 147, episode reward: 272.196, mean reward: 1.158 [-9.946, 100.000], mean action: 1.613 [0.000, 3.000], mean observation: -0.016 [-0.763, 1.409], loss: 6.328146, mae: 53.121384, mean_q: 70.920761
  594819/1100000: episode: 1329, duration: 1.446s, episode steps: 214, steps per second: 148, episode reward: 243.235, mean reward: 1.137 [-2.531, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.111 [-1.059, 1.404], loss: 8.325397, mae: 53.281445, mean_q: 71.247375
  595126/1100000: episode: 1330, duration: 2.101s, episode steps: 307, steps per second: 146, episode reward: 183.700, mean reward: 0.598 [-20.430, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.154 [-0.729, 1.410], loss: 7.594414, mae: 53.378551, mean_q: 71.119514
  595501/1100000: episode: 1331, duration: 2.642s, episode steps: 375, steps per second: 142, episode reward: 300.800, mean reward: 0.802 [-17.475, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.115 [-0.563, 1.401], loss: 7.935422, mae: 53.447613, mean_q: 71.199928
  595922/1100000: episode: 1332, duration: 3.007s, episode steps: 421, steps per second: 140, episode reward: 212.838, mean reward: 0.506 [-11.319, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.013 [-0.649, 1.401], loss: 8.019640, mae: 53.188019, mean_q: 70.457176
  596034/1100000: episode: 1333, duration: 0.752s, episode steps: 112, steps per second: 149, episode reward: 31.754, mean reward: 0.284 [-100.000, 15.652], mean action: 1.821 [0.000, 3.000], mean observation: 0.025 [-0.729, 1.402], loss: 9.537541, mae: 54.315647, mean_q: 72.353531
  596317/1100000: episode: 1334, duration: 1.918s, episode steps: 283, steps per second: 148, episode reward: 226.002, mean reward: 0.799 [-17.375, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: -0.003 [-0.749, 1.410], loss: 7.234626, mae: 53.931351, mean_q: 71.900688
  596920/1100000: episode: 1335, duration: 4.329s, episode steps: 603, steps per second: 139, episode reward: -163.293, mean reward: -0.271 [-100.000, 11.472], mean action: 1.242 [0.000, 3.000], mean observation: -0.030 [-1.530, 1.499], loss: 5.931792, mae: 53.936550, mean_q: 71.721436
  597213/1100000: episode: 1336, duration: 1.996s, episode steps: 293, steps per second: 147, episode reward: 254.379, mean reward: 0.868 [-20.890, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.017 [-0.644, 1.460], loss: 6.312164, mae: 53.698639, mean_q: 71.045143
  597496/1100000: episode: 1337, duration: 1.933s, episode steps: 283, steps per second: 146, episode reward: 250.816, mean reward: 0.886 [-9.566, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.172 [-0.592, 1.422], loss: 5.309767, mae: 54.178246, mean_q: 71.851486
  597712/1100000: episode: 1338, duration: 1.455s, episode steps: 216, steps per second: 148, episode reward: 265.633, mean reward: 1.230 [-3.074, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.090 [-0.978, 1.413], loss: 9.399997, mae: 54.506268, mean_q: 72.508080
  598027/1100000: episode: 1339, duration: 2.166s, episode steps: 315, steps per second: 145, episode reward: 286.971, mean reward: 0.911 [-2.702, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: -0.029 [-0.784, 1.386], loss: 4.816916, mae: 53.771038, mean_q: 71.154984
  598306/1100000: episode: 1340, duration: 1.917s, episode steps: 279, steps per second: 146, episode reward: 259.679, mean reward: 0.931 [-18.708, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.017 [-0.709, 1.387], loss: 9.928700, mae: 53.610252, mean_q: 71.174110
  598662/1100000: episode: 1341, duration: 2.426s, episode steps: 356, steps per second: 147, episode reward: 228.420, mean reward: 0.642 [-18.244, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.004 [-0.905, 1.395], loss: 6.363167, mae: 54.050365, mean_q: 71.794800
  599118/1100000: episode: 1342, duration: 3.301s, episode steps: 456, steps per second: 138, episode reward: 243.860, mean reward: 0.535 [-17.762, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.025 [-1.053, 1.502], loss: 5.349349, mae: 53.500118, mean_q: 71.258789
  599750/1100000: episode: 1343, duration: 4.491s, episode steps: 632, steps per second: 141, episode reward: 186.944, mean reward: 0.296 [-14.188, 100.000], mean action: 0.797 [0.000, 3.000], mean observation: 0.043 [-1.103, 1.414], loss: 5.728024, mae: 53.502960, mean_q: 71.042824
  599997/1100000: episode: 1344, duration: 1.665s, episode steps: 247, steps per second: 148, episode reward: 263.916, mean reward: 1.068 [-2.636, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: 0.093 [-0.582, 1.473], loss: 6.793026, mae: 53.159008, mean_q: 70.828186
  600506/1100000: episode: 1345, duration: 3.547s, episode steps: 509, steps per second: 144, episode reward: 272.105, mean reward: 0.535 [-19.980, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.155 [-0.851, 1.448], loss: 7.167261, mae: 53.563152, mean_q: 71.301308
  600674/1100000: episode: 1346, duration: 1.121s, episode steps: 168, steps per second: 150, episode reward: -186.111, mean reward: -1.108 [-100.000, 2.331], mean action: 1.268 [0.000, 3.000], mean observation: 0.386 [-0.157, 1.935], loss: 6.890425, mae: 53.718651, mean_q: 71.463760
  600849/1100000: episode: 1347, duration: 1.193s, episode steps: 175, steps per second: 147, episode reward: -6.650, mean reward: -0.038 [-100.000, 20.115], mean action: 1.794 [0.000, 3.000], mean observation: -0.062 [-1.047, 1.390], loss: 6.978683, mae: 54.427948, mean_q: 72.739265
  601201/1100000: episode: 1348, duration: 2.387s, episode steps: 352, steps per second: 147, episode reward: 286.813, mean reward: 0.815 [-18.729, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.112 [-1.014, 1.411], loss: 5.953923, mae: 53.782200, mean_q: 71.622955
  601537/1100000: episode: 1349, duration: 2.389s, episode steps: 336, steps per second: 141, episode reward: 243.136, mean reward: 0.724 [-15.554, 100.000], mean action: 2.074 [0.000, 3.000], mean observation: 0.172 [-0.543, 1.389], loss: 8.066157, mae: 53.785480, mean_q: 71.934258
  602019/1100000: episode: 1350, duration: 3.331s, episode steps: 482, steps per second: 145, episode reward: 219.561, mean reward: 0.456 [-11.281, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.020 [-1.258, 1.439], loss: 6.784033, mae: 53.900837, mean_q: 71.931618
  602327/1100000: episode: 1351, duration: 2.114s, episode steps: 308, steps per second: 146, episode reward: 259.901, mean reward: 0.844 [-18.600, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.198 [-1.373, 1.503], loss: 7.152297, mae: 53.725586, mean_q: 71.579536
  602539/1100000: episode: 1352, duration: 1.422s, episode steps: 212, steps per second: 149, episode reward: 6.246, mean reward: 0.029 [-100.000, 12.464], mean action: 1.679 [0.000, 3.000], mean observation: 0.023 [-1.228, 1.514], loss: 5.577825, mae: 53.918503, mean_q: 71.828430
  602694/1100000: episode: 1353, duration: 1.034s, episode steps: 155, steps per second: 150, episode reward: 21.521, mean reward: 0.139 [-100.000, 17.554], mean action: 1.439 [0.000, 3.000], mean observation: -0.013 [-0.956, 1.408], loss: 25.920591, mae: 53.679134, mean_q: 71.481705
  602997/1100000: episode: 1354, duration: 2.117s, episode steps: 303, steps per second: 143, episode reward: 197.408, mean reward: 0.652 [-14.255, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: -0.029 [-0.960, 1.438], loss: 10.015950, mae: 53.726894, mean_q: 71.511765
  603212/1100000: episode: 1355, duration: 1.433s, episode steps: 215, steps per second: 150, episode reward: -168.122, mean reward: -0.782 [-100.000, 4.772], mean action: 1.600 [0.000, 3.000], mean observation: 0.067 [-0.757, 1.470], loss: 6.682497, mae: 53.602875, mean_q: 71.228386
  603808/1100000: episode: 1356, duration: 4.171s, episode steps: 596, steps per second: 143, episode reward: 227.667, mean reward: 0.382 [-22.605, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.052 [-0.741, 1.424], loss: 6.729501, mae: 53.728039, mean_q: 71.248505
  604419/1100000: episode: 1357, duration: 4.392s, episode steps: 611, steps per second: 139, episode reward: 259.892, mean reward: 0.425 [-20.122, 100.000], mean action: 0.797 [0.000, 3.000], mean observation: 0.136 [-0.806, 1.387], loss: 6.800021, mae: 53.696030, mean_q: 71.519844
  604927/1100000: episode: 1358, duration: 3.523s, episode steps: 508, steps per second: 144, episode reward: 264.314, mean reward: 0.520 [-18.413, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.238 [-0.690, 1.407], loss: 9.452491, mae: 53.667854, mean_q: 71.592224
  605322/1100000: episode: 1359, duration: 2.815s, episode steps: 395, steps per second: 140, episode reward: 153.828, mean reward: 0.389 [-15.520, 100.000], mean action: 2.015 [0.000, 3.000], mean observation: 0.160 [-0.692, 1.417], loss: 11.615655, mae: 53.893444, mean_q: 71.718826
  605595/1100000: episode: 1360, duration: 1.907s, episode steps: 273, steps per second: 143, episode reward: -214.151, mean reward: -0.784 [-100.000, 4.317], mean action: 1.484 [0.000, 3.000], mean observation: 0.042 [-0.608, 1.408], loss: 7.980925, mae: 54.249485, mean_q: 71.992989
  605772/1100000: episode: 1361, duration: 1.180s, episode steps: 177, steps per second: 150, episode reward: 255.579, mean reward: 1.444 [-7.588, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.054 [-0.740, 1.436], loss: 7.143722, mae: 53.957993, mean_q: 71.781998
  606145/1100000: episode: 1362, duration: 2.552s, episode steps: 373, steps per second: 146, episode reward: 234.128, mean reward: 0.628 [-21.130, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: -0.008 [-0.705, 1.406], loss: 9.747741, mae: 53.921886, mean_q: 71.570930
  606544/1100000: episode: 1363, duration: 2.774s, episode steps: 399, steps per second: 144, episode reward: 254.194, mean reward: 0.637 [-18.531, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.213 [-1.023, 1.535], loss: 7.086785, mae: 53.839725, mean_q: 71.672676
  607066/1100000: episode: 1364, duration: 3.658s, episode steps: 522, steps per second: 143, episode reward: 242.135, mean reward: 0.464 [-9.430, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.156 [-0.854, 1.386], loss: 7.668647, mae: 53.862606, mean_q: 71.639160
  607604/1100000: episode: 1365, duration: 3.715s, episode steps: 538, steps per second: 145, episode reward: 287.556, mean reward: 0.534 [-17.972, 100.000], mean action: 0.682 [0.000, 3.000], mean observation: 0.251 [-0.839, 1.389], loss: 11.492437, mae: 53.817699, mean_q: 71.528633
  607859/1100000: episode: 1366, duration: 1.720s, episode steps: 255, steps per second: 148, episode reward: 286.347, mean reward: 1.123 [-8.303, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.073 [-0.681, 1.411], loss: 8.033630, mae: 53.867973, mean_q: 71.615730
  608451/1100000: episode: 1367, duration: 4.252s, episode steps: 592, steps per second: 139, episode reward: 199.464, mean reward: 0.337 [-16.372, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.151 [-0.861, 1.392], loss: 6.592607, mae: 54.017418, mean_q: 71.605629
  609031/1100000: episode: 1368, duration: 4.276s, episode steps: 580, steps per second: 136, episode reward: 129.202, mean reward: 0.223 [-14.691, 100.000], mean action: 1.903 [0.000, 3.000], mean observation: 0.112 [-0.895, 1.423], loss: 7.608648, mae: 53.398731, mean_q: 71.017815
  609765/1100000: episode: 1369, duration: 5.290s, episode steps: 734, steps per second: 139, episode reward: 263.863, mean reward: 0.359 [-17.194, 100.000], mean action: 2.123 [0.000, 3.000], mean observation: 0.138 [-0.933, 1.386], loss: 10.358305, mae: 53.311653, mean_q: 71.026489
  609981/1100000: episode: 1370, duration: 1.454s, episode steps: 216, steps per second: 149, episode reward: -221.900, mean reward: -1.027 [-100.000, 63.506], mean action: 1.292 [0.000, 3.000], mean observation: -0.009 [-1.834, 1.425], loss: 7.095042, mae: 53.477104, mean_q: 71.196999
  610481/1100000: episode: 1371, duration: 3.631s, episode steps: 500, steps per second: 138, episode reward: 246.721, mean reward: 0.493 [-17.817, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.036 [-0.587, 1.491], loss: 6.357901, mae: 53.288414, mean_q: 70.997765
  611423/1100000: episode: 1372, duration: 6.969s, episode steps: 942, steps per second: 135, episode reward: -312.205, mean reward: -0.331 [-100.000, 15.680], mean action: 1.559 [0.000, 3.000], mean observation: -0.097 [-1.669, 1.525], loss: 6.906145, mae: 53.451710, mean_q: 71.126640
  612225/1100000: episode: 1373, duration: 6.559s, episode steps: 802, steps per second: 122, episode reward: 165.663, mean reward: 0.207 [-13.461, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: -0.059 [-0.931, 1.387], loss: 7.965480, mae: 53.735477, mean_q: 71.274849
  612301/1100000: episode: 1374, duration: 0.505s, episode steps: 76, steps per second: 151, episode reward: -93.858, mean reward: -1.235 [-100.000, 18.985], mean action: 1.500 [0.000, 3.000], mean observation: 0.203 [-1.568, 1.414], loss: 6.108608, mae: 54.383698, mean_q: 72.173058
  612405/1100000: episode: 1375, duration: 0.710s, episode steps: 104, steps per second: 146, episode reward: 11.479, mean reward: 0.110 [-100.000, 20.057], mean action: 1.865 [0.000, 3.000], mean observation: 0.043 [-1.677, 1.385], loss: 4.508963, mae: 53.878475, mean_q: 72.052368
  612731/1100000: episode: 1376, duration: 2.323s, episode steps: 326, steps per second: 140, episode reward: 258.629, mean reward: 0.793 [-10.028, 100.000], mean action: 2.077 [0.000, 3.000], mean observation: 0.205 [-0.705, 1.404], loss: 6.475448, mae: 53.886562, mean_q: 71.225739
  612801/1100000: episode: 1377, duration: 0.469s, episode steps: 70, steps per second: 149, episode reward: -79.170, mean reward: -1.131 [-100.000, 11.070], mean action: 1.214 [0.000, 3.000], mean observation: 0.184 [-3.601, 1.404], loss: 4.589280, mae: 52.517319, mean_q: 69.246971
  613367/1100000: episode: 1378, duration: 3.941s, episode steps: 566, steps per second: 144, episode reward: 231.069, mean reward: 0.408 [-18.992, 100.000], mean action: 1.002 [0.000, 3.000], mean observation: 0.003 [-1.022, 1.398], loss: 9.464169, mae: 53.998833, mean_q: 71.877663
  614014/1100000: episode: 1379, duration: 4.871s, episode steps: 647, steps per second: 133, episode reward: 252.612, mean reward: 0.390 [-18.273, 100.000], mean action: 2.297 [0.000, 3.000], mean observation: 0.119 [-1.137, 1.390], loss: 7.201890, mae: 53.587990, mean_q: 71.072586
  614248/1100000: episode: 1380, duration: 1.571s, episode steps: 234, steps per second: 149, episode reward: 264.933, mean reward: 1.132 [-2.785, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.115 [-1.220, 1.421], loss: 7.838456, mae: 53.503975, mean_q: 70.961021
  614873/1100000: episode: 1381, duration: 4.575s, episode steps: 625, steps per second: 137, episode reward: 238.522, mean reward: 0.382 [-21.444, 100.000], mean action: 0.682 [0.000, 3.000], mean observation: 0.134 [-0.920, 1.409], loss: 5.717885, mae: 53.410801, mean_q: 70.955948
  615609/1100000: episode: 1382, duration: 5.210s, episode steps: 736, steps per second: 141, episode reward: 215.313, mean reward: 0.293 [-18.901, 100.000], mean action: 1.746 [0.000, 3.000], mean observation: 0.118 [-0.617, 1.394], loss: 7.236318, mae: 53.365334, mean_q: 70.769981
  615791/1100000: episode: 1383, duration: 1.228s, episode steps: 182, steps per second: 148, episode reward: -54.648, mean reward: -0.300 [-100.000, 13.748], mean action: 1.879 [0.000, 3.000], mean observation: -0.093 [-1.299, 1.386], loss: 4.508146, mae: 52.914314, mean_q: 70.446945
  615979/1100000: episode: 1384, duration: 1.267s, episode steps: 188, steps per second: 148, episode reward: 49.908, mean reward: 0.265 [-100.000, 10.724], mean action: 1.819 [0.000, 3.000], mean observation: 0.178 [-0.610, 1.518], loss: 10.360508, mae: 53.097904, mean_q: 70.521667
  616096/1100000: episode: 1385, duration: 0.784s, episode steps: 117, steps per second: 149, episode reward: -32.241, mean reward: -0.276 [-100.000, 18.089], mean action: 1.632 [0.000, 3.000], mean observation: 0.060 [-1.256, 1.405], loss: 11.085011, mae: 52.992550, mean_q: 70.641373
  616571/1100000: episode: 1386, duration: 3.257s, episode steps: 475, steps per second: 146, episode reward: 262.135, mean reward: 0.552 [-20.179, 100.000], mean action: 0.981 [0.000, 3.000], mean observation: 0.273 [-1.121, 1.386], loss: 7.978547, mae: 52.943687, mean_q: 70.404976
  617073/1100000: episode: 1387, duration: 3.467s, episode steps: 502, steps per second: 145, episode reward: 221.948, mean reward: 0.442 [-18.246, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.208 [-0.645, 1.400], loss: 7.447237, mae: 52.938549, mean_q: 70.437080
  617317/1100000: episode: 1388, duration: 1.653s, episode steps: 244, steps per second: 148, episode reward: 258.381, mean reward: 1.059 [-8.720, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.039 [-0.732, 1.393], loss: 6.423636, mae: 52.901791, mean_q: 70.184280
  617723/1100000: episode: 1389, duration: 2.813s, episode steps: 406, steps per second: 144, episode reward: 204.391, mean reward: 0.503 [-15.779, 100.000], mean action: 1.892 [0.000, 3.000], mean observation: 0.152 [-0.715, 1.516], loss: 6.812875, mae: 52.763199, mean_q: 70.228508
  618127/1100000: episode: 1390, duration: 2.897s, episode steps: 404, steps per second: 139, episode reward: 189.763, mean reward: 0.470 [-17.390, 100.000], mean action: 2.428 [0.000, 3.000], mean observation: 0.123 [-0.656, 1.441], loss: 5.375143, mae: 52.758041, mean_q: 70.359184
  618697/1100000: episode: 1391, duration: 3.983s, episode steps: 570, steps per second: 143, episode reward: 201.241, mean reward: 0.353 [-18.575, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: -0.014 [-1.338, 1.476], loss: 9.240457, mae: 52.400238, mean_q: 69.569336
  619220/1100000: episode: 1392, duration: 3.693s, episode steps: 523, steps per second: 142, episode reward: 275.817, mean reward: 0.527 [-15.058, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.052 [-0.703, 1.411], loss: 6.445377, mae: 52.488968, mean_q: 69.914894
  619650/1100000: episode: 1393, duration: 3.015s, episode steps: 430, steps per second: 143, episode reward: 229.136, mean reward: 0.533 [-11.317, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: -0.068 [-0.694, 1.392], loss: 8.655393, mae: 52.505619, mean_q: 69.826134
  619953/1100000: episode: 1394, duration: 2.076s, episode steps: 303, steps per second: 146, episode reward: -235.481, mean reward: -0.777 [-100.000, 14.238], mean action: 1.614 [0.000, 3.000], mean observation: -0.124 [-1.829, 1.407], loss: 4.294438, mae: 52.378365, mean_q: 69.382477
  620719/1100000: episode: 1395, duration: 5.625s, episode steps: 766, steps per second: 136, episode reward: 187.673, mean reward: 0.245 [-18.068, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: -0.057 [-0.744, 1.406], loss: 7.307170, mae: 52.386311, mean_q: 69.541718
  621159/1100000: episode: 1396, duration: 3.203s, episode steps: 440, steps per second: 137, episode reward: 251.266, mean reward: 0.571 [-24.033, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: -0.013 [-0.675, 1.393], loss: 5.543796, mae: 52.232712, mean_q: 69.249733
  621350/1100000: episode: 1397, duration: 1.284s, episode steps: 191, steps per second: 149, episode reward: 281.808, mean reward: 1.475 [-9.814, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.051 [-0.733, 1.400], loss: 6.815946, mae: 52.160175, mean_q: 69.349030
  621633/1100000: episode: 1398, duration: 1.908s, episode steps: 283, steps per second: 148, episode reward: 234.907, mean reward: 0.830 [-13.013, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.066 [-0.668, 1.434], loss: 6.079937, mae: 52.068626, mean_q: 69.433769
  622289/1100000: episode: 1399, duration: 4.773s, episode steps: 656, steps per second: 137, episode reward: 222.253, mean reward: 0.339 [-20.368, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: -0.013 [-0.751, 1.423], loss: 8.440884, mae: 52.098167, mean_q: 69.199432
  622556/1100000: episode: 1400, duration: 1.817s, episode steps: 267, steps per second: 147, episode reward: 223.068, mean reward: 0.835 [-9.510, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: -0.028 [-1.306, 1.402], loss: 7.121499, mae: 52.427879, mean_q: 69.916153
  623307/1100000: episode: 1401, duration: 5.627s, episode steps: 751, steps per second: 133, episode reward: 187.303, mean reward: 0.249 [-19.616, 100.000], mean action: 1.975 [0.000, 3.000], mean observation: 0.003 [-0.676, 1.420], loss: 7.071182, mae: 51.965176, mean_q: 69.284264
  623997/1100000: episode: 1402, duration: 5.079s, episode steps: 690, steps per second: 136, episode reward: 191.182, mean reward: 0.277 [-17.609, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: -0.055 [-0.679, 1.400], loss: 7.607947, mae: 52.071880, mean_q: 69.375336
  624338/1100000: episode: 1403, duration: 2.391s, episode steps: 341, steps per second: 143, episode reward: 259.360, mean reward: 0.761 [-11.884, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.129 [-0.801, 1.468], loss: 4.359346, mae: 51.851662, mean_q: 69.179428
  624776/1100000: episode: 1404, duration: 3.099s, episode steps: 438, steps per second: 141, episode reward: 214.628, mean reward: 0.490 [-20.066, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.018 [-1.209, 1.485], loss: 9.480933, mae: 52.139866, mean_q: 69.423576
  625290/1100000: episode: 1405, duration: 3.573s, episode steps: 514, steps per second: 144, episode reward: 243.954, mean reward: 0.475 [-19.175, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.143 [-0.663, 1.429], loss: 9.640026, mae: 51.632980, mean_q: 68.638756
  625638/1100000: episode: 1406, duration: 2.417s, episode steps: 348, steps per second: 144, episode reward: 229.012, mean reward: 0.658 [-17.458, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.133 [-0.673, 1.404], loss: 6.303906, mae: 51.805367, mean_q: 68.563843
  625792/1100000: episode: 1407, duration: 1.031s, episode steps: 154, steps per second: 149, episode reward: -265.741, mean reward: -1.726 [-100.000, 3.649], mean action: 1.266 [0.000, 3.000], mean observation: 0.034 [-1.355, 1.431], loss: 4.093534, mae: 51.928963, mean_q: 68.952705
  626089/1100000: episode: 1408, duration: 2.044s, episode steps: 297, steps per second: 145, episode reward: 248.570, mean reward: 0.837 [-10.623, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: -0.090 [-0.773, 1.401], loss: 7.302822, mae: 51.442730, mean_q: 68.635063
  626556/1100000: episode: 1409, duration: 3.178s, episode steps: 467, steps per second: 147, episode reward: 274.034, mean reward: 0.587 [-10.380, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.150 [-0.705, 1.485], loss: 7.094564, mae: 51.475441, mean_q: 68.442375
  626813/1100000: episode: 1410, duration: 1.743s, episode steps: 257, steps per second: 147, episode reward: -18.725, mean reward: -0.073 [-100.000, 14.534], mean action: 1.825 [0.000, 3.000], mean observation: 0.130 [-0.726, 1.447], loss: 6.034527, mae: 51.491642, mean_q: 68.232094
  627138/1100000: episode: 1411, duration: 2.234s, episode steps: 325, steps per second: 146, episode reward: 233.276, mean reward: 0.718 [-9.483, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: -0.040 [-0.682, 1.471], loss: 6.744323, mae: 51.640854, mean_q: 68.765984
  627862/1100000: episode: 1412, duration: 5.007s, episode steps: 724, steps per second: 145, episode reward: 239.486, mean reward: 0.331 [-18.832, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.028 [-0.894, 1.473], loss: 8.130208, mae: 51.458336, mean_q: 68.618820
  628256/1100000: episode: 1413, duration: 2.758s, episode steps: 394, steps per second: 143, episode reward: 242.911, mean reward: 0.617 [-18.806, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.006 [-0.941, 1.387], loss: 6.714314, mae: 51.153965, mean_q: 67.794510
  628575/1100000: episode: 1414, duration: 2.245s, episode steps: 319, steps per second: 142, episode reward: 181.125, mean reward: 0.568 [-18.236, 100.000], mean action: 1.928 [0.000, 3.000], mean observation: 0.139 [-0.853, 1.404], loss: 7.913952, mae: 51.504421, mean_q: 68.382721
  629131/1100000: episode: 1415, duration: 3.839s, episode steps: 556, steps per second: 145, episode reward: 243.551, mean reward: 0.438 [-19.075, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.020 [-0.922, 1.419], loss: 8.223541, mae: 51.331474, mean_q: 68.313087
  629426/1100000: episode: 1416, duration: 2.050s, episode steps: 295, steps per second: 144, episode reward: 286.429, mean reward: 0.971 [-8.724, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.093 [-0.753, 1.481], loss: 9.592314, mae: 51.223167, mean_q: 68.388176
  630107/1100000: episode: 1417, duration: 5.116s, episode steps: 681, steps per second: 133, episode reward: 174.372, mean reward: 0.256 [-19.029, 100.000], mean action: 2.391 [0.000, 3.000], mean observation: 0.187 [-0.810, 1.391], loss: 6.901743, mae: 50.865078, mean_q: 68.026886
  630357/1100000: episode: 1418, duration: 1.704s, episode steps: 250, steps per second: 147, episode reward: 206.950, mean reward: 0.828 [-2.772, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.153 [-0.646, 1.402], loss: 9.770495, mae: 51.396008, mean_q: 68.686012
  630869/1100000: episode: 1419, duration: 3.618s, episode steps: 512, steps per second: 142, episode reward: 171.197, mean reward: 0.334 [-18.164, 100.000], mean action: 2.299 [0.000, 3.000], mean observation: 0.174 [-0.800, 1.391], loss: 8.333170, mae: 50.860485, mean_q: 67.628036
  631778/1100000: episode: 1420, duration: 6.791s, episode steps: 909, steps per second: 134, episode reward: 133.283, mean reward: 0.147 [-19.226, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: 0.029 [-0.894, 1.459], loss: 6.776978, mae: 50.575836, mean_q: 67.447289
  632100/1100000: episode: 1421, duration: 2.245s, episode steps: 322, steps per second: 143, episode reward: 226.404, mean reward: 0.703 [-12.139, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.139 [-0.775, 1.437], loss: 9.530127, mae: 50.600933, mean_q: 67.483826
  632260/1100000: episode: 1422, duration: 1.091s, episode steps: 160, steps per second: 147, episode reward: -15.748, mean reward: -0.098 [-100.000, 25.056], mean action: 2.000 [0.000, 3.000], mean observation: -0.038 [-1.359, 1.417], loss: 6.050475, mae: 50.633587, mean_q: 67.513039
  632686/1100000: episode: 1423, duration: 2.922s, episode steps: 426, steps per second: 146, episode reward: 230.753, mean reward: 0.542 [-19.593, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: -0.049 [-0.870, 1.469], loss: 6.264464, mae: 50.304375, mean_q: 67.118134
  632970/1100000: episode: 1424, duration: 1.929s, episode steps: 284, steps per second: 147, episode reward: 248.400, mean reward: 0.875 [-8.848, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: -0.056 [-0.663, 1.394], loss: 7.758426, mae: 49.785080, mean_q: 66.523026
  633265/1100000: episode: 1425, duration: 2.004s, episode steps: 295, steps per second: 147, episode reward: 232.091, mean reward: 0.787 [-18.798, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: -0.018 [-1.020, 1.522], loss: 6.921130, mae: 50.292255, mean_q: 67.270912
  633367/1100000: episode: 1426, duration: 0.689s, episode steps: 102, steps per second: 148, episode reward: -22.907, mean reward: -0.225 [-100.000, 20.175], mean action: 1.824 [0.000, 3.000], mean observation: -0.149 [-1.132, 3.238], loss: 6.063014, mae: 50.689610, mean_q: 67.306213
  633515/1100000: episode: 1427, duration: 0.988s, episode steps: 148, steps per second: 150, episode reward: -42.968, mean reward: -0.290 [-100.000, 13.885], mean action: 1.507 [0.000, 3.000], mean observation: 0.047 [-0.904, 1.499], loss: 8.346028, mae: 50.422852, mean_q: 66.899956
  633802/1100000: episode: 1428, duration: 1.991s, episode steps: 287, steps per second: 144, episode reward: 186.502, mean reward: 0.650 [-19.267, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: -0.064 [-1.186, 1.408], loss: 6.844254, mae: 50.402283, mean_q: 67.202408
  634074/1100000: episode: 1429, duration: 1.852s, episode steps: 272, steps per second: 147, episode reward: 261.754, mean reward: 0.962 [-8.978, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.195 [-0.619, 1.395], loss: 7.341353, mae: 50.398510, mean_q: 67.148796
  634331/1100000: episode: 1430, duration: 1.785s, episode steps: 257, steps per second: 144, episode reward: 269.940, mean reward: 1.050 [-17.189, 100.000], mean action: 2.214 [0.000, 3.000], mean observation: 0.140 [-0.617, 1.394], loss: 7.676342, mae: 50.448498, mean_q: 67.136902
  635031/1100000: episode: 1431, duration: 4.969s, episode steps: 700, steps per second: 141, episode reward: 284.532, mean reward: 0.406 [-19.847, 100.000], mean action: 0.927 [0.000, 3.000], mean observation: 0.150 [-0.735, 1.474], loss: 5.853668, mae: 50.978447, mean_q: 68.016434
  635426/1100000: episode: 1432, duration: 2.675s, episode steps: 395, steps per second: 148, episode reward: 212.027, mean reward: 0.537 [-17.266, 100.000], mean action: 0.851 [0.000, 3.000], mean observation: 0.180 [-1.106, 1.413], loss: 6.981874, mae: 51.121483, mean_q: 68.023636
  636250/1100000: episode: 1433, duration: 5.990s, episode steps: 824, steps per second: 138, episode reward: 204.659, mean reward: 0.248 [-21.098, 100.000], mean action: 2.212 [0.000, 3.000], mean observation: 0.189 [-0.569, 1.412], loss: 6.268422, mae: 50.969006, mean_q: 67.827835
  636417/1100000: episode: 1434, duration: 1.130s, episode steps: 167, steps per second: 148, episode reward: 11.359, mean reward: 0.068 [-100.000, 9.994], mean action: 1.719 [0.000, 3.000], mean observation: 0.037 [-1.236, 1.399], loss: 6.154656, mae: 51.075951, mean_q: 67.888206
  636542/1100000: episode: 1435, duration: 0.843s, episode steps: 125, steps per second: 148, episode reward: -92.117, mean reward: -0.737 [-100.000, 60.028], mean action: 1.624 [0.000, 3.000], mean observation: 0.061 [-1.036, 1.596], loss: 10.978218, mae: 50.968029, mean_q: 67.983261
  637158/1100000: episode: 1436, duration: 4.341s, episode steps: 616, steps per second: 142, episode reward: 223.136, mean reward: 0.362 [-22.156, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.037 [-0.761, 1.388], loss: 8.995636, mae: 51.068989, mean_q: 68.047165
  637495/1100000: episode: 1437, duration: 2.319s, episode steps: 337, steps per second: 145, episode reward: 253.412, mean reward: 0.752 [-10.460, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.225 [-0.641, 1.525], loss: 9.567411, mae: 51.037285, mean_q: 68.084091
  637685/1100000: episode: 1438, duration: 1.296s, episode steps: 190, steps per second: 147, episode reward: 234.984, mean reward: 1.237 [-3.546, 100.000], mean action: 2.026 [0.000, 3.000], mean observation: 0.115 [-0.677, 1.390], loss: 6.561070, mae: 50.940487, mean_q: 67.996956
  638118/1100000: episode: 1439, duration: 3.098s, episode steps: 433, steps per second: 140, episode reward: 216.580, mean reward: 0.500 [-20.347, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: -0.021 [-0.681, 1.396], loss: 8.945788, mae: 50.985386, mean_q: 67.799461
  638595/1100000: episode: 1440, duration: 3.277s, episode steps: 477, steps per second: 146, episode reward: 277.390, mean reward: 0.582 [-14.386, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.184 [-0.852, 1.385], loss: 10.338896, mae: 50.967014, mean_q: 68.030777
  639595/1100000: episode: 1441, duration: 7.592s, episode steps: 1000, steps per second: 132, episode reward: -13.871, mean reward: -0.014 [-19.483, 22.063], mean action: 1.048 [0.000, 3.000], mean observation: 0.052 [-0.909, 1.455], loss: 6.047101, mae: 50.513462, mean_q: 67.225334
  639850/1100000: episode: 1442, duration: 1.721s, episode steps: 255, steps per second: 148, episode reward: 270.567, mean reward: 1.061 [-18.125, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.099 [-1.031, 1.433], loss: 6.250569, mae: 50.764633, mean_q: 67.699814
  639995/1100000: episode: 1443, duration: 0.978s, episode steps: 145, steps per second: 148, episode reward: -134.333, mean reward: -0.926 [-100.000, 46.703], mean action: 1.524 [0.000, 3.000], mean observation: 0.002 [-1.525, 1.386], loss: 10.335483, mae: 50.246880, mean_q: 66.996918
  640346/1100000: episode: 1444, duration: 2.436s, episode steps: 351, steps per second: 144, episode reward: -81.493, mean reward: -0.232 [-100.000, 8.271], mean action: 1.510 [0.000, 3.000], mean observation: 0.040 [-0.780, 1.845], loss: 7.052696, mae: 50.305290, mean_q: 67.168236
  640579/1100000: episode: 1445, duration: 1.594s, episode steps: 233, steps per second: 146, episode reward: 262.452, mean reward: 1.126 [-2.787, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.118 [-0.727, 1.399], loss: 11.340282, mae: 49.959335, mean_q: 66.551544
  640945/1100000: episode: 1446, duration: 2.571s, episode steps: 366, steps per second: 142, episode reward: -189.959, mean reward: -0.519 [-100.000, 13.775], mean action: 1.016 [0.000, 3.000], mean observation: 0.067 [-1.567, 1.399], loss: 10.009574, mae: 50.602192, mean_q: 67.323196
  641102/1100000: episode: 1447, duration: 1.056s, episode steps: 157, steps per second: 149, episode reward: -116.693, mean reward: -0.743 [-100.000, 9.963], mean action: 1.592 [0.000, 3.000], mean observation: 0.037 [-1.308, 4.860], loss: 4.405223, mae: 50.665150, mean_q: 67.712532
  641552/1100000: episode: 1448, duration: 3.125s, episode steps: 450, steps per second: 144, episode reward: 186.457, mean reward: 0.414 [-9.509, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.183 [-0.745, 1.406], loss: 8.442838, mae: 50.636406, mean_q: 67.241531
  641839/1100000: episode: 1449, duration: 1.972s, episode steps: 287, steps per second: 146, episode reward: 221.096, mean reward: 0.770 [-12.962, 100.000], mean action: 1.557 [0.000, 3.000], mean observation: 0.121 [-0.798, 1.398], loss: 7.505900, mae: 50.638432, mean_q: 67.153511
  642839/1100000: episode: 1450, duration: 7.648s, episode steps: 1000, steps per second: 131, episode reward: 23.373, mean reward: 0.023 [-24.762, 27.640], mean action: 1.418 [0.000, 3.000], mean observation: 0.110 [-0.732, 1.393], loss: 7.924059, mae: 50.275570, mean_q: 66.717247
  643444/1100000: episode: 1451, duration: 4.287s, episode steps: 605, steps per second: 141, episode reward: 234.115, mean reward: 0.387 [-17.967, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.015 [-0.788, 1.420], loss: 7.867715, mae: 50.096119, mean_q: 66.788727
  643777/1100000: episode: 1452, duration: 2.275s, episode steps: 333, steps per second: 146, episode reward: 252.454, mean reward: 0.758 [-5.791, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: -0.018 [-0.647, 1.431], loss: 5.149208, mae: 50.421322, mean_q: 66.820923
  643990/1100000: episode: 1453, duration: 1.445s, episode steps: 213, steps per second: 147, episode reward: 10.451, mean reward: 0.049 [-100.000, 17.416], mean action: 1.568 [0.000, 3.000], mean observation: -0.074 [-0.684, 1.518], loss: 7.420608, mae: 50.267422, mean_q: 66.917252
  644142/1100000: episode: 1454, duration: 1.014s, episode steps: 152, steps per second: 150, episode reward: 56.794, mean reward: 0.374 [-100.000, 15.505], mean action: 1.599 [0.000, 3.000], mean observation: 0.012 [-1.346, 1.412], loss: 8.385609, mae: 50.433643, mean_q: 66.924782
  644636/1100000: episode: 1455, duration: 3.400s, episode steps: 494, steps per second: 145, episode reward: 212.954, mean reward: 0.431 [-20.160, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.195 [-0.727, 1.468], loss: 6.631950, mae: 50.477554, mean_q: 67.255028
  645536/1100000: episode: 1456, duration: 6.575s, episode steps: 900, steps per second: 137, episode reward: 156.947, mean reward: 0.174 [-20.173, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.255 [-1.074, 1.399], loss: 7.124110, mae: 49.895607, mean_q: 66.344894
  646218/1100000: episode: 1457, duration: 4.859s, episode steps: 682, steps per second: 140, episode reward: 232.685, mean reward: 0.341 [-19.136, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.273 [-0.903, 1.463], loss: 7.681789, mae: 50.005852, mean_q: 66.175095
  646516/1100000: episode: 1458, duration: 2.125s, episode steps: 298, steps per second: 140, episode reward: 283.410, mean reward: 0.951 [-23.535, 100.000], mean action: 1.963 [0.000, 3.000], mean observation: 0.129 [-0.779, 1.427], loss: 4.734523, mae: 50.155266, mean_q: 66.769623
  646707/1100000: episode: 1459, duration: 1.305s, episode steps: 191, steps per second: 146, episode reward: -1.325, mean reward: -0.007 [-100.000, 9.047], mean action: 1.670 [0.000, 3.000], mean observation: 0.058 [-0.931, 1.478], loss: 8.688849, mae: 50.023888, mean_q: 66.106285
  647707/1100000: episode: 1460, duration: 7.086s, episode steps: 1000, steps per second: 141, episode reward: 105.688, mean reward: 0.106 [-19.798, 22.899], mean action: 1.105 [0.000, 3.000], mean observation: 0.172 [-0.970, 1.483], loss: 8.461283, mae: 49.862396, mean_q: 66.032623
  648283/1100000: episode: 1461, duration: 4.086s, episode steps: 576, steps per second: 141, episode reward: 249.577, mean reward: 0.433 [-19.695, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.209 [-0.754, 1.414], loss: 9.468454, mae: 49.893883, mean_q: 66.101997
  648373/1100000: episode: 1462, duration: 0.603s, episode steps: 90, steps per second: 149, episode reward: -48.031, mean reward: -0.534 [-100.000, 15.802], mean action: 1.633 [0.000, 3.000], mean observation: -0.049 [-0.946, 1.405], loss: 9.699475, mae: 49.811211, mean_q: 66.051956
  648825/1100000: episode: 1463, duration: 3.200s, episode steps: 452, steps per second: 141, episode reward: 269.366, mean reward: 0.596 [-18.967, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.115 [-0.671, 1.412], loss: 7.674116, mae: 49.827076, mean_q: 66.001495
  649559/1100000: episode: 1464, duration: 5.354s, episode steps: 734, steps per second: 137, episode reward: 260.817, mean reward: 0.355 [-18.047, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.127 [-1.241, 1.398], loss: 9.977525, mae: 49.494919, mean_q: 65.413124
  649903/1100000: episode: 1465, duration: 2.390s, episode steps: 344, steps per second: 144, episode reward: 282.606, mean reward: 0.822 [-11.337, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.111 [-1.320, 1.501], loss: 7.466932, mae: 49.273914, mean_q: 65.616333
  650080/1100000: episode: 1466, duration: 1.199s, episode steps: 177, steps per second: 148, episode reward: -64.427, mean reward: -0.364 [-100.000, 8.673], mean action: 1.638 [0.000, 3.000], mean observation: -0.068 [-0.933, 2.340], loss: 4.357192, mae: 49.729740, mean_q: 66.148689
  650325/1100000: episode: 1467, duration: 1.661s, episode steps: 245, steps per second: 148, episode reward: 258.602, mean reward: 1.056 [-5.810, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.187 [-0.639, 1.435], loss: 5.198271, mae: 49.680637, mean_q: 66.414139
  650877/1100000: episode: 1468, duration: 3.899s, episode steps: 552, steps per second: 142, episode reward: 228.351, mean reward: 0.414 [-18.676, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.214 [-0.882, 1.387], loss: 6.734625, mae: 49.869713, mean_q: 66.389374
  651251/1100000: episode: 1469, duration: 2.594s, episode steps: 374, steps per second: 144, episode reward: 288.380, mean reward: 0.771 [-18.020, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.108 [-1.264, 1.400], loss: 8.519209, mae: 49.909489, mean_q: 66.392258
  651690/1100000: episode: 1470, duration: 3.068s, episode steps: 439, steps per second: 143, episode reward: 277.578, mean reward: 0.632 [-19.538, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.149 [-0.718, 1.522], loss: 6.698323, mae: 49.913910, mean_q: 66.488678
  651957/1100000: episode: 1471, duration: 1.823s, episode steps: 267, steps per second: 146, episode reward: 207.940, mean reward: 0.779 [-10.516, 100.000], mean action: 2.255 [0.000, 3.000], mean observation: 0.002 [-0.638, 1.443], loss: 8.560254, mae: 50.301064, mean_q: 66.681213
  652411/1100000: episode: 1472, duration: 3.162s, episode steps: 454, steps per second: 144, episode reward: -66.863, mean reward: -0.147 [-100.000, 17.521], mean action: 1.385 [0.000, 3.000], mean observation: 0.122 [-1.437, 1.477], loss: 6.004478, mae: 50.054008, mean_q: 66.383728
  652905/1100000: episode: 1473, duration: 3.568s, episode steps: 494, steps per second: 138, episode reward: 255.855, mean reward: 0.518 [-19.351, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.055 [-0.698, 1.387], loss: 10.651946, mae: 50.000870, mean_q: 66.396225
  653438/1100000: episode: 1474, duration: 3.869s, episode steps: 533, steps per second: 138, episode reward: 198.562, mean reward: 0.373 [-19.360, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: -0.001 [-0.679, 1.438], loss: 6.855821, mae: 49.765926, mean_q: 66.228867
  653968/1100000: episode: 1475, duration: 3.663s, episode steps: 530, steps per second: 145, episode reward: 235.145, mean reward: 0.444 [-10.265, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.152 [-1.473, 1.385], loss: 8.383451, mae: 49.851185, mean_q: 66.272110
  654085/1100000: episode: 1476, duration: 0.780s, episode steps: 117, steps per second: 150, episode reward: -36.725, mean reward: -0.314 [-100.000, 11.482], mean action: 1.462 [0.000, 3.000], mean observation: 0.083 [-0.697, 1.540], loss: 14.138350, mae: 49.873936, mean_q: 66.343460
  654471/1100000: episode: 1477, duration: 2.679s, episode steps: 386, steps per second: 144, episode reward: 290.123, mean reward: 0.752 [-11.521, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.072 [-0.772, 1.405], loss: 12.001743, mae: 50.043575, mean_q: 66.620987
  654684/1100000: episode: 1478, duration: 1.447s, episode steps: 213, steps per second: 147, episode reward: -239.415, mean reward: -1.124 [-100.000, 51.770], mean action: 1.798 [0.000, 3.000], mean observation: 0.049 [-0.812, 2.039], loss: 10.859200, mae: 50.353607, mean_q: 67.004288
  654837/1100000: episode: 1479, duration: 1.047s, episode steps: 153, steps per second: 146, episode reward: -51.533, mean reward: -0.337 [-100.000, 37.689], mean action: 1.647 [0.000, 3.000], mean observation: 0.062 [-0.708, 1.461], loss: 6.498640, mae: 50.360306, mean_q: 67.148132
  655441/1100000: episode: 1480, duration: 4.523s, episode steps: 604, steps per second: 134, episode reward: 181.921, mean reward: 0.301 [-18.958, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: -0.037 [-0.625, 1.395], loss: 9.694962, mae: 49.876980, mean_q: 66.406105
  656125/1100000: episode: 1481, duration: 4.752s, episode steps: 684, steps per second: 144, episode reward: 249.317, mean reward: 0.364 [-19.542, 100.000], mean action: 0.832 [0.000, 3.000], mean observation: 0.188 [-1.031, 1.400], loss: 6.165720, mae: 50.004093, mean_q: 66.421921
  656195/1100000: episode: 1482, duration: 0.465s, episode steps: 70, steps per second: 150, episode reward: -121.798, mean reward: -1.740 [-100.000, 5.450], mean action: 1.457 [0.000, 3.000], mean observation: 0.042 [-4.056, 1.393], loss: 3.656169, mae: 50.694996, mean_q: 67.288116
  656426/1100000: episode: 1483, duration: 1.569s, episode steps: 231, steps per second: 147, episode reward: 221.300, mean reward: 0.958 [-9.316, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.010 [-1.015, 1.398], loss: 5.065312, mae: 50.393402, mean_q: 67.074890
  656636/1100000: episode: 1484, duration: 1.414s, episode steps: 210, steps per second: 149, episode reward: 248.155, mean reward: 1.182 [-9.054, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.070 [-0.672, 1.399], loss: 10.746324, mae: 49.969082, mean_q: 66.354462
  656893/1100000: episode: 1485, duration: 1.747s, episode steps: 257, steps per second: 147, episode reward: -109.124, mean reward: -0.425 [-100.000, 29.123], mean action: 1.770 [0.000, 3.000], mean observation: 0.028 [-0.998, 1.471], loss: 7.626681, mae: 50.507698, mean_q: 67.034180
  657128/1100000: episode: 1486, duration: 1.623s, episode steps: 235, steps per second: 145, episode reward: 155.186, mean reward: 0.660 [-14.013, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: -0.109 [-0.976, 1.394], loss: 9.596101, mae: 50.028023, mean_q: 66.656960
  657328/1100000: episode: 1487, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: 266.856, mean reward: 1.334 [-10.485, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.026 [-0.744, 1.387], loss: 9.876465, mae: 49.723228, mean_q: 65.828865
  657807/1100000: episode: 1488, duration: 3.450s, episode steps: 479, steps per second: 139, episode reward: -57.725, mean reward: -0.121 [-100.000, 13.916], mean action: 1.718 [0.000, 3.000], mean observation: 0.058 [-0.824, 1.497], loss: 4.966621, mae: 50.160686, mean_q: 66.582748
  658243/1100000: episode: 1489, duration: 3.090s, episode steps: 436, steps per second: 141, episode reward: 254.517, mean reward: 0.584 [-3.241, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: -0.042 [-0.823, 1.398], loss: 7.294437, mae: 49.828648, mean_q: 66.215248
  658593/1100000: episode: 1490, duration: 2.427s, episode steps: 350, steps per second: 144, episode reward: 230.741, mean reward: 0.659 [-2.662, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: -0.028 [-0.633, 1.446], loss: 6.695586, mae: 49.714954, mean_q: 66.122902
  658925/1100000: episode: 1491, duration: 2.284s, episode steps: 332, steps per second: 145, episode reward: -250.745, mean reward: -0.755 [-100.000, 12.386], mean action: 1.377 [0.000, 3.000], mean observation: 0.068 [-1.152, 1.432], loss: 9.065660, mae: 50.124683, mean_q: 66.453209
  659136/1100000: episode: 1492, duration: 1.432s, episode steps: 211, steps per second: 147, episode reward: -55.523, mean reward: -0.263 [-100.000, 19.366], mean action: 1.730 [0.000, 3.000], mean observation: -0.034 [-0.773, 1.387], loss: 5.863482, mae: 50.112328, mean_q: 66.580513
  659419/1100000: episode: 1493, duration: 1.933s, episode steps: 283, steps per second: 146, episode reward: 246.568, mean reward: 0.871 [-10.148, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: -0.037 [-0.826, 1.438], loss: 7.330076, mae: 50.444527, mean_q: 66.880272
  659814/1100000: episode: 1494, duration: 2.718s, episode steps: 395, steps per second: 145, episode reward: 234.243, mean reward: 0.593 [-9.996, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: 0.004 [-0.600, 1.394], loss: 10.369833, mae: 50.101059, mean_q: 66.358444
  660179/1100000: episode: 1495, duration: 2.484s, episode steps: 365, steps per second: 147, episode reward: 291.894, mean reward: 0.800 [-18.557, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.136 [-1.031, 1.452], loss: 8.004714, mae: 49.845249, mean_q: 66.292900
  660373/1100000: episode: 1496, duration: 1.302s, episode steps: 194, steps per second: 149, episode reward: 35.896, mean reward: 0.185 [-100.000, 13.371], mean action: 1.634 [0.000, 3.000], mean observation: 0.127 [-0.771, 1.504], loss: 8.022367, mae: 49.425545, mean_q: 65.725876
  660631/1100000: episode: 1497, duration: 1.762s, episode steps: 258, steps per second: 146, episode reward: 255.528, mean reward: 0.990 [-11.519, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.117 [-1.199, 1.427], loss: 13.625713, mae: 49.581463, mean_q: 66.135735
  660762/1100000: episode: 1498, duration: 0.882s, episode steps: 131, steps per second: 148, episode reward: -53.485, mean reward: -0.408 [-100.000, 17.141], mean action: 1.901 [0.000, 3.000], mean observation: -0.060 [-0.772, 1.409], loss: 9.516960, mae: 50.113743, mean_q: 66.924896
  661762/1100000: episode: 1499, duration: 8.037s, episode steps: 1000, steps per second: 124, episode reward: 65.516, mean reward: 0.066 [-24.181, 23.338], mean action: 1.976 [0.000, 3.000], mean observation: 0.035 [-1.050, 1.392], loss: 8.602936, mae: 49.825298, mean_q: 66.098297
  662335/1100000: episode: 1500, duration: 4.079s, episode steps: 573, steps per second: 140, episode reward: 189.512, mean reward: 0.331 [-18.933, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.019 [-0.600, 1.402], loss: 7.262429, mae: 49.692837, mean_q: 65.907097
  662638/1100000: episode: 1501, duration: 2.065s, episode steps: 303, steps per second: 147, episode reward: 241.752, mean reward: 0.798 [-10.602, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.131 [-0.540, 1.505], loss: 6.091845, mae: 49.498375, mean_q: 65.837212
  662802/1100000: episode: 1502, duration: 1.105s, episode steps: 164, steps per second: 148, episode reward: 18.842, mean reward: 0.115 [-100.000, 15.990], mean action: 1.591 [0.000, 3.000], mean observation: 0.204 [-1.181, 1.454], loss: 15.202889, mae: 49.917458, mean_q: 66.148209
  662963/1100000: episode: 1503, duration: 1.079s, episode steps: 161, steps per second: 149, episode reward: 63.987, mean reward: 0.397 [-100.000, 13.898], mean action: 1.571 [0.000, 3.000], mean observation: 0.009 [-0.660, 1.391], loss: 16.593699, mae: 50.325558, mean_q: 66.988525
  663289/1100000: episode: 1504, duration: 2.238s, episode steps: 326, steps per second: 146, episode reward: 259.452, mean reward: 0.796 [-12.934, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.192 [-0.615, 1.500], loss: 8.752939, mae: 50.401337, mean_q: 66.738304
  663704/1100000: episode: 1505, duration: 3.029s, episode steps: 415, steps per second: 137, episode reward: 270.370, mean reward: 0.651 [-9.345, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: -0.054 [-0.840, 1.387], loss: 9.823210, mae: 49.877251, mean_q: 66.122833
  664056/1100000: episode: 1506, duration: 2.403s, episode steps: 352, steps per second: 146, episode reward: 282.849, mean reward: 0.804 [-17.941, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.143 [-1.050, 1.393], loss: 9.842074, mae: 50.450199, mean_q: 67.130676
  665056/1100000: episode: 1507, duration: 7.567s, episode steps: 1000, steps per second: 132, episode reward: 87.347, mean reward: 0.087 [-18.502, 22.340], mean action: 1.318 [0.000, 3.000], mean observation: 0.206 [-1.399, 1.480], loss: 6.234723, mae: 50.444847, mean_q: 66.870857
  665204/1100000: episode: 1508, duration: 0.995s, episode steps: 148, steps per second: 149, episode reward: 25.490, mean reward: 0.172 [-100.000, 19.632], mean action: 1.655 [0.000, 3.000], mean observation: 0.167 [-1.389, 1.393], loss: 9.685118, mae: 50.513969, mean_q: 66.670815
  665329/1100000: episode: 1509, duration: 0.844s, episode steps: 125, steps per second: 148, episode reward: -121.511, mean reward: -0.972 [-100.000, 7.779], mean action: 1.736 [0.000, 3.000], mean observation: 0.004 [-0.775, 3.321], loss: 15.423944, mae: 51.022266, mean_q: 67.285728
  665849/1100000: episode: 1510, duration: 3.726s, episode steps: 520, steps per second: 140, episode reward: 180.331, mean reward: 0.347 [-11.418, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: -0.048 [-1.250, 1.389], loss: 8.739820, mae: 50.653576, mean_q: 67.221588
  666604/1100000: episode: 1511, duration: 5.977s, episode steps: 755, steps per second: 126, episode reward: 141.827, mean reward: 0.188 [-18.072, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: -0.063 [-0.820, 1.467], loss: 9.487223, mae: 50.324142, mean_q: 66.932175
  666749/1100000: episode: 1512, duration: 0.973s, episode steps: 145, steps per second: 149, episode reward: -126.061, mean reward: -0.869 [-100.000, 7.960], mean action: 1.448 [0.000, 3.000], mean observation: -0.078 [-1.044, 3.458], loss: 10.856417, mae: 50.624245, mean_q: 67.128212
  666918/1100000: episode: 1513, duration: 1.129s, episode steps: 169, steps per second: 150, episode reward: -76.103, mean reward: -0.450 [-100.000, 9.378], mean action: 1.592 [0.000, 3.000], mean observation: -0.124 [-0.960, 3.511], loss: 9.999991, mae: 50.428463, mean_q: 66.666054
  667241/1100000: episode: 1514, duration: 2.220s, episode steps: 323, steps per second: 145, episode reward: 286.381, mean reward: 0.887 [-9.496, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.179 [-0.559, 1.397], loss: 8.773627, mae: 50.703568, mean_q: 66.959961
  667624/1100000: episode: 1515, duration: 2.729s, episode steps: 383, steps per second: 140, episode reward: 220.847, mean reward: 0.577 [-10.014, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.064 [-0.632, 1.443], loss: 7.731439, mae: 50.381268, mean_q: 66.587532
  667846/1100000: episode: 1516, duration: 1.499s, episode steps: 222, steps per second: 148, episode reward: -152.722, mean reward: -0.688 [-100.000, 18.755], mean action: 1.541 [0.000, 3.000], mean observation: 0.084 [-1.637, 1.408], loss: 8.157285, mae: 50.271214, mean_q: 66.402550
  668184/1100000: episode: 1517, duration: 2.329s, episode steps: 338, steps per second: 145, episode reward: 219.485, mean reward: 0.649 [-10.343, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: -0.050 [-1.056, 1.498], loss: 6.529245, mae: 50.834076, mean_q: 67.144875
  668410/1100000: episode: 1518, duration: 1.522s, episode steps: 226, steps per second: 149, episode reward: 48.051, mean reward: 0.213 [-100.000, 18.189], mean action: 1.473 [0.000, 3.000], mean observation: 0.123 [-0.885, 1.823], loss: 10.604909, mae: 50.580315, mean_q: 66.713905
  668609/1100000: episode: 1519, duration: 1.366s, episode steps: 199, steps per second: 146, episode reward: -14.574, mean reward: -0.073 [-100.000, 11.378], mean action: 1.884 [0.000, 3.000], mean observation: -0.073 [-0.730, 1.385], loss: 5.515790, mae: 50.915504, mean_q: 67.573524
  668838/1100000: episode: 1520, duration: 1.556s, episode steps: 229, steps per second: 147, episode reward: 280.503, mean reward: 1.225 [-10.811, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.095 [-0.788, 1.389], loss: 13.868590, mae: 50.868900, mean_q: 67.427101
  669035/1100000: episode: 1521, duration: 1.331s, episode steps: 197, steps per second: 148, episode reward: -215.104, mean reward: -1.092 [-100.000, 42.144], mean action: 1.365 [0.000, 3.000], mean observation: 0.003 [-0.800, 2.046], loss: 16.168818, mae: 50.570026, mean_q: 67.047813
  669190/1100000: episode: 1522, duration: 1.045s, episode steps: 155, steps per second: 148, episode reward: -235.806, mean reward: -1.521 [-100.000, 42.431], mean action: 1.800 [0.000, 3.000], mean observation: 0.024 [-0.787, 2.052], loss: 7.678256, mae: 51.323196, mean_q: 68.404228
  669737/1100000: episode: 1523, duration: 3.893s, episode steps: 547, steps per second: 141, episode reward: 225.700, mean reward: 0.413 [-12.938, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.154 [-0.566, 1.396], loss: 9.845441, mae: 50.864395, mean_q: 67.144920
  669849/1100000: episode: 1524, duration: 0.743s, episode steps: 112, steps per second: 151, episode reward: -210.828, mean reward: -1.882 [-100.000, 47.129], mean action: 1.134 [0.000, 3.000], mean observation: -0.186 [-3.382, 1.478], loss: 4.386518, mae: 50.782017, mean_q: 67.117271
  670302/1100000: episode: 1525, duration: 3.158s, episode steps: 453, steps per second: 143, episode reward: -122.696, mean reward: -0.271 [-100.000, 9.921], mean action: 1.627 [0.000, 3.000], mean observation: 0.061 [-0.627, 1.402], loss: 6.293792, mae: 50.754368, mean_q: 66.872437
  670618/1100000: episode: 1526, duration: 2.176s, episode steps: 316, steps per second: 145, episode reward: 267.543, mean reward: 0.847 [-20.192, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.118 [-0.912, 1.474], loss: 12.819819, mae: 50.652367, mean_q: 66.927238
  671013/1100000: episode: 1527, duration: 2.707s, episode steps: 395, steps per second: 146, episode reward: 226.974, mean reward: 0.575 [-4.970, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: -0.035 [-0.680, 1.472], loss: 7.781891, mae: 50.825691, mean_q: 67.284775
  671449/1100000: episode: 1528, duration: 2.999s, episode steps: 436, steps per second: 145, episode reward: 236.301, mean reward: 0.542 [-14.708, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.153 [-1.050, 1.415], loss: 10.515488, mae: 50.385445, mean_q: 66.173416
  671951/1100000: episode: 1529, duration: 3.588s, episode steps: 502, steps per second: 140, episode reward: 232.285, mean reward: 0.463 [-19.331, 100.000], mean action: 1.625 [0.000, 3.000], mean observation: 0.185 [-0.831, 1.402], loss: 9.662663, mae: 50.541878, mean_q: 66.595963
  672151/1100000: episode: 1530, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: 276.310, mean reward: 1.382 [-9.405, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.050 [-0.648, 1.411], loss: 6.394609, mae: 50.419701, mean_q: 66.383484
  672479/1100000: episode: 1531, duration: 2.268s, episode steps: 328, steps per second: 145, episode reward: 301.145, mean reward: 0.918 [-19.589, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.115 [-0.667, 1.527], loss: 9.099237, mae: 50.617931, mean_q: 66.313713
  673479/1100000: episode: 1532, duration: 7.198s, episode steps: 1000, steps per second: 139, episode reward: 116.354, mean reward: 0.116 [-23.419, 21.973], mean action: 0.903 [0.000, 3.000], mean observation: 0.050 [-0.944, 1.388], loss: 9.375379, mae: 50.405369, mean_q: 66.410797
  673897/1100000: episode: 1533, duration: 2.973s, episode steps: 418, steps per second: 141, episode reward: 201.196, mean reward: 0.481 [-19.853, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.203 [-0.816, 1.411], loss: 8.466205, mae: 50.292595, mean_q: 66.052177
  674515/1100000: episode: 1534, duration: 4.349s, episode steps: 618, steps per second: 142, episode reward: 176.842, mean reward: 0.286 [-18.741, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.142 [-0.694, 1.400], loss: 9.155169, mae: 50.238388, mean_q: 66.108727
  675461/1100000: episode: 1535, duration: 6.732s, episode steps: 946, steps per second: 141, episode reward: 121.899, mean reward: 0.129 [-22.837, 100.000], mean action: 1.801 [0.000, 3.000], mean observation: 0.179 [-0.918, 1.414], loss: 7.956849, mae: 50.184032, mean_q: 66.350578
  676127/1100000: episode: 1536, duration: 4.774s, episode steps: 666, steps per second: 140, episode reward: 243.229, mean reward: 0.365 [-19.289, 100.000], mean action: 0.749 [0.000, 3.000], mean observation: 0.121 [-1.146, 1.449], loss: 6.757639, mae: 50.216167, mean_q: 66.430016
  676445/1100000: episode: 1537, duration: 2.169s, episode steps: 318, steps per second: 147, episode reward: 293.664, mean reward: 0.923 [-19.830, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.100 [-0.864, 1.397], loss: 7.676179, mae: 50.929569, mean_q: 67.021660
  676882/1100000: episode: 1538, duration: 3.074s, episode steps: 437, steps per second: 142, episode reward: 243.111, mean reward: 0.556 [-11.243, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.183 [-1.068, 1.410], loss: 6.945869, mae: 50.128872, mean_q: 66.366768
  677014/1100000: episode: 1539, duration: 0.878s, episode steps: 132, steps per second: 150, episode reward: 18.995, mean reward: 0.144 [-100.000, 14.155], mean action: 1.409 [0.000, 3.000], mean observation: -0.012 [-0.701, 1.406], loss: 6.685529, mae: 50.200386, mean_q: 66.240868
  677576/1100000: episode: 1540, duration: 4.138s, episode steps: 562, steps per second: 136, episode reward: 189.385, mean reward: 0.337 [-19.889, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.138 [-0.609, 1.406], loss: 8.105412, mae: 50.389866, mean_q: 66.803909
  677716/1100000: episode: 1541, duration: 0.924s, episode steps: 140, steps per second: 152, episode reward: 44.956, mean reward: 0.321 [-100.000, 15.874], mean action: 1.179 [0.000, 3.000], mean observation: 0.120 [-0.768, 1.481], loss: 13.997227, mae: 51.140606, mean_q: 67.732979
  678235/1100000: episode: 1542, duration: 3.729s, episode steps: 519, steps per second: 139, episode reward: 191.073, mean reward: 0.368 [-19.044, 100.000], mean action: 1.942 [0.000, 3.000], mean observation: -0.003 [-0.615, 1.411], loss: 7.696210, mae: 50.990646, mean_q: 66.969063
  678600/1100000: episode: 1543, duration: 2.555s, episode steps: 365, steps per second: 143, episode reward: 268.354, mean reward: 0.735 [-17.533, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.124 [-0.730, 1.482], loss: 9.579139, mae: 51.018055, mean_q: 67.529877
  679330/1100000: episode: 1544, duration: 5.654s, episode steps: 730, steps per second: 129, episode reward: 97.190, mean reward: 0.133 [-12.942, 100.000], mean action: 1.741 [0.000, 3.000], mean observation: 0.015 [-1.357, 1.413], loss: 11.697525, mae: 51.279205, mean_q: 67.728050
  679636/1100000: episode: 1545, duration: 2.098s, episode steps: 306, steps per second: 146, episode reward: 227.789, mean reward: 0.744 [-9.920, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.008 [-0.600, 1.456], loss: 5.414104, mae: 51.041367, mean_q: 67.603340
  679844/1100000: episode: 1546, duration: 1.405s, episode steps: 208, steps per second: 148, episode reward: -102.621, mean reward: -0.493 [-100.000, 12.150], mean action: 1.394 [0.000, 3.000], mean observation: 0.195 [-0.877, 1.408], loss: 7.290653, mae: 51.504086, mean_q: 68.811310
  680040/1100000: episode: 1547, duration: 1.314s, episode steps: 196, steps per second: 149, episode reward: 258.427, mean reward: 1.319 [-17.779, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.156 [-0.682, 1.387], loss: 6.493332, mae: 51.157497, mean_q: 67.918106
  680475/1100000: episode: 1548, duration: 2.991s, episode steps: 435, steps per second: 145, episode reward: 249.666, mean reward: 0.574 [-19.944, 100.000], mean action: 0.538 [0.000, 3.000], mean observation: 0.127 [-0.735, 1.435], loss: 7.465106, mae: 52.174946, mean_q: 69.245247
  680690/1100000: episode: 1549, duration: 1.464s, episode steps: 215, steps per second: 147, episode reward: -3.890, mean reward: -0.018 [-100.000, 13.071], mean action: 1.898 [0.000, 3.000], mean observation: 0.184 [-1.495, 1.493], loss: 5.811074, mae: 51.888554, mean_q: 68.859673
  681004/1100000: episode: 1550, duration: 2.176s, episode steps: 314, steps per second: 144, episode reward: 197.315, mean reward: 0.628 [-17.242, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: -0.002 [-0.965, 1.443], loss: 11.753971, mae: 52.065941, mean_q: 68.920509
  681868/1100000: episode: 1551, duration: 6.143s, episode steps: 864, steps per second: 141, episode reward: 238.178, mean reward: 0.276 [-19.239, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.271 [-0.802, 1.388], loss: 10.791312, mae: 52.350197, mean_q: 69.553558
  681952/1100000: episode: 1552, duration: 0.560s, episode steps: 84, steps per second: 150, episode reward: -499.188, mean reward: -5.943 [-100.000, -1.673], mean action: 0.798 [0.000, 3.000], mean observation: -0.017 [-6.632, 1.461], loss: 5.476491, mae: 52.411030, mean_q: 69.383415
  682443/1100000: episode: 1553, duration: 3.340s, episode steps: 491, steps per second: 147, episode reward: 242.198, mean reward: 0.493 [-20.011, 100.000], mean action: 0.764 [0.000, 3.000], mean observation: 0.161 [-0.826, 1.448], loss: 8.134554, mae: 52.253349, mean_q: 69.178284
  682753/1100000: episode: 1554, duration: 2.132s, episode steps: 310, steps per second: 145, episode reward: 280.980, mean reward: 0.906 [-10.944, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.098 [-0.663, 1.424], loss: 11.155988, mae: 52.436993, mean_q: 69.369301
  683080/1100000: episode: 1555, duration: 2.301s, episode steps: 327, steps per second: 142, episode reward: 275.794, mean reward: 0.843 [-17.456, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.109 [-1.156, 1.386], loss: 9.819263, mae: 52.059555, mean_q: 68.932556
  683479/1100000: episode: 1556, duration: 2.715s, episode steps: 399, steps per second: 147, episode reward: 289.555, mean reward: 0.726 [-19.859, 100.000], mean action: 0.817 [0.000, 3.000], mean observation: 0.115 [-0.756, 1.386], loss: 11.056910, mae: 52.851006, mean_q: 69.594612
  683707/1100000: episode: 1557, duration: 1.526s, episode steps: 228, steps per second: 149, episode reward: 269.601, mean reward: 1.182 [-17.475, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.095 [-0.961, 1.428], loss: 4.697259, mae: 52.309120, mean_q: 69.313332
  684130/1100000: episode: 1558, duration: 2.932s, episode steps: 423, steps per second: 144, episode reward: 220.881, mean reward: 0.522 [-9.553, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: -0.071 [-0.754, 1.407], loss: 9.021982, mae: 52.351494, mean_q: 69.316330
  684547/1100000: episode: 1559, duration: 2.873s, episode steps: 417, steps per second: 145, episode reward: 269.940, mean reward: 0.647 [-17.340, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.134 [-0.623, 1.453], loss: 9.944635, mae: 52.549274, mean_q: 69.222969
  684872/1100000: episode: 1560, duration: 2.243s, episode steps: 325, steps per second: 145, episode reward: 253.019, mean reward: 0.779 [-12.545, 100.000], mean action: 1.569 [0.000, 3.000], mean observation: 0.160 [-0.550, 1.472], loss: 4.481178, mae: 52.286724, mean_q: 69.480209
  685051/1100000: episode: 1561, duration: 1.191s, episode steps: 179, steps per second: 150, episode reward: 266.038, mean reward: 1.486 [-2.234, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.077 [-0.902, 1.396], loss: 6.610372, mae: 52.470936, mean_q: 69.622009
  685273/1100000: episode: 1562, duration: 1.495s, episode steps: 222, steps per second: 149, episode reward: 307.010, mean reward: 1.383 [-6.688, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.040 [-0.774, 1.396], loss: 4.208854, mae: 52.252411, mean_q: 69.428314
  685587/1100000: episode: 1563, duration: 2.193s, episode steps: 314, steps per second: 143, episode reward: 216.283, mean reward: 0.689 [-10.728, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.010 [-0.740, 1.426], loss: 7.790929, mae: 52.057739, mean_q: 68.961754
  685779/1100000: episode: 1564, duration: 1.297s, episode steps: 192, steps per second: 148, episode reward: -170.922, mean reward: -0.890 [-100.000, 5.156], mean action: 1.568 [0.000, 3.000], mean observation: 0.258 [-0.721, 1.485], loss: 11.326772, mae: 52.577473, mean_q: 69.571815
  686297/1100000: episode: 1565, duration: 3.643s, episode steps: 518, steps per second: 142, episode reward: 223.804, mean reward: 0.432 [-19.271, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.214 [-1.547, 1.514], loss: 8.772014, mae: 52.122154, mean_q: 68.995178
  687269/1100000: episode: 1566, duration: 7.139s, episode steps: 972, steps per second: 136, episode reward: 211.111, mean reward: 0.217 [-19.744, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.067 [-0.762, 1.438], loss: 9.431679, mae: 51.857983, mean_q: 68.690666
  687763/1100000: episode: 1567, duration: 3.629s, episode steps: 494, steps per second: 136, episode reward: 210.880, mean reward: 0.427 [-4.285, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: -0.021 [-1.218, 1.391], loss: 9.620160, mae: 51.424885, mean_q: 68.264732
  688262/1100000: episode: 1568, duration: 3.475s, episode steps: 499, steps per second: 144, episode reward: 241.938, mean reward: 0.485 [-14.299, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.183 [-0.822, 1.471], loss: 11.545746, mae: 51.655769, mean_q: 68.368225
  688527/1100000: episode: 1569, duration: 1.794s, episode steps: 265, steps per second: 148, episode reward: -24.768, mean reward: -0.093 [-100.000, 10.234], mean action: 1.408 [0.000, 3.000], mean observation: 0.112 [-0.890, 1.407], loss: 6.797994, mae: 51.560059, mean_q: 68.146301
  688893/1100000: episode: 1570, duration: 2.537s, episode steps: 366, steps per second: 144, episode reward: 232.480, mean reward: 0.635 [-12.761, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.173 [-0.787, 1.389], loss: 9.345336, mae: 51.506233, mean_q: 68.494438
  689673/1100000: episode: 1571, duration: 6.047s, episode steps: 780, steps per second: 129, episode reward: 209.580, mean reward: 0.269 [-19.586, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: 0.038 [-1.128, 1.399], loss: 10.021119, mae: 51.886738, mean_q: 68.869743
  690504/1100000: episode: 1572, duration: 6.032s, episode steps: 831, steps per second: 138, episode reward: 219.882, mean reward: 0.265 [-20.034, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.204 [-0.720, 1.397], loss: 8.816860, mae: 51.505489, mean_q: 68.203323
  690740/1100000: episode: 1573, duration: 1.579s, episode steps: 236, steps per second: 149, episode reward: 231.851, mean reward: 0.982 [-14.845, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.130 [-0.750, 1.390], loss: 8.162174, mae: 51.588627, mean_q: 68.717033
  691443/1100000: episode: 1574, duration: 5.049s, episode steps: 703, steps per second: 139, episode reward: 159.288, mean reward: 0.227 [-19.515, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.118 [-1.034, 1.436], loss: 7.047369, mae: 51.778828, mean_q: 68.932358
  691691/1100000: episode: 1575, duration: 1.678s, episode steps: 248, steps per second: 148, episode reward: 296.708, mean reward: 1.196 [-17.479, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.112 [-0.754, 1.459], loss: 9.249492, mae: 52.522297, mean_q: 69.793533
  691901/1100000: episode: 1576, duration: 1.419s, episode steps: 210, steps per second: 148, episode reward: -294.251, mean reward: -1.401 [-100.000, 4.126], mean action: 1.733 [0.000, 3.000], mean observation: 0.094 [-1.361, 1.625], loss: 9.830058, mae: 52.430031, mean_q: 69.611252
  692541/1100000: episode: 1577, duration: 4.641s, episode steps: 640, steps per second: 138, episode reward: 180.323, mean reward: 0.282 [-17.652, 100.000], mean action: 2.041 [0.000, 3.000], mean observation: 0.160 [-0.649, 1.468], loss: 8.500631, mae: 52.554432, mean_q: 69.820107
  693541/1100000: episode: 1578, duration: 7.429s, episode steps: 1000, steps per second: 135, episode reward: 66.627, mean reward: 0.067 [-19.938, 14.326], mean action: 1.259 [0.000, 3.000], mean observation: 0.271 [-0.998, 1.410], loss: 10.395018, mae: 52.803307, mean_q: 69.782951
  694100/1100000: episode: 1579, duration: 4.063s, episode steps: 559, steps per second: 138, episode reward: -212.416, mean reward: -0.380 [-100.000, 21.321], mean action: 1.894 [0.000, 3.000], mean observation: 0.017 [-1.278, 1.577], loss: 10.549981, mae: 53.195297, mean_q: 70.053230
  694817/1100000: episode: 1580, duration: 4.901s, episode steps: 717, steps per second: 146, episode reward: 211.920, mean reward: 0.296 [-18.862, 100.000], mean action: 0.640 [0.000, 3.000], mean observation: 0.032 [-0.796, 1.412], loss: 8.076302, mae: 52.770996, mean_q: 69.714676
  695513/1100000: episode: 1581, duration: 5.401s, episode steps: 696, steps per second: 129, episode reward: 198.781, mean reward: 0.286 [-17.654, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: -0.077 [-0.804, 1.400], loss: 10.037711, mae: 52.345264, mean_q: 68.862259
  695812/1100000: episode: 1582, duration: 2.029s, episode steps: 299, steps per second: 147, episode reward: 264.961, mean reward: 0.886 [-12.093, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.193 [-0.807, 1.396], loss: 9.913864, mae: 52.846245, mean_q: 69.159180
  696258/1100000: episode: 1583, duration: 3.169s, episode steps: 446, steps per second: 141, episode reward: 253.203, mean reward: 0.568 [-19.099, 100.000], mean action: 0.870 [0.000, 3.000], mean observation: 0.219 [-1.209, 1.397], loss: 7.798669, mae: 52.249191, mean_q: 68.725739
  696509/1100000: episode: 1584, duration: 1.705s, episode steps: 251, steps per second: 147, episode reward: 207.733, mean reward: 0.828 [-13.076, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.165 [-0.666, 1.430], loss: 10.210277, mae: 52.120060, mean_q: 69.021660
  697022/1100000: episode: 1585, duration: 3.703s, episode steps: 513, steps per second: 139, episode reward: 240.628, mean reward: 0.469 [-18.933, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: -0.051 [-0.866, 1.423], loss: 11.196771, mae: 52.512863, mean_q: 69.057976
  697633/1100000: episode: 1586, duration: 4.543s, episode steps: 611, steps per second: 134, episode reward: 232.204, mean reward: 0.380 [-18.678, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: -0.024 [-0.864, 1.386], loss: 9.234292, mae: 52.325630, mean_q: 69.040405
  698239/1100000: episode: 1587, duration: 4.624s, episode steps: 606, steps per second: 131, episode reward: 158.055, mean reward: 0.261 [-18.841, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: -0.029 [-0.842, 1.406], loss: 9.125082, mae: 52.461967, mean_q: 68.926979
  699054/1100000: episode: 1588, duration: 6.299s, episode steps: 815, steps per second: 129, episode reward: 174.666, mean reward: 0.214 [-20.250, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: -0.012 [-0.791, 1.409], loss: 8.047401, mae: 52.438721, mean_q: 69.182701
  699674/1100000: episode: 1589, duration: 4.387s, episode steps: 620, steps per second: 141, episode reward: 240.950, mean reward: 0.389 [-18.504, 100.000], mean action: 0.997 [0.000, 3.000], mean observation: 0.231 [-0.660, 1.444], loss: 7.306192, mae: 52.077499, mean_q: 68.850502
  700096/1100000: episode: 1590, duration: 3.002s, episode steps: 422, steps per second: 141, episode reward: 254.478, mean reward: 0.603 [-10.983, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: -0.019 [-0.675, 1.389], loss: 8.608181, mae: 51.772743, mean_q: 68.473213
  700305/1100000: episode: 1591, duration: 1.420s, episode steps: 209, steps per second: 147, episode reward: -143.866, mean reward: -0.688 [-100.000, 9.882], mean action: 1.713 [0.000, 3.000], mean observation: 0.125 [-1.919, 1.426], loss: 21.502277, mae: 52.234291, mean_q: 68.394249
  700644/1100000: episode: 1592, duration: 2.365s, episode steps: 339, steps per second: 143, episode reward: 230.044, mean reward: 0.679 [-9.666, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: -0.031 [-0.887, 1.414], loss: 9.490791, mae: 52.075764, mean_q: 68.747375
  701015/1100000: episode: 1593, duration: 2.598s, episode steps: 371, steps per second: 143, episode reward: 176.149, mean reward: 0.475 [-15.795, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.012 [-0.897, 1.409], loss: 8.111692, mae: 52.035908, mean_q: 68.707703
  701267/1100000: episode: 1594, duration: 1.719s, episode steps: 252, steps per second: 147, episode reward: 260.907, mean reward: 1.035 [-6.816, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.069 [-0.704, 1.407], loss: 9.896756, mae: 52.342194, mean_q: 68.769562
  701906/1100000: episode: 1595, duration: 4.628s, episode steps: 639, steps per second: 138, episode reward: 260.316, mean reward: 0.407 [-20.384, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.168 [-0.720, 1.405], loss: 8.736323, mae: 51.919186, mean_q: 68.723145
  702135/1100000: episode: 1596, duration: 1.561s, episode steps: 229, steps per second: 147, episode reward: 269.579, mean reward: 1.177 [-11.015, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.067 [-0.829, 1.391], loss: 7.044502, mae: 51.679867, mean_q: 68.029732
  702581/1100000: episode: 1597, duration: 3.158s, episode steps: 446, steps per second: 141, episode reward: 162.715, mean reward: 0.365 [-18.118, 100.000], mean action: 2.002 [0.000, 3.000], mean observation: 0.130 [-0.766, 1.401], loss: 7.758062, mae: 52.034805, mean_q: 68.848709
  702905/1100000: episode: 1598, duration: 2.231s, episode steps: 324, steps per second: 145, episode reward: 243.301, mean reward: 0.751 [-18.269, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: -0.034 [-0.676, 1.392], loss: 8.361302, mae: 51.695831, mean_q: 68.325302
  703905/1100000: episode: 1599, duration: 7.456s, episode steps: 1000, steps per second: 134, episode reward: 117.771, mean reward: 0.118 [-22.515, 17.130], mean action: 2.413 [0.000, 3.000], mean observation: 0.220 [-1.186, 1.515], loss: 7.688759, mae: 51.543385, mean_q: 67.886703
  704109/1100000: episode: 1600, duration: 1.383s, episode steps: 204, steps per second: 148, episode reward: -123.192, mean reward: -0.604 [-100.000, 7.099], mean action: 1.809 [0.000, 3.000], mean observation: 0.077 [-0.695, 1.489], loss: 6.578548, mae: 51.486347, mean_q: 68.115456
  704287/1100000: episode: 1601, duration: 1.226s, episode steps: 178, steps per second: 145, episode reward: 244.570, mean reward: 1.374 [-9.005, 100.000], mean action: 1.663 [0.000, 3.000], mean observation: 0.072 [-0.667, 1.391], loss: 9.076833, mae: 51.636860, mean_q: 67.738586
  704797/1100000: episode: 1602, duration: 3.497s, episode steps: 510, steps per second: 146, episode reward: 250.031, mean reward: 0.490 [-17.741, 100.000], mean action: 0.802 [0.000, 3.000], mean observation: 0.023 [-0.707, 1.389], loss: 12.647128, mae: 51.701294, mean_q: 68.494286
  704952/1100000: episode: 1603, duration: 1.047s, episode steps: 155, steps per second: 148, episode reward: -74.978, mean reward: -0.484 [-100.000, 18.319], mean action: 1.510 [0.000, 3.000], mean observation: 0.082 [-1.172, 1.848], loss: 6.187713, mae: 51.719425, mean_q: 68.698944
  705234/1100000: episode: 1604, duration: 1.941s, episode steps: 282, steps per second: 145, episode reward: 236.329, mean reward: 0.838 [-10.625, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.081 [-0.697, 1.431], loss: 11.806953, mae: 51.503075, mean_q: 68.602402
  705479/1100000: episode: 1605, duration: 1.672s, episode steps: 245, steps per second: 147, episode reward: 257.530, mean reward: 1.051 [-10.043, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.057 [-0.658, 1.418], loss: 10.364933, mae: 51.248680, mean_q: 67.820450
  705990/1100000: episode: 1606, duration: 3.757s, episode steps: 511, steps per second: 136, episode reward: 204.602, mean reward: 0.400 [-17.798, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.243 [-0.743, 1.408], loss: 11.020748, mae: 51.042816, mean_q: 67.790207
  706148/1100000: episode: 1607, duration: 1.066s, episode steps: 158, steps per second: 148, episode reward: 2.542, mean reward: 0.016 [-100.000, 13.907], mean action: 1.842 [0.000, 3.000], mean observation: 0.195 [-1.535, 1.388], loss: 8.249884, mae: 51.363979, mean_q: 68.850395
  706542/1100000: episode: 1608, duration: 2.729s, episode steps: 394, steps per second: 144, episode reward: 280.993, mean reward: 0.713 [-17.124, 100.000], mean action: 0.799 [0.000, 3.000], mean observation: 0.153 [-0.747, 1.389], loss: 9.394476, mae: 50.894451, mean_q: 67.842575
  706675/1100000: episode: 1609, duration: 0.892s, episode steps: 133, steps per second: 149, episode reward: -29.129, mean reward: -0.219 [-100.000, 18.496], mean action: 1.827 [0.000, 3.000], mean observation: 0.135 [-1.635, 1.457], loss: 7.226847, mae: 51.455517, mean_q: 68.453819
  707345/1100000: episode: 1610, duration: 4.734s, episode steps: 670, steps per second: 142, episode reward: 221.490, mean reward: 0.331 [-23.613, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: -0.012 [-1.175, 1.392], loss: 8.179459, mae: 50.922855, mean_q: 67.710014
  707902/1100000: episode: 1611, duration: 3.957s, episode steps: 557, steps per second: 141, episode reward: 171.002, mean reward: 0.307 [-18.660, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.017 [-0.743, 1.419], loss: 9.207449, mae: 51.314098, mean_q: 68.373199
  708355/1100000: episode: 1612, duration: 3.190s, episode steps: 453, steps per second: 142, episode reward: 162.197, mean reward: 0.358 [-22.719, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.121 [-0.900, 1.449], loss: 9.583714, mae: 51.056145, mean_q: 67.961113
  709037/1100000: episode: 1613, duration: 4.983s, episode steps: 682, steps per second: 137, episode reward: 208.131, mean reward: 0.305 [-17.098, 100.000], mean action: 2.235 [0.000, 3.000], mean observation: 0.257 [-0.779, 1.391], loss: 9.221399, mae: 50.451672, mean_q: 67.116005
  709456/1100000: episode: 1614, duration: 2.958s, episode steps: 419, steps per second: 142, episode reward: 235.908, mean reward: 0.563 [-19.451, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.032 [-0.800, 1.414], loss: 10.132314, mae: 50.817116, mean_q: 67.285942
  709638/1100000: episode: 1615, duration: 1.241s, episode steps: 182, steps per second: 147, episode reward: 12.545, mean reward: 0.069 [-100.000, 18.886], mean action: 1.918 [0.000, 3.000], mean observation: 0.157 [-0.719, 1.411], loss: 10.151573, mae: 51.178646, mean_q: 68.208611
  709993/1100000: episode: 1616, duration: 2.492s, episode steps: 355, steps per second: 142, episode reward: 216.987, mean reward: 0.611 [-10.250, 100.000], mean action: 2.118 [0.000, 3.000], mean observation: 0.031 [-0.703, 1.508], loss: 7.126801, mae: 50.886024, mean_q: 67.609818
  710245/1100000: episode: 1617, duration: 1.730s, episode steps: 252, steps per second: 146, episode reward: 222.300, mean reward: 0.882 [-10.281, 100.000], mean action: 2.131 [0.000, 3.000], mean observation: 0.116 [-0.686, 1.426], loss: 7.604489, mae: 51.003365, mean_q: 67.728256
  710444/1100000: episode: 1618, duration: 1.350s, episode steps: 199, steps per second: 147, episode reward: 272.947, mean reward: 1.372 [-9.879, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.077 [-0.871, 1.389], loss: 14.862241, mae: 51.357460, mean_q: 68.259598
  710695/1100000: episode: 1619, duration: 1.710s, episode steps: 251, steps per second: 147, episode reward: 236.770, mean reward: 0.943 [-8.392, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.094 [-0.767, 1.409], loss: 11.308887, mae: 51.264271, mean_q: 68.208908
  711399/1100000: episode: 1620, duration: 5.392s, episode steps: 704, steps per second: 131, episode reward: 201.677, mean reward: 0.286 [-19.664, 100.000], mean action: 2.036 [0.000, 3.000], mean observation: 0.168 [-1.427, 1.471], loss: 8.908048, mae: 51.233353, mean_q: 68.097000
  711680/1100000: episode: 1621, duration: 1.909s, episode steps: 281, steps per second: 147, episode reward: -301.496, mean reward: -1.073 [-100.000, 7.514], mean action: 1.552 [0.000, 3.000], mean observation: 0.122 [-0.799, 2.347], loss: 9.178269, mae: 50.798046, mean_q: 67.644684
  711988/1100000: episode: 1622, duration: 2.093s, episode steps: 308, steps per second: 147, episode reward: 296.535, mean reward: 0.963 [-17.338, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.129 [-0.796, 1.449], loss: 6.186887, mae: 51.444874, mean_q: 68.155693
  712401/1100000: episode: 1623, duration: 2.959s, episode steps: 413, steps per second: 140, episode reward: 284.901, mean reward: 0.690 [-18.275, 100.000], mean action: 0.843 [0.000, 3.000], mean observation: 0.124 [-0.726, 1.415], loss: 8.675155, mae: 51.889988, mean_q: 68.922737
  712878/1100000: episode: 1624, duration: 3.506s, episode steps: 477, steps per second: 136, episode reward: 246.319, mean reward: 0.516 [-5.333, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: -0.029 [-0.620, 1.419], loss: 10.285661, mae: 52.015602, mean_q: 69.027466
  713878/1100000: episode: 1625, duration: 7.378s, episode steps: 1000, steps per second: 136, episode reward: -84.684, mean reward: -0.085 [-5.607, 4.448], mean action: 1.695 [0.000, 3.000], mean observation: 0.082 [-0.735, 1.386], loss: 8.231575, mae: 51.535172, mean_q: 68.527580
  714320/1100000: episode: 1626, duration: 3.125s, episode steps: 442, steps per second: 141, episode reward: 290.610, mean reward: 0.657 [-19.940, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.141 [-1.044, 1.433], loss: 8.379696, mae: 51.369526, mean_q: 68.235016
  714639/1100000: episode: 1627, duration: 2.179s, episode steps: 319, steps per second: 146, episode reward: 242.543, mean reward: 0.760 [-10.730, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.180 [-0.707, 1.459], loss: 7.211154, mae: 51.253967, mean_q: 68.139572
  714740/1100000: episode: 1628, duration: 0.674s, episode steps: 101, steps per second: 150, episode reward: -50.941, mean reward: -0.504 [-100.000, 12.676], mean action: 1.436 [0.000, 3.000], mean observation: 0.035 [-1.326, 1.456], loss: 5.431812, mae: 51.273785, mean_q: 68.114502
  715112/1100000: episode: 1629, duration: 2.533s, episode steps: 372, steps per second: 147, episode reward: 230.362, mean reward: 0.619 [-18.342, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.182 [-0.604, 1.400], loss: 7.944646, mae: 51.323517, mean_q: 68.439751
  715804/1100000: episode: 1630, duration: 5.455s, episode steps: 692, steps per second: 127, episode reward: 223.431, mean reward: 0.323 [-20.329, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: -0.007 [-0.652, 1.398], loss: 5.942518, mae: 51.096176, mean_q: 68.178375
  716186/1100000: episode: 1631, duration: 2.766s, episode steps: 382, steps per second: 138, episode reward: 246.600, mean reward: 0.646 [-18.523, 100.000], mean action: 1.822 [0.000, 3.000], mean observation: 0.204 [-0.730, 1.394], loss: 7.061866, mae: 50.905308, mean_q: 67.808075
  716510/1100000: episode: 1632, duration: 2.262s, episode steps: 324, steps per second: 143, episode reward: 260.399, mean reward: 0.804 [-17.891, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.100 [-0.793, 1.395], loss: 10.062008, mae: 51.099964, mean_q: 68.137352
  716710/1100000: episode: 1633, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -43.378, mean reward: -0.217 [-100.000, 27.425], mean action: 1.355 [0.000, 3.000], mean observation: 0.068 [-0.799, 1.405], loss: 9.600458, mae: 50.985577, mean_q: 67.801834
  717266/1100000: episode: 1634, duration: 3.969s, episode steps: 556, steps per second: 140, episode reward: 203.084, mean reward: 0.365 [-18.387, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.165 [-0.709, 1.413], loss: 8.696141, mae: 51.524456, mean_q: 68.601166
  717377/1100000: episode: 1635, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: -217.790, mean reward: -1.962 [-100.000, 22.951], mean action: 1.369 [0.000, 3.000], mean observation: -0.146 [-4.640, 1.448], loss: 7.735168, mae: 51.313038, mean_q: 68.526878
  717483/1100000: episode: 1636, duration: 0.710s, episode steps: 106, steps per second: 149, episode reward: -236.682, mean reward: -2.233 [-100.000, 3.327], mean action: 1.698 [0.000, 3.000], mean observation: 0.036 [-1.658, 1.399], loss: 7.671461, mae: 50.704052, mean_q: 67.374672
  717851/1100000: episode: 1637, duration: 2.539s, episode steps: 368, steps per second: 145, episode reward: 307.420, mean reward: 0.835 [-18.427, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.095 [-0.707, 1.478], loss: 16.733482, mae: 51.273602, mean_q: 67.993736
  718317/1100000: episode: 1638, duration: 3.305s, episode steps: 466, steps per second: 141, episode reward: 237.052, mean reward: 0.509 [-18.671, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: -0.005 [-0.725, 1.446], loss: 12.453220, mae: 50.936836, mean_q: 67.713654
  718792/1100000: episode: 1639, duration: 3.290s, episode steps: 475, steps per second: 144, episode reward: 146.463, mean reward: 0.308 [-19.415, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: -0.050 [-1.431, 1.432], loss: 8.935442, mae: 50.717064, mean_q: 67.474045
  719109/1100000: episode: 1640, duration: 2.175s, episode steps: 317, steps per second: 146, episode reward: 241.696, mean reward: 0.762 [-21.070, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.198 [-0.944, 1.388], loss: 16.218801, mae: 50.990742, mean_q: 67.783035
  719330/1100000: episode: 1641, duration: 1.495s, episode steps: 221, steps per second: 148, episode reward: -117.570, mean reward: -0.532 [-100.000, 9.184], mean action: 1.869 [0.000, 3.000], mean observation: -0.132 [-1.001, 1.480], loss: 7.403373, mae: 51.183716, mean_q: 68.146912
  719492/1100000: episode: 1642, duration: 1.094s, episode steps: 162, steps per second: 148, episode reward: -38.711, mean reward: -0.239 [-100.000, 15.891], mean action: 1.340 [0.000, 3.000], mean observation: -0.093 [-0.770, 2.720], loss: 6.451285, mae: 50.985760, mean_q: 67.679588
  719706/1100000: episode: 1643, duration: 1.447s, episode steps: 214, steps per second: 148, episode reward: -12.009, mean reward: -0.056 [-100.000, 10.342], mean action: 1.556 [0.000, 3.000], mean observation: 0.123 [-0.693, 1.413], loss: 11.297713, mae: 50.442337, mean_q: 67.259117
  720056/1100000: episode: 1644, duration: 2.416s, episode steps: 350, steps per second: 145, episode reward: -236.871, mean reward: -0.677 [-100.000, 54.131], mean action: 1.366 [0.000, 3.000], mean observation: -0.047 [-1.747, 1.397], loss: 11.512880, mae: 51.085369, mean_q: 68.305870
  720120/1100000: episode: 1645, duration: 0.423s, episode steps: 64, steps per second: 151, episode reward: -126.457, mean reward: -1.976 [-100.000, 7.063], mean action: 0.750 [0.000, 3.000], mean observation: -0.010 [-1.750, 1.400], loss: 6.754084, mae: 49.533962, mean_q: 66.098213
  720623/1100000: episode: 1646, duration: 3.671s, episode steps: 503, steps per second: 137, episode reward: 163.216, mean reward: 0.324 [-11.318, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: -0.075 [-0.600, 1.400], loss: 9.758645, mae: 50.687172, mean_q: 67.588890
  720842/1100000: episode: 1647, duration: 1.486s, episode steps: 219, steps per second: 147, episode reward: 256.354, mean reward: 1.171 [-9.611, 100.000], mean action: 1.680 [0.000, 3.000], mean observation: 0.165 [-0.725, 1.414], loss: 12.123760, mae: 50.385380, mean_q: 66.956619
  721105/1100000: episode: 1648, duration: 1.822s, episode steps: 263, steps per second: 144, episode reward: -163.838, mean reward: -0.623 [-100.000, 10.709], mean action: 1.426 [0.000, 3.000], mean observation: -0.058 [-1.550, 1.481], loss: 12.977758, mae: 50.991222, mean_q: 67.820709
  721199/1100000: episode: 1649, duration: 0.622s, episode steps: 94, steps per second: 151, episode reward: -254.622, mean reward: -2.709 [-100.000, 1.118], mean action: 1.298 [0.000, 3.000], mean observation: -0.125 [-1.791, 1.416], loss: 12.655233, mae: 50.433632, mean_q: 67.117432
  721454/1100000: episode: 1650, duration: 1.757s, episode steps: 255, steps per second: 145, episode reward: -96.571, mean reward: -0.379 [-100.000, 16.929], mean action: 1.831 [0.000, 3.000], mean observation: -0.028 [-1.263, 1.392], loss: 14.316117, mae: 51.118042, mean_q: 67.786446
  721823/1100000: episode: 1651, duration: 2.592s, episode steps: 369, steps per second: 142, episode reward: 258.462, mean reward: 0.700 [-3.900, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: -0.018 [-0.811, 1.429], loss: 11.968282, mae: 50.796429, mean_q: 67.314629
  722169/1100000: episode: 1652, duration: 2.356s, episode steps: 346, steps per second: 147, episode reward: 238.882, mean reward: 0.690 [-17.634, 100.000], mean action: 0.775 [0.000, 3.000], mean observation: 0.073 [-0.618, 1.413], loss: 9.980701, mae: 50.982224, mean_q: 67.961143
  722486/1100000: episode: 1653, duration: 2.230s, episode steps: 317, steps per second: 142, episode reward: -7.061, mean reward: -0.022 [-100.000, 14.534], mean action: 1.864 [0.000, 3.000], mean observation: 0.116 [-0.707, 1.391], loss: 8.497871, mae: 50.169762, mean_q: 66.860039
  722577/1100000: episode: 1654, duration: 0.620s, episode steps: 91, steps per second: 147, episode reward: -178.482, mean reward: -1.961 [-100.000, 3.364], mean action: 2.220 [0.000, 3.000], mean observation: -0.127 [-1.567, 1.412], loss: 12.229218, mae: 50.804585, mean_q: 68.059639
  723010/1100000: episode: 1655, duration: 3.005s, episode steps: 433, steps per second: 144, episode reward: 201.148, mean reward: 0.465 [-18.298, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: 0.218 [-1.013, 1.408], loss: 9.634949, mae: 50.412601, mean_q: 67.078720
  723336/1100000: episode: 1656, duration: 2.295s, episode steps: 326, steps per second: 142, episode reward: 205.719, mean reward: 0.631 [-21.656, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.053 [-1.095, 1.426], loss: 7.029117, mae: 50.729557, mean_q: 67.320122
  723524/1100000: episode: 1657, duration: 1.267s, episode steps: 188, steps per second: 148, episode reward: -170.281, mean reward: -0.906 [-100.000, 4.517], mean action: 1.654 [0.000, 3.000], mean observation: -0.141 [-1.003, 1.416], loss: 7.451312, mae: 50.754204, mean_q: 67.142319
  723628/1100000: episode: 1658, duration: 0.699s, episode steps: 104, steps per second: 149, episode reward: -167.447, mean reward: -1.610 [-100.000, 3.268], mean action: 1.913 [0.000, 3.000], mean observation: -0.069 [-1.440, 1.388], loss: 12.206981, mae: 51.102024, mean_q: 67.782501
  724124/1100000: episode: 1659, duration: 3.697s, episode steps: 496, steps per second: 134, episode reward: 234.351, mean reward: 0.472 [-20.215, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.155 [-0.752, 1.452], loss: 13.253161, mae: 50.765358, mean_q: 67.438118
  724277/1100000: episode: 1660, duration: 1.034s, episode steps: 153, steps per second: 148, episode reward: -51.809, mean reward: -0.339 [-100.000, 16.431], mean action: 1.987 [0.000, 3.000], mean observation: -0.097 [-1.081, 1.433], loss: 6.043024, mae: 51.106453, mean_q: 67.929573
  724634/1100000: episode: 1661, duration: 2.470s, episode steps: 357, steps per second: 145, episode reward: -278.138, mean reward: -0.779 [-100.000, 21.064], mean action: 1.835 [0.000, 3.000], mean observation: -0.073 [-1.276, 1.396], loss: 18.065147, mae: 51.082249, mean_q: 67.804558
  724809/1100000: episode: 1662, duration: 1.184s, episode steps: 175, steps per second: 148, episode reward: -54.028, mean reward: -0.309 [-100.000, 9.464], mean action: 1.720 [0.000, 3.000], mean observation: 0.114 [-0.950, 1.409], loss: 13.390216, mae: 51.284431, mean_q: 67.828491
  725385/1100000: episode: 1663, duration: 4.340s, episode steps: 576, steps per second: 133, episode reward: 201.492, mean reward: 0.350 [-18.501, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: -0.030 [-0.826, 1.394], loss: 9.010403, mae: 50.600605, mean_q: 67.378853
  725836/1100000: episode: 1664, duration: 3.228s, episode steps: 451, steps per second: 140, episode reward: 252.445, mean reward: 0.560 [-18.674, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.084 [-0.615, 1.493], loss: 9.853259, mae: 50.674160, mean_q: 67.418808
  726117/1100000: episode: 1665, duration: 1.934s, episode steps: 281, steps per second: 145, episode reward: 259.056, mean reward: 0.922 [-17.377, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.081 [-0.700, 1.400], loss: 22.199987, mae: 50.711788, mean_q: 67.802521
  726875/1100000: episode: 1666, duration: 5.667s, episode steps: 758, steps per second: 134, episode reward: 225.866, mean reward: 0.298 [-20.121, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.114 [-1.011, 1.407], loss: 10.015351, mae: 50.882214, mean_q: 67.942596
  727003/1100000: episode: 1667, duration: 0.881s, episode steps: 128, steps per second: 145, episode reward: -173.674, mean reward: -1.357 [-100.000, 5.140], mean action: 1.648 [0.000, 3.000], mean observation: 0.091 [-1.703, 5.784], loss: 16.126625, mae: 51.421871, mean_q: 68.140930
  727406/1100000: episode: 1668, duration: 2.817s, episode steps: 403, steps per second: 143, episode reward: -111.426, mean reward: -0.276 [-100.000, 20.636], mean action: 2.114 [0.000, 3.000], mean observation: -0.040 [-1.002, 1.390], loss: 14.392293, mae: 51.069401, mean_q: 68.332466
  727666/1100000: episode: 1669, duration: 1.768s, episode steps: 260, steps per second: 147, episode reward: 246.415, mean reward: 0.948 [-11.320, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.208 [-0.934, 1.396], loss: 15.864066, mae: 51.014160, mean_q: 68.084099
  727927/1100000: episode: 1670, duration: 1.778s, episode steps: 261, steps per second: 147, episode reward: 244.715, mean reward: 0.938 [-10.688, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.189 [-0.691, 1.417], loss: 20.687977, mae: 50.913792, mean_q: 67.584015
  728918/1100000: episode: 1671, duration: 7.103s, episode steps: 991, steps per second: 140, episode reward: 224.967, mean reward: 0.227 [-20.733, 100.000], mean action: 0.811 [0.000, 3.000], mean observation: 0.050 [-0.600, 1.498], loss: 8.820271, mae: 50.791553, mean_q: 67.645103
  729368/1100000: episode: 1672, duration: 3.234s, episode steps: 450, steps per second: 139, episode reward: -156.430, mean reward: -0.348 [-100.000, 19.696], mean action: 1.749 [0.000, 3.000], mean observation: -0.033 [-1.992, 1.409], loss: 8.385038, mae: 50.803280, mean_q: 67.680214
  729777/1100000: episode: 1673, duration: 2.901s, episode steps: 409, steps per second: 141, episode reward: 190.662, mean reward: 0.466 [-11.656, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: -0.031 [-1.187, 1.445], loss: 7.885233, mae: 51.478199, mean_q: 68.669113
  730038/1100000: episode: 1674, duration: 1.785s, episode steps: 261, steps per second: 146, episode reward: -127.375, mean reward: -0.488 [-100.000, 19.812], mean action: 1.996 [0.000, 3.000], mean observation: 0.107 [-0.786, 1.691], loss: 9.036030, mae: 51.424713, mean_q: 68.774696
  730420/1100000: episode: 1675, duration: 2.642s, episode steps: 382, steps per second: 145, episode reward: 233.846, mean reward: 0.612 [-19.967, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: 0.139 [-0.810, 1.388], loss: 11.780956, mae: 51.954197, mean_q: 69.520340
  730730/1100000: episode: 1676, duration: 2.112s, episode steps: 310, steps per second: 147, episode reward: -179.362, mean reward: -0.579 [-100.000, 13.468], mean action: 1.481 [0.000, 3.000], mean observation: 0.136 [-1.000, 1.454], loss: 7.625129, mae: 51.808613, mean_q: 69.194000
  731019/1100000: episode: 1677, duration: 2.058s, episode steps: 289, steps per second: 140, episode reward: 181.517, mean reward: 0.628 [-12.864, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.070 [-0.890, 1.396], loss: 11.398970, mae: 51.258022, mean_q: 68.083900
  731406/1100000: episode: 1678, duration: 2.653s, episode steps: 387, steps per second: 146, episode reward: 290.887, mean reward: 0.752 [-19.810, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.083 [-0.870, 1.401], loss: 11.445675, mae: 51.381878, mean_q: 68.177223
  731663/1100000: episode: 1679, duration: 1.744s, episode steps: 257, steps per second: 147, episode reward: 301.173, mean reward: 1.172 [-9.169, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.070 [-0.951, 1.388], loss: 8.307361, mae: 51.726082, mean_q: 68.492966
  731884/1100000: episode: 1680, duration: 1.502s, episode steps: 221, steps per second: 147, episode reward: -196.051, mean reward: -0.887 [-100.000, 72.543], mean action: 1.552 [0.000, 3.000], mean observation: -0.143 [-2.071, 1.405], loss: 8.167420, mae: 51.847214, mean_q: 68.775955
  732206/1100000: episode: 1681, duration: 2.201s, episode steps: 322, steps per second: 146, episode reward: 222.713, mean reward: 0.692 [-10.388, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.139 [-0.632, 1.494], loss: 8.597807, mae: 51.595909, mean_q: 68.349983
  732360/1100000: episode: 1682, duration: 1.040s, episode steps: 154, steps per second: 148, episode reward: -324.367, mean reward: -2.106 [-100.000, 5.914], mean action: 1.825 [0.000, 3.000], mean observation: -0.066 [-4.826, 1.410], loss: 14.093940, mae: 51.937473, mean_q: 68.381607
  732578/1100000: episode: 1683, duration: 1.479s, episode steps: 218, steps per second: 147, episode reward: 279.444, mean reward: 1.282 [-10.922, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.213 [-0.605, 1.389], loss: 10.887355, mae: 51.542606, mean_q: 68.601593
  733234/1100000: episode: 1684, duration: 4.783s, episode steps: 656, steps per second: 137, episode reward: 260.729, mean reward: 0.397 [-19.229, 100.000], mean action: 0.852 [0.000, 3.000], mean observation: 0.140 [-0.601, 1.443], loss: 10.593427, mae: 51.492008, mean_q: 68.483734
  733450/1100000: episode: 1685, duration: 1.443s, episode steps: 216, steps per second: 150, episode reward: 265.345, mean reward: 1.228 [-6.927, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.187 [-1.079, 1.390], loss: 11.995592, mae: 51.214153, mean_q: 68.124916
  733670/1100000: episode: 1686, duration: 1.479s, episode steps: 220, steps per second: 149, episode reward: 232.896, mean reward: 1.059 [-3.066, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.163 [-0.782, 1.463], loss: 7.368356, mae: 51.509094, mean_q: 68.435135
  734294/1100000: episode: 1687, duration: 4.499s, episode steps: 624, steps per second: 139, episode reward: 174.039, mean reward: 0.279 [-17.959, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.090 [-0.787, 1.406], loss: 6.593918, mae: 51.164864, mean_q: 68.132988
  734684/1100000: episode: 1688, duration: 2.652s, episode steps: 390, steps per second: 147, episode reward: 235.537, mean reward: 0.604 [-11.800, 100.000], mean action: 0.846 [0.000, 3.000], mean observation: 0.154 [-0.679, 1.400], loss: 12.233179, mae: 51.761940, mean_q: 68.710464
  734953/1100000: episode: 1689, duration: 1.842s, episode steps: 269, steps per second: 146, episode reward: 198.054, mean reward: 0.736 [-10.085, 100.000], mean action: 2.030 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.408], loss: 6.825848, mae: 51.494488, mean_q: 68.305748
  735052/1100000: episode: 1690, duration: 0.654s, episode steps: 99, steps per second: 151, episode reward: -280.278, mean reward: -2.831 [-100.000, 4.926], mean action: 1.687 [0.000, 3.000], mean observation: -0.055 [-2.071, 1.432], loss: 5.312635, mae: 51.917999, mean_q: 68.926552
  735521/1100000: episode: 1691, duration: 3.344s, episode steps: 469, steps per second: 140, episode reward: 271.539, mean reward: 0.579 [-11.737, 100.000], mean action: 0.889 [0.000, 3.000], mean observation: 0.134 [-0.695, 1.459], loss: 5.933918, mae: 51.624783, mean_q: 68.434723
  735625/1100000: episode: 1692, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -78.582, mean reward: -0.756 [-100.000, 8.356], mean action: 1.394 [0.000, 3.000], mean observation: 0.120 [-1.031, 1.501], loss: 4.620350, mae: 52.615768, mean_q: 69.738823
  736006/1100000: episode: 1693, duration: 2.671s, episode steps: 381, steps per second: 143, episode reward: 257.899, mean reward: 0.677 [-2.715, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: -0.019 [-0.600, 1.465], loss: 7.545211, mae: 51.939716, mean_q: 69.145592
  736797/1100000: episode: 1694, duration: 5.741s, episode steps: 791, steps per second: 138, episode reward: 228.906, mean reward: 0.289 [-17.069, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.131 [-0.634, 1.396], loss: 9.285333, mae: 51.955841, mean_q: 69.087524
  737342/1100000: episode: 1695, duration: 3.881s, episode steps: 545, steps per second: 140, episode reward: 212.878, mean reward: 0.391 [-19.487, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.108 [-0.701, 1.463], loss: 8.783715, mae: 51.797485, mean_q: 68.643257
  737841/1100000: episode: 1696, duration: 3.544s, episode steps: 499, steps per second: 141, episode reward: 294.535, mean reward: 0.590 [-18.813, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.110 [-0.786, 1.394], loss: 6.836115, mae: 51.863281, mean_q: 68.754196
  738134/1100000: episode: 1697, duration: 2.021s, episode steps: 293, steps per second: 145, episode reward: -13.822, mean reward: -0.047 [-100.000, 19.141], mean action: 1.768 [0.000, 3.000], mean observation: -0.011 [-0.837, 1.854], loss: 11.648717, mae: 51.820259, mean_q: 68.526039
  738854/1100000: episode: 1698, duration: 5.175s, episode steps: 720, steps per second: 139, episode reward: 216.778, mean reward: 0.301 [-17.935, 100.000], mean action: 2.249 [0.000, 3.000], mean observation: 0.088 [-0.778, 2.081], loss: 7.085229, mae: 51.929806, mean_q: 68.818672
  739438/1100000: episode: 1699, duration: 4.173s, episode steps: 584, steps per second: 140, episode reward: 237.611, mean reward: 0.407 [-19.598, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.241 [-0.527, 1.398], loss: 9.133383, mae: 52.302288, mean_q: 69.225288
  739799/1100000: episode: 1700, duration: 2.534s, episode steps: 361, steps per second: 142, episode reward: -132.610, mean reward: -0.367 [-100.000, 12.518], mean action: 1.972 [0.000, 3.000], mean observation: 0.138 [-0.760, 1.386], loss: 10.106140, mae: 52.151142, mean_q: 68.902611
  740415/1100000: episode: 1701, duration: 4.536s, episode steps: 616, steps per second: 136, episode reward: 182.215, mean reward: 0.296 [-20.003, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.210 [-0.719, 1.424], loss: 7.062633, mae: 52.196362, mean_q: 69.166298
  740516/1100000: episode: 1702, duration: 0.675s, episode steps: 101, steps per second: 150, episode reward: -173.132, mean reward: -1.714 [-100.000, 18.631], mean action: 1.307 [0.000, 3.000], mean observation: -0.070 [-4.804, 1.442], loss: 11.817951, mae: 52.297890, mean_q: 69.106537
  741012/1100000: episode: 1703, duration: 3.424s, episode steps: 496, steps per second: 145, episode reward: 212.450, mean reward: 0.428 [-18.897, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.204 [-0.897, 1.389], loss: 8.771729, mae: 52.387650, mean_q: 69.216560
  741324/1100000: episode: 1704, duration: 2.166s, episode steps: 312, steps per second: 144, episode reward: 256.432, mean reward: 0.822 [-19.237, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: -0.013 [-0.880, 1.408], loss: 9.906388, mae: 52.330750, mean_q: 69.073395
  741510/1100000: episode: 1705, duration: 1.245s, episode steps: 186, steps per second: 149, episode reward: -131.135, mean reward: -0.705 [-100.000, 2.702], mean action: 1.892 [0.000, 3.000], mean observation: 0.088 [-1.002, 1.527], loss: 9.672935, mae: 52.585152, mean_q: 69.280891
  741676/1100000: episode: 1706, duration: 1.114s, episode steps: 166, steps per second: 149, episode reward: -141.602, mean reward: -0.853 [-100.000, 3.323], mean action: 1.669 [0.000, 3.000], mean observation: 0.051 [-1.001, 1.416], loss: 7.130126, mae: 52.501858, mean_q: 69.015251
  742010/1100000: episode: 1707, duration: 2.358s, episode steps: 334, steps per second: 142, episode reward: 215.938, mean reward: 0.647 [-18.430, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: -0.013 [-0.854, 1.386], loss: 11.928862, mae: 52.537853, mean_q: 69.624115
  742284/1100000: episode: 1708, duration: 1.892s, episode steps: 274, steps per second: 145, episode reward: -231.083, mean reward: -0.843 [-100.000, 3.713], mean action: 1.332 [0.000, 3.000], mean observation: 0.098 [-1.011, 1.831], loss: 5.714005, mae: 53.308052, mean_q: 70.583344
  742406/1100000: episode: 1709, duration: 0.815s, episode steps: 122, steps per second: 150, episode reward: 30.902, mean reward: 0.253 [-100.000, 14.356], mean action: 1.803 [0.000, 3.000], mean observation: -0.017 [-0.780, 1.387], loss: 11.470824, mae: 52.964134, mean_q: 70.606888
  742547/1100000: episode: 1710, duration: 0.939s, episode steps: 141, steps per second: 150, episode reward: -155.274, mean reward: -1.101 [-100.000, 4.158], mean action: 1.135 [0.000, 3.000], mean observation: 0.010 [-1.007, 1.390], loss: 7.780362, mae: 52.424236, mean_q: 69.237312
  742879/1100000: episode: 1711, duration: 2.269s, episode steps: 332, steps per second: 146, episode reward: -205.868, mean reward: -0.620 [-100.000, 106.168], mean action: 1.599 [0.000, 3.000], mean observation: 0.055 [-2.014, 1.462], loss: 9.853242, mae: 53.374672, mean_q: 70.404961
  743231/1100000: episode: 1712, duration: 2.446s, episode steps: 352, steps per second: 144, episode reward: -223.179, mean reward: -0.634 [-100.000, 19.022], mean action: 1.653 [0.000, 3.000], mean observation: 0.079 [-1.099, 1.813], loss: 9.120555, mae: 52.931950, mean_q: 69.953545
  743434/1100000: episode: 1713, duration: 1.366s, episode steps: 203, steps per second: 149, episode reward: 252.749, mean reward: 1.245 [-14.483, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.017 [-1.288, 1.472], loss: 9.978779, mae: 53.501125, mean_q: 70.377251
  744232/1100000: episode: 1714, duration: 5.771s, episode steps: 798, steps per second: 138, episode reward: 223.762, mean reward: 0.280 [-18.920, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.233 [-0.760, 1.630], loss: 7.683479, mae: 52.975555, mean_q: 69.809898
  744421/1100000: episode: 1715, duration: 1.264s, episode steps: 189, steps per second: 150, episode reward: 242.851, mean reward: 1.285 [-12.521, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.083 [-0.850, 1.405], loss: 8.479074, mae: 53.040218, mean_q: 69.580910
  744908/1100000: episode: 1716, duration: 3.561s, episode steps: 487, steps per second: 137, episode reward: 170.789, mean reward: 0.351 [-18.797, 100.000], mean action: 1.957 [0.000, 3.000], mean observation: 0.137 [-0.690, 1.387], loss: 8.373442, mae: 52.953220, mean_q: 69.583099
  745204/1100000: episode: 1717, duration: 2.020s, episode steps: 296, steps per second: 147, episode reward: 224.623, mean reward: 0.759 [-18.763, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: -0.005 [-0.719, 1.410], loss: 12.020452, mae: 53.315632, mean_q: 69.779373
  745891/1100000: episode: 1718, duration: 5.006s, episode steps: 687, steps per second: 137, episode reward: 263.942, mean reward: 0.384 [-20.235, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.139 [-0.851, 1.400], loss: 11.473210, mae: 53.981419, mean_q: 70.861504
  746538/1100000: episode: 1719, duration: 4.654s, episode steps: 647, steps per second: 139, episode reward: 261.268, mean reward: 0.404 [-19.725, 100.000], mean action: 2.085 [0.000, 3.000], mean observation: 0.156 [-0.769, 1.402], loss: 9.106945, mae: 53.874935, mean_q: 70.849739
  746803/1100000: episode: 1720, duration: 1.805s, episode steps: 265, steps per second: 147, episode reward: 274.029, mean reward: 1.034 [-14.195, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: -0.008 [-1.247, 1.501], loss: 7.065888, mae: 54.294231, mean_q: 71.334770
  747063/1100000: episode: 1721, duration: 1.759s, episode steps: 260, steps per second: 148, episode reward: -66.806, mean reward: -0.257 [-100.000, 15.764], mean action: 1.454 [0.000, 3.000], mean observation: 0.152 [-1.394, 1.701], loss: 7.575585, mae: 53.989941, mean_q: 71.607239
  747437/1100000: episode: 1722, duration: 2.582s, episode steps: 374, steps per second: 145, episode reward: 238.135, mean reward: 0.637 [-20.108, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.123 [-0.756, 1.411], loss: 8.554304, mae: 54.697056, mean_q: 71.913017
  748100/1100000: episode: 1723, duration: 4.947s, episode steps: 663, steps per second: 134, episode reward: 258.918, mean reward: 0.391 [-18.867, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.108 [-1.242, 1.452], loss: 10.998892, mae: 54.093792, mean_q: 71.415535
  748224/1100000: episode: 1724, duration: 0.825s, episode steps: 124, steps per second: 150, episode reward: -535.318, mean reward: -4.317 [-100.000, 59.420], mean action: 2.089 [0.000, 3.000], mean observation: -0.105 [-4.516, 4.842], loss: 8.692954, mae: 54.701626, mean_q: 70.727692
  748535/1100000: episode: 1725, duration: 2.156s, episode steps: 311, steps per second: 144, episode reward: 194.475, mean reward: 0.625 [-10.522, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: -0.053 [-1.080, 1.483], loss: 14.506989, mae: 54.967884, mean_q: 71.715073
  748662/1100000: episode: 1726, duration: 0.848s, episode steps: 127, steps per second: 150, episode reward: 6.426, mean reward: 0.051 [-100.000, 18.668], mean action: 1.811 [0.000, 3.000], mean observation: 0.099 [-1.328, 1.422], loss: 14.751619, mae: 55.465511, mean_q: 71.342133
  748783/1100000: episode: 1727, duration: 0.813s, episode steps: 121, steps per second: 149, episode reward: 8.868, mean reward: 0.073 [-100.000, 17.133], mean action: 1.950 [0.000, 3.000], mean observation: 0.086 [-0.963, 1.433], loss: 8.316721, mae: 55.040066, mean_q: 72.356361
  748879/1100000: episode: 1728, duration: 0.637s, episode steps: 96, steps per second: 151, episode reward: -143.572, mean reward: -1.496 [-100.000, 3.113], mean action: 1.542 [0.000, 3.000], mean observation: -0.031 [-1.414, 1.393], loss: 18.169916, mae: 55.312313, mean_q: 72.341156
  749237/1100000: episode: 1729, duration: 2.485s, episode steps: 358, steps per second: 144, episode reward: 190.895, mean reward: 0.533 [-11.942, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: -0.042 [-0.727, 1.394], loss: 7.876956, mae: 55.170719, mean_q: 71.696602
  749752/1100000: episode: 1730, duration: 3.541s, episode steps: 515, steps per second: 145, episode reward: 259.978, mean reward: 0.505 [-19.298, 100.000], mean action: 0.596 [0.000, 3.000], mean observation: 0.219 [-0.939, 1.505], loss: 7.759785, mae: 55.134327, mean_q: 72.006660
  749902/1100000: episode: 1731, duration: 1.001s, episode steps: 150, steps per second: 150, episode reward: -32.512, mean reward: -0.217 [-100.000, 13.554], mean action: 1.620 [0.000, 3.000], mean observation: -0.029 [-0.688, 1.395], loss: 15.128049, mae: 55.186577, mean_q: 71.746681
  750010/1100000: episode: 1732, duration: 0.731s, episode steps: 108, steps per second: 148, episode reward: 30.991, mean reward: 0.287 [-100.000, 12.089], mean action: 1.852 [0.000, 3.000], mean observation: 0.003 [-0.710, 1.395], loss: 14.304590, mae: 54.634148, mean_q: 70.732834
  750135/1100000: episode: 1733, duration: 0.841s, episode steps: 125, steps per second: 149, episode reward: -39.535, mean reward: -0.316 [-100.000, 9.020], mean action: 1.800 [0.000, 3.000], mean observation: -0.015 [-1.857, 1.387], loss: 7.175071, mae: 54.812469, mean_q: 71.562904
  750546/1100000: episode: 1734, duration: 2.901s, episode steps: 411, steps per second: 142, episode reward: -67.318, mean reward: -0.164 [-100.000, 14.476], mean action: 1.895 [0.000, 3.000], mean observation: 0.111 [-1.383, 1.411], loss: 12.689026, mae: 54.746456, mean_q: 71.564362
  751390/1100000: episode: 1735, duration: 6.263s, episode steps: 844, steps per second: 135, episode reward: 246.945, mean reward: 0.293 [-19.794, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.065 [-0.600, 1.397], loss: 13.909520, mae: 54.438663, mean_q: 70.896324
  751532/1100000: episode: 1736, duration: 0.960s, episode steps: 142, steps per second: 148, episode reward: -92.073, mean reward: -0.648 [-100.000, 20.013], mean action: 1.965 [0.000, 3.000], mean observation: 0.208 [-1.704, 1.398], loss: 7.633268, mae: 53.574444, mean_q: 70.479706
  752131/1100000: episode: 1737, duration: 4.418s, episode steps: 599, steps per second: 136, episode reward: 219.815, mean reward: 0.367 [-20.989, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: -0.015 [-0.607, 1.387], loss: 11.599545, mae: 54.302307, mean_q: 70.830849
  752398/1100000: episode: 1738, duration: 1.810s, episode steps: 267, steps per second: 148, episode reward: -179.090, mean reward: -0.671 [-100.000, 10.138], mean action: 1.824 [0.000, 3.000], mean observation: 0.013 [-1.006, 1.411], loss: 10.245269, mae: 53.743618, mean_q: 70.582497
  752863/1100000: episode: 1739, duration: 3.383s, episode steps: 465, steps per second: 137, episode reward: 264.455, mean reward: 0.569 [-19.028, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.065 [-0.643, 1.445], loss: 9.016121, mae: 54.096050, mean_q: 70.468620
  753015/1100000: episode: 1740, duration: 1.018s, episode steps: 152, steps per second: 149, episode reward: -7.289, mean reward: -0.048 [-100.000, 13.361], mean action: 1.908 [0.000, 3.000], mean observation: -0.040 [-1.424, 1.388], loss: 8.693637, mae: 54.154720, mean_q: 70.308495
  753091/1100000: episode: 1741, duration: 0.513s, episode steps: 76, steps per second: 148, episode reward: -218.022, mean reward: -2.869 [-100.000, 56.966], mean action: 2.000 [1.000, 3.000], mean observation: 0.301 [-1.698, 4.496], loss: 9.984764, mae: 53.892761, mean_q: 69.058426
  753721/1100000: episode: 1742, duration: 4.463s, episode steps: 630, steps per second: 141, episode reward: -96.911, mean reward: -0.154 [-100.000, 13.385], mean action: 1.824 [0.000, 3.000], mean observation: 0.002 [-0.931, 1.753], loss: 16.290493, mae: 53.751934, mean_q: 70.142746
  753856/1100000: episode: 1743, duration: 0.905s, episode steps: 135, steps per second: 149, episode reward: -390.986, mean reward: -2.896 [-100.000, 3.756], mean action: 1.919 [0.000, 3.000], mean observation: 0.256 [-0.994, 2.297], loss: 7.473470, mae: 53.840900, mean_q: 70.402573
  754245/1100000: episode: 1744, duration: 2.752s, episode steps: 389, steps per second: 141, episode reward: 254.023, mean reward: 0.653 [-14.735, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.123 [-0.720, 1.476], loss: 12.447389, mae: 53.817371, mean_q: 69.731171
  754560/1100000: episode: 1745, duration: 2.165s, episode steps: 315, steps per second: 145, episode reward: 281.018, mean reward: 0.892 [-9.928, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.119 [-1.540, 1.405], loss: 12.868526, mae: 53.518284, mean_q: 69.449425
  754974/1100000: episode: 1746, duration: 2.955s, episode steps: 414, steps per second: 140, episode reward: 163.225, mean reward: 0.394 [-8.993, 100.000], mean action: 1.966 [0.000, 3.000], mean observation: 0.133 [-0.536, 1.404], loss: 12.727445, mae: 53.257198, mean_q: 69.530266
  755428/1100000: episode: 1747, duration: 3.302s, episode steps: 454, steps per second: 137, episode reward: 281.994, mean reward: 0.621 [-18.463, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.104 [-0.623, 1.513], loss: 11.469744, mae: 52.923733, mean_q: 68.618889
  756159/1100000: episode: 1748, duration: 5.514s, episode steps: 731, steps per second: 133, episode reward: 160.173, mean reward: 0.219 [-19.681, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: -0.030 [-0.742, 1.395], loss: 10.382833, mae: 53.222866, mean_q: 69.372871
  756724/1100000: episode: 1749, duration: 4.123s, episode steps: 565, steps per second: 137, episode reward: 191.716, mean reward: 0.339 [-18.853, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.067 [-0.787, 1.405], loss: 10.469766, mae: 52.943600, mean_q: 69.447945
  757006/1100000: episode: 1750, duration: 1.923s, episode steps: 282, steps per second: 147, episode reward: 268.356, mean reward: 0.952 [-20.812, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.069 [-0.756, 1.393], loss: 9.967693, mae: 52.734474, mean_q: 68.875984
  757413/1100000: episode: 1751, duration: 2.881s, episode steps: 407, steps per second: 141, episode reward: 135.015, mean reward: 0.332 [-23.545, 100.000], mean action: 1.757 [0.000, 3.000], mean observation: -0.004 [-0.997, 1.394], loss: 9.087169, mae: 52.685905, mean_q: 68.438568
  757970/1100000: episode: 1752, duration: 3.988s, episode steps: 557, steps per second: 140, episode reward: 231.207, mean reward: 0.415 [-10.078, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.119 [-0.693, 1.886], loss: 10.285639, mae: 52.410442, mean_q: 68.483917
  758539/1100000: episode: 1753, duration: 4.080s, episode steps: 569, steps per second: 139, episode reward: -160.819, mean reward: -0.283 [-100.000, 15.597], mean action: 1.524 [0.000, 3.000], mean observation: 0.102 [-1.580, 1.672], loss: 8.600773, mae: 52.645992, mean_q: 68.917778
  759167/1100000: episode: 1754, duration: 4.647s, episode steps: 628, steps per second: 135, episode reward: 139.602, mean reward: 0.222 [-11.695, 100.000], mean action: 1.932 [0.000, 3.000], mean observation: 0.155 [-1.010, 1.409], loss: 8.302862, mae: 52.598488, mean_q: 68.693306
  760167/1100000: episode: 1755, duration: 7.356s, episode steps: 1000, steps per second: 136, episode reward: 209.106, mean reward: 0.209 [-19.114, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.025 [-0.600, 1.445], loss: 12.566580, mae: 52.808392, mean_q: 68.933739
  760777/1100000: episode: 1756, duration: 4.470s, episode steps: 610, steps per second: 136, episode reward: 223.393, mean reward: 0.366 [-19.839, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.122 [-0.847, 1.422], loss: 7.913493, mae: 52.436028, mean_q: 68.496681
  761777/1100000: episode: 1757, duration: 7.712s, episode steps: 1000, steps per second: 130, episode reward: 96.524, mean reward: 0.097 [-21.677, 22.376], mean action: 1.375 [0.000, 3.000], mean observation: 0.111 [-0.645, 1.412], loss: 10.193429, mae: 52.070118, mean_q: 67.856987
  762140/1100000: episode: 1758, duration: 2.554s, episode steps: 363, steps per second: 142, episode reward: -123.050, mean reward: -0.339 [-100.000, 4.434], mean action: 1.702 [0.000, 3.000], mean observation: 0.250 [-0.523, 1.390], loss: 9.821847, mae: 51.996407, mean_q: 68.054634
  762660/1100000: episode: 1759, duration: 3.734s, episode steps: 520, steps per second: 139, episode reward: 231.673, mean reward: 0.446 [-23.737, 100.000], mean action: 2.387 [0.000, 3.000], mean observation: 0.223 [-0.654, 1.387], loss: 13.623533, mae: 51.798706, mean_q: 67.624428
  762920/1100000: episode: 1760, duration: 1.771s, episode steps: 260, steps per second: 147, episode reward: 275.211, mean reward: 1.059 [-17.346, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.089 [-0.693, 1.462], loss: 8.910729, mae: 51.641136, mean_q: 67.729103
  763463/1100000: episode: 1761, duration: 3.995s, episode steps: 543, steps per second: 136, episode reward: 235.159, mean reward: 0.433 [-19.165, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.110 [-0.696, 1.409], loss: 9.306409, mae: 51.767437, mean_q: 68.034515
  764202/1100000: episode: 1762, duration: 5.455s, episode steps: 739, steps per second: 135, episode reward: 173.443, mean reward: 0.235 [-7.408, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.185 [-1.299, 1.390], loss: 10.606586, mae: 51.935444, mean_q: 68.265770
  764490/1100000: episode: 1763, duration: 2.021s, episode steps: 288, steps per second: 142, episode reward: 256.194, mean reward: 0.890 [-18.882, 100.000], mean action: 2.042 [0.000, 3.000], mean observation: 0.107 [-0.797, 1.387], loss: 9.069373, mae: 51.958084, mean_q: 68.305634
  764861/1100000: episode: 1764, duration: 2.697s, episode steps: 371, steps per second: 138, episode reward: 258.239, mean reward: 0.696 [-13.048, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: -0.013 [-0.706, 1.409], loss: 11.332897, mae: 52.066090, mean_q: 68.801262
  765250/1100000: episode: 1765, duration: 2.785s, episode steps: 389, steps per second: 140, episode reward: 212.973, mean reward: 0.547 [-18.138, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.004 [-0.636, 1.400], loss: 11.388313, mae: 52.169685, mean_q: 68.629608
  765582/1100000: episode: 1766, duration: 2.328s, episode steps: 332, steps per second: 143, episode reward: 261.450, mean reward: 0.787 [-17.688, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.112 [-0.769, 1.468], loss: 8.312194, mae: 52.384720, mean_q: 69.448204
  765812/1100000: episode: 1767, duration: 1.558s, episode steps: 230, steps per second: 148, episode reward: 261.902, mean reward: 1.139 [-9.686, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.071 [-0.786, 1.396], loss: 10.452772, mae: 52.021381, mean_q: 68.270126
  766227/1100000: episode: 1768, duration: 2.920s, episode steps: 415, steps per second: 142, episode reward: 229.050, mean reward: 0.552 [-3.328, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.005 [-0.710, 1.395], loss: 9.621704, mae: 52.297478, mean_q: 69.105942
  767227/1100000: episode: 1769, duration: 7.644s, episode steps: 1000, steps per second: 131, episode reward: -33.445, mean reward: -0.033 [-12.483, 25.671], mean action: 1.875 [0.000, 3.000], mean observation: 0.149 [-1.112, 1.526], loss: 8.836110, mae: 52.074688, mean_q: 68.571266
  767544/1100000: episode: 1770, duration: 2.205s, episode steps: 317, steps per second: 144, episode reward: 259.567, mean reward: 0.819 [-11.411, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.047 [-0.737, 1.510], loss: 11.293966, mae: 52.281818, mean_q: 68.746834
  767753/1100000: episode: 1771, duration: 1.408s, episode steps: 209, steps per second: 148, episode reward: -638.148, mean reward: -3.053 [-100.000, 4.907], mean action: 1.684 [0.000, 3.000], mean observation: 0.080 [-3.788, 1.896], loss: 11.213289, mae: 52.243198, mean_q: 68.600632
  768333/1100000: episode: 1772, duration: 4.085s, episode steps: 580, steps per second: 142, episode reward: 154.829, mean reward: 0.267 [-12.124, 100.000], mean action: 1.781 [0.000, 3.000], mean observation: 0.002 [-0.600, 1.392], loss: 8.789727, mae: 52.989120, mean_q: 69.736519
  768703/1100000: episode: 1773, duration: 2.587s, episode steps: 370, steps per second: 143, episode reward: 1.268, mean reward: 0.003 [-100.000, 11.643], mean action: 1.746 [0.000, 3.000], mean observation: -0.019 [-0.600, 1.527], loss: 8.588001, mae: 53.056843, mean_q: 70.157906
  768797/1100000: episode: 1774, duration: 0.627s, episode steps: 94, steps per second: 150, episode reward: 26.511, mean reward: 0.282 [-100.000, 18.618], mean action: 1.660 [0.000, 3.000], mean observation: 0.025 [-0.839, 1.403], loss: 19.079124, mae: 53.312431, mean_q: 70.299171
  768961/1100000: episode: 1775, duration: 1.101s, episode steps: 164, steps per second: 149, episode reward: 4.883, mean reward: 0.030 [-100.000, 34.596], mean action: 1.427 [0.000, 3.000], mean observation: 0.014 [-1.012, 2.029], loss: 8.720283, mae: 52.288864, mean_q: 69.120728
  769273/1100000: episode: 1776, duration: 2.178s, episode steps: 312, steps per second: 143, episode reward: -57.594, mean reward: -0.185 [-100.000, 15.598], mean action: 1.856 [0.000, 3.000], mean observation: 0.201 [-1.166, 1.628], loss: 13.528870, mae: 52.801426, mean_q: 69.690338
  769663/1100000: episode: 1777, duration: 2.734s, episode steps: 390, steps per second: 143, episode reward: 219.472, mean reward: 0.563 [-19.655, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.163 [-0.675, 1.516], loss: 8.488648, mae: 52.885555, mean_q: 70.133301
  770201/1100000: episode: 1778, duration: 4.097s, episode steps: 538, steps per second: 131, episode reward: 237.066, mean reward: 0.441 [-18.586, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.046 [-0.669, 1.441], loss: 8.229677, mae: 52.534828, mean_q: 69.284714
  770547/1100000: episode: 1779, duration: 2.364s, episode steps: 346, steps per second: 146, episode reward: 215.170, mean reward: 0.622 [-8.196, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.018 [-1.095, 1.483], loss: 12.985919, mae: 52.422852, mean_q: 69.007523
  771028/1100000: episode: 1780, duration: 3.372s, episode steps: 481, steps per second: 143, episode reward: 191.435, mean reward: 0.398 [-19.685, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.011 [-0.611, 1.426], loss: 10.442931, mae: 52.437881, mean_q: 69.094406
  771900/1100000: episode: 1781, duration: 6.321s, episode steps: 872, steps per second: 138, episode reward: 234.729, mean reward: 0.269 [-18.285, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.040 [-0.722, 1.493], loss: 8.289889, mae: 52.184235, mean_q: 68.879486
  772557/1100000: episode: 1782, duration: 4.814s, episode steps: 657, steps per second: 136, episode reward: 190.409, mean reward: 0.290 [-21.488, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: -0.030 [-0.600, 1.412], loss: 9.014299, mae: 52.147758, mean_q: 68.743614
  772769/1100000: episode: 1783, duration: 1.444s, episode steps: 212, steps per second: 147, episode reward: -196.314, mean reward: -0.926 [-100.000, 17.625], mean action: 1.642 [0.000, 3.000], mean observation: 0.157 [-2.019, 1.387], loss: 9.253182, mae: 51.772346, mean_q: 67.956909
  773583/1100000: episode: 1784, duration: 6.389s, episode steps: 814, steps per second: 127, episode reward: 122.889, mean reward: 0.151 [-12.385, 100.000], mean action: 1.549 [0.000, 3.000], mean observation: 0.169 [-0.482, 1.423], loss: 8.377782, mae: 52.221409, mean_q: 68.870018
  774425/1100000: episode: 1785, duration: 6.191s, episode steps: 842, steps per second: 136, episode reward: 179.695, mean reward: 0.213 [-18.458, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: -0.031 [-0.769, 1.406], loss: 9.926314, mae: 52.468296, mean_q: 69.276115
  775425/1100000: episode: 1786, duration: 7.823s, episode steps: 1000, steps per second: 128, episode reward: 10.337, mean reward: 0.010 [-20.862, 13.219], mean action: 1.421 [0.000, 3.000], mean observation: 0.161 [-0.624, 1.414], loss: 8.770098, mae: 52.485592, mean_q: 69.086555
  775667/1100000: episode: 1787, duration: 1.654s, episode steps: 242, steps per second: 146, episode reward: -199.293, mean reward: -0.824 [-100.000, 26.865], mean action: 1.426 [0.000, 3.000], mean observation: 0.005 [-0.868, 1.617], loss: 11.410664, mae: 52.161255, mean_q: 68.539848
  776667/1100000: episode: 1788, duration: 7.570s, episode steps: 1000, steps per second: 132, episode reward: 36.584, mean reward: 0.037 [-20.536, 19.842], mean action: 1.750 [0.000, 3.000], mean observation: 0.217 [-0.473, 1.420], loss: 9.786736, mae: 52.115906, mean_q: 68.474266
  777000/1100000: episode: 1789, duration: 2.298s, episode steps: 333, steps per second: 145, episode reward: 281.991, mean reward: 0.847 [-11.965, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.071 [-0.722, 1.456], loss: 8.167150, mae: 51.789272, mean_q: 68.038017
  777577/1100000: episode: 1790, duration: 4.064s, episode steps: 577, steps per second: 142, episode reward: 187.110, mean reward: 0.324 [-17.398, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: -0.033 [-0.634, 1.407], loss: 13.356697, mae: 51.857853, mean_q: 67.781303
  777846/1100000: episode: 1791, duration: 1.858s, episode steps: 269, steps per second: 145, episode reward: 264.670, mean reward: 0.984 [-17.839, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.073 [-0.979, 1.408], loss: 8.596099, mae: 52.194122, mean_q: 68.372437
  778185/1100000: episode: 1792, duration: 2.328s, episode steps: 339, steps per second: 146, episode reward: 254.748, mean reward: 0.751 [-19.276, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.467], loss: 11.420403, mae: 52.535282, mean_q: 68.906990
  778474/1100000: episode: 1793, duration: 1.991s, episode steps: 289, steps per second: 145, episode reward: 195.223, mean reward: 0.676 [-17.322, 100.000], mean action: 2.107 [0.000, 3.000], mean observation: 0.157 [-0.580, 1.399], loss: 8.845758, mae: 52.127739, mean_q: 68.474342
  778790/1100000: episode: 1794, duration: 2.170s, episode steps: 316, steps per second: 146, episode reward: 288.422, mean reward: 0.913 [-9.164, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.050 [-0.781, 1.426], loss: 7.697325, mae: 52.145615, mean_q: 67.978264
  779242/1100000: episode: 1795, duration: 3.100s, episode steps: 452, steps per second: 146, episode reward: 229.414, mean reward: 0.508 [-20.737, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.201 [-0.696, 1.399], loss: 7.209282, mae: 52.159985, mean_q: 68.747459
  779632/1100000: episode: 1796, duration: 2.675s, episode steps: 390, steps per second: 146, episode reward: -57.120, mean reward: -0.146 [-100.000, 13.127], mean action: 1.756 [0.000, 3.000], mean observation: 0.108 [-0.800, 1.432], loss: 9.162143, mae: 52.430439, mean_q: 68.558563
  780048/1100000: episode: 1797, duration: 2.971s, episode steps: 416, steps per second: 140, episode reward: 261.955, mean reward: 0.630 [-13.387, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: -0.008 [-0.636, 1.386], loss: 7.820633, mae: 52.285259, mean_q: 68.660774
  780470/1100000: episode: 1798, duration: 2.991s, episode steps: 422, steps per second: 141, episode reward: 266.678, mean reward: 0.632 [-19.586, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.102 [-0.749, 1.520], loss: 7.660064, mae: 52.256157, mean_q: 68.575623
  780845/1100000: episode: 1799, duration: 2.600s, episode steps: 375, steps per second: 144, episode reward: 231.054, mean reward: 0.616 [-19.135, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: -0.028 [-0.657, 1.427], loss: 9.420461, mae: 52.307606, mean_q: 68.623665
  781102/1100000: episode: 1800, duration: 1.760s, episode steps: 257, steps per second: 146, episode reward: -148.362, mean reward: -0.577 [-100.000, 53.468], mean action: 1.794 [0.000, 3.000], mean observation: 0.079 [-1.622, 1.438], loss: 8.188335, mae: 52.371399, mean_q: 68.485909
  781557/1100000: episode: 1801, duration: 3.374s, episode steps: 455, steps per second: 135, episode reward: 203.361, mean reward: 0.447 [-13.036, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: -0.033 [-0.600, 1.438], loss: 8.498036, mae: 52.030521, mean_q: 68.156662
  781768/1100000: episode: 1802, duration: 1.432s, episode steps: 211, steps per second: 147, episode reward: 228.848, mean reward: 1.085 [-9.293, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: 0.054 [-1.252, 1.493], loss: 9.085850, mae: 52.087002, mean_q: 68.012558
  782002/1100000: episode: 1803, duration: 1.585s, episode steps: 234, steps per second: 148, episode reward: 267.019, mean reward: 1.141 [-7.865, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.119 [-1.172, 1.398], loss: 7.972681, mae: 52.007824, mean_q: 68.283501
  782177/1100000: episode: 1804, duration: 1.182s, episode steps: 175, steps per second: 148, episode reward: -164.416, mean reward: -0.940 [-100.000, 72.218], mean action: 1.423 [0.000, 3.000], mean observation: -0.085 [-1.932, 2.028], loss: 15.518763, mae: 52.854576, mean_q: 68.792267
  782594/1100000: episode: 1805, duration: 2.938s, episode steps: 417, steps per second: 142, episode reward: 199.613, mean reward: 0.479 [-18.097, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: -0.019 [-0.624, 1.392], loss: 10.274506, mae: 52.193420, mean_q: 67.933929
  782941/1100000: episode: 1806, duration: 2.355s, episode steps: 347, steps per second: 147, episode reward: 296.416, mean reward: 0.854 [-11.920, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.119 [-0.566, 1.459], loss: 9.741797, mae: 51.881073, mean_q: 67.539467
  783314/1100000: episode: 1807, duration: 2.600s, episode steps: 373, steps per second: 143, episode reward: 247.368, mean reward: 0.663 [-5.043, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.206 [-0.767, 1.438], loss: 7.886063, mae: 52.254463, mean_q: 68.352478
  783808/1100000: episode: 1808, duration: 3.529s, episode steps: 494, steps per second: 140, episode reward: 207.486, mean reward: 0.420 [-19.005, 100.000], mean action: 1.721 [0.000, 3.000], mean observation: 0.201 [-0.661, 1.411], loss: 10.038489, mae: 51.961166, mean_q: 67.793106
  784138/1100000: episode: 1809, duration: 2.269s, episode steps: 330, steps per second: 145, episode reward: 278.182, mean reward: 0.843 [-10.381, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.026 [-0.741, 1.399], loss: 9.207098, mae: 51.574528, mean_q: 67.493599
  785016/1100000: episode: 1810, duration: 6.504s, episode steps: 878, steps per second: 135, episode reward: 206.597, mean reward: 0.235 [-19.078, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.188 [-0.632, 1.393], loss: 9.054979, mae: 51.967918, mean_q: 68.165657
  786016/1100000: episode: 1811, duration: 7.805s, episode steps: 1000, steps per second: 128, episode reward: 87.064, mean reward: 0.087 [-19.369, 13.227], mean action: 1.137 [0.000, 3.000], mean observation: 0.014 [-0.651, 1.399], loss: 8.086070, mae: 51.073837, mean_q: 67.019806
  786742/1100000: episode: 1812, duration: 5.212s, episode steps: 726, steps per second: 139, episode reward: 229.075, mean reward: 0.316 [-18.898, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.237 [-0.551, 1.432], loss: 8.961726, mae: 51.175758, mean_q: 67.234581
  787097/1100000: episode: 1813, duration: 2.440s, episode steps: 355, steps per second: 145, episode reward: 219.911, mean reward: 0.619 [-15.903, 100.000], mean action: 2.138 [0.000, 3.000], mean observation: 0.030 [-0.632, 1.389], loss: 9.500609, mae: 51.573986, mean_q: 67.688812
  787484/1100000: episode: 1814, duration: 2.704s, episode steps: 387, steps per second: 143, episode reward: -80.362, mean reward: -0.208 [-100.000, 18.604], mean action: 1.734 [0.000, 3.000], mean observation: 0.150 [-0.968, 1.637], loss: 6.282794, mae: 52.099785, mean_q: 68.461044
  788236/1100000: episode: 1815, duration: 5.666s, episode steps: 752, steps per second: 133, episode reward: 213.056, mean reward: 0.283 [-19.512, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: -0.021 [-0.760, 1.437], loss: 7.906842, mae: 51.428242, mean_q: 68.013435
  788825/1100000: episode: 1816, duration: 4.311s, episode steps: 589, steps per second: 137, episode reward: 245.638, mean reward: 0.417 [-23.744, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.101 [-0.799, 1.439], loss: 7.713999, mae: 51.486027, mean_q: 67.945709
  789309/1100000: episode: 1817, duration: 3.493s, episode steps: 484, steps per second: 139, episode reward: 189.568, mean reward: 0.392 [-9.403, 100.000], mean action: 1.671 [0.000, 3.000], mean observation: -0.044 [-1.137, 1.391], loss: 11.411903, mae: 51.792419, mean_q: 68.437126
  789594/1100000: episode: 1818, duration: 2.054s, episode steps: 285, steps per second: 139, episode reward: -40.788, mean reward: -0.143 [-100.000, 14.620], mean action: 1.930 [0.000, 3.000], mean observation: 0.117 [-1.757, 1.495], loss: 5.418667, mae: 51.384930, mean_q: 68.353249
  790391/1100000: episode: 1819, duration: 5.793s, episode steps: 797, steps per second: 138, episode reward: 195.372, mean reward: 0.245 [-19.140, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.038 [-0.690, 1.438], loss: 7.952811, mae: 51.428688, mean_q: 68.082764
  791122/1100000: episode: 1820, duration: 5.405s, episode steps: 731, steps per second: 135, episode reward: 212.772, mean reward: 0.291 [-19.649, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.043 [-0.600, 1.490], loss: 7.960668, mae: 51.041691, mean_q: 67.565025
  791675/1100000: episode: 1821, duration: 3.943s, episode steps: 553, steps per second: 140, episode reward: -321.303, mean reward: -0.581 [-100.000, 14.284], mean action: 1.825 [0.000, 3.000], mean observation: 0.008 [-1.352, 2.849], loss: 7.305295, mae: 50.807571, mean_q: 67.223724
  792675/1100000: episode: 1822, duration: 7.530s, episode steps: 1000, steps per second: 133, episode reward: 81.462, mean reward: 0.081 [-18.544, 14.204], mean action: 2.185 [0.000, 3.000], mean observation: 0.237 [-0.771, 1.408], loss: 9.063557, mae: 50.990608, mean_q: 67.351089
  793061/1100000: episode: 1823, duration: 2.672s, episode steps: 386, steps per second: 144, episode reward: 261.184, mean reward: 0.677 [-10.696, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: -0.022 [-0.721, 1.530], loss: 8.363447, mae: 50.659267, mean_q: 67.007294
  793328/1100000: episode: 1824, duration: 1.835s, episode steps: 267, steps per second: 145, episode reward: -4.488, mean reward: -0.017 [-100.000, 12.137], mean action: 1.801 [0.000, 3.000], mean observation: 0.108 [-0.990, 1.390], loss: 6.250277, mae: 50.382198, mean_q: 66.686844
  793727/1100000: episode: 1825, duration: 2.804s, episode steps: 399, steps per second: 142, episode reward: 232.684, mean reward: 0.583 [-19.442, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: 0.085 [-0.873, 1.401], loss: 9.351359, mae: 50.473095, mean_q: 66.813751
  794417/1100000: episode: 1826, duration: 5.020s, episode steps: 690, steps per second: 137, episode reward: 228.021, mean reward: 0.330 [-19.151, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.216 [-0.564, 1.493], loss: 7.598454, mae: 50.672485, mean_q: 66.887909
  794596/1100000: episode: 1827, duration: 1.206s, episode steps: 179, steps per second: 148, episode reward: -43.421, mean reward: -0.243 [-100.000, 10.043], mean action: 1.771 [0.000, 3.000], mean observation: 0.118 [-1.492, 1.419], loss: 7.299325, mae: 50.896156, mean_q: 67.410408
  794975/1100000: episode: 1828, duration: 2.637s, episode steps: 379, steps per second: 144, episode reward: 273.825, mean reward: 0.722 [-10.827, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.110 [-1.033, 1.523], loss: 9.350414, mae: 50.331329, mean_q: 66.612137
  795264/1100000: episode: 1829, duration: 1.981s, episode steps: 289, steps per second: 146, episode reward: 240.326, mean reward: 0.832 [-19.048, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.068 [-0.667, 1.403], loss: 6.853678, mae: 50.725174, mean_q: 67.299225
  796034/1100000: episode: 1830, duration: 5.588s, episode steps: 770, steps per second: 138, episode reward: 248.677, mean reward: 0.323 [-23.265, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.031 [-0.788, 1.409], loss: 8.181332, mae: 50.270172, mean_q: 66.258652
  796366/1100000: episode: 1831, duration: 2.336s, episode steps: 332, steps per second: 142, episode reward: 269.917, mean reward: 0.813 [-10.743, 100.000], mean action: 2.235 [0.000, 3.000], mean observation: 0.100 [-1.253, 1.392], loss: 6.464694, mae: 50.106163, mean_q: 66.119484
  796721/1100000: episode: 1832, duration: 2.523s, episode steps: 355, steps per second: 141, episode reward: 230.512, mean reward: 0.649 [-19.038, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.067 [-0.855, 1.404], loss: 11.362394, mae: 50.629391, mean_q: 66.959686
  797050/1100000: episode: 1833, duration: 2.286s, episode steps: 329, steps per second: 144, episode reward: 248.876, mean reward: 0.756 [-10.682, 100.000], mean action: 1.954 [0.000, 3.000], mean observation: -0.018 [-0.674, 1.394], loss: 7.857822, mae: 50.051647, mean_q: 66.099236
  797862/1100000: episode: 1834, duration: 5.802s, episode steps: 812, steps per second: 140, episode reward: 210.138, mean reward: 0.259 [-21.361, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.040 [-0.743, 1.444], loss: 6.671986, mae: 49.669842, mean_q: 65.868004
  798403/1100000: episode: 1835, duration: 3.927s, episode steps: 541, steps per second: 138, episode reward: 162.510, mean reward: 0.300 [-3.987, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.162 [-0.744, 1.407], loss: 8.584207, mae: 50.075199, mean_q: 66.275803
  798748/1100000: episode: 1836, duration: 2.378s, episode steps: 345, steps per second: 145, episode reward: 219.509, mean reward: 0.636 [-18.104, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: -0.019 [-0.696, 1.435], loss: 7.281035, mae: 49.910027, mean_q: 65.972862
  799277/1100000: episode: 1837, duration: 3.754s, episode steps: 529, steps per second: 141, episode reward: 232.594, mean reward: 0.440 [-18.415, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.019 [-0.820, 1.391], loss: 7.743582, mae: 49.335533, mean_q: 65.430267
  799873/1100000: episode: 1838, duration: 4.273s, episode steps: 596, steps per second: 139, episode reward: 248.637, mean reward: 0.417 [-21.603, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.209 [-1.186, 1.401], loss: 7.582139, mae: 49.641289, mean_q: 65.680176
  800873/1100000: episode: 1839, duration: 7.732s, episode steps: 1000, steps per second: 129, episode reward: 138.496, mean reward: 0.138 [-19.305, 14.137], mean action: 1.365 [0.000, 3.000], mean observation: 0.243 [-0.489, 1.502], loss: 7.209027, mae: 49.089291, mean_q: 65.158630
  801023/1100000: episode: 1840, duration: 1.005s, episode steps: 150, steps per second: 149, episode reward: 52.956, mean reward: 0.353 [-100.000, 15.851], mean action: 1.940 [0.000, 3.000], mean observation: 0.021 [-1.552, 1.479], loss: 5.879781, mae: 48.460476, mean_q: 64.382416
  801599/1100000: episode: 1841, duration: 3.986s, episode steps: 576, steps per second: 144, episode reward: 50.193, mean reward: 0.087 [-100.000, 20.779], mean action: 0.766 [0.000, 3.000], mean observation: 0.070 [-0.815, 1.399], loss: 11.275258, mae: 48.788731, mean_q: 64.714127
  802599/1100000: episode: 1842, duration: 7.488s, episode steps: 1000, steps per second: 134, episode reward: 41.551, mean reward: 0.042 [-18.323, 12.674], mean action: 1.574 [0.000, 3.000], mean observation: 0.214 [-0.798, 1.400], loss: 8.073335, mae: 48.778656, mean_q: 64.832474
  802741/1100000: episode: 1843, duration: 0.948s, episode steps: 142, steps per second: 150, episode reward: -100.918, mean reward: -0.711 [-100.000, 18.715], mean action: 1.782 [0.000, 3.000], mean observation: -0.043 [-1.424, 1.393], loss: 6.302238, mae: 48.469963, mean_q: 64.470673
  803033/1100000: episode: 1844, duration: 2.010s, episode steps: 292, steps per second: 145, episode reward: 256.487, mean reward: 0.878 [-17.385, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.075 [-0.763, 1.400], loss: 7.838794, mae: 48.594398, mean_q: 64.254295
  804033/1100000: episode: 1845, duration: 7.691s, episode steps: 1000, steps per second: 130, episode reward: -38.010, mean reward: -0.038 [-19.243, 22.496], mean action: 1.448 [0.000, 3.000], mean observation: -0.007 [-0.667, 1.428], loss: 9.339774, mae: 48.665474, mean_q: 64.664093
  804442/1100000: episode: 1846, duration: 2.898s, episode steps: 409, steps per second: 141, episode reward: -17.010, mean reward: -0.042 [-100.000, 9.728], mean action: 1.689 [0.000, 3.000], mean observation: -0.050 [-0.873, 1.468], loss: 9.925173, mae: 48.668438, mean_q: 64.791862
  804635/1100000: episode: 1847, duration: 1.287s, episode steps: 193, steps per second: 150, episode reward: -109.971, mean reward: -0.570 [-100.000, 12.389], mean action: 1.793 [0.000, 3.000], mean observation: -0.033 [-1.053, 1.493], loss: 10.118248, mae: 49.306561, mean_q: 65.458839
  805251/1100000: episode: 1848, duration: 4.510s, episode steps: 616, steps per second: 137, episode reward: -338.176, mean reward: -0.549 [-100.000, 13.701], mean action: 1.766 [0.000, 3.000], mean observation: 0.074 [-1.312, 1.975], loss: 10.859468, mae: 49.175598, mean_q: 65.663368
  805557/1100000: episode: 1849, duration: 2.088s, episode steps: 306, steps per second: 147, episode reward: 281.195, mean reward: 0.919 [-20.404, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: 0.049 [-1.191, 1.427], loss: 6.618621, mae: 49.353127, mean_q: 65.984848
  805648/1100000: episode: 1850, duration: 0.611s, episode steps: 91, steps per second: 149, episode reward: -73.012, mean reward: -0.802 [-100.000, 13.010], mean action: 1.648 [0.000, 3.000], mean observation: -0.003 [-1.382, 1.391], loss: 8.511628, mae: 49.267246, mean_q: 65.672546
  805879/1100000: episode: 1851, duration: 1.579s, episode steps: 231, steps per second: 146, episode reward: -189.341, mean reward: -0.820 [-100.000, 49.503], mean action: 1.645 [0.000, 3.000], mean observation: -0.116 [-1.612, 1.402], loss: 8.296861, mae: 49.875614, mean_q: 66.714615
  806416/1100000: episode: 1852, duration: 3.973s, episode steps: 537, steps per second: 135, episode reward: 222.512, mean reward: 0.414 [-19.130, 100.000], mean action: 2.009 [0.000, 3.000], mean observation: 0.061 [-0.843, 1.776], loss: 12.064403, mae: 49.799492, mean_q: 66.281281
  806993/1100000: episode: 1853, duration: 4.296s, episode steps: 577, steps per second: 134, episode reward: 256.240, mean reward: 0.444 [-19.676, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.087 [-0.677, 1.388], loss: 13.623487, mae: 49.620857, mean_q: 66.321800
  807478/1100000: episode: 1854, duration: 3.456s, episode steps: 485, steps per second: 140, episode reward: -72.407, mean reward: -0.149 [-100.000, 26.483], mean action: 1.596 [0.000, 3.000], mean observation: 0.011 [-1.192, 1.515], loss: 6.393180, mae: 49.643509, mean_q: 66.123108
  807735/1100000: episode: 1855, duration: 1.761s, episode steps: 257, steps per second: 146, episode reward: -73.360, mean reward: -0.285 [-100.000, 18.338], mean action: 1.615 [0.000, 3.000], mean observation: 0.094 [-1.240, 1.717], loss: 10.911786, mae: 49.997772, mean_q: 66.661690
  808408/1100000: episode: 1856, duration: 5.237s, episode steps: 673, steps per second: 129, episode reward: 184.444, mean reward: 0.274 [-9.283, 100.000], mean action: 1.673 [0.000, 3.000], mean observation: 0.177 [-0.462, 1.449], loss: 12.025503, mae: 49.320370, mean_q: 65.949532
  808824/1100000: episode: 1857, duration: 2.974s, episode steps: 416, steps per second: 140, episode reward: 167.498, mean reward: 0.403 [-7.584, 100.000], mean action: 1.901 [0.000, 3.000], mean observation: -0.017 [-0.704, 1.406], loss: 13.487560, mae: 49.334480, mean_q: 66.001984
  809198/1100000: episode: 1858, duration: 2.579s, episode steps: 374, steps per second: 145, episode reward: -330.024, mean reward: -0.882 [-100.000, 18.276], mean action: 1.439 [0.000, 3.000], mean observation: -0.084 [-2.380, 1.386], loss: 9.489883, mae: 49.722542, mean_q: 66.540619
  809684/1100000: episode: 1859, duration: 3.423s, episode steps: 486, steps per second: 142, episode reward: 254.491, mean reward: 0.524 [-19.304, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.057 [-1.674, 1.389], loss: 11.560854, mae: 49.742615, mean_q: 66.269936
  810284/1100000: episode: 1860, duration: 4.328s, episode steps: 600, steps per second: 139, episode reward: 236.579, mean reward: 0.394 [-20.388, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.015 [-0.600, 1.477], loss: 14.232106, mae: 50.050636, mean_q: 66.754700
  810381/1100000: episode: 1861, duration: 0.644s, episode steps: 97, steps per second: 151, episode reward: -316.506, mean reward: -3.263 [-100.000, 140.327], mean action: 1.588 [0.000, 3.000], mean observation: -0.220 [-5.332, 2.555], loss: 13.145066, mae: 49.015385, mean_q: 65.101501
  810479/1100000: episode: 1862, duration: 0.657s, episode steps: 98, steps per second: 149, episode reward: -88.127, mean reward: -0.899 [-100.000, 15.712], mean action: 1.765 [0.000, 3.000], mean observation: 0.127 [-1.143, 3.042], loss: 5.781313, mae: 49.910431, mean_q: 66.716286
  811238/1100000: episode: 1863, duration: 5.575s, episode steps: 759, steps per second: 136, episode reward: 190.539, mean reward: 0.251 [-18.397, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.124 [-0.995, 1.399], loss: 12.492352, mae: 49.583828, mean_q: 66.021339
  811345/1100000: episode: 1864, duration: 0.724s, episode steps: 107, steps per second: 148, episode reward: -7.955, mean reward: -0.074 [-100.000, 19.354], mean action: 1.963 [0.000, 3.000], mean observation: 0.133 [-2.015, 1.426], loss: 8.279621, mae: 49.102028, mean_q: 65.523514
  812345/1100000: episode: 1865, duration: 8.295s, episode steps: 1000, steps per second: 121, episode reward: -17.119, mean reward: -0.017 [-4.852, 4.853], mean action: 1.800 [0.000, 3.000], mean observation: 0.153 [-0.475, 1.392], loss: 10.469773, mae: 49.292210, mean_q: 65.673561
  812602/1100000: episode: 1866, duration: 1.731s, episode steps: 257, steps per second: 148, episode reward: 36.098, mean reward: 0.140 [-100.000, 14.833], mean action: 1.584 [0.000, 3.000], mean observation: -0.040 [-0.688, 1.526], loss: 8.369850, mae: 49.367306, mean_q: 65.646912
  813602/1100000: episode: 1867, duration: 7.889s, episode steps: 1000, steps per second: 127, episode reward: 2.766, mean reward: 0.003 [-12.266, 13.848], mean action: 1.774 [0.000, 3.000], mean observation: 0.154 [-0.485, 1.495], loss: 7.654850, mae: 48.885273, mean_q: 65.053436
  814467/1100000: episode: 1868, duration: 6.714s, episode steps: 865, steps per second: 129, episode reward: 236.042, mean reward: 0.273 [-19.675, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.124 [-0.702, 1.452], loss: 11.365550, mae: 48.134121, mean_q: 64.013000
  815007/1100000: episode: 1869, duration: 3.787s, episode steps: 540, steps per second: 143, episode reward: -81.712, mean reward: -0.151 [-100.000, 10.216], mean action: 1.687 [0.000, 3.000], mean observation: -0.034 [-0.851, 1.429], loss: 8.035808, mae: 47.938179, mean_q: 64.113686
  815551/1100000: episode: 1870, duration: 3.928s, episode steps: 544, steps per second: 138, episode reward: 123.673, mean reward: 0.227 [-17.785, 100.000], mean action: 2.066 [0.000, 3.000], mean observation: -0.004 [-0.600, 1.420], loss: 7.484379, mae: 48.173222, mean_q: 64.431847
  816212/1100000: episode: 1871, duration: 4.772s, episode steps: 661, steps per second: 139, episode reward: 202.709, mean reward: 0.307 [-17.152, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.003 [-0.627, 1.467], loss: 19.518847, mae: 48.001488, mean_q: 63.961262
  816852/1100000: episode: 1872, duration: 4.927s, episode steps: 640, steps per second: 130, episode reward: 234.358, mean reward: 0.366 [-17.406, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.103 [-0.706, 1.391], loss: 8.290085, mae: 48.168598, mean_q: 64.283096
  817037/1100000: episode: 1873, duration: 1.251s, episode steps: 185, steps per second: 148, episode reward: -471.107, mean reward: -2.547 [-100.000, 4.593], mean action: 1.968 [0.000, 3.000], mean observation: -0.024 [-6.766, 1.517], loss: 6.560888, mae: 48.058605, mean_q: 64.112068
  817836/1100000: episode: 1874, duration: 6.023s, episode steps: 799, steps per second: 133, episode reward: 239.625, mean reward: 0.300 [-17.619, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.017 [-0.600, 1.456], loss: 8.071309, mae: 48.522625, mean_q: 64.658112
  818504/1100000: episode: 1875, duration: 4.812s, episode steps: 668, steps per second: 139, episode reward: 227.454, mean reward: 0.341 [-21.741, 100.000], mean action: 0.936 [0.000, 3.000], mean observation: 0.081 [-0.838, 1.410], loss: 7.515505, mae: 47.948513, mean_q: 63.937580
  819196/1100000: episode: 1876, duration: 5.002s, episode steps: 692, steps per second: 138, episode reward: 216.147, mean reward: 0.312 [-19.843, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: 0.247 [-1.371, 1.485], loss: 8.522019, mae: 47.791660, mean_q: 63.526535
  819779/1100000: episode: 1877, duration: 4.224s, episode steps: 583, steps per second: 138, episode reward: 280.002, mean reward: 0.480 [-19.525, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.076 [-0.633, 1.388], loss: 8.476528, mae: 47.439899, mean_q: 63.491734
  820448/1100000: episode: 1878, duration: 4.969s, episode steps: 669, steps per second: 135, episode reward: 179.345, mean reward: 0.268 [-21.610, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: -0.054 [-0.651, 1.434], loss: 7.783338, mae: 47.072216, mean_q: 62.850883
  820759/1100000: episode: 1879, duration: 2.152s, episode steps: 311, steps per second: 145, episode reward: 234.474, mean reward: 0.754 [-8.982, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.005 [-0.600, 1.398], loss: 23.808216, mae: 47.172428, mean_q: 62.665119
  821490/1100000: episode: 1880, duration: 5.756s, episode steps: 731, steps per second: 127, episode reward: 186.633, mean reward: 0.255 [-18.493, 100.000], mean action: 1.510 [0.000, 3.000], mean observation: 0.075 [-0.815, 1.497], loss: 6.534481, mae: 46.775742, mean_q: 62.205368
  821952/1100000: episode: 1881, duration: 3.358s, episode steps: 462, steps per second: 138, episode reward: 246.393, mean reward: 0.533 [-10.291, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.078 [-0.743, 1.413], loss: 7.611954, mae: 45.981880, mean_q: 60.980988
  822117/1100000: episode: 1882, duration: 1.111s, episode steps: 165, steps per second: 149, episode reward: -230.945, mean reward: -1.400 [-100.000, 5.196], mean action: 1.382 [0.000, 3.000], mean observation: 0.107 [-0.659, 1.474], loss: 4.790895, mae: 46.204876, mean_q: 61.208714
  823117/1100000: episode: 1883, duration: 7.847s, episode steps: 1000, steps per second: 127, episode reward: 26.903, mean reward: 0.027 [-12.241, 15.623], mean action: 1.542 [0.000, 3.000], mean observation: 0.013 [-1.257, 1.399], loss: 8.696465, mae: 46.043900, mean_q: 61.033813
  824117/1100000: episode: 1884, duration: 8.185s, episode steps: 1000, steps per second: 122, episode reward: -17.274, mean reward: -0.017 [-5.380, 5.441], mean action: 1.543 [0.000, 3.000], mean observation: 0.084 [-0.603, 1.388], loss: 11.631152, mae: 45.198235, mean_q: 60.148941
  824484/1100000: episode: 1885, duration: 2.543s, episode steps: 367, steps per second: 144, episode reward: 254.689, mean reward: 0.694 [-19.609, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.071 [-0.596, 1.406], loss: 6.428650, mae: 45.132042, mean_q: 60.201698
  825050/1100000: episode: 1886, duration: 4.106s, episode steps: 566, steps per second: 138, episode reward: 224.794, mean reward: 0.397 [-17.382, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.208 [-0.553, 1.403], loss: 7.463371, mae: 45.064812, mean_q: 60.039574
  825449/1100000: episode: 1887, duration: 2.812s, episode steps: 399, steps per second: 142, episode reward: 248.689, mean reward: 0.623 [-19.984, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.119 [-0.769, 1.401], loss: 7.100867, mae: 45.277443, mean_q: 60.378815
  825753/1100000: episode: 1888, duration: 2.138s, episode steps: 304, steps per second: 142, episode reward: 270.615, mean reward: 0.890 [-9.580, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.042 [-0.803, 1.408], loss: 8.079031, mae: 45.840038, mean_q: 61.120857
  826182/1100000: episode: 1889, duration: 3.005s, episode steps: 429, steps per second: 143, episode reward: 260.071, mean reward: 0.606 [-8.761, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.108 [-0.650, 1.414], loss: 5.817311, mae: 46.067047, mean_q: 61.371540
  826773/1100000: episode: 1890, duration: 4.419s, episode steps: 591, steps per second: 134, episode reward: 235.581, mean reward: 0.399 [-22.383, 100.000], mean action: 2.235 [0.000, 3.000], mean observation: 0.016 [-0.788, 1.408], loss: 8.361328, mae: 45.956551, mean_q: 61.074039
  827773/1100000: episode: 1891, duration: 8.424s, episode steps: 1000, steps per second: 119, episode reward: -70.400, mean reward: -0.070 [-5.084, 4.580], mean action: 1.870 [0.000, 3.000], mean observation: 0.212 [-0.503, 1.396], loss: 7.478510, mae: 46.420895, mean_q: 61.800381
  828245/1100000: episode: 1892, duration: 3.348s, episode steps: 472, steps per second: 141, episode reward: 224.408, mean reward: 0.475 [-18.503, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.106 [-0.723, 1.417], loss: 5.442948, mae: 46.218975, mean_q: 61.626324
  828463/1100000: episode: 1893, duration: 1.481s, episode steps: 218, steps per second: 147, episode reward: 4.032, mean reward: 0.018 [-100.000, 7.341], mean action: 1.688 [0.000, 3.000], mean observation: -0.041 [-0.600, 1.411], loss: 8.214268, mae: 46.221607, mean_q: 61.662621
  828806/1100000: episode: 1894, duration: 2.408s, episode steps: 343, steps per second: 142, episode reward: 285.201, mean reward: 0.831 [-9.896, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: -0.020 [-1.168, 1.388], loss: 7.161468, mae: 46.144974, mean_q: 61.641594
  829414/1100000: episode: 1895, duration: 4.391s, episode steps: 608, steps per second: 138, episode reward: 227.410, mean reward: 0.374 [-10.994, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.114 [-0.727, 1.470], loss: 11.867762, mae: 46.722309, mean_q: 62.380363
  829814/1100000: episode: 1896, duration: 2.850s, episode steps: 400, steps per second: 140, episode reward: 261.358, mean reward: 0.653 [-18.332, 100.000], mean action: 0.835 [0.000, 3.000], mean observation: 0.123 [-0.763, 1.390], loss: 7.912861, mae: 46.725060, mean_q: 62.310509
  830694/1100000: episode: 1897, duration: 6.522s, episode steps: 880, steps per second: 135, episode reward: 152.020, mean reward: 0.173 [-22.586, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.180 [-1.305, 1.460], loss: 8.439039, mae: 46.938156, mean_q: 62.555222
  831092/1100000: episode: 1898, duration: 2.807s, episode steps: 398, steps per second: 142, episode reward: 223.391, mean reward: 0.561 [-12.516, 100.000], mean action: 1.915 [0.000, 3.000], mean observation: -0.041 [-0.696, 1.410], loss: 9.018481, mae: 47.144905, mean_q: 62.706757
  831592/1100000: episode: 1899, duration: 3.518s, episode steps: 500, steps per second: 142, episode reward: 257.727, mean reward: 0.515 [-17.783, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.110 [-0.691, 1.419], loss: 5.150053, mae: 47.169571, mean_q: 62.966873
  832030/1100000: episode: 1900, duration: 3.107s, episode steps: 438, steps per second: 141, episode reward: 245.832, mean reward: 0.561 [-17.807, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.079 [-0.935, 1.403], loss: 6.783172, mae: 47.397144, mean_q: 63.309402
  832516/1100000: episode: 1901, duration: 3.568s, episode steps: 486, steps per second: 136, episode reward: 258.710, mean reward: 0.532 [-18.776, 100.000], mean action: 0.784 [0.000, 3.000], mean observation: 0.113 [-0.695, 1.445], loss: 5.876172, mae: 47.763088, mean_q: 63.858147
  832790/1100000: episode: 1902, duration: 1.872s, episode steps: 274, steps per second: 146, episode reward: 268.545, mean reward: 0.980 [-7.864, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.086 [-0.726, 1.466], loss: 6.162877, mae: 47.958958, mean_q: 64.189285
  833561/1100000: episode: 1903, duration: 5.578s, episode steps: 771, steps per second: 138, episode reward: -199.280, mean reward: -0.258 [-100.000, 19.005], mean action: 1.904 [0.000, 3.000], mean observation: 0.079 [-1.323, 1.479], loss: 6.910892, mae: 47.480736, mean_q: 63.569752
  833811/1100000: episode: 1904, duration: 1.714s, episode steps: 250, steps per second: 146, episode reward: 266.307, mean reward: 1.065 [-9.520, 100.000], mean action: 1.524 [0.000, 3.000], mean observation: 0.037 [-0.782, 1.398], loss: 6.734838, mae: 47.942711, mean_q: 64.033211
  834235/1100000: episode: 1905, duration: 2.993s, episode steps: 424, steps per second: 142, episode reward: -207.239, mean reward: -0.489 [-100.000, 11.432], mean action: 1.618 [0.000, 3.000], mean observation: 0.063 [-3.009, 1.397], loss: 6.691751, mae: 47.682949, mean_q: 63.679474
  834696/1100000: episode: 1906, duration: 3.204s, episode steps: 461, steps per second: 144, episode reward: 296.994, mean reward: 0.644 [-17.802, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.096 [-0.705, 1.490], loss: 8.788145, mae: 47.824257, mean_q: 63.874851
  835101/1100000: episode: 1907, duration: 2.856s, episode steps: 405, steps per second: 142, episode reward: 252.400, mean reward: 0.623 [-9.618, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: -0.036 [-0.675, 1.395], loss: 10.325500, mae: 48.178230, mean_q: 64.480354
  835383/1100000: episode: 1908, duration: 1.927s, episode steps: 282, steps per second: 146, episode reward: -281.832, mean reward: -0.999 [-100.000, 6.315], mean action: 1.851 [0.000, 3.000], mean observation: -0.096 [-2.553, 1.412], loss: 7.944648, mae: 48.218842, mean_q: 64.687027
  835841/1100000: episode: 1909, duration: 3.155s, episode steps: 458, steps per second: 145, episode reward: -222.046, mean reward: -0.485 [-100.000, 21.840], mean action: 1.572 [0.000, 3.000], mean observation: 0.063 [-2.206, 1.459], loss: 6.142518, mae: 48.341003, mean_q: 64.779701
  836482/1100000: episode: 1910, duration: 4.700s, episode steps: 641, steps per second: 136, episode reward: 206.471, mean reward: 0.322 [-19.739, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: -0.016 [-0.718, 1.386], loss: 9.981041, mae: 47.996586, mean_q: 64.134956
  836822/1100000: episode: 1911, duration: 2.379s, episode steps: 340, steps per second: 143, episode reward: 263.328, mean reward: 0.774 [-9.234, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.191 [-1.340, 1.386], loss: 6.739570, mae: 48.123966, mean_q: 64.400848
  837221/1100000: episode: 1912, duration: 2.773s, episode steps: 399, steps per second: 144, episode reward: 227.991, mean reward: 0.571 [-9.514, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: -0.030 [-0.627, 1.408], loss: 6.561154, mae: 48.079132, mean_q: 64.272041
  837995/1100000: episode: 1913, duration: 5.692s, episode steps: 774, steps per second: 136, episode reward: 208.834, mean reward: 0.270 [-19.455, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.259 [-0.661, 1.445], loss: 8.175574, mae: 48.000885, mean_q: 64.043373
  838612/1100000: episode: 1914, duration: 4.601s, episode steps: 617, steps per second: 134, episode reward: -334.501, mean reward: -0.542 [-100.000, 5.725], mean action: 1.869 [0.000, 3.000], mean observation: -0.062 [-1.428, 1.416], loss: 7.525812, mae: 47.624107, mean_q: 63.585075
  839123/1100000: episode: 1915, duration: 3.791s, episode steps: 511, steps per second: 135, episode reward: 233.246, mean reward: 0.456 [-12.641, 100.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.016 [-0.600, 1.399], loss: 6.207715, mae: 48.113304, mean_q: 64.175659
  839468/1100000: episode: 1916, duration: 2.373s, episode steps: 345, steps per second: 145, episode reward: -108.559, mean reward: -0.315 [-100.000, 13.072], mean action: 1.620 [0.000, 3.000], mean observation: -0.073 [-0.892, 1.456], loss: 12.739328, mae: 48.303673, mean_q: 64.368233
  840451/1100000: episode: 1917, duration: 7.406s, episode steps: 983, steps per second: 133, episode reward: 108.989, mean reward: 0.111 [-20.939, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.133 [-1.448, 1.411], loss: 8.154992, mae: 48.289467, mean_q: 64.565918
  841294/1100000: episode: 1918, duration: 6.234s, episode steps: 843, steps per second: 135, episode reward: 172.578, mean reward: 0.205 [-19.280, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.186 [-0.614, 1.389], loss: 7.322650, mae: 48.071609, mean_q: 64.302994
  841602/1100000: episode: 1919, duration: 2.125s, episode steps: 308, steps per second: 145, episode reward: 59.510, mean reward: 0.193 [-100.000, 11.120], mean action: 2.065 [0.000, 3.000], mean observation: 0.021 [-0.726, 1.391], loss: 6.466366, mae: 48.696278, mean_q: 65.197952
  841923/1100000: episode: 1920, duration: 2.244s, episode steps: 321, steps per second: 143, episode reward: 245.798, mean reward: 0.766 [-13.602, 100.000], mean action: 1.813 [0.000, 3.000], mean observation: -0.011 [-0.845, 1.387], loss: 7.471455, mae: 47.982082, mean_q: 64.221428
  842089/1100000: episode: 1921, duration: 1.127s, episode steps: 166, steps per second: 147, episode reward: 4.288, mean reward: 0.026 [-100.000, 16.681], mean action: 1.813 [0.000, 3.000], mean observation: -0.040 [-0.834, 1.386], loss: 6.897297, mae: 47.772274, mean_q: 64.036224
  842340/1100000: episode: 1922, duration: 1.711s, episode steps: 251, steps per second: 147, episode reward: -149.407, mean reward: -0.595 [-100.000, 4.413], mean action: 1.466 [0.000, 3.000], mean observation: 0.064 [-0.667, 1.410], loss: 9.687493, mae: 48.176655, mean_q: 64.373993
  843048/1100000: episode: 1923, duration: 5.321s, episode steps: 708, steps per second: 133, episode reward: 156.640, mean reward: 0.221 [-20.505, 100.000], mean action: 1.722 [0.000, 3.000], mean observation: 0.178 [-0.752, 1.410], loss: 11.975601, mae: 48.523643, mean_q: 65.004776
  843390/1100000: episode: 1924, duration: 2.352s, episode steps: 342, steps per second: 145, episode reward: 232.592, mean reward: 0.680 [-2.909, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.012 [-0.600, 1.411], loss: 6.104544, mae: 48.460751, mean_q: 64.816795
  844390/1100000: episode: 1925, duration: 7.785s, episode steps: 1000, steps per second: 128, episode reward: -17.124, mean reward: -0.017 [-5.516, 6.126], mean action: 1.642 [0.000, 3.000], mean observation: 0.093 [-0.788, 1.453], loss: 8.256684, mae: 48.888573, mean_q: 65.316383
  845115/1100000: episode: 1926, duration: 5.247s, episode steps: 725, steps per second: 138, episode reward: 212.736, mean reward: 0.293 [-11.759, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: -0.000 [-0.600, 1.522], loss: 8.313824, mae: 48.614819, mean_q: 64.828629
  845703/1100000: episode: 1927, duration: 4.256s, episode steps: 588, steps per second: 138, episode reward: 265.682, mean reward: 0.452 [-19.886, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.096 [-0.602, 1.396], loss: 6.395421, mae: 48.434235, mean_q: 64.879860
  846101/1100000: episode: 1928, duration: 2.805s, episode steps: 398, steps per second: 142, episode reward: 202.379, mean reward: 0.508 [-18.149, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: -0.034 [-0.600, 1.399], loss: 8.427851, mae: 48.302547, mean_q: 64.706238
  846512/1100000: episode: 1929, duration: 2.946s, episode steps: 411, steps per second: 140, episode reward: 267.274, mean reward: 0.650 [-18.174, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.090 [-0.736, 1.402], loss: 7.142419, mae: 48.196217, mean_q: 64.622658
  846695/1100000: episode: 1930, duration: 1.226s, episode steps: 183, steps per second: 149, episode reward: -218.303, mean reward: -1.193 [-100.000, 5.036], mean action: 1.574 [0.000, 3.000], mean observation: 0.148 [-0.600, 1.670], loss: 4.503278, mae: 48.940109, mean_q: 65.377502
  846980/1100000: episode: 1931, duration: 2.007s, episode steps: 285, steps per second: 142, episode reward: 237.250, mean reward: 0.832 [-3.595, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.089 [-0.612, 1.413], loss: 9.100246, mae: 48.543037, mean_q: 65.026443
  847980/1100000: episode: 1932, duration: 7.749s, episode steps: 1000, steps per second: 129, episode reward: -62.595, mean reward: -0.063 [-6.155, 5.258], mean action: 1.801 [0.000, 3.000], mean observation: 0.090 [-0.599, 1.414], loss: 7.015214, mae: 48.594952, mean_q: 65.122169
  848580/1100000: episode: 1933, duration: 4.143s, episode steps: 600, steps per second: 145, episode reward: 210.349, mean reward: 0.351 [-17.547, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: -0.004 [-0.600, 1.392], loss: 5.003720, mae: 48.304558, mean_q: 64.832672
  849051/1100000: episode: 1934, duration: 3.264s, episode steps: 471, steps per second: 144, episode reward: 227.283, mean reward: 0.483 [-19.096, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.007 [-0.623, 1.388], loss: 19.417721, mae: 48.627895, mean_q: 65.211388
  849181/1100000: episode: 1935, duration: 0.873s, episode steps: 130, steps per second: 149, episode reward: -374.803, mean reward: -2.883 [-100.000, 2.056], mean action: 1.362 [0.000, 3.000], mean observation: 0.173 [-1.839, 2.070], loss: 8.462391, mae: 47.935558, mean_q: 63.992561
  849766/1100000: episode: 1936, duration: 4.087s, episode steps: 585, steps per second: 143, episode reward: 232.015, mean reward: 0.397 [-20.478, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.133 [-0.634, 1.411], loss: 13.856656, mae: 48.448097, mean_q: 64.953728
  850766/1100000: episode: 1937, duration: 7.322s, episode steps: 1000, steps per second: 137, episode reward: 149.659, mean reward: 0.150 [-21.831, 21.558], mean action: 2.010 [0.000, 3.000], mean observation: 0.143 [-0.607, 1.398], loss: 12.089255, mae: 49.437180, mean_q: 66.461998
  851565/1100000: episode: 1938, duration: 5.625s, episode steps: 799, steps per second: 142, episode reward: 277.550, mean reward: 0.347 [-19.509, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.170 [-0.875, 1.393], loss: 9.781462, mae: 49.477287, mean_q: 66.444717
  851748/1100000: episode: 1939, duration: 1.250s, episode steps: 183, steps per second: 146, episode reward: 20.871, mean reward: 0.114 [-100.000, 12.612], mean action: 1.880 [0.000, 3.000], mean observation: 0.073 [-1.208, 1.450], loss: 4.238239, mae: 50.093952, mean_q: 67.269196
  851995/1100000: episode: 1940, duration: 1.687s, episode steps: 247, steps per second: 146, episode reward: -142.766, mean reward: -0.578 [-100.000, 4.886], mean action: 1.660 [0.000, 3.000], mean observation: 0.031 [-1.039, 1.455], loss: 8.095922, mae: 50.175819, mean_q: 67.474709
  852275/1100000: episode: 1941, duration: 1.940s, episode steps: 280, steps per second: 144, episode reward: 261.205, mean reward: 0.933 [-8.437, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.088 [-0.625, 1.409], loss: 4.513658, mae: 49.953033, mean_q: 67.186600
  852396/1100000: episode: 1942, duration: 0.814s, episode steps: 121, steps per second: 149, episode reward: -744.058, mean reward: -6.149 [-100.000, 1.767], mean action: 1.364 [0.000, 3.000], mean observation: 0.285 [-1.796, 6.729], loss: 7.640075, mae: 49.849598, mean_q: 67.042099
  852604/1100000: episode: 1943, duration: 1.417s, episode steps: 208, steps per second: 147, episode reward: -210.668, mean reward: -1.013 [-100.000, 5.929], mean action: 1.971 [0.000, 3.000], mean observation: 0.035 [-1.006, 1.390], loss: 17.452232, mae: 50.364864, mean_q: 67.498611
  853604/1100000: episode: 1944, duration: 7.479s, episode steps: 1000, steps per second: 134, episode reward: 44.457, mean reward: 0.044 [-20.055, 22.076], mean action: 0.997 [0.000, 3.000], mean observation: 0.065 [-0.764, 1.411], loss: 20.047337, mae: 50.943134, mean_q: 68.513405
  853668/1100000: episode: 1945, duration: 0.434s, episode steps: 64, steps per second: 148, episode reward: -458.478, mean reward: -7.164 [-100.000, 1.098], mean action: 2.938 [1.000, 3.000], mean observation: -0.222 [-3.375, 1.401], loss: 5.652097, mae: 50.170654, mean_q: 66.986412
  854056/1100000: episode: 1946, duration: 2.684s, episode steps: 388, steps per second: 145, episode reward: 263.809, mean reward: 0.680 [-17.457, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.127 [-0.683, 1.394], loss: 60.220295, mae: 51.134071, mean_q: 68.622131
  854958/1100000: episode: 1947, duration: 6.927s, episode steps: 902, steps per second: 130, episode reward: 159.334, mean reward: 0.177 [-23.590, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: -0.015 [-0.789, 1.387], loss: 24.763784, mae: 50.822819, mean_q: 68.187416
  855125/1100000: episode: 1948, duration: 1.135s, episode steps: 167, steps per second: 147, episode reward: -56.564, mean reward: -0.339 [-100.000, 7.156], mean action: 1.473 [0.000, 3.000], mean observation: -0.066 [-0.894, 1.393], loss: 6.931365, mae: 52.381596, mean_q: 70.512207
  855181/1100000: episode: 1949, duration: 0.384s, episode steps: 56, steps per second: 146, episode reward: -363.998, mean reward: -6.500 [-100.000, 3.986], mean action: 2.893 [1.000, 3.000], mean observation: -0.245 [-5.963, 1.394], loss: 2.769902, mae: 51.305935, mean_q: 69.107437
  855309/1100000: episode: 1950, duration: 0.902s, episode steps: 128, steps per second: 142, episode reward: -143.137, mean reward: -1.118 [-100.000, 80.629], mean action: 1.852 [0.000, 3.000], mean observation: 0.186 [-1.208, 1.555], loss: 12.417185, mae: 51.867996, mean_q: 69.922546
  855683/1100000: episode: 1951, duration: 2.650s, episode steps: 374, steps per second: 141, episode reward: 261.103, mean reward: 0.698 [-18.246, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.121 [-0.571, 1.394], loss: 12.914463, mae: 51.877033, mean_q: 69.475761
  856337/1100000: episode: 1952, duration: 4.881s, episode steps: 654, steps per second: 134, episode reward: 166.694, mean reward: 0.255 [-10.479, 100.000], mean action: 1.951 [0.000, 3.000], mean observation: -0.005 [-0.709, 1.415], loss: 9.494514, mae: 51.656704, mean_q: 69.269287
  856416/1100000: episode: 1953, duration: 0.528s, episode steps: 79, steps per second: 150, episode reward: -622.970, mean reward: -7.886 [-100.000, 1.980], mean action: 2.633 [0.000, 3.000], mean observation: -0.296 [-4.979, 3.922], loss: 6.776315, mae: 50.904713, mean_q: 68.166679
  856492/1100000: episode: 1954, duration: 0.513s, episode steps: 76, steps per second: 148, episode reward: -514.118, mean reward: -6.765 [-100.000, 124.290], mean action: 2.789 [0.000, 3.000], mean observation: -0.328 [-5.197, 4.062], loss: 11.049858, mae: 53.302357, mean_q: 71.737518
  856604/1100000: episode: 1955, duration: 0.753s, episode steps: 112, steps per second: 149, episode reward: -573.646, mean reward: -5.122 [-100.000, 5.225], mean action: 1.536 [0.000, 2.000], mean observation: 0.378 [-2.210, 4.412], loss: 10.884260, mae: 52.197071, mean_q: 70.151054
  856724/1100000: episode: 1956, duration: 0.805s, episode steps: 120, steps per second: 149, episode reward: -663.576, mean reward: -5.530 [-100.000, 1.897], mean action: 1.583 [0.000, 3.000], mean observation: 0.230 [-2.659, 5.048], loss: 50.934002, mae: 53.354744, mean_q: 71.821327
  856788/1100000: episode: 1957, duration: 0.431s, episode steps: 64, steps per second: 148, episode reward: -475.731, mean reward: -7.433 [-100.000, 0.897], mean action: 2.641 [0.000, 3.000], mean observation: -0.241 [-3.216, 7.258], loss: 7.631386, mae: 53.189884, mean_q: 71.478249
  856898/1100000: episode: 1958, duration: 0.735s, episode steps: 110, steps per second: 150, episode reward: -851.433, mean reward: -7.740 [-100.000, 2.117], mean action: 1.300 [0.000, 3.000], mean observation: 0.426 [-1.998, 7.953], loss: 26.348742, mae: 52.441368, mean_q: 70.168610
  857707/1100000: episode: 1959, duration: 5.851s, episode steps: 809, steps per second: 138, episode reward: 157.845, mean reward: 0.195 [-20.944, 100.000], mean action: 2.372 [0.000, 3.000], mean observation: 0.087 [-0.648, 1.485], loss: 18.255457, mae: 53.548172, mean_q: 71.607948
  857796/1100000: episode: 1960, duration: 0.594s, episode steps: 89, steps per second: 150, episode reward: -665.478, mean reward: -7.477 [-100.000, 1.989], mean action: 2.506 [0.000, 3.000], mean observation: -0.342 [-6.116, 1.522], loss: 7.314083, mae: 53.827660, mean_q: 72.309471
  857897/1100000: episode: 1961, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -414.999, mean reward: -4.109 [-100.000, 3.135], mean action: 1.267 [0.000, 3.000], mean observation: 0.300 [-3.606, 3.131], loss: 101.960495, mae: 54.571564, mean_q: 72.901543
  857985/1100000: episode: 1962, duration: 0.591s, episode steps: 88, steps per second: 149, episode reward: -735.122, mean reward: -8.354 [-100.000, 2.202], mean action: 1.000 [0.000, 3.000], mean observation: 0.522 [-5.201, 6.403], loss: 18.157099, mae: 53.576893, mean_q: 72.052681
  858128/1100000: episode: 1963, duration: 0.954s, episode steps: 143, steps per second: 150, episode reward: -804.952, mean reward: -5.629 [-100.000, 27.865], mean action: 1.413 [0.000, 3.000], mean observation: 0.430 [-1.782, 7.510], loss: 9.674463, mae: 54.227379, mean_q: 72.885689
  858208/1100000: episode: 1964, duration: 0.537s, episode steps: 80, steps per second: 149, episode reward: -429.594, mean reward: -5.370 [-100.000, 116.895], mean action: 2.737 [0.000, 3.000], mean observation: -0.278 [-5.020, 3.829], loss: 16.579336, mae: 55.091087, mean_q: 73.722214
  858532/1100000: episode: 1965, duration: 2.243s, episode steps: 324, steps per second: 144, episode reward: -1790.152, mean reward: -5.525 [-100.000, 3.499], mean action: 1.448 [0.000, 3.000], mean observation: 0.445 [-2.277, 15.728], loss: 15.157605, mae: 53.992802, mean_q: 72.176437
  858685/1100000: episode: 1966, duration: 1.035s, episode steps: 153, steps per second: 148, episode reward: -630.050, mean reward: -4.118 [-100.000, 115.194], mean action: 1.601 [0.000, 3.000], mean observation: 0.412 [-1.504, 5.260], loss: 33.329639, mae: 55.021767, mean_q: 72.875412
  859356/1100000: episode: 1967, duration: 4.838s, episode steps: 671, steps per second: 139, episode reward: -386.790, mean reward: -0.576 [-100.000, 5.196], mean action: 1.696 [0.000, 3.000], mean observation: -0.022 [-1.999, 1.400], loss: 26.790300, mae: 54.845539, mean_q: 72.293213
  859454/1100000: episode: 1968, duration: 0.657s, episode steps: 98, steps per second: 149, episode reward: -537.802, mean reward: -5.488 [-100.000, 1.659], mean action: 1.571 [0.000, 3.000], mean observation: 0.106 [-2.291, 2.358], loss: 29.120102, mae: 55.059425, mean_q: 72.741432
  859584/1100000: episode: 1969, duration: 0.876s, episode steps: 130, steps per second: 148, episode reward: -366.820, mean reward: -2.822 [-100.000, 2.635], mean action: 1.500 [0.000, 3.000], mean observation: 0.070 [-1.892, 1.449], loss: 14.821949, mae: 55.015835, mean_q: 73.243286
  860528/1100000: episode: 1970, duration: 7.419s, episode steps: 944, steps per second: 127, episode reward: -288.024, mean reward: -0.305 [-100.000, 15.832], mean action: 1.702 [0.000, 3.000], mean observation: -0.012 [-0.813, 1.408], loss: 21.816013, mae: 55.367500, mean_q: 73.230255
  860646/1100000: episode: 1971, duration: 0.790s, episode steps: 118, steps per second: 149, episode reward: -108.059, mean reward: -0.916 [-100.000, 3.938], mean action: 1.458 [0.000, 3.000], mean observation: -0.152 [-1.001, 1.397], loss: 34.508587, mae: 54.042252, mean_q: 71.615822
  860744/1100000: episode: 1972, duration: 0.656s, episode steps: 98, steps per second: 149, episode reward: -487.995, mean reward: -4.980 [-100.000, 103.655], mean action: 1.367 [0.000, 3.000], mean observation: 0.356 [-4.597, 4.503], loss: 11.137239, mae: 55.003006, mean_q: 72.495209
  861023/1100000: episode: 1973, duration: 1.905s, episode steps: 279, steps per second: 146, episode reward: 231.303, mean reward: 0.829 [-2.588, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.083 [-1.003, 1.407], loss: 27.289965, mae: 55.009529, mean_q: 72.719254
  861118/1100000: episode: 1974, duration: 0.638s, episode steps: 95, steps per second: 149, episode reward: -501.413, mean reward: -5.278 [-100.000, 1.658], mean action: 1.716 [0.000, 3.000], mean observation: 0.279 [-2.062, 8.004], loss: 5.219124, mae: 55.108620, mean_q: 72.779282
  861354/1100000: episode: 1975, duration: 1.601s, episode steps: 236, steps per second: 147, episode reward: -556.635, mean reward: -2.359 [-100.000, 3.947], mean action: 1.381 [0.000, 3.000], mean observation: 0.162 [-1.848, 4.093], loss: 15.252977, mae: 55.060486, mean_q: 72.645714
  861465/1100000: episode: 1976, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -562.087, mean reward: -5.064 [-100.000, 1.994], mean action: 1.018 [0.000, 3.000], mean observation: 0.169 [-6.867, 3.168], loss: 25.063839, mae: 54.222290, mean_q: 72.072746
  861723/1100000: episode: 1977, duration: 1.742s, episode steps: 258, steps per second: 148, episode reward: -1179.554, mean reward: -4.572 [-100.000, 4.057], mean action: 1.364 [0.000, 3.000], mean observation: 0.234 [-2.339, 8.951], loss: 52.727722, mae: 55.215122, mean_q: 72.393074
  861861/1100000: episode: 1978, duration: 0.928s, episode steps: 138, steps per second: 149, episode reward: -271.786, mean reward: -1.969 [-100.000, 4.394], mean action: 1.891 [0.000, 3.000], mean observation: 0.047 [-1.787, 1.499], loss: 13.376596, mae: 55.127964, mean_q: 72.922966
  862172/1100000: episode: 1979, duration: 2.124s, episode steps: 311, steps per second: 146, episode reward: -670.607, mean reward: -2.156 [-100.000, 2.699], mean action: 1.662 [0.000, 3.000], mean observation: 0.234 [-2.575, 3.945], loss: 27.965425, mae: 55.445198, mean_q: 72.962624
  862594/1100000: episode: 1980, duration: 2.982s, episode steps: 422, steps per second: 142, episode reward: 225.287, mean reward: 0.534 [-8.648, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.075 [-0.494, 1.398], loss: 20.465084, mae: 55.490364, mean_q: 72.670708
  863132/1100000: episode: 1981, duration: 3.808s, episode steps: 538, steps per second: 141, episode reward: 223.652, mean reward: 0.416 [-20.732, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.124 [-0.618, 1.457], loss: 14.582150, mae: 55.507793, mean_q: 72.889069
  863496/1100000: episode: 1982, duration: 2.572s, episode steps: 364, steps per second: 141, episode reward: -954.695, mean reward: -2.623 [-100.000, 4.219], mean action: 1.591 [0.000, 3.000], mean observation: -0.083 [-7.150, 4.407], loss: 15.224936, mae: 54.920235, mean_q: 72.253105
  863722/1100000: episode: 1983, duration: 1.615s, episode steps: 226, steps per second: 140, episode reward: -792.776, mean reward: -3.508 [-100.000, 6.644], mean action: 1.628 [0.000, 3.000], mean observation: 0.265 [-2.691, 6.295], loss: 41.119457, mae: 55.442020, mean_q: 71.346573
  864081/1100000: episode: 1984, duration: 2.544s, episode steps: 359, steps per second: 141, episode reward: -141.164, mean reward: -0.393 [-100.000, 20.852], mean action: 1.822 [0.000, 3.000], mean observation: 0.092 [-1.829, 1.388], loss: 16.180983, mae: 55.240894, mean_q: 72.051491
  864447/1100000: episode: 1985, duration: 2.480s, episode steps: 366, steps per second: 148, episode reward: 220.009, mean reward: 0.601 [-18.866, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.035 [-0.600, 1.401], loss: 16.588781, mae: 55.048264, mean_q: 71.427559
  864540/1100000: episode: 1986, duration: 0.622s, episode steps: 93, steps per second: 150, episode reward: -387.570, mean reward: -4.167 [-100.000, 2.484], mean action: 1.516 [0.000, 3.000], mean observation: 0.127 [-2.033, 3.678], loss: 12.522384, mae: 55.677116, mean_q: 72.830078
  864660/1100000: episode: 1987, duration: 0.804s, episode steps: 120, steps per second: 149, episode reward: -92.944, mean reward: -0.775 [-100.000, 23.715], mean action: 1.392 [0.000, 3.000], mean observation: 0.003 [-1.574, 1.465], loss: 12.578526, mae: 54.970116, mean_q: 72.055092
  864731/1100000: episode: 1988, duration: 0.478s, episode steps: 71, steps per second: 149, episode reward: -298.384, mean reward: -4.203 [-100.000, 2.530], mean action: 1.324 [0.000, 3.000], mean observation: 0.081 [-1.735, 5.070], loss: 9.337213, mae: 55.173225, mean_q: 71.817657
  865293/1100000: episode: 1989, duration: 4.083s, episode steps: 562, steps per second: 138, episode reward: 241.692, mean reward: 0.430 [-18.973, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.100 [-0.635, 1.466], loss: 12.241541, mae: 55.612835, mean_q: 71.725250
  865556/1100000: episode: 1990, duration: 1.774s, episode steps: 263, steps per second: 148, episode reward: -285.951, mean reward: -1.087 [-100.000, 5.532], mean action: 1.669 [0.000, 3.000], mean observation: 0.079 [-0.600, 1.480], loss: 14.336910, mae: 55.419277, mean_q: 71.508202
  865688/1100000: episode: 1991, duration: 0.879s, episode steps: 132, steps per second: 150, episode reward: -735.109, mean reward: -5.569 [-100.000, 1.845], mean action: 1.326 [0.000, 3.000], mean observation: 0.396 [-4.490, 5.075], loss: 12.789618, mae: 55.237667, mean_q: 71.456985
  865802/1100000: episode: 1992, duration: 0.759s, episode steps: 114, steps per second: 150, episode reward: -723.203, mean reward: -6.344 [-100.000, 2.601], mean action: 1.193 [0.000, 3.000], mean observation: 0.423 [-2.322, 4.896], loss: 17.914589, mae: 56.624760, mean_q: 72.633385
  865938/1100000: episode: 1993, duration: 0.908s, episode steps: 136, steps per second: 150, episode reward: -223.981, mean reward: -1.647 [-100.000, 20.248], mean action: 1.191 [0.000, 3.000], mean observation: 0.162 [-1.192, 5.047], loss: 14.713343, mae: 55.609741, mean_q: 71.267143
  866697/1100000: episode: 1994, duration: 5.886s, episode steps: 759, steps per second: 129, episode reward: 194.964, mean reward: 0.257 [-20.573, 100.000], mean action: 1.627 [0.000, 3.000], mean observation: 0.093 [-0.929, 1.435], loss: 25.113510, mae: 56.281044, mean_q: 72.534111
  867062/1100000: episode: 1995, duration: 2.598s, episode steps: 365, steps per second: 140, episode reward: 272.394, mean reward: 0.746 [-19.856, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.104 [-0.534, 1.386], loss: 23.551365, mae: 56.435570, mean_q: 72.663338
  867553/1100000: episode: 1996, duration: 3.466s, episode steps: 491, steps per second: 142, episode reward: 242.233, mean reward: 0.493 [-9.201, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.136 [-0.606, 1.426], loss: 13.646967, mae: 56.241886, mean_q: 72.310989
  867643/1100000: episode: 1997, duration: 0.607s, episode steps: 90, steps per second: 148, episode reward: -361.569, mean reward: -4.017 [-100.000, 3.647], mean action: 1.522 [0.000, 3.000], mean observation: 0.335 [-0.600, 2.458], loss: 9.371086, mae: 56.245102, mean_q: 72.151360
  868224/1100000: episode: 1998, duration: 4.285s, episode steps: 581, steps per second: 136, episode reward: 226.498, mean reward: 0.390 [-20.524, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.020 [-0.665, 1.412], loss: 38.785324, mae: 56.895882, mean_q: 72.088776
  868560/1100000: episode: 1999, duration: 2.302s, episode steps: 336, steps per second: 146, episode reward: -210.631, mean reward: -0.627 [-100.000, 2.543], mean action: 1.485 [0.000, 3.000], mean observation: 0.192 [-1.186, 1.703], loss: 13.292653, mae: 56.893242, mean_q: 72.504433
  868834/1100000: episode: 2000, duration: 1.892s, episode steps: 274, steps per second: 145, episode reward: -173.955, mean reward: -0.635 [-100.000, 5.325], mean action: 1.573 [0.000, 3.000], mean observation: 0.161 [-0.582, 1.456], loss: 16.857746, mae: 57.166748, mean_q: 72.095108
  868939/1100000: episode: 2001, duration: 0.785s, episode steps: 105, steps per second: 134, episode reward: -302.632, mean reward: -2.882 [-100.000, 2.209], mean action: 1.152 [0.000, 3.000], mean observation: 0.329 [-0.600, 2.483], loss: 13.897305, mae: 57.616661, mean_q: 72.983086
  869363/1100000: episode: 2002, duration: 3.018s, episode steps: 424, steps per second: 140, episode reward: 252.796, mean reward: 0.596 [-9.994, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.001 [-0.600, 1.392], loss: 39.836967, mae: 57.762402, mean_q: 72.454163
  869490/1100000: episode: 2003, duration: 0.859s, episode steps: 127, steps per second: 148, episode reward: -567.665, mean reward: -4.470 [-100.000, 1.515], mean action: 2.346 [1.000, 3.000], mean observation: 0.217 [-2.258, 2.754], loss: 9.894210, mae: 58.519585, mean_q: 73.110535
  870015/1100000: episode: 2004, duration: 3.700s, episode steps: 525, steps per second: 142, episode reward: 246.979, mean reward: 0.470 [-19.633, 100.000], mean action: 0.834 [0.000, 3.000], mean observation: 0.124 [-0.654, 1.450], loss: 15.559966, mae: 58.264664, mean_q: 72.581139
  870337/1100000: episode: 2005, duration: 2.218s, episode steps: 322, steps per second: 145, episode reward: 253.452, mean reward: 0.787 [-4.284, 100.000], mean action: 1.516 [0.000, 3.000], mean observation: 0.022 [-0.674, 1.399], loss: 15.988603, mae: 58.804859, mean_q: 71.851006
  870505/1100000: episode: 2006, duration: 1.120s, episode steps: 168, steps per second: 150, episode reward: -208.347, mean reward: -1.240 [-100.000, 2.231], mean action: 1.375 [0.000, 3.000], mean observation: 0.298 [-0.296, 1.439], loss: 15.067466, mae: 58.405575, mean_q: 71.340424
  871505/1100000: episode: 2007, duration: 7.387s, episode steps: 1000, steps per second: 135, episode reward: -120.477, mean reward: -0.120 [-4.653, 5.445], mean action: 1.855 [0.000, 3.000], mean observation: 0.097 [-0.651, 1.496], loss: 24.616974, mae: 59.092274, mean_q: 71.926590
  871677/1100000: episode: 2008, duration: 1.161s, episode steps: 172, steps per second: 148, episode reward: -150.745, mean reward: -0.876 [-100.000, 2.310], mean action: 1.523 [0.000, 3.000], mean observation: 0.098 [-1.001, 1.621], loss: 13.589318, mae: 58.699936, mean_q: 70.353447
  872083/1100000: episode: 2009, duration: 2.965s, episode steps: 406, steps per second: 137, episode reward: 239.048, mean reward: 0.589 [-17.877, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.081 [-0.823, 1.393], loss: 9.865370, mae: 58.711704, mean_q: 71.545868
  873083/1100000: episode: 2010, duration: 7.463s, episode steps: 1000, steps per second: 134, episode reward: -162.216, mean reward: -0.162 [-4.801, 4.724], mean action: 1.740 [0.000, 3.000], mean observation: 0.134 [-0.757, 1.441], loss: 24.328981, mae: 58.792007, mean_q: 71.189247
  873206/1100000: episode: 2011, duration: 0.825s, episode steps: 123, steps per second: 149, episode reward: -267.855, mean reward: -2.178 [-100.000, 1.497], mean action: 1.325 [0.000, 3.000], mean observation: 0.079 [-1.010, 2.067], loss: 10.091074, mae: 58.361851, mean_q: 70.467110
  873282/1100000: episode: 2012, duration: 0.506s, episode steps: 76, steps per second: 150, episode reward: -4.263, mean reward: -0.056 [-100.000, 10.725], mean action: 1.987 [0.000, 3.000], mean observation: -0.060 [-1.210, 2.679], loss: 52.292133, mae: 58.453114, mean_q: 72.085083
  873833/1100000: episode: 2013, duration: 3.881s, episode steps: 551, steps per second: 142, episode reward: -242.693, mean reward: -0.440 [-100.000, 4.645], mean action: 1.701 [0.000, 3.000], mean observation: 0.005 [-1.000, 1.648], loss: 19.005419, mae: 58.071781, mean_q: 70.226425
  873950/1100000: episode: 2014, duration: 0.778s, episode steps: 117, steps per second: 150, episode reward: -135.651, mean reward: -1.159 [-100.000, 2.368], mean action: 1.214 [0.000, 3.000], mean observation: 0.310 [-0.514, 1.440], loss: 16.325909, mae: 58.315666, mean_q: 71.814026
  874265/1100000: episode: 2015, duration: 2.181s, episode steps: 315, steps per second: 144, episode reward: -460.304, mean reward: -1.461 [-100.000, 71.900], mean action: 1.330 [0.000, 3.000], mean observation: 0.006 [-3.580, 1.637], loss: 15.856081, mae: 58.740540, mean_q: 72.417328
  874944/1100000: episode: 2016, duration: 4.940s, episode steps: 679, steps per second: 137, episode reward: 174.780, mean reward: 0.257 [-20.345, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.090 [-0.601, 1.447], loss: 11.333194, mae: 58.432415, mean_q: 70.969406
  875124/1100000: episode: 2017, duration: 1.237s, episode steps: 180, steps per second: 146, episode reward: -198.954, mean reward: -1.105 [-100.000, 4.669], mean action: 1.944 [0.000, 3.000], mean observation: 0.103 [-0.600, 1.410], loss: 7.615146, mae: 58.436592, mean_q: 71.268204
  875331/1100000: episode: 2018, duration: 1.401s, episode steps: 207, steps per second: 148, episode reward: -219.357, mean reward: -1.060 [-100.000, 4.492], mean action: 1.918 [0.000, 3.000], mean observation: 0.083 [-0.600, 1.406], loss: 17.458075, mae: 58.313690, mean_q: 71.951416
  875803/1100000: episode: 2019, duration: 3.269s, episode steps: 472, steps per second: 144, episode reward: 274.581, mean reward: 0.582 [-18.352, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.125 [-1.370, 1.391], loss: 8.295181, mae: 58.392464, mean_q: 70.893738
  876355/1100000: episode: 2020, duration: 3.916s, episode steps: 552, steps per second: 141, episode reward: 244.187, mean reward: 0.442 [-10.413, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.084 [-0.670, 1.657], loss: 14.100944, mae: 58.806068, mean_q: 71.395813
  876896/1100000: episode: 2021, duration: 4.046s, episode steps: 541, steps per second: 134, episode reward: 214.438, mean reward: 0.396 [-4.223, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.072 [-0.516, 1.432], loss: 13.459087, mae: 58.115929, mean_q: 70.743126
  877532/1100000: episode: 2022, duration: 4.523s, episode steps: 636, steps per second: 141, episode reward: 247.657, mean reward: 0.389 [-17.512, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.029 [-0.600, 1.388], loss: 10.388667, mae: 58.268826, mean_q: 70.556747
  878532/1100000: episode: 2023, duration: 7.512s, episode steps: 1000, steps per second: 133, episode reward: -83.564, mean reward: -0.084 [-5.699, 5.544], mean action: 1.677 [0.000, 3.000], mean observation: 0.132 [-0.535, 1.462], loss: 13.187355, mae: 57.723095, mean_q: 69.929825
  879243/1100000: episode: 2024, duration: 5.263s, episode steps: 711, steps per second: 135, episode reward: -98.885, mean reward: -0.139 [-100.000, 15.086], mean action: 1.598 [0.000, 3.000], mean observation: 0.114 [-0.724, 1.385], loss: 13.054699, mae: 57.597942, mean_q: 70.820732
  880243/1100000: episode: 2025, duration: 7.523s, episode steps: 1000, steps per second: 133, episode reward: 8.847, mean reward: 0.009 [-19.919, 19.964], mean action: 1.482 [0.000, 3.000], mean observation: 0.000 [-0.755, 1.522], loss: 22.766930, mae: 57.472569, mean_q: 70.997971
  880357/1100000: episode: 2026, duration: 0.790s, episode steps: 114, steps per second: 144, episode reward: -2.290, mean reward: -0.020 [-100.000, 21.201], mean action: 1.789 [0.000, 3.000], mean observation: 0.054 [-0.917, 1.451], loss: 8.523431, mae: 55.957924, mean_q: 70.610611
  880520/1100000: episode: 2027, duration: 1.108s, episode steps: 163, steps per second: 147, episode reward: 16.004, mean reward: 0.098 [-100.000, 23.276], mean action: 1.620 [0.000, 3.000], mean observation: -0.087 [-0.819, 1.500], loss: 9.806517, mae: 57.073475, mean_q: 70.489548
  881520/1100000: episode: 2028, duration: 7.822s, episode steps: 1000, steps per second: 128, episode reward: -105.969, mean reward: -0.106 [-7.219, 5.062], mean action: 1.678 [0.000, 3.000], mean observation: 0.102 [-0.759, 1.395], loss: 11.092568, mae: 56.286774, mean_q: 70.364670
  881671/1100000: episode: 2029, duration: 1.019s, episode steps: 151, steps per second: 148, episode reward: 4.815, mean reward: 0.032 [-100.000, 11.329], mean action: 1.960 [0.000, 3.000], mean observation: -0.097 [-1.315, 1.405], loss: 13.038146, mae: 55.857964, mean_q: 69.225670
  882079/1100000: episode: 2030, duration: 2.869s, episode steps: 408, steps per second: 142, episode reward: 190.516, mean reward: 0.467 [-15.372, 100.000], mean action: 1.792 [0.000, 3.000], mean observation: 0.107 [-1.145, 1.389], loss: 10.397796, mae: 56.360428, mean_q: 70.671394
  883079/1100000: episode: 2031, duration: 7.353s, episode steps: 1000, steps per second: 136, episode reward: -9.306, mean reward: -0.009 [-19.517, 20.683], mean action: 1.567 [0.000, 3.000], mean observation: 0.126 [-0.578, 1.391], loss: 7.829246, mae: 56.117012, mean_q: 70.217766
  883367/1100000: episode: 2032, duration: 1.975s, episode steps: 288, steps per second: 146, episode reward: -7.041, mean reward: -0.024 [-100.000, 9.646], mean action: 1.694 [0.000, 3.000], mean observation: 0.070 [-1.361, 1.420], loss: 57.774857, mae: 56.280666, mean_q: 68.864708
  884087/1100000: episode: 2033, duration: 5.162s, episode steps: 720, steps per second: 139, episode reward: 197.912, mean reward: 0.275 [-18.306, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.152 [-0.536, 1.393], loss: 10.977081, mae: 55.625145, mean_q: 68.686066
  884998/1100000: episode: 2034, duration: 6.630s, episode steps: 911, steps per second: 137, episode reward: 151.869, mean reward: 0.167 [-10.160, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.176 [-0.699, 1.466], loss: 10.764002, mae: 55.619915, mean_q: 68.464859
  885466/1100000: episode: 2035, duration: 3.215s, episode steps: 468, steps per second: 146, episode reward: 269.984, mean reward: 0.577 [-19.764, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: 0.082 [-0.876, 1.445], loss: 12.089859, mae: 55.336563, mean_q: 67.695717
  885611/1100000: episode: 2036, duration: 0.973s, episode steps: 145, steps per second: 149, episode reward: -467.799, mean reward: -3.226 [-100.000, 28.578], mean action: 1.766 [0.000, 3.000], mean observation: 0.011 [-2.396, 3.348], loss: 12.874791, mae: 55.879013, mean_q: 68.620316
  886244/1100000: episode: 2037, duration: 4.479s, episode steps: 633, steps per second: 141, episode reward: -49.434, mean reward: -0.078 [-100.000, 9.648], mean action: 1.624 [0.000, 3.000], mean observation: 0.081 [-1.215, 1.399], loss: 16.972773, mae: 55.286385, mean_q: 66.743988
  886652/1100000: episode: 2038, duration: 2.897s, episode steps: 408, steps per second: 141, episode reward: 23.231, mean reward: 0.057 [-100.000, 19.508], mean action: 1.674 [0.000, 3.000], mean observation: 0.017 [-0.774, 1.388], loss: 9.258529, mae: 55.375858, mean_q: 66.810013
  886790/1100000: episode: 2039, duration: 0.923s, episode steps: 138, steps per second: 150, episode reward: -15.750, mean reward: -0.114 [-100.000, 20.877], mean action: 1.442 [0.000, 3.000], mean observation: -0.113 [-0.875, 1.586], loss: 7.268854, mae: 55.500210, mean_q: 65.671906
  887405/1100000: episode: 2040, duration: 4.374s, episode steps: 615, steps per second: 141, episode reward: -53.040, mean reward: -0.086 [-100.000, 16.215], mean action: 1.641 [0.000, 3.000], mean observation: 0.028 [-0.720, 1.584], loss: 14.087136, mae: 55.257786, mean_q: 66.761963
  888123/1100000: episode: 2041, duration: 5.189s, episode steps: 718, steps per second: 138, episode reward: 252.522, mean reward: 0.352 [-18.956, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.091 [-1.117, 1.397], loss: 16.916819, mae: 55.216030, mean_q: 65.717148
  888305/1100000: episode: 2042, duration: 1.230s, episode steps: 182, steps per second: 148, episode reward: -23.670, mean reward: -0.130 [-100.000, 9.959], mean action: 1.731 [0.000, 3.000], mean observation: -0.035 [-2.349, 1.385], loss: 10.744465, mae: 54.157619, mean_q: 64.992752
  888992/1100000: episode: 2043, duration: 5.107s, episode steps: 687, steps per second: 135, episode reward: 201.560, mean reward: 0.293 [-20.391, 100.000], mean action: 1.521 [0.000, 3.000], mean observation: 0.189 [-0.861, 1.412], loss: 10.796969, mae: 55.229008, mean_q: 65.304893
  889619/1100000: episode: 2044, duration: 4.595s, episode steps: 627, steps per second: 136, episode reward: 172.307, mean reward: 0.275 [-19.351, 100.000], mean action: 1.856 [0.000, 3.000], mean observation: 0.161 [-0.574, 1.394], loss: 11.718298, mae: 55.072056, mean_q: 64.875435
  890040/1100000: episode: 2045, duration: 2.879s, episode steps: 421, steps per second: 146, episode reward: -2.284, mean reward: -0.005 [-100.000, 8.136], mean action: 1.570 [0.000, 3.000], mean observation: 0.119 [-0.637, 1.393], loss: 10.638677, mae: 54.849731, mean_q: 65.438675
  891040/1100000: episode: 2046, duration: 7.954s, episode steps: 1000, steps per second: 126, episode reward: 43.550, mean reward: 0.044 [-18.588, 12.495], mean action: 1.371 [0.000, 3.000], mean observation: 0.128 [-0.975, 1.511], loss: 11.328984, mae: 54.198071, mean_q: 63.958195
  891429/1100000: episode: 2047, duration: 2.676s, episode steps: 389, steps per second: 145, episode reward: -39.309, mean reward: -0.101 [-100.000, 8.175], mean action: 1.581 [0.000, 3.000], mean observation: 0.119 [-0.568, 1.405], loss: 12.172839, mae: 53.806740, mean_q: 63.788914
  891581/1100000: episode: 2048, duration: 1.016s, episode steps: 152, steps per second: 150, episode reward: -420.265, mean reward: -2.765 [-100.000, 1.514], mean action: 1.730 [0.000, 3.000], mean observation: 0.456 [-0.639, 3.348], loss: 12.331954, mae: 54.363461, mean_q: 63.665924
  892581/1100000: episode: 2049, duration: 7.505s, episode steps: 1000, steps per second: 133, episode reward: 15.241, mean reward: 0.015 [-19.193, 14.654], mean action: 1.263 [0.000, 3.000], mean observation: 0.152 [-0.832, 1.386], loss: 15.306152, mae: 52.550499, mean_q: 62.332603
  893483/1100000: episode: 2050, duration: 6.547s, episode steps: 902, steps per second: 138, episode reward: 267.584, mean reward: 0.297 [-20.984, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.156 [-0.440, 1.472], loss: 13.063002, mae: 51.888611, mean_q: 61.954174
  894483/1100000: episode: 2051, duration: 7.858s, episode steps: 1000, steps per second: 127, episode reward: -169.689, mean reward: -0.170 [-6.677, 5.936], mean action: 1.642 [0.000, 3.000], mean observation: -0.021 [-0.650, 1.460], loss: 8.648274, mae: 51.461342, mean_q: 61.283733
  894714/1100000: episode: 2052, duration: 1.584s, episode steps: 231, steps per second: 146, episode reward: -135.313, mean reward: -0.586 [-100.000, 3.934], mean action: 1.619 [0.000, 3.000], mean observation: 0.084 [-0.600, 1.435], loss: 39.804840, mae: 51.984112, mean_q: 60.899815
  895290/1100000: episode: 2053, duration: 4.074s, episode steps: 576, steps per second: 141, episode reward: 144.147, mean reward: 0.250 [-20.544, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.159 [-0.829, 1.597], loss: 16.617893, mae: 51.060246, mean_q: 60.697170
  895655/1100000: episode: 2054, duration: 2.521s, episode steps: 365, steps per second: 145, episode reward: -3.029, mean reward: -0.008 [-100.000, 20.991], mean action: 1.753 [0.000, 3.000], mean observation: 0.108 [-1.290, 1.397], loss: 8.784848, mae: 51.515491, mean_q: 61.384243
  896499/1100000: episode: 2055, duration: 6.174s, episode steps: 844, steps per second: 137, episode reward: 220.793, mean reward: 0.262 [-8.909, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.182 [-1.060, 1.897], loss: 11.327973, mae: 51.187248, mean_q: 60.442581
  896929/1100000: episode: 2056, duration: 3.115s, episode steps: 430, steps per second: 138, episode reward: 177.814, mean reward: 0.414 [-19.568, 100.000], mean action: 1.972 [0.000, 3.000], mean observation: 0.164 [-0.577, 1.394], loss: 9.738525, mae: 51.014606, mean_q: 59.822304
  897356/1100000: episode: 2057, duration: 2.983s, episode steps: 427, steps per second: 143, episode reward: 222.561, mean reward: 0.521 [-3.455, 100.000], mean action: 1.731 [0.000, 3.000], mean observation: 0.170 [-0.504, 1.448], loss: 10.860220, mae: 51.098675, mean_q: 59.309921
  898356/1100000: episode: 2058, duration: 8.215s, episode steps: 1000, steps per second: 122, episode reward: -10.815, mean reward: -0.011 [-23.906, 20.692], mean action: 1.660 [0.000, 3.000], mean observation: -0.060 [-0.692, 1.439], loss: 13.010597, mae: 50.796150, mean_q: 59.183853
  898871/1100000: episode: 2059, duration: 3.602s, episode steps: 515, steps per second: 143, episode reward: 163.830, mean reward: 0.318 [-14.844, 100.000], mean action: 1.693 [0.000, 3.000], mean observation: 0.030 [-0.629, 1.503], loss: 9.603999, mae: 50.550205, mean_q: 59.232063
  899871/1100000: episode: 2060, duration: 7.271s, episode steps: 1000, steps per second: 138, episode reward: -98.407, mean reward: -0.098 [-5.064, 5.632], mean action: 1.605 [0.000, 3.000], mean observation: -0.040 [-0.600, 1.422], loss: 10.999107, mae: 50.393139, mean_q: 58.923985
  900386/1100000: episode: 2061, duration: 3.723s, episode steps: 515, steps per second: 138, episode reward: -58.341, mean reward: -0.113 [-100.000, 12.248], mean action: 1.670 [0.000, 3.000], mean observation: -0.056 [-0.689, 1.433], loss: 9.700558, mae: 50.365108, mean_q: 59.447552
  901165/1100000: episode: 2062, duration: 6.318s, episode steps: 779, steps per second: 123, episode reward: 167.310, mean reward: 0.215 [-17.524, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: -0.086 [-0.730, 1.413], loss: 8.717035, mae: 50.920120, mean_q: 59.488075
  902165/1100000: episode: 2063, duration: 7.805s, episode steps: 1000, steps per second: 128, episode reward: -59.549, mean reward: -0.060 [-4.482, 4.932], mean action: 1.786 [0.000, 3.000], mean observation: -0.013 [-0.673, 1.519], loss: 16.281179, mae: 50.564297, mean_q: 58.851170
  902576/1100000: episode: 2064, duration: 2.862s, episode steps: 411, steps per second: 144, episode reward: 18.928, mean reward: 0.046 [-100.000, 15.821], mean action: 1.735 [0.000, 3.000], mean observation: 0.063 [-0.771, 1.386], loss: 8.119132, mae: 50.322357, mean_q: 58.676319
  903561/1100000: episode: 2065, duration: 7.576s, episode steps: 985, steps per second: 130, episode reward: 208.342, mean reward: 0.212 [-22.075, 100.000], mean action: 1.703 [0.000, 3.000], mean observation: 0.216 [-1.093, 1.525], loss: 11.235888, mae: 50.164803, mean_q: 58.305984
  904561/1100000: episode: 2066, duration: 7.974s, episode steps: 1000, steps per second: 125, episode reward: -43.548, mean reward: -0.044 [-5.076, 5.183], mean action: 1.807 [0.000, 3.000], mean observation: -0.088 [-0.600, 1.402], loss: 14.449241, mae: 50.397243, mean_q: 57.303181
  905561/1100000: episode: 2067, duration: 8.770s, episode steps: 1000, steps per second: 114, episode reward: -13.974, mean reward: -0.014 [-4.913, 4.569], mean action: 1.664 [0.000, 3.000], mean observation: -0.077 [-0.600, 1.406], loss: 9.320082, mae: 50.455086, mean_q: 57.578915
  906561/1100000: episode: 2068, duration: 7.343s, episode steps: 1000, steps per second: 136, episode reward: -130.539, mean reward: -0.131 [-5.140, 4.875], mean action: 1.869 [0.000, 3.000], mean observation: -0.002 [-0.612, 1.418], loss: 10.549188, mae: 50.247799, mean_q: 57.807510
  907561/1100000: episode: 2069, duration: 7.900s, episode steps: 1000, steps per second: 127, episode reward: -72.587, mean reward: -0.073 [-22.980, 23.869], mean action: 1.949 [0.000, 3.000], mean observation: 0.063 [-1.577, 1.411], loss: 12.789182, mae: 50.367741, mean_q: 58.683491
  908561/1100000: episode: 2070, duration: 8.016s, episode steps: 1000, steps per second: 125, episode reward: 22.932, mean reward: 0.023 [-4.885, 5.228], mean action: 1.774 [0.000, 3.000], mean observation: -0.064 [-0.978, 2.040], loss: 9.180843, mae: 49.839767, mean_q: 58.636890
  909561/1100000: episode: 2071, duration: 7.880s, episode steps: 1000, steps per second: 127, episode reward: -59.864, mean reward: -0.060 [-5.125, 4.377], mean action: 1.935 [0.000, 3.000], mean observation: 0.088 [-0.429, 1.412], loss: 7.440006, mae: 49.956921, mean_q: 58.479763
  910016/1100000: episode: 2072, duration: 3.119s, episode steps: 455, steps per second: 146, episode reward: 211.685, mean reward: 0.465 [-18.669, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: -0.008 [-0.600, 1.460], loss: 6.707955, mae: 50.002995, mean_q: 59.286770
  910419/1100000: episode: 2073, duration: 2.825s, episode steps: 403, steps per second: 143, episode reward: 187.061, mean reward: 0.464 [-18.332, 100.000], mean action: 1.596 [0.000, 3.000], mean observation: 0.200 [-1.172, 1.410], loss: 9.766765, mae: 50.493565, mean_q: 59.154003
  911076/1100000: episode: 2074, duration: 4.686s, episode steps: 657, steps per second: 140, episode reward: -128.049, mean reward: -0.195 [-100.000, 19.983], mean action: 1.591 [0.000, 3.000], mean observation: -0.027 [-1.382, 1.677], loss: 17.958239, mae: 50.551300, mean_q: 59.246731
  911843/1100000: episode: 2075, duration: 6.152s, episode steps: 767, steps per second: 125, episode reward: 92.976, mean reward: 0.121 [-11.824, 100.000], mean action: 1.975 [0.000, 3.000], mean observation: 0.189 [-0.657, 1.389], loss: 8.194664, mae: 50.628979, mean_q: 59.133091
  912497/1100000: episode: 2076, duration: 4.709s, episode steps: 654, steps per second: 139, episode reward: 236.038, mean reward: 0.361 [-19.621, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.045 [-0.653, 1.441], loss: 9.789966, mae: 50.554008, mean_q: 59.110378
  913181/1100000: episode: 2077, duration: 5.181s, episode steps: 684, steps per second: 132, episode reward: -256.191, mean reward: -0.375 [-100.000, 17.263], mean action: 1.661 [0.000, 3.000], mean observation: 0.120 [-1.618, 2.240], loss: 8.608925, mae: 50.529938, mean_q: 59.226860
  913379/1100000: episode: 2078, duration: 1.348s, episode steps: 198, steps per second: 147, episode reward: 33.906, mean reward: 0.171 [-100.000, 15.184], mean action: 1.783 [0.000, 3.000], mean observation: -0.028 [-0.876, 1.388], loss: 8.106478, mae: 50.282734, mean_q: 58.647125
  913551/1100000: episode: 2079, duration: 1.155s, episode steps: 172, steps per second: 149, episode reward: -57.946, mean reward: -0.337 [-100.000, 8.838], mean action: 1.523 [0.000, 3.000], mean observation: 0.091 [-1.209, 1.410], loss: 11.294149, mae: 50.595657, mean_q: 58.215939
  914272/1100000: episode: 2080, duration: 5.503s, episode steps: 721, steps per second: 131, episode reward: -77.538, mean reward: -0.108 [-100.000, 11.997], mean action: 1.649 [0.000, 3.000], mean observation: -0.079 [-0.972, 1.469], loss: 8.265689, mae: 50.766850, mean_q: 58.298115
  914517/1100000: episode: 2081, duration: 1.679s, episode steps: 245, steps per second: 146, episode reward: 18.377, mean reward: 0.075 [-100.000, 10.980], mean action: 1.841 [0.000, 3.000], mean observation: 0.115 [-1.316, 1.396], loss: 8.679502, mae: 51.114269, mean_q: 58.806911
  915165/1100000: episode: 2082, duration: 4.698s, episode steps: 648, steps per second: 138, episode reward: 229.358, mean reward: 0.354 [-9.021, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.156 [-1.276, 2.489], loss: 9.276339, mae: 51.202305, mean_q: 58.102848
  915758/1100000: episode: 2083, duration: 4.409s, episode steps: 593, steps per second: 134, episode reward: 173.487, mean reward: 0.293 [-20.214, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.104 [-1.370, 1.421], loss: 15.041718, mae: 51.160866, mean_q: 58.680016
  915876/1100000: episode: 2084, duration: 0.798s, episode steps: 118, steps per second: 148, episode reward: -15.932, mean reward: -0.135 [-100.000, 6.807], mean action: 1.669 [0.000, 3.000], mean observation: 0.004 [-0.862, 1.408], loss: 7.582317, mae: 51.403584, mean_q: 58.516075
  916447/1100000: episode: 2085, duration: 4.126s, episode steps: 571, steps per second: 138, episode reward: 207.458, mean reward: 0.363 [-11.518, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: -0.036 [-0.603, 1.405], loss: 10.949409, mae: 51.314281, mean_q: 57.511375
  916898/1100000: episode: 2086, duration: 3.154s, episode steps: 451, steps per second: 143, episode reward: 234.600, mean reward: 0.520 [-19.617, 100.000], mean action: 1.956 [0.000, 3.000], mean observation: 0.141 [-0.928, 1.392], loss: 9.565592, mae: 51.342312, mean_q: 58.617374
  917853/1100000: episode: 2087, duration: 7.537s, episode steps: 955, steps per second: 127, episode reward: 202.889, mean reward: 0.212 [-20.532, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.123 [-0.707, 1.418], loss: 12.886074, mae: 51.378635, mean_q: 58.525383
  918853/1100000: episode: 2088, duration: 8.253s, episode steps: 1000, steps per second: 121, episode reward: -5.910, mean reward: -0.006 [-10.059, 12.545], mean action: 1.670 [0.000, 3.000], mean observation: -0.020 [-0.772, 1.414], loss: 14.183714, mae: 51.568352, mean_q: 59.660397
  919640/1100000: episode: 2089, duration: 5.781s, episode steps: 787, steps per second: 136, episode reward: -98.527, mean reward: -0.125 [-100.000, 12.511], mean action: 1.653 [0.000, 3.000], mean observation: -0.013 [-0.732, 1.401], loss: 10.374377, mae: 51.459599, mean_q: 60.756969
  919792/1100000: episode: 2090, duration: 1.029s, episode steps: 152, steps per second: 148, episode reward: 7.114, mean reward: 0.047 [-100.000, 11.182], mean action: 1.947 [0.000, 3.000], mean observation: -0.046 [-0.760, 1.398], loss: 6.811573, mae: 52.088425, mean_q: 62.221493
  920792/1100000: episode: 2091, duration: 7.717s, episode steps: 1000, steps per second: 130, episode reward: -13.309, mean reward: -0.013 [-20.086, 21.756], mean action: 1.538 [0.000, 3.000], mean observation: 0.060 [-0.578, 1.411], loss: 9.973158, mae: 52.162323, mean_q: 61.646202
  921792/1100000: episode: 2092, duration: 7.743s, episode steps: 1000, steps per second: 129, episode reward: -33.564, mean reward: -0.034 [-5.371, 6.237], mean action: 1.767 [0.000, 3.000], mean observation: 0.066 [-0.851, 1.487], loss: 16.787443, mae: 52.278439, mean_q: 62.099998
  922360/1100000: episode: 2093, duration: 4.009s, episode steps: 568, steps per second: 142, episode reward: 177.516, mean reward: 0.313 [-17.396, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.134 [-0.992, 1.413], loss: 9.700377, mae: 52.614872, mean_q: 63.144600
  922764/1100000: episode: 2094, duration: 2.889s, episode steps: 404, steps per second: 140, episode reward: 265.526, mean reward: 0.657 [-21.868, 100.000], mean action: 1.718 [0.000, 3.000], mean observation: 0.117 [-0.616, 1.527], loss: 8.006005, mae: 52.914246, mean_q: 63.216442
  922921/1100000: episode: 2095, duration: 1.058s, episode steps: 157, steps per second: 148, episode reward: -146.302, mean reward: -0.932 [-100.000, 31.670], mean action: 1.694 [0.000, 3.000], mean observation: 0.027 [-1.951, 1.469], loss: 13.046020, mae: 53.519245, mean_q: 64.631241
  923042/1100000: episode: 2096, duration: 0.816s, episode steps: 121, steps per second: 148, episode reward: -130.724, mean reward: -1.080 [-100.000, 56.262], mean action: 1.504 [0.000, 3.000], mean observation: 0.005 [-1.544, 1.412], loss: 9.517583, mae: 53.138710, mean_q: 64.665680
  923234/1100000: episode: 2097, duration: 1.297s, episode steps: 192, steps per second: 148, episode reward: 18.833, mean reward: 0.098 [-100.000, 12.799], mean action: 1.625 [0.000, 3.000], mean observation: 0.091 [-1.848, 1.411], loss: 11.530446, mae: 53.254810, mean_q: 65.278488
  923362/1100000: episode: 2098, duration: 0.858s, episode steps: 128, steps per second: 149, episode reward: -20.597, mean reward: -0.161 [-100.000, 28.326], mean action: 1.602 [0.000, 3.000], mean observation: 0.159 [-1.541, 1.450], loss: 13.211822, mae: 53.212955, mean_q: 65.166824
  923448/1100000: episode: 2099, duration: 0.576s, episode steps: 86, steps per second: 149, episode reward: -20.219, mean reward: -0.235 [-100.000, 16.586], mean action: 1.430 [0.000, 3.000], mean observation: -0.046 [-1.184, 1.639], loss: 5.776716, mae: 53.416386, mean_q: 65.980347
  923605/1100000: episode: 2100, duration: 1.067s, episode steps: 157, steps per second: 147, episode reward: -198.337, mean reward: -1.263 [-100.000, 38.763], mean action: 1.822 [0.000, 3.000], mean observation: 0.027 [-1.630, 1.411], loss: 11.226521, mae: 53.178799, mean_q: 65.574875
  923905/1100000: episode: 2101, duration: 2.090s, episode steps: 300, steps per second: 144, episode reward: 275.996, mean reward: 0.920 [-17.409, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.087 [-1.550, 1.390], loss: 9.960160, mae: 53.257080, mean_q: 65.953445
  924004/1100000: episode: 2102, duration: 0.664s, episode steps: 99, steps per second: 149, episode reward: 24.786, mean reward: 0.250 [-100.000, 17.566], mean action: 1.343 [0.000, 3.000], mean observation: 0.096 [-0.906, 1.396], loss: 14.176328, mae: 53.209793, mean_q: 66.457390
  924409/1100000: episode: 2103, duration: 2.812s, episode steps: 405, steps per second: 144, episode reward: -192.190, mean reward: -0.475 [-100.000, 7.166], mean action: 1.741 [0.000, 3.000], mean observation: 0.066 [-0.765, 1.445], loss: 9.531728, mae: 53.358036, mean_q: 66.640381
  924557/1100000: episode: 2104, duration: 0.993s, episode steps: 148, steps per second: 149, episode reward: -14.546, mean reward: -0.098 [-100.000, 11.287], mean action: 1.757 [0.000, 3.000], mean observation: 0.139 [-0.679, 1.395], loss: 7.425978, mae: 53.237232, mean_q: 66.664589
  925029/1100000: episode: 2105, duration: 3.357s, episode steps: 472, steps per second: 141, episode reward: -114.297, mean reward: -0.242 [-100.000, 15.856], mean action: 1.678 [0.000, 3.000], mean observation: 0.010 [-0.774, 1.426], loss: 15.385069, mae: 53.127850, mean_q: 66.835251
  925608/1100000: episode: 2106, duration: 4.071s, episode steps: 579, steps per second: 142, episode reward: 174.747, mean reward: 0.302 [-17.550, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: -0.043 [-1.112, 1.431], loss: 12.551076, mae: 53.473408, mean_q: 67.705246
  926007/1100000: episode: 2107, duration: 2.834s, episode steps: 399, steps per second: 141, episode reward: -181.506, mean reward: -0.455 [-100.000, 13.635], mean action: 1.805 [0.000, 3.000], mean observation: 0.082 [-0.792, 1.394], loss: 14.505345, mae: 53.400505, mean_q: 67.613617
  926115/1100000: episode: 2108, duration: 0.723s, episode steps: 108, steps per second: 149, episode reward: -11.629, mean reward: -0.108 [-100.000, 15.082], mean action: 1.778 [0.000, 3.000], mean observation: 0.139 [-1.486, 1.394], loss: 18.682705, mae: 53.092075, mean_q: 67.212776
  926505/1100000: episode: 2109, duration: 2.727s, episode steps: 390, steps per second: 143, episode reward: 259.325, mean reward: 0.665 [-10.572, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.091 [-0.738, 1.396], loss: 12.651751, mae: 53.411160, mean_q: 67.757645
  927185/1100000: episode: 2110, duration: 4.947s, episode steps: 680, steps per second: 137, episode reward: 217.799, mean reward: 0.320 [-24.824, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.007 [-0.839, 1.390], loss: 12.237259, mae: 53.879539, mean_q: 68.403793
  927717/1100000: episode: 2111, duration: 3.708s, episode steps: 532, steps per second: 143, episode reward: 229.477, mean reward: 0.431 [-19.820, 100.000], mean action: 0.883 [0.000, 3.000], mean observation: 0.056 [-0.705, 1.401], loss: 8.036082, mae: 53.227577, mean_q: 68.623543
  928161/1100000: episode: 2112, duration: 3.165s, episode steps: 444, steps per second: 140, episode reward: -187.363, mean reward: -0.422 [-100.000, 4.262], mean action: 1.649 [0.000, 3.000], mean observation: 0.248 [-0.458, 1.408], loss: 9.108909, mae: 53.004505, mean_q: 68.907402
  928731/1100000: episode: 2113, duration: 4.113s, episode steps: 570, steps per second: 139, episode reward: -286.936, mean reward: -0.503 [-100.000, 5.050], mean action: 1.591 [0.000, 3.000], mean observation: 0.209 [-1.623, 1.426], loss: 9.374999, mae: 53.182167, mean_q: 69.110542
  929114/1100000: episode: 2114, duration: 2.754s, episode steps: 383, steps per second: 139, episode reward: 208.363, mean reward: 0.544 [-13.017, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.093 [-0.661, 1.391], loss: 12.225596, mae: 53.239662, mean_q: 69.388451
  929878/1100000: episode: 2115, duration: 6.122s, episode steps: 764, steps per second: 125, episode reward: 157.334, mean reward: 0.206 [-11.776, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.222 [-1.361, 2.039], loss: 8.274561, mae: 52.932796, mean_q: 69.221825
  930782/1100000: episode: 2116, duration: 6.517s, episode steps: 904, steps per second: 139, episode reward: -179.424, mean reward: -0.198 [-100.000, 23.116], mean action: 1.124 [0.000, 3.000], mean observation: 0.137 [-0.644, 1.402], loss: 12.859829, mae: 52.446342, mean_q: 68.568794
  931782/1100000: episode: 2117, duration: 7.279s, episode steps: 1000, steps per second: 137, episode reward: -10.654, mean reward: -0.011 [-4.891, 5.003], mean action: 1.787 [0.000, 3.000], mean observation: 0.123 [-0.896, 2.033], loss: 9.422902, mae: 52.299576, mean_q: 68.930885
  932782/1100000: episode: 2118, duration: 7.463s, episode steps: 1000, steps per second: 134, episode reward: 75.138, mean reward: 0.075 [-20.977, 21.458], mean action: 1.042 [0.000, 3.000], mean observation: 0.212 [-0.603, 1.409], loss: 10.021915, mae: 51.638191, mean_q: 67.902924
  933475/1100000: episode: 2119, duration: 5.062s, episode steps: 693, steps per second: 137, episode reward: 162.818, mean reward: 0.235 [-19.327, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.136 [-0.602, 1.393], loss: 7.879483, mae: 51.499977, mean_q: 68.270760
  933913/1100000: episode: 2120, duration: 3.013s, episode steps: 438, steps per second: 145, episode reward: -450.344, mean reward: -1.028 [-100.000, 6.595], mean action: 1.304 [0.000, 3.000], mean observation: 0.367 [-2.216, 3.166], loss: 9.732638, mae: 51.311455, mean_q: 67.900024
  934913/1100000: episode: 2121, duration: 8.141s, episode steps: 1000, steps per second: 123, episode reward: 26.197, mean reward: 0.026 [-20.885, 26.543], mean action: 1.408 [0.000, 3.000], mean observation: -0.023 [-0.774, 1.401], loss: 9.481063, mae: 51.771103, mean_q: 68.548721
  935599/1100000: episode: 2122, duration: 5.027s, episode steps: 686, steps per second: 136, episode reward: 164.441, mean reward: 0.240 [-23.944, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.150 [-0.922, 1.403], loss: 7.196597, mae: 51.602589, mean_q: 68.674461
  936599/1100000: episode: 2123, duration: 7.369s, episode steps: 1000, steps per second: 136, episode reward: 70.988, mean reward: 0.071 [-19.936, 22.367], mean action: 1.482 [0.000, 3.000], mean observation: 0.196 [-0.581, 1.412], loss: 9.639661, mae: 50.996563, mean_q: 67.790779
  937556/1100000: episode: 2124, duration: 7.330s, episode steps: 957, steps per second: 131, episode reward: 53.613, mean reward: 0.056 [-14.097, 100.000], mean action: 1.950 [0.000, 3.000], mean observation: 0.018 [-0.646, 1.409], loss: 8.554863, mae: 50.624577, mean_q: 67.055458
  937766/1100000: episode: 2125, duration: 1.409s, episode steps: 210, steps per second: 149, episode reward: -10.575, mean reward: -0.050 [-100.000, 15.893], mean action: 1.457 [0.000, 3.000], mean observation: 0.129 [-0.600, 1.417], loss: 8.846158, mae: 50.642254, mean_q: 66.882050
  938766/1100000: episode: 2126, duration: 8.211s, episode steps: 1000, steps per second: 122, episode reward: 29.251, mean reward: 0.029 [-19.625, 15.313], mean action: 1.716 [0.000, 3.000], mean observation: -0.045 [-0.659, 1.394], loss: 9.564983, mae: 49.923008, mean_q: 66.070564
  939111/1100000: episode: 2127, duration: 2.340s, episode steps: 345, steps per second: 147, episode reward: 16.845, mean reward: 0.049 [-100.000, 5.382], mean action: 1.739 [0.000, 3.000], mean observation: 0.085 [-0.688, 1.388], loss: 8.085492, mae: 49.807621, mean_q: 65.859818
  940111/1100000: episode: 2128, duration: 7.894s, episode steps: 1000, steps per second: 127, episode reward: -26.080, mean reward: -0.026 [-12.229, 16.490], mean action: 1.659 [0.000, 3.000], mean observation: -0.089 [-0.677, 1.406], loss: 7.148434, mae: 49.540642, mean_q: 65.988602
  940502/1100000: episode: 2129, duration: 2.732s, episode steps: 391, steps per second: 143, episode reward: 247.313, mean reward: 0.633 [-18.775, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.124 [-0.774, 1.406], loss: 6.260384, mae: 49.528332, mean_q: 66.131905
  941039/1100000: episode: 2130, duration: 3.799s, episode steps: 537, steps per second: 141, episode reward: 267.606, mean reward: 0.498 [-20.878, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.131 [-0.728, 1.522], loss: 9.007446, mae: 49.424931, mean_q: 66.127220
  941586/1100000: episode: 2131, duration: 3.910s, episode steps: 547, steps per second: 140, episode reward: 181.984, mean reward: 0.333 [-20.539, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.005 [-0.787, 1.425], loss: 11.145553, mae: 49.164028, mean_q: 65.572258
  942586/1100000: episode: 2132, duration: 7.303s, episode steps: 1000, steps per second: 137, episode reward: 125.535, mean reward: 0.126 [-19.894, 25.414], mean action: 1.437 [0.000, 3.000], mean observation: 0.252 [-1.123, 1.391], loss: 7.037828, mae: 48.813278, mean_q: 65.051186
  943543/1100000: episode: 2133, duration: 7.974s, episode steps: 957, steps per second: 120, episode reward: 134.103, mean reward: 0.140 [-17.644, 100.000], mean action: 1.860 [0.000, 3.000], mean observation: 0.103 [-0.679, 1.494], loss: 8.756433, mae: 48.100559, mean_q: 64.185425
  943865/1100000: episode: 2134, duration: 2.247s, episode steps: 322, steps per second: 143, episode reward: 255.599, mean reward: 0.794 [-19.876, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.086 [-0.736, 1.398], loss: 7.604403, mae: 47.589199, mean_q: 63.526169
  944865/1100000: episode: 2135, duration: 7.807s, episode steps: 1000, steps per second: 128, episode reward: -52.320, mean reward: -0.052 [-5.217, 6.135], mean action: 1.625 [0.000, 3.000], mean observation: 0.008 [-0.756, 1.501], loss: 8.205635, mae: 47.486012, mean_q: 63.333672
  945865/1100000: episode: 2136, duration: 8.670s, episode steps: 1000, steps per second: 115, episode reward: -38.769, mean reward: -0.039 [-20.695, 20.075], mean action: 1.830 [0.000, 3.000], mean observation: 0.095 [-0.563, 1.418], loss: 8.117029, mae: 46.652832, mean_q: 62.171295
  946365/1100000: episode: 2137, duration: 3.569s, episode steps: 500, steps per second: 140, episode reward: -244.428, mean reward: -0.489 [-100.000, 27.662], mean action: 1.714 [0.000, 3.000], mean observation: -0.070 [-0.805, 2.050], loss: 7.727672, mae: 46.172619, mean_q: 61.736240
  946985/1100000: episode: 2138, duration: 4.587s, episode steps: 620, steps per second: 135, episode reward: 226.967, mean reward: 0.366 [-19.047, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.174 [-0.701, 1.523], loss: 8.180120, mae: 45.853031, mean_q: 61.012695
  947396/1100000: episode: 2139, duration: 2.867s, episode steps: 411, steps per second: 143, episode reward: 7.485, mean reward: 0.018 [-100.000, 12.283], mean action: 1.895 [0.000, 3.000], mean observation: 0.004 [-0.777, 1.452], loss: 5.061044, mae: 45.673618, mean_q: 60.978245
  948050/1100000: episode: 2140, duration: 4.937s, episode steps: 654, steps per second: 132, episode reward: 189.684, mean reward: 0.290 [-18.418, 100.000], mean action: 1.615 [0.000, 3.000], mean observation: -0.060 [-0.759, 1.404], loss: 8.885410, mae: 45.358006, mean_q: 60.491932
  948302/1100000: episode: 2141, duration: 1.728s, episode steps: 252, steps per second: 146, episode reward: -80.850, mean reward: -0.321 [-100.000, 17.748], mean action: 1.655 [0.000, 3.000], mean observation: 0.060 [-0.991, 1.445], loss: 5.838292, mae: 45.492092, mean_q: 60.373852
  948956/1100000: episode: 2142, duration: 4.621s, episode steps: 654, steps per second: 142, episode reward: 248.833, mean reward: 0.380 [-19.366, 100.000], mean action: 0.807 [0.000, 3.000], mean observation: 0.144 [-1.128, 1.400], loss: 7.283123, mae: 45.018543, mean_q: 59.841637
  949408/1100000: episode: 2143, duration: 3.286s, episode steps: 452, steps per second: 138, episode reward: -418.822, mean reward: -0.927 [-100.000, 5.859], mean action: 1.597 [0.000, 3.000], mean observation: -0.046 [-1.157, 3.937], loss: 7.183846, mae: 44.493553, mean_q: 59.239922
  949698/1100000: episode: 2144, duration: 1.980s, episode steps: 290, steps per second: 146, episode reward: 270.413, mean reward: 0.932 [-21.220, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.133 [-0.843, 1.440], loss: 8.510406, mae: 44.357052, mean_q: 59.072704
  950159/1100000: episode: 2145, duration: 3.226s, episode steps: 461, steps per second: 143, episode reward: 244.699, mean reward: 0.531 [-19.644, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.207 [-0.668, 1.442], loss: 10.358206, mae: 44.491001, mean_q: 59.111404
  950564/1100000: episode: 2146, duration: 2.791s, episode steps: 405, steps per second: 145, episode reward: 152.225, mean reward: 0.376 [-21.204, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.188 [-0.728, 1.424], loss: 7.222392, mae: 44.555275, mean_q: 59.453239
  950663/1100000: episode: 2147, duration: 0.665s, episode steps: 99, steps per second: 149, episode reward: -7.283, mean reward: -0.074 [-100.000, 20.115], mean action: 1.606 [0.000, 3.000], mean observation: 0.030 [-1.196, 1.405], loss: 12.533504, mae: 44.946293, mean_q: 59.434235
  951663/1100000: episode: 2148, duration: 7.592s, episode steps: 1000, steps per second: 132, episode reward: 26.330, mean reward: 0.026 [-23.842, 15.990], mean action: 2.119 [0.000, 3.000], mean observation: 0.137 [-0.864, 1.391], loss: 8.302214, mae: 44.373009, mean_q: 59.043613
  951953/1100000: episode: 2149, duration: 2.045s, episode steps: 290, steps per second: 142, episode reward: 309.469, mean reward: 1.067 [-10.797, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.092 [-0.988, 1.395], loss: 8.391219, mae: 44.286640, mean_q: 59.026340
  952697/1100000: episode: 2150, duration: 5.743s, episode steps: 744, steps per second: 130, episode reward: 127.585, mean reward: 0.171 [-11.254, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: -0.070 [-0.702, 1.411], loss: 7.044868, mae: 44.223705, mean_q: 58.914890
  952909/1100000: episode: 2151, duration: 1.513s, episode steps: 212, steps per second: 140, episode reward: -30.115, mean reward: -0.142 [-100.000, 13.460], mean action: 1.722 [0.000, 3.000], mean observation: -0.012 [-0.750, 1.459], loss: 6.776205, mae: 43.579899, mean_q: 58.077583
  953447/1100000: episode: 2152, duration: 3.850s, episode steps: 538, steps per second: 140, episode reward: 253.901, mean reward: 0.472 [-19.927, 100.000], mean action: 1.742 [0.000, 3.000], mean observation: 0.109 [-0.938, 1.390], loss: 7.254436, mae: 43.798004, mean_q: 58.282753
  953627/1100000: episode: 2153, duration: 1.209s, episode steps: 180, steps per second: 149, episode reward: 12.783, mean reward: 0.071 [-100.000, 18.066], mean action: 1.550 [0.000, 3.000], mean observation: 0.106 [-1.027, 1.490], loss: 7.043332, mae: 43.157330, mean_q: 57.137966
  954153/1100000: episode: 2154, duration: 3.720s, episode steps: 526, steps per second: 141, episode reward: 168.442, mean reward: 0.320 [-17.832, 100.000], mean action: 2.316 [0.000, 3.000], mean observation: 0.183 [-0.613, 1.395], loss: 11.156767, mae: 43.801895, mean_q: 58.269711
  954695/1100000: episode: 2155, duration: 3.936s, episode steps: 542, steps per second: 138, episode reward: -38.978, mean reward: -0.072 [-100.000, 18.627], mean action: 1.548 [0.000, 3.000], mean observation: 0.079 [-1.077, 1.513], loss: 9.878734, mae: 43.405602, mean_q: 57.648655
  954863/1100000: episode: 2156, duration: 1.121s, episode steps: 168, steps per second: 150, episode reward: -33.834, mean reward: -0.201 [-100.000, 14.773], mean action: 1.554 [0.000, 3.000], mean observation: 0.001 [-0.861, 1.414], loss: 8.316464, mae: 43.366402, mean_q: 57.668858
  954985/1100000: episode: 2157, duration: 0.822s, episode steps: 122, steps per second: 148, episode reward: 5.782, mean reward: 0.047 [-100.000, 17.011], mean action: 1.918 [0.000, 3.000], mean observation: 0.068 [-1.409, 1.448], loss: 9.039598, mae: 42.974548, mean_q: 57.142452
  955126/1100000: episode: 2158, duration: 0.952s, episode steps: 141, steps per second: 148, episode reward: -87.169, mean reward: -0.618 [-100.000, 14.121], mean action: 1.667 [0.000, 3.000], mean observation: 0.070 [-2.106, 1.397], loss: 9.063616, mae: 43.939922, mean_q: 57.968872
  955842/1100000: episode: 2159, duration: 5.150s, episode steps: 716, steps per second: 139, episode reward: 158.925, mean reward: 0.222 [-16.091, 100.000], mean action: 1.747 [0.000, 3.000], mean observation: -0.018 [-0.655, 1.446], loss: 10.314546, mae: 43.406754, mean_q: 57.728226
  956134/1100000: episode: 2160, duration: 2.015s, episode steps: 292, steps per second: 145, episode reward: 250.407, mean reward: 0.858 [-10.743, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.037 [-0.804, 1.402], loss: 8.186172, mae: 43.746731, mean_q: 58.308716
  956230/1100000: episode: 2161, duration: 0.650s, episode steps: 96, steps per second: 148, episode reward: -123.609, mean reward: -1.288 [-100.000, 2.273], mean action: 1.719 [0.000, 3.000], mean observation: -0.125 [-1.319, 1.392], loss: 7.954687, mae: 43.029530, mean_q: 57.152863
  956882/1100000: episode: 2162, duration: 4.672s, episode steps: 652, steps per second: 140, episode reward: 183.251, mean reward: 0.281 [-19.180, 100.000], mean action: 1.840 [0.000, 3.000], mean observation: -0.041 [-0.681, 1.519], loss: 10.021491, mae: 43.021961, mean_q: 57.376270
  957317/1100000: episode: 2163, duration: 3.131s, episode steps: 435, steps per second: 139, episode reward: 190.177, mean reward: 0.437 [-13.051, 100.000], mean action: 2.101 [0.000, 3.000], mean observation: -0.001 [-0.669, 1.393], loss: 10.627164, mae: 43.187622, mean_q: 57.302963
  957661/1100000: episode: 2164, duration: 2.420s, episode steps: 344, steps per second: 142, episode reward: 254.500, mean reward: 0.740 [-8.661, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.082 [-0.806, 1.465], loss: 9.384704, mae: 42.572289, mean_q: 56.645981
  958073/1100000: episode: 2165, duration: 2.960s, episode steps: 412, steps per second: 139, episode reward: 219.680, mean reward: 0.533 [-13.734, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.002 [-0.976, 1.393], loss: 9.227110, mae: 42.429642, mean_q: 56.400932
  958553/1100000: episode: 2166, duration: 3.409s, episode steps: 480, steps per second: 141, episode reward: 183.753, mean reward: 0.383 [-13.444, 100.000], mean action: 1.854 [0.000, 3.000], mean observation: -0.034 [-0.816, 1.389], loss: 7.327532, mae: 42.558300, mean_q: 56.739666
  958670/1100000: episode: 2167, duration: 0.792s, episode steps: 117, steps per second: 148, episode reward: 35.895, mean reward: 0.307 [-100.000, 18.003], mean action: 1.889 [0.000, 3.000], mean observation: 0.064 [-0.878, 1.392], loss: 9.750759, mae: 42.746735, mean_q: 56.821777
  959559/1100000: episode: 2168, duration: 6.852s, episode steps: 889, steps per second: 130, episode reward: 178.946, mean reward: 0.201 [-17.121, 100.000], mean action: 1.826 [0.000, 3.000], mean observation: 0.175 [-0.829, 1.400], loss: 6.988166, mae: 42.897842, mean_q: 57.174999
  959750/1100000: episode: 2169, duration: 1.302s, episode steps: 191, steps per second: 147, episode reward: 25.825, mean reward: 0.135 [-100.000, 5.178], mean action: 1.895 [0.000, 3.000], mean observation: -0.063 [-0.743, 1.387], loss: 11.243062, mae: 42.610607, mean_q: 56.801670
  960167/1100000: episode: 2170, duration: 2.969s, episode steps: 417, steps per second: 140, episode reward: 234.321, mean reward: 0.562 [-8.358, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.071 [-0.979, 1.515], loss: 7.801443, mae: 42.508862, mean_q: 56.818954
  960388/1100000: episode: 2171, duration: 1.501s, episode steps: 221, steps per second: 147, episode reward: -27.883, mean reward: -0.126 [-100.000, 9.384], mean action: 1.570 [0.000, 3.000], mean observation: 0.158 [-1.741, 1.399], loss: 7.863772, mae: 42.450253, mean_q: 56.490898
  960816/1100000: episode: 2172, duration: 2.978s, episode steps: 428, steps per second: 144, episode reward: 172.780, mean reward: 0.404 [-15.441, 100.000], mean action: 2.185 [0.000, 3.000], mean observation: 0.201 [-0.817, 1.424], loss: 8.067645, mae: 42.657112, mean_q: 56.795303
  961002/1100000: episode: 2173, duration: 1.264s, episode steps: 186, steps per second: 147, episode reward: -19.360, mean reward: -0.104 [-100.000, 17.517], mean action: 1.731 [0.000, 3.000], mean observation: 0.015 [-1.432, 1.469], loss: 8.257104, mae: 42.753910, mean_q: 57.119953
  962002/1100000: episode: 2174, duration: 7.487s, episode steps: 1000, steps per second: 134, episode reward: -58.343, mean reward: -0.058 [-4.900, 4.747], mean action: 1.472 [0.000, 3.000], mean observation: -0.049 [-0.711, 1.400], loss: 8.852830, mae: 42.423218, mean_q: 56.489079
  962204/1100000: episode: 2175, duration: 1.363s, episode steps: 202, steps per second: 148, episode reward: -102.385, mean reward: -0.507 [-100.000, 54.179], mean action: 1.619 [0.000, 3.000], mean observation: 0.141 [-0.714, 1.561], loss: 10.392653, mae: 41.688572, mean_q: 55.624119
  962710/1100000: episode: 2176, duration: 3.519s, episode steps: 506, steps per second: 144, episode reward: 279.710, mean reward: 0.553 [-18.079, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.147 [-0.686, 1.534], loss: 7.866291, mae: 42.061745, mean_q: 55.991138
  963710/1100000: episode: 2177, duration: 8.098s, episode steps: 1000, steps per second: 123, episode reward: -32.196, mean reward: -0.032 [-18.570, 21.574], mean action: 1.893 [0.000, 3.000], mean observation: 0.081 [-0.763, 1.458], loss: 6.781659, mae: 41.909042, mean_q: 56.000496
  964002/1100000: episode: 2178, duration: 2.056s, episode steps: 292, steps per second: 142, episode reward: -186.654, mean reward: -0.639 [-100.000, 4.494], mean action: 1.911 [0.000, 3.000], mean observation: 0.298 [-0.786, 1.592], loss: 8.214266, mae: 42.185677, mean_q: 56.107853
  964337/1100000: episode: 2179, duration: 2.321s, episode steps: 335, steps per second: 144, episode reward: -252.488, mean reward: -0.754 [-100.000, 4.790], mean action: 1.561 [0.000, 3.000], mean observation: 0.253 [-0.584, 1.629], loss: 12.881661, mae: 42.401428, mean_q: 56.218323
  964886/1100000: episode: 2180, duration: 3.908s, episode steps: 549, steps per second: 140, episode reward: 128.822, mean reward: 0.235 [-15.145, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: -0.043 [-0.829, 1.411], loss: 9.308701, mae: 42.277985, mean_q: 56.414806
  965366/1100000: episode: 2181, duration: 3.496s, episode steps: 480, steps per second: 137, episode reward: 179.247, mean reward: 0.373 [-13.991, 100.000], mean action: 2.285 [0.000, 3.000], mean observation: 0.108 [-0.871, 1.388], loss: 7.563296, mae: 42.298985, mean_q: 56.392693
  965942/1100000: episode: 2182, duration: 4.332s, episode steps: 576, steps per second: 133, episode reward: -303.993, mean reward: -0.528 [-100.000, 4.386], mean action: 1.641 [0.000, 3.000], mean observation: 0.199 [-0.714, 1.407], loss: 8.084188, mae: 41.999668, mean_q: 56.010826
  966382/1100000: episode: 2183, duration: 3.082s, episode steps: 440, steps per second: 143, episode reward: 233.475, mean reward: 0.531 [-12.330, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.148 [-0.857, 1.404], loss: 9.854336, mae: 42.559887, mean_q: 56.744095
  966479/1100000: episode: 2184, duration: 0.657s, episode steps: 97, steps per second: 148, episode reward: -44.260, mean reward: -0.456 [-100.000, 20.037], mean action: 1.876 [0.000, 3.000], mean observation: 0.060 [-1.020, 2.600], loss: 6.236688, mae: 42.590866, mean_q: 56.891647
  966752/1100000: episode: 2185, duration: 1.903s, episode steps: 273, steps per second: 143, episode reward: -166.041, mean reward: -0.608 [-100.000, 29.556], mean action: 1.766 [0.000, 3.000], mean observation: 0.058 [-1.894, 1.391], loss: 10.719092, mae: 42.554317, mean_q: 56.715546
  966876/1100000: episode: 2186, duration: 0.842s, episode steps: 124, steps per second: 147, episode reward: -210.505, mean reward: -1.698 [-100.000, 5.284], mean action: 1.806 [0.000, 3.000], mean observation: 0.148 [-0.867, 1.545], loss: 5.512378, mae: 43.114491, mean_q: 57.482368
  967876/1100000: episode: 2187, duration: 8.433s, episode steps: 1000, steps per second: 119, episode reward: -40.287, mean reward: -0.040 [-19.575, 10.967], mean action: 1.792 [0.000, 3.000], mean observation: -0.080 [-0.718, 1.406], loss: 8.770037, mae: 42.753693, mean_q: 57.039936
  968876/1100000: episode: 2188, duration: 7.713s, episode steps: 1000, steps per second: 130, episode reward: -27.790, mean reward: -0.028 [-8.404, 16.926], mean action: 1.717 [0.000, 3.000], mean observation: -0.077 [-0.860, 1.387], loss: 8.900539, mae: 43.000557, mean_q: 57.268204
  969388/1100000: episode: 2189, duration: 3.702s, episode steps: 512, steps per second: 138, episode reward: 246.076, mean reward: 0.481 [-9.227, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.155 [-1.121, 1.401], loss: 7.786264, mae: 42.459812, mean_q: 56.603466
  969492/1100000: episode: 2190, duration: 0.700s, episode steps: 104, steps per second: 149, episode reward: -5.223, mean reward: -0.050 [-100.000, 17.381], mean action: 1.885 [0.000, 3.000], mean observation: -0.041 [-1.040, 1.664], loss: 5.985802, mae: 42.852848, mean_q: 57.011070
  970060/1100000: episode: 2191, duration: 4.194s, episode steps: 568, steps per second: 135, episode reward: 200.795, mean reward: 0.354 [-11.467, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: -0.059 [-0.704, 1.486], loss: 7.660010, mae: 42.934761, mean_q: 57.164288
  971060/1100000: episode: 2192, duration: 7.725s, episode steps: 1000, steps per second: 129, episode reward: -25.162, mean reward: -0.025 [-5.334, 5.143], mean action: 1.743 [0.000, 3.000], mean observation: -0.064 [-0.729, 1.458], loss: 9.939856, mae: 42.329849, mean_q: 56.265385
  971463/1100000: episode: 2193, duration: 2.857s, episode steps: 403, steps per second: 141, episode reward: 258.903, mean reward: 0.642 [-18.728, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.238 [-0.752, 1.387], loss: 11.780184, mae: 42.598412, mean_q: 56.445335
  971962/1100000: episode: 2194, duration: 3.566s, episode steps: 499, steps per second: 140, episode reward: 211.112, mean reward: 0.423 [-17.973, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.172 [-0.631, 1.503], loss: 9.625354, mae: 42.154800, mean_q: 55.979340
  972422/1100000: episode: 2195, duration: 3.323s, episode steps: 460, steps per second: 138, episode reward: 227.592, mean reward: 0.495 [-9.139, 100.000], mean action: 2.067 [0.000, 3.000], mean observation: 0.164 [-0.657, 1.390], loss: 9.276340, mae: 41.556053, mean_q: 55.335285
  973016/1100000: episode: 2196, duration: 4.524s, episode steps: 594, steps per second: 131, episode reward: 185.106, mean reward: 0.312 [-18.905, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: -0.036 [-0.738, 1.396], loss: 9.386728, mae: 41.705532, mean_q: 55.393532
  973472/1100000: episode: 2197, duration: 3.234s, episode steps: 456, steps per second: 141, episode reward: 201.887, mean reward: 0.443 [-19.202, 100.000], mean action: 2.015 [0.000, 3.000], mean observation: 0.011 [-0.799, 1.423], loss: 9.629290, mae: 41.891457, mean_q: 55.290348
  973603/1100000: episode: 2198, duration: 0.909s, episode steps: 131, steps per second: 144, episode reward: -560.983, mean reward: -4.282 [-100.000, 1.414], mean action: 1.840 [0.000, 3.000], mean observation: 0.272 [-1.423, 3.667], loss: 6.559746, mae: 41.703705, mean_q: 55.480354
  973704/1100000: episode: 2199, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -291.844, mean reward: -2.890 [-100.000, 1.912], mean action: 2.099 [0.000, 3.000], mean observation: 0.129 [-1.196, 2.091], loss: 13.904856, mae: 42.289600, mean_q: 56.164246
  974037/1100000: episode: 2200, duration: 2.348s, episode steps: 333, steps per second: 142, episode reward: 230.529, mean reward: 0.692 [-8.525, 100.000], mean action: 2.030 [0.000, 3.000], mean observation: 0.002 [-0.710, 1.516], loss: 9.194578, mae: 42.107929, mean_q: 55.738041
  974639/1100000: episode: 2201, duration: 4.288s, episode steps: 602, steps per second: 140, episode reward: 126.431, mean reward: 0.210 [-23.137, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.165 [-0.652, 1.407], loss: 8.613612, mae: 42.510181, mean_q: 56.275402
  974970/1100000: episode: 2202, duration: 2.331s, episode steps: 331, steps per second: 142, episode reward: -43.385, mean reward: -0.131 [-100.000, 14.682], mean action: 1.731 [0.000, 3.000], mean observation: -0.048 [-1.023, 1.402], loss: 10.366728, mae: 42.380512, mean_q: 56.314377
  975613/1100000: episode: 2203, duration: 4.546s, episode steps: 643, steps per second: 141, episode reward: 251.039, mean reward: 0.390 [-18.263, 100.000], mean action: 1.501 [0.000, 3.000], mean observation: 0.017 [-0.688, 1.419], loss: 8.074295, mae: 42.903564, mean_q: 56.838615
  975994/1100000: episode: 2204, duration: 2.632s, episode steps: 381, steps per second: 145, episode reward: 221.714, mean reward: 0.582 [-22.647, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.223 [-1.160, 1.404], loss: 9.122747, mae: 42.793846, mean_q: 56.853767
  976759/1100000: episode: 2205, duration: 5.858s, episode steps: 765, steps per second: 131, episode reward: -249.850, mean reward: -0.327 [-100.000, 31.072], mean action: 1.429 [0.000, 3.000], mean observation: -0.015 [-0.957, 1.968], loss: 9.969995, mae: 42.645195, mean_q: 56.368362
  977511/1100000: episode: 2206, duration: 5.453s, episode steps: 752, steps per second: 138, episode reward: 244.713, mean reward: 0.325 [-19.498, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.052 [-0.606, 1.496], loss: 8.918065, mae: 42.108971, mean_q: 55.543743
  978371/1100000: episode: 2207, duration: 6.338s, episode steps: 860, steps per second: 136, episode reward: 198.250, mean reward: 0.231 [-21.913, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.015 [-0.704, 1.487], loss: 8.978482, mae: 41.696365, mean_q: 54.980808
  979127/1100000: episode: 2208, duration: 5.416s, episode steps: 756, steps per second: 140, episode reward: 205.895, mean reward: 0.272 [-20.440, 100.000], mean action: 2.257 [0.000, 3.000], mean observation: 0.060 [-0.810, 1.448], loss: 8.479506, mae: 41.458488, mean_q: 54.501293
  979407/1100000: episode: 2209, duration: 1.909s, episode steps: 280, steps per second: 147, episode reward: -189.318, mean reward: -0.676 [-100.000, 2.808], mean action: 1.575 [0.000, 3.000], mean observation: 0.328 [-0.301, 1.795], loss: 7.859951, mae: 41.534462, mean_q: 54.845093
  979865/1100000: episode: 2210, duration: 3.234s, episode steps: 458, steps per second: 142, episode reward: 226.981, mean reward: 0.496 [-16.965, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: -0.050 [-0.886, 1.399], loss: 7.497993, mae: 41.608189, mean_q: 54.960102
  980400/1100000: episode: 2211, duration: 3.776s, episode steps: 535, steps per second: 142, episode reward: 245.279, mean reward: 0.458 [-12.140, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.205 [-0.661, 1.461], loss: 9.920607, mae: 40.951038, mean_q: 53.798271
  980640/1100000: episode: 2212, duration: 1.632s, episode steps: 240, steps per second: 147, episode reward: 268.185, mean reward: 1.117 [-17.472, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.233 [-1.485, 1.407], loss: 10.311240, mae: 41.283298, mean_q: 54.495922
  980829/1100000: episode: 2213, duration: 1.285s, episode steps: 189, steps per second: 147, episode reward: -64.952, mean reward: -0.344 [-100.000, 7.223], mean action: 1.794 [0.000, 3.000], mean observation: 0.057 [-0.755, 1.642], loss: 9.607547, mae: 41.570827, mean_q: 54.845402
  981218/1100000: episode: 2214, duration: 2.709s, episode steps: 389, steps per second: 144, episode reward: -179.546, mean reward: -0.462 [-100.000, 3.749], mean action: 1.478 [0.000, 3.000], mean observation: 0.255 [-0.294, 1.848], loss: 7.563334, mae: 41.549515, mean_q: 54.867661
  981363/1100000: episode: 2215, duration: 0.983s, episode steps: 145, steps per second: 147, episode reward: -230.471, mean reward: -1.589 [-100.000, 113.872], mean action: 1.490 [0.000, 3.000], mean observation: -0.029 [-1.994, 1.408], loss: 7.751307, mae: 42.112858, mean_q: 55.164337
  981447/1100000: episode: 2216, duration: 0.557s, episode steps: 84, steps per second: 151, episode reward: -173.643, mean reward: -2.067 [-100.000, 6.535], mean action: 0.298 [0.000, 3.000], mean observation: 0.028 [-1.523, 1.438], loss: 10.140105, mae: 41.551060, mean_q: 54.077160
  982447/1100000: episode: 2217, duration: 7.299s, episode steps: 1000, steps per second: 137, episode reward: 117.679, mean reward: 0.118 [-18.854, 21.096], mean action: 1.831 [0.000, 3.000], mean observation: 0.250 [-0.575, 1.510], loss: 7.912330, mae: 42.031265, mean_q: 55.187111
  983447/1100000: episode: 2218, duration: 7.187s, episode steps: 1000, steps per second: 139, episode reward: 103.190, mean reward: 0.103 [-17.229, 14.233], mean action: 1.941 [0.000, 3.000], mean observation: 0.075 [-0.774, 1.397], loss: 8.450727, mae: 41.963364, mean_q: 55.271881
  983577/1100000: episode: 2219, duration: 0.873s, episode steps: 130, steps per second: 149, episode reward: -119.791, mean reward: -0.921 [-100.000, 11.079], mean action: 1.777 [0.000, 3.000], mean observation: -0.024 [-0.974, 2.000], loss: 11.927272, mae: 42.866501, mean_q: 56.220058
  984126/1100000: episode: 2220, duration: 3.876s, episode steps: 549, steps per second: 142, episode reward: 223.623, mean reward: 0.407 [-17.697, 100.000], mean action: 0.767 [0.000, 3.000], mean observation: 0.259 [-0.796, 1.392], loss: 8.403939, mae: 42.304283, mean_q: 55.553986
  984510/1100000: episode: 2221, duration: 2.700s, episode steps: 384, steps per second: 142, episode reward: 207.573, mean reward: 0.541 [-10.920, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.019 [-0.723, 1.416], loss: 7.314112, mae: 42.400494, mean_q: 55.772182
  985062/1100000: episode: 2222, duration: 3.989s, episode steps: 552, steps per second: 138, episode reward: 239.830, mean reward: 0.434 [-12.251, 100.000], mean action: 1.841 [0.000, 3.000], mean observation: 0.135 [-0.995, 1.388], loss: 8.290271, mae: 42.081295, mean_q: 54.997581
  985712/1100000: episode: 2223, duration: 4.932s, episode steps: 650, steps per second: 132, episode reward: 168.917, mean reward: 0.260 [-19.075, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.126 [-0.636, 1.413], loss: 8.698905, mae: 42.145546, mean_q: 55.246815
  986005/1100000: episode: 2224, duration: 2.109s, episode steps: 293, steps per second: 139, episode reward: 0.171, mean reward: 0.001 [-100.000, 17.763], mean action: 1.700 [0.000, 3.000], mean observation: 0.006 [-0.867, 1.695], loss: 6.217880, mae: 42.535343, mean_q: 55.848396
  986422/1100000: episode: 2225, duration: 3.026s, episode steps: 417, steps per second: 138, episode reward: 235.203, mean reward: 0.564 [-18.600, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.235 [-0.675, 1.397], loss: 7.843197, mae: 42.531532, mean_q: 55.694836
  986868/1100000: episode: 2226, duration: 3.207s, episode steps: 446, steps per second: 139, episode reward: -94.444, mean reward: -0.212 [-100.000, 15.162], mean action: 1.807 [0.000, 3.000], mean observation: 0.117 [-1.327, 1.442], loss: 7.738443, mae: 42.798923, mean_q: 56.171547
  987262/1100000: episode: 2227, duration: 2.745s, episode steps: 394, steps per second: 144, episode reward: 255.099, mean reward: 0.647 [-10.385, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.081 [-0.673, 1.418], loss: 7.272305, mae: 42.801598, mean_q: 56.046761
  988262/1100000: episode: 2228, duration: 7.525s, episode steps: 1000, steps per second: 133, episode reward: 85.594, mean reward: 0.086 [-19.048, 22.821], mean action: 1.827 [0.000, 3.000], mean observation: 0.248 [-0.757, 1.424], loss: 8.990889, mae: 42.308262, mean_q: 55.767719
  988780/1100000: episode: 2229, duration: 3.732s, episode steps: 518, steps per second: 139, episode reward: 196.029, mean reward: 0.378 [-18.080, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.167 [-0.982, 1.399], loss: 7.140955, mae: 42.243637, mean_q: 55.892788
  989074/1100000: episode: 2230, duration: 2.041s, episode steps: 294, steps per second: 144, episode reward: -52.212, mean reward: -0.178 [-100.000, 9.836], mean action: 1.881 [0.000, 3.000], mean observation: -0.095 [-1.082, 1.404], loss: 6.141664, mae: 42.741982, mean_q: 56.400524
  989612/1100000: episode: 2231, duration: 3.756s, episode steps: 538, steps per second: 143, episode reward: 282.722, mean reward: 0.526 [-20.793, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.143 [-0.757, 1.405], loss: 10.216688, mae: 42.728626, mean_q: 56.326225
  990060/1100000: episode: 2232, duration: 3.223s, episode steps: 448, steps per second: 139, episode reward: 223.147, mean reward: 0.498 [-18.320, 100.000], mean action: 2.096 [0.000, 3.000], mean observation: 0.129 [-1.588, 1.389], loss: 6.542474, mae: 42.622810, mean_q: 56.115593
  990308/1100000: episode: 2233, duration: 1.682s, episode steps: 248, steps per second: 147, episode reward: 208.782, mean reward: 0.842 [-9.880, 100.000], mean action: 2.278 [0.000, 3.000], mean observation: 0.025 [-0.799, 1.431], loss: 6.666965, mae: 42.607334, mean_q: 56.359692
  990891/1100000: episode: 2234, duration: 4.177s, episode steps: 583, steps per second: 140, episode reward: 240.591, mean reward: 0.413 [-18.650, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.002 [-0.739, 1.387], loss: 7.215851, mae: 43.039497, mean_q: 56.669716
  991339/1100000: episode: 2235, duration: 3.212s, episode steps: 448, steps per second: 139, episode reward: 237.933, mean reward: 0.531 [-17.748, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.102 [-0.512, 1.417], loss: 7.665198, mae: 42.592747, mean_q: 56.128960
  991967/1100000: episode: 2236, duration: 4.586s, episode steps: 628, steps per second: 137, episode reward: -86.709, mean reward: -0.138 [-100.000, 10.750], mean action: 1.787 [0.000, 3.000], mean observation: -0.057 [-0.945, 1.424], loss: 7.583765, mae: 42.386719, mean_q: 55.890980
  992208/1100000: episode: 2237, duration: 1.639s, episode steps: 241, steps per second: 147, episode reward: 18.645, mean reward: 0.077 [-100.000, 11.780], mean action: 1.817 [0.000, 3.000], mean observation: 0.101 [-0.696, 1.407], loss: 7.175785, mae: 42.490192, mean_q: 55.378857
  992465/1100000: episode: 2238, duration: 1.740s, episode steps: 257, steps per second: 148, episode reward: 278.049, mean reward: 1.082 [-12.261, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.076 [-1.083, 1.423], loss: 10.013668, mae: 42.230186, mean_q: 55.711426
  993042/1100000: episode: 2239, duration: 4.141s, episode steps: 577, steps per second: 139, episode reward: 213.691, mean reward: 0.370 [-9.400, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: -0.030 [-0.657, 1.420], loss: 7.471739, mae: 41.921860, mean_q: 55.283436
  993528/1100000: episode: 2240, duration: 3.390s, episode steps: 486, steps per second: 143, episode reward: -104.137, mean reward: -0.214 [-100.000, 13.668], mean action: 1.788 [0.000, 3.000], mean observation: -0.075 [-1.237, 1.400], loss: 6.105142, mae: 41.860283, mean_q: 55.286411
  994528/1100000: episode: 2241, duration: 8.305s, episode steps: 1000, steps per second: 120, episode reward: -65.268, mean reward: -0.065 [-5.773, 6.382], mean action: 1.736 [0.000, 3.000], mean observation: 0.173 [-0.773, 1.400], loss: 7.115528, mae: 41.752499, mean_q: 55.251915
  995528/1100000: episode: 2242, duration: 7.738s, episode steps: 1000, steps per second: 129, episode reward: 29.995, mean reward: 0.030 [-19.306, 13.323], mean action: 1.767 [0.000, 3.000], mean observation: 0.173 [-0.958, 1.389], loss: 7.767342, mae: 41.853073, mean_q: 55.489929
  996283/1100000: episode: 2243, duration: 5.514s, episode steps: 755, steps per second: 137, episode reward: 186.619, mean reward: 0.247 [-18.684, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: -0.042 [-0.762, 1.399], loss: 6.624898, mae: 41.669861, mean_q: 55.315952
  996587/1100000: episode: 2244, duration: 2.063s, episode steps: 304, steps per second: 147, episode reward: 206.794, mean reward: 0.680 [-18.928, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.184 [-0.582, 1.434], loss: 5.299246, mae: 41.258732, mean_q: 54.988514
  996941/1100000: episode: 2245, duration: 2.458s, episode steps: 354, steps per second: 144, episode reward: 242.682, mean reward: 0.686 [-11.382, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.197 [-0.845, 1.392], loss: 6.261189, mae: 41.525482, mean_q: 55.261570
  997733/1100000: episode: 2246, duration: 5.739s, episode steps: 792, steps per second: 138, episode reward: 162.018, mean reward: 0.205 [-18.068, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.174 [-0.714, 1.386], loss: 7.013829, mae: 41.816467, mean_q: 55.530544
  998049/1100000: episode: 2247, duration: 2.168s, episode steps: 316, steps per second: 146, episode reward: 208.821, mean reward: 0.661 [-10.373, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.089 [-0.686, 1.436], loss: 6.330913, mae: 42.145996, mean_q: 55.918896
  998429/1100000: episode: 2248, duration: 2.678s, episode steps: 380, steps per second: 142, episode reward: 212.178, mean reward: 0.558 [-12.569, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.117 [-0.599, 1.486], loss: 7.990139, mae: 42.105953, mean_q: 56.204811
  998748/1100000: episode: 2249, duration: 2.231s, episode steps: 319, steps per second: 143, episode reward: 275.512, mean reward: 0.864 [-17.890, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.114 [-0.539, 1.393], loss: 7.000076, mae: 42.402443, mean_q: 56.507793
  999076/1100000: episode: 2250, duration: 2.263s, episode steps: 328, steps per second: 145, episode reward: 241.486, mean reward: 0.736 [-9.894, 100.000], mean action: 1.640 [0.000, 3.000], mean observation: 0.148 [-0.594, 1.405], loss: 7.792329, mae: 42.113342, mean_q: 56.129456
  999432/1100000: episode: 2251, duration: 2.468s, episode steps: 356, steps per second: 144, episode reward: 273.375, mean reward: 0.768 [-20.871, 100.000], mean action: 1.846 [0.000, 3.000], mean observation: -0.005 [-0.849, 1.385], loss: 5.693429, mae: 42.159779, mean_q: 56.460522
 1000037/1100000: episode: 2252, duration: 4.256s, episode steps: 605, steps per second: 142, episode reward: 222.467, mean reward: 0.368 [-12.737, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.224 [-0.520, 1.487], loss: 7.625663, mae: 42.530315, mean_q: 56.734058
 1000380/1100000: episode: 2253, duration: 2.407s, episode steps: 343, steps per second: 142, episode reward: 267.664, mean reward: 0.780 [-18.246, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.034 [-0.788, 1.416], loss: 6.068901, mae: 42.028175, mean_q: 56.231846
 1000712/1100000: episode: 2254, duration: 2.280s, episode steps: 332, steps per second: 146, episode reward: 239.612, mean reward: 0.722 [-10.068, 100.000], mean action: 1.955 [0.000, 3.000], mean observation: 0.014 [-0.819, 1.411], loss: 8.176474, mae: 42.810249, mean_q: 56.917603
 1000987/1100000: episode: 2255, duration: 1.901s, episode steps: 275, steps per second: 145, episode reward: 278.854, mean reward: 1.014 [-7.995, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.062 [-1.082, 1.394], loss: 5.699989, mae: 42.707897, mean_q: 56.918728
 1001139/1100000: episode: 2256, duration: 1.014s, episode steps: 152, steps per second: 150, episode reward: -253.241, mean reward: -1.666 [-100.000, 5.537], mean action: 1.664 [0.000, 3.000], mean observation: 0.200 [-0.918, 2.177], loss: 10.039960, mae: 42.350155, mean_q: 56.358437
 1002139/1100000: episode: 2257, duration: 7.605s, episode steps: 1000, steps per second: 131, episode reward: -78.861, mean reward: -0.079 [-5.082, 5.742], mean action: 1.570 [0.000, 3.000], mean observation: 0.176 [-0.851, 1.524], loss: 7.096702, mae: 43.129494, mean_q: 57.519279
 1002537/1100000: episode: 2258, duration: 2.848s, episode steps: 398, steps per second: 140, episode reward: -5.697, mean reward: -0.014 [-100.000, 8.562], mean action: 1.656 [0.000, 3.000], mean observation: -0.035 [-0.672, 1.421], loss: 6.308769, mae: 42.852623, mean_q: 57.136372
 1003537/1100000: episode: 2259, duration: 7.981s, episode steps: 1000, steps per second: 125, episode reward: -93.186, mean reward: -0.093 [-5.051, 5.686], mean action: 1.553 [0.000, 3.000], mean observation: 0.162 [-0.778, 1.430], loss: 7.404484, mae: 43.126476, mean_q: 57.356075
 1003776/1100000: episode: 2260, duration: 1.628s, episode steps: 239, steps per second: 147, episode reward: 5.017, mean reward: 0.021 [-100.000, 8.008], mean action: 1.669 [0.000, 3.000], mean observation: 0.063 [-0.536, 1.399], loss: 6.378837, mae: 43.424702, mean_q: 57.363003
 1004268/1100000: episode: 2261, duration: 3.537s, episode steps: 492, steps per second: 139, episode reward: 238.159, mean reward: 0.484 [-18.691, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.093 [-0.545, 1.435], loss: 7.584191, mae: 43.045197, mean_q: 57.045876
 1004639/1100000: episode: 2262, duration: 2.587s, episode steps: 371, steps per second: 143, episode reward: -70.086, mean reward: -0.189 [-100.000, 18.690], mean action: 1.426 [0.000, 3.000], mean observation: -0.025 [-1.020, 1.415], loss: 6.076863, mae: 43.211849, mean_q: 57.032215
 1005473/1100000: episode: 2263, duration: 6.435s, episode steps: 834, steps per second: 130, episode reward: 195.711, mean reward: 0.235 [-20.508, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.183 [-0.663, 1.408], loss: 6.425406, mae: 43.386971, mean_q: 57.435703
 1006135/1100000: episode: 2264, duration: 4.746s, episode steps: 662, steps per second: 139, episode reward: 267.540, mean reward: 0.404 [-19.484, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.065 [-0.661, 1.449], loss: 7.454009, mae: 43.135811, mean_q: 57.225010
 1006575/1100000: episode: 2265, duration: 3.088s, episode steps: 440, steps per second: 142, episode reward: 198.449, mean reward: 0.451 [-19.066, 100.000], mean action: 2.343 [0.000, 3.000], mean observation: 0.043 [-0.642, 1.406], loss: 5.757783, mae: 43.580101, mean_q: 57.626865
 1006812/1100000: episode: 2266, duration: 1.603s, episode steps: 237, steps per second: 148, episode reward: -92.265, mean reward: -0.389 [-100.000, 23.495], mean action: 1.599 [0.000, 3.000], mean observation: 0.068 [-1.898, 1.466], loss: 7.845309, mae: 43.120049, mean_q: 57.162144
 1007347/1100000: episode: 2267, duration: 3.853s, episode steps: 535, steps per second: 139, episode reward: 249.643, mean reward: 0.467 [-21.666, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.088 [-1.123, 1.582], loss: 7.257594, mae: 43.501797, mean_q: 57.569523
 1007925/1100000: episode: 2268, duration: 4.244s, episode steps: 578, steps per second: 136, episode reward: 210.810, mean reward: 0.365 [-18.716, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.072 [-0.682, 1.395], loss: 8.769311, mae: 43.728886, mean_q: 57.883598
 1008213/1100000: episode: 2269, duration: 1.965s, episode steps: 288, steps per second: 147, episode reward: 237.956, mean reward: 0.826 [-6.149, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.177 [-0.742, 1.457], loss: 6.799094, mae: 43.807053, mean_q: 58.048531
 1008764/1100000: episode: 2270, duration: 3.852s, episode steps: 551, steps per second: 143, episode reward: -256.989, mean reward: -0.466 [-100.000, 8.843], mean action: 1.809 [0.000, 3.000], mean observation: -0.015 [-1.317, 1.497], loss: 6.518231, mae: 43.607533, mean_q: 57.840618
 1008970/1100000: episode: 2271, duration: 1.392s, episode steps: 206, steps per second: 148, episode reward: -192.814, mean reward: -0.936 [-100.000, 10.557], mean action: 1.748 [0.000, 3.000], mean observation: 0.042 [-1.804, 1.401], loss: 7.404133, mae: 44.563030, mean_q: 58.907948
 1009801/1100000: episode: 2272, duration: 6.819s, episode steps: 831, steps per second: 122, episode reward: 150.457, mean reward: 0.181 [-18.163, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.202 [-0.610, 1.387], loss: 8.955166, mae: 43.834648, mean_q: 57.896225
 1010249/1100000: episode: 2273, duration: 3.258s, episode steps: 448, steps per second: 138, episode reward: 185.095, mean reward: 0.413 [-17.218, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: -0.036 [-0.774, 1.388], loss: 6.115078, mae: 43.431923, mean_q: 57.453640
 1010599/1100000: episode: 2274, duration: 2.489s, episode steps: 350, steps per second: 141, episode reward: 254.385, mean reward: 0.727 [-17.853, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.075 [-0.674, 1.498], loss: 8.019402, mae: 44.399532, mean_q: 58.957489
 1010800/1100000: episode: 2275, duration: 1.378s, episode steps: 201, steps per second: 146, episode reward: -24.778, mean reward: -0.123 [-100.000, 11.310], mean action: 1.866 [0.000, 3.000], mean observation: 0.081 [-0.764, 1.402], loss: 8.929655, mae: 44.021717, mean_q: 58.162086
 1011530/1100000: episode: 2276, duration: 5.311s, episode steps: 730, steps per second: 137, episode reward: 227.112, mean reward: 0.311 [-19.125, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.157 [-0.734, 1.402], loss: 8.394110, mae: 44.080799, mean_q: 58.512547
 1012530/1100000: episode: 2277, duration: 7.936s, episode steps: 1000, steps per second: 126, episode reward: 51.778, mean reward: 0.052 [-20.515, 22.840], mean action: 1.492 [0.000, 3.000], mean observation: -0.026 [-0.707, 1.422], loss: 8.128762, mae: 44.095638, mean_q: 58.502350
 1013530/1100000: episode: 2278, duration: 7.301s, episode steps: 1000, steps per second: 137, episode reward: 64.089, mean reward: 0.064 [-23.412, 14.251], mean action: 2.002 [0.000, 3.000], mean observation: 0.012 [-0.697, 1.395], loss: 7.332787, mae: 43.761993, mean_q: 58.072563
 1014530/1100000: episode: 2279, duration: 8.167s, episode steps: 1000, steps per second: 122, episode reward: -21.701, mean reward: -0.022 [-20.066, 19.609], mean action: 1.665 [0.000, 3.000], mean observation: 0.176 [-0.691, 1.424], loss: 8.729936, mae: 43.292610, mean_q: 57.168560
 1014784/1100000: episode: 2280, duration: 1.728s, episode steps: 254, steps per second: 147, episode reward: -96.421, mean reward: -0.380 [-100.000, 18.390], mean action: 1.740 [0.000, 3.000], mean observation: 0.040 [-1.002, 1.439], loss: 6.030566, mae: 43.058002, mean_q: 56.904228
 1015162/1100000: episode: 2281, duration: 2.665s, episode steps: 378, steps per second: 142, episode reward: 299.384, mean reward: 0.792 [-17.414, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.095 [-1.145, 1.390], loss: 8.961637, mae: 43.493809, mean_q: 57.472282
 1015894/1100000: episode: 2282, duration: 5.260s, episode steps: 732, steps per second: 139, episode reward: 227.712, mean reward: 0.311 [-18.266, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.010 [-0.685, 1.395], loss: 8.043918, mae: 43.737850, mean_q: 57.762039
 1016839/1100000: episode: 2283, duration: 7.118s, episode steps: 945, steps per second: 133, episode reward: 230.797, mean reward: 0.244 [-20.614, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.126 [-1.307, 1.477], loss: 6.073574, mae: 43.401791, mean_q: 57.621788
 1017689/1100000: episode: 2284, duration: 6.486s, episode steps: 850, steps per second: 131, episode reward: 194.700, mean reward: 0.229 [-20.696, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.021 [-0.712, 1.398], loss: 8.300142, mae: 43.243675, mean_q: 57.271778
 1018185/1100000: episode: 2285, duration: 3.558s, episode steps: 496, steps per second: 139, episode reward: 217.303, mean reward: 0.438 [-17.347, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.071 [-0.728, 1.453], loss: 5.491562, mae: 42.982845, mean_q: 56.649830
 1018539/1100000: episode: 2286, duration: 2.608s, episode steps: 354, steps per second: 136, episode reward: 2.702, mean reward: 0.008 [-100.000, 10.366], mean action: 1.582 [0.000, 3.000], mean observation: 0.076 [-0.546, 1.465], loss: 5.598511, mae: 43.187378, mean_q: 57.255924
 1019539/1100000: episode: 2287, duration: 7.607s, episode steps: 1000, steps per second: 131, episode reward: 21.485, mean reward: 0.021 [-20.065, 11.641], mean action: 1.753 [0.000, 3.000], mean observation: -0.008 [-0.667, 1.488], loss: 7.213162, mae: 42.931709, mean_q: 56.905952
 1020319/1100000: episode: 2288, duration: 5.949s, episode steps: 780, steps per second: 131, episode reward: 234.443, mean reward: 0.301 [-18.569, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.220 [-0.988, 1.394], loss: 8.175147, mae: 42.603718, mean_q: 56.365463
 1020862/1100000: episode: 2289, duration: 3.932s, episode steps: 543, steps per second: 138, episode reward: 208.239, mean reward: 0.383 [-19.015, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.206 [-0.492, 1.406], loss: 6.338757, mae: 42.282990, mean_q: 55.975082
 1021673/1100000: episode: 2290, duration: 5.777s, episode steps: 811, steps per second: 140, episode reward: 187.904, mean reward: 0.232 [-20.261, 100.000], mean action: 1.599 [0.000, 3.000], mean observation: 0.225 [-0.724, 1.410], loss: 6.525560, mae: 42.451817, mean_q: 56.342815
 1021794/1100000: episode: 2291, duration: 0.813s, episode steps: 121, steps per second: 149, episode reward: -68.023, mean reward: -0.562 [-100.000, 11.020], mean action: 1.479 [0.000, 3.000], mean observation: 0.106 [-1.928, 1.411], loss: 7.182055, mae: 41.922153, mean_q: 55.568501
 1022072/1100000: episode: 2292, duration: 1.923s, episode steps: 278, steps per second: 145, episode reward: -3.483, mean reward: -0.013 [-100.000, 17.108], mean action: 1.935 [0.000, 3.000], mean observation: -0.062 [-1.336, 1.396], loss: 6.279215, mae: 42.248905, mean_q: 55.892788
 1023072/1100000: episode: 2293, duration: 7.312s, episode steps: 1000, steps per second: 137, episode reward: 89.317, mean reward: 0.089 [-18.666, 14.824], mean action: 1.948 [0.000, 3.000], mean observation: 0.093 [-0.686, 1.388], loss: 6.540163, mae: 42.622593, mean_q: 56.351578
 1023701/1100000: episode: 2294, duration: 4.389s, episode steps: 629, steps per second: 143, episode reward: 181.764, mean reward: 0.289 [-14.641, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.016 [-0.728, 1.403], loss: 5.894731, mae: 42.746555, mean_q: 56.815346
 1024118/1100000: episode: 2295, duration: 2.974s, episode steps: 417, steps per second: 140, episode reward: 208.626, mean reward: 0.500 [-17.434, 100.000], mean action: 2.353 [0.000, 3.000], mean observation: 0.013 [-0.715, 1.399], loss: 9.923735, mae: 42.517086, mean_q: 56.129436
 1024448/1100000: episode: 2296, duration: 2.279s, episode steps: 330, steps per second: 145, episode reward: 252.410, mean reward: 0.765 [-11.007, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.054 [-0.744, 1.493], loss: 8.344426, mae: 42.857998, mean_q: 56.725990
 1024591/1100000: episode: 2297, duration: 0.962s, episode steps: 143, steps per second: 149, episode reward: 7.786, mean reward: 0.054 [-100.000, 7.581], mean action: 1.671 [0.000, 3.000], mean observation: 0.107 [-0.665, 1.422], loss: 4.425250, mae: 42.782478, mean_q: 57.142376
 1024696/1100000: episode: 2298, duration: 0.710s, episode steps: 105, steps per second: 148, episode reward: 50.511, mean reward: 0.481 [-100.000, 18.054], mean action: 1.762 [0.000, 3.000], mean observation: 0.046 [-0.968, 1.388], loss: 9.035033, mae: 43.517082, mean_q: 57.669796
 1025002/1100000: episode: 2299, duration: 2.120s, episode steps: 306, steps per second: 144, episode reward: 257.936, mean reward: 0.843 [-10.615, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.097 [-0.627, 1.433], loss: 4.573376, mae: 43.057529, mean_q: 57.305206
 1025281/1100000: episode: 2300, duration: 1.897s, episode steps: 279, steps per second: 147, episode reward: 289.723, mean reward: 1.038 [-17.374, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.094 [-0.771, 1.393], loss: 8.470269, mae: 43.531139, mean_q: 58.108124
 1025660/1100000: episode: 2301, duration: 2.734s, episode steps: 379, steps per second: 139, episode reward: 257.398, mean reward: 0.679 [-10.334, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.117 [-0.472, 1.402], loss: 8.367110, mae: 43.229614, mean_q: 57.591743
 1025935/1100000: episode: 2302, duration: 1.934s, episode steps: 275, steps per second: 142, episode reward: 262.949, mean reward: 0.956 [-11.130, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.041 [-0.700, 1.391], loss: 6.265359, mae: 43.761276, mean_q: 57.936287
 1026286/1100000: episode: 2303, duration: 2.418s, episode steps: 351, steps per second: 145, episode reward: 204.655, mean reward: 0.583 [-12.798, 100.000], mean action: 1.718 [0.000, 3.000], mean observation: 0.146 [-0.871, 1.445], loss: 4.391706, mae: 43.349678, mean_q: 57.644226
 1026830/1100000: episode: 2304, duration: 4.104s, episode steps: 544, steps per second: 133, episode reward: -822.799, mean reward: -1.512 [-100.000, 10.296], mean action: 1.697 [0.000, 3.000], mean observation: -0.106 [-5.083, 2.442], loss: 7.334605, mae: 44.140507, mean_q: 58.462753
 1027261/1100000: episode: 2305, duration: 3.116s, episode steps: 431, steps per second: 138, episode reward: 215.691, mean reward: 0.500 [-16.335, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.226 [-0.649, 1.470], loss: 7.084886, mae: 44.320656, mean_q: 58.247738
 1027785/1100000: episode: 2306, duration: 3.773s, episode steps: 524, steps per second: 139, episode reward: 255.660, mean reward: 0.488 [-17.873, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.235 [-0.631, 1.442], loss: 10.316429, mae: 44.040607, mean_q: 57.728817
 1028239/1100000: episode: 2307, duration: 3.233s, episode steps: 454, steps per second: 140, episode reward: 235.647, mean reward: 0.519 [-17.186, 100.000], mean action: 1.793 [0.000, 3.000], mean observation: 0.153 [-0.700, 1.399], loss: 12.954197, mae: 43.700699, mean_q: 57.222446
 1028567/1100000: episode: 2308, duration: 2.250s, episode steps: 328, steps per second: 146, episode reward: 285.937, mean reward: 0.872 [-18.480, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.077 [-0.736, 1.412], loss: 11.598219, mae: 43.677876, mean_q: 57.666016
 1028955/1100000: episode: 2309, duration: 2.744s, episode steps: 388, steps per second: 141, episode reward: 248.529, mean reward: 0.641 [-17.664, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.194 [-0.586, 1.400], loss: 5.572770, mae: 43.497864, mean_q: 57.474880
 1029098/1100000: episode: 2310, duration: 0.964s, episode steps: 143, steps per second: 148, episode reward: -60.677, mean reward: -0.424 [-100.000, 14.119], mean action: 1.734 [0.000, 3.000], mean observation: 0.043 [-2.297, 1.412], loss: 6.227048, mae: 43.644455, mean_q: 57.722744
 1029540/1100000: episode: 2311, duration: 3.142s, episode steps: 442, steps per second: 141, episode reward: -184.431, mean reward: -0.417 [-100.000, 4.497], mean action: 1.735 [0.000, 3.000], mean observation: 0.277 [-0.467, 1.495], loss: 6.946368, mae: 43.851086, mean_q: 58.108673
 1029907/1100000: episode: 2312, duration: 2.500s, episode steps: 367, steps per second: 147, episode reward: 219.270, mean reward: 0.597 [-19.752, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.197 [-0.874, 1.437], loss: 7.296169, mae: 43.886959, mean_q: 58.320072
 1030281/1100000: episode: 2313, duration: 2.571s, episode steps: 374, steps per second: 145, episode reward: 280.461, mean reward: 0.750 [-9.634, 100.000], mean action: 0.770 [0.000, 3.000], mean observation: 0.111 [-0.805, 1.523], loss: 5.289954, mae: 43.640388, mean_q: 57.801929
 1030377/1100000: episode: 2314, duration: 0.639s, episode steps: 96, steps per second: 150, episode reward: -206.965, mean reward: -2.156 [-100.000, 6.762], mean action: 1.146 [0.000, 3.000], mean observation: -0.085 [-1.669, 1.911], loss: 5.950136, mae: 44.839386, mean_q: 59.715137
 1030802/1100000: episode: 2315, duration: 2.997s, episode steps: 425, steps per second: 142, episode reward: 243.625, mean reward: 0.573 [-10.843, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.215 [-0.604, 1.523], loss: 7.353058, mae: 43.933640, mean_q: 58.354481
 1031041/1100000: episode: 2316, duration: 1.632s, episode steps: 239, steps per second: 146, episode reward: 264.139, mean reward: 1.105 [-8.951, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.086 [-0.652, 1.397], loss: 4.616053, mae: 44.228436, mean_q: 58.353260
 1032041/1100000: episode: 2317, duration: 7.177s, episode steps: 1000, steps per second: 139, episode reward: 110.174, mean reward: 0.110 [-20.773, 25.255], mean action: 0.896 [0.000, 3.000], mean observation: 0.213 [-0.632, 1.458], loss: 6.606408, mae: 44.455540, mean_q: 58.725719
 1032402/1100000: episode: 2318, duration: 2.528s, episode steps: 361, steps per second: 143, episode reward: 271.406, mean reward: 0.752 [-9.268, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.031 [-0.800, 1.407], loss: 6.270188, mae: 44.562756, mean_q: 59.090168
 1032632/1100000: episode: 2319, duration: 1.571s, episode steps: 230, steps per second: 146, episode reward: 230.179, mean reward: 1.001 [-20.307, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.084 [-0.685, 1.410], loss: 6.742492, mae: 45.480331, mean_q: 59.805367
 1033147/1100000: episode: 2320, duration: 3.755s, episode steps: 515, steps per second: 137, episode reward: 212.958, mean reward: 0.414 [-24.488, 100.000], mean action: 1.645 [0.000, 3.000], mean observation: 0.137 [-0.579, 1.472], loss: 8.326106, mae: 45.300892, mean_q: 60.168709
 1033270/1100000: episode: 2321, duration: 0.831s, episode steps: 123, steps per second: 148, episode reward: -10.292, mean reward: -0.084 [-100.000, 21.174], mean action: 1.699 [0.000, 3.000], mean observation: 0.060 [-1.491, 1.400], loss: 6.314153, mae: 45.450665, mean_q: 60.452267
 1033518/1100000: episode: 2322, duration: 1.670s, episode steps: 248, steps per second: 149, episode reward: 260.779, mean reward: 1.052 [-10.775, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.242 [-0.675, 1.419], loss: 6.948809, mae: 45.357441, mean_q: 60.080883
 1033734/1100000: episode: 2323, duration: 1.447s, episode steps: 216, steps per second: 149, episode reward: 249.238, mean reward: 1.154 [-9.597, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.085 [-0.690, 1.416], loss: 5.801075, mae: 45.430561, mean_q: 60.379658
 1034206/1100000: episode: 2324, duration: 3.314s, episode steps: 472, steps per second: 142, episode reward: 298.638, mean reward: 0.633 [-18.502, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.143 [-0.648, 1.448], loss: 7.284930, mae: 46.277637, mean_q: 61.600849
 1034843/1100000: episode: 2325, duration: 4.690s, episode steps: 637, steps per second: 136, episode reward: 248.927, mean reward: 0.391 [-9.723, 100.000], mean action: 1.918 [0.000, 3.000], mean observation: -0.047 [-0.749, 1.408], loss: 7.567214, mae: 46.419785, mean_q: 61.916492
 1035035/1100000: episode: 2326, duration: 1.311s, episode steps: 192, steps per second: 146, episode reward: -0.714, mean reward: -0.004 [-100.000, 15.696], mean action: 1.745 [0.000, 3.000], mean observation: -0.037 [-0.823, 1.398], loss: 3.598172, mae: 46.632549, mean_q: 62.148937
 1035610/1100000: episode: 2327, duration: 4.075s, episode steps: 575, steps per second: 141, episode reward: 187.235, mean reward: 0.326 [-18.353, 100.000], mean action: 1.835 [0.000, 3.000], mean observation: 0.040 [-0.600, 1.397], loss: 7.602860, mae: 46.843288, mean_q: 62.231373
 1035842/1100000: episode: 2328, duration: 1.566s, episode steps: 232, steps per second: 148, episode reward: -149.150, mean reward: -0.643 [-100.000, 9.438], mean action: 1.694 [0.000, 3.000], mean observation: 0.051 [-1.001, 1.525], loss: 9.635270, mae: 47.316216, mean_q: 63.004353
 1036157/1100000: episode: 2329, duration: 2.200s, episode steps: 315, steps per second: 143, episode reward: -47.015, mean reward: -0.149 [-100.000, 18.526], mean action: 1.768 [0.000, 3.000], mean observation: -0.027 [-0.828, 1.424], loss: 5.338495, mae: 47.596588, mean_q: 63.080227
 1036580/1100000: episode: 2330, duration: 2.928s, episode steps: 423, steps per second: 144, episode reward: 272.172, mean reward: 0.643 [-17.656, 100.000], mean action: 0.981 [0.000, 3.000], mean observation: 0.129 [-1.077, 1.395], loss: 8.934454, mae: 47.866489, mean_q: 63.517048
 1036916/1100000: episode: 2331, duration: 2.337s, episode steps: 336, steps per second: 144, episode reward: 221.400, mean reward: 0.659 [-5.127, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: -0.023 [-0.602, 1.431], loss: 9.555147, mae: 47.811893, mean_q: 63.723534
 1037108/1100000: episode: 2332, duration: 1.298s, episode steps: 192, steps per second: 148, episode reward: -94.241, mean reward: -0.491 [-100.000, 10.979], mean action: 1.646 [0.000, 3.000], mean observation: -0.035 [-0.919, 1.400], loss: 5.986799, mae: 48.524990, mean_q: 64.175774
 1037349/1100000: episode: 2333, duration: 1.684s, episode steps: 241, steps per second: 143, episode reward: -196.433, mean reward: -0.815 [-100.000, 39.199], mean action: 1.759 [0.000, 3.000], mean observation: 0.003 [-0.701, 2.382], loss: 6.810398, mae: 48.493534, mean_q: 64.228539
 1037962/1100000: episode: 2334, duration: 4.408s, episode steps: 613, steps per second: 139, episode reward: 190.663, mean reward: 0.311 [-19.412, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.102 [-1.270, 1.428], loss: 8.258127, mae: 48.236095, mean_q: 64.010925
 1038190/1100000: episode: 2335, duration: 1.547s, episode steps: 228, steps per second: 147, episode reward: 30.564, mean reward: 0.134 [-100.000, 10.112], mean action: 1.579 [0.000, 3.000], mean observation: 0.203 [-0.799, 1.861], loss: 8.773519, mae: 48.459335, mean_q: 64.367592
 1038875/1100000: episode: 2336, duration: 4.801s, episode steps: 685, steps per second: 143, episode reward: 259.853, mean reward: 0.379 [-18.888, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.024 [-0.600, 1.459], loss: 10.230302, mae: 48.820278, mean_q: 64.626358
 1039490/1100000: episode: 2337, duration: 4.478s, episode steps: 615, steps per second: 137, episode reward: 236.728, mean reward: 0.385 [-17.549, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.142 [-0.584, 1.583], loss: 9.284897, mae: 48.826004, mean_q: 64.800072
 1039756/1100000: episode: 2338, duration: 1.813s, episode steps: 266, steps per second: 147, episode reward: 262.206, mean reward: 0.986 [-17.639, 100.000], mean action: 2.071 [0.000, 3.000], mean observation: 0.182 [-0.801, 1.391], loss: 9.333115, mae: 48.779812, mean_q: 64.462761
 1040100/1100000: episode: 2339, duration: 2.402s, episode steps: 344, steps per second: 143, episode reward: 145.451, mean reward: 0.423 [-19.927, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: 0.169 [-1.051, 1.414], loss: 6.126630, mae: 48.542843, mean_q: 64.393158
 1040475/1100000: episode: 2340, duration: 2.556s, episode steps: 375, steps per second: 147, episode reward: 204.701, mean reward: 0.546 [-10.852, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.187 [-1.008, 1.408], loss: 11.073487, mae: 49.053593, mean_q: 65.057426
 1041015/1100000: episode: 2341, duration: 3.866s, episode steps: 540, steps per second: 140, episode reward: -180.319, mean reward: -0.334 [-100.000, 5.384], mean action: 1.931 [0.000, 3.000], mean observation: 0.061 [-1.001, 1.394], loss: 8.354808, mae: 49.718258, mean_q: 66.042755
 1041238/1100000: episode: 2342, duration: 1.521s, episode steps: 223, steps per second: 147, episode reward: 28.703, mean reward: 0.129 [-100.000, 8.917], mean action: 1.592 [0.000, 3.000], mean observation: 0.108 [-1.279, 1.424], loss: 14.027300, mae: 50.072269, mean_q: 66.274437
 1041488/1100000: episode: 2343, duration: 1.708s, episode steps: 250, steps per second: 146, episode reward: 257.161, mean reward: 1.029 [-17.494, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.094 [-1.223, 1.414], loss: 8.470701, mae: 50.008408, mean_q: 66.106743
 1041639/1100000: episode: 2344, duration: 1.025s, episode steps: 151, steps per second: 147, episode reward: 21.539, mean reward: 0.143 [-100.000, 15.846], mean action: 1.477 [0.000, 3.000], mean observation: 0.105 [-0.793, 1.385], loss: 9.100727, mae: 50.437973, mean_q: 66.985359
 1042507/1100000: episode: 2345, duration: 6.335s, episode steps: 868, steps per second: 137, episode reward: 201.777, mean reward: 0.232 [-20.127, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.241 [-0.676, 1.400], loss: 11.371400, mae: 50.413540, mean_q: 67.102905
 1042629/1100000: episode: 2346, duration: 0.819s, episode steps: 122, steps per second: 149, episode reward: -78.571, mean reward: -0.644 [-100.000, 6.900], mean action: 1.615 [0.000, 3.000], mean observation: -0.058 [-1.247, 1.481], loss: 7.086728, mae: 50.342014, mean_q: 66.797356
 1043018/1100000: episode: 2347, duration: 2.707s, episode steps: 389, steps per second: 144, episode reward: 185.339, mean reward: 0.476 [-9.542, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.199 [-0.600, 1.410], loss: 6.752126, mae: 49.907398, mean_q: 66.549187
 1043143/1100000: episode: 2348, duration: 0.842s, episode steps: 125, steps per second: 148, episode reward: -41.488, mean reward: -0.332 [-100.000, 11.426], mean action: 1.728 [0.000, 3.000], mean observation: -0.069 [-0.976, 1.436], loss: 7.839265, mae: 49.691059, mean_q: 65.899719
 1044143/1100000: episode: 2349, duration: 7.353s, episode steps: 1000, steps per second: 136, episode reward: 128.322, mean reward: 0.128 [-19.666, 23.119], mean action: 0.782 [0.000, 3.000], mean observation: 0.284 [-0.686, 1.436], loss: 8.663644, mae: 49.795380, mean_q: 66.146782
 1044484/1100000: episode: 2350, duration: 2.437s, episode steps: 341, steps per second: 140, episode reward: 221.212, mean reward: 0.649 [-17.406, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.079 [-0.580, 1.390], loss: 10.146875, mae: 49.191360, mean_q: 65.259911
 1044939/1100000: episode: 2351, duration: 3.347s, episode steps: 455, steps per second: 136, episode reward: 193.940, mean reward: 0.426 [-14.695, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.084 [-0.749, 1.397], loss: 7.302357, mae: 49.078899, mean_q: 65.223007
 1045939/1100000: episode: 2352, duration: 7.270s, episode steps: 1000, steps per second: 138, episode reward: -71.094, mean reward: -0.071 [-13.927, 11.238], mean action: 1.720 [0.000, 3.000], mean observation: 0.003 [-0.600, 1.414], loss: 9.440444, mae: 48.760117, mean_q: 64.775787
 1046478/1100000: episode: 2353, duration: 3.905s, episode steps: 539, steps per second: 138, episode reward: 275.913, mean reward: 0.512 [-18.061, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.059 [-1.005, 1.455], loss: 8.273023, mae: 48.264606, mean_q: 64.319359
 1046853/1100000: episode: 2354, duration: 2.583s, episode steps: 375, steps per second: 145, episode reward: 224.477, mean reward: 0.599 [-9.694, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.146 [-0.921, 1.396], loss: 5.442985, mae: 48.463024, mean_q: 64.626823
 1047485/1100000: episode: 2355, duration: 4.478s, episode steps: 632, steps per second: 141, episode reward: 266.259, mean reward: 0.421 [-17.694, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.101 [-0.570, 1.459], loss: 7.191430, mae: 48.146656, mean_q: 64.202431
 1047815/1100000: episode: 2356, duration: 2.297s, episode steps: 330, steps per second: 144, episode reward: 215.981, mean reward: 0.654 [-18.135, 100.000], mean action: 2.097 [0.000, 3.000], mean observation: 0.046 [-0.600, 1.442], loss: 7.743254, mae: 47.733921, mean_q: 63.610718
 1048196/1100000: episode: 2357, duration: 2.684s, episode steps: 381, steps per second: 142, episode reward: 261.765, mean reward: 0.687 [-9.529, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.203 [-0.731, 1.522], loss: 9.437216, mae: 48.393620, mean_q: 64.103371
 1049007/1100000: episode: 2358, duration: 6.427s, episode steps: 811, steps per second: 126, episode reward: 169.114, mean reward: 0.209 [-9.622, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.017 [-0.872, 1.482], loss: 7.288606, mae: 48.640610, mean_q: 64.858551
 1049671/1100000: episode: 2359, duration: 4.772s, episode steps: 664, steps per second: 139, episode reward: 202.058, mean reward: 0.304 [-19.675, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: 0.237 [-0.732, 1.393], loss: 8.156011, mae: 48.876896, mean_q: 65.283653
 1050017/1100000: episode: 2360, duration: 2.410s, episode steps: 346, steps per second: 144, episode reward: 217.337, mean reward: 0.628 [-19.785, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.104 [-0.536, 1.447], loss: 8.605561, mae: 48.482914, mean_q: 64.717369
 1051017/1100000: episode: 2361, duration: 7.502s, episode steps: 1000, steps per second: 133, episode reward: 124.271, mean reward: 0.124 [-19.683, 22.218], mean action: 1.399 [0.000, 3.000], mean observation: 0.269 [-0.576, 1.440], loss: 5.850147, mae: 48.279640, mean_q: 64.420464
 1051543/1100000: episode: 2362, duration: 3.641s, episode steps: 526, steps per second: 144, episode reward: -216.962, mean reward: -0.412 [-100.000, 6.578], mean action: 1.529 [0.000, 3.000], mean observation: 0.067 [-0.922, 2.656], loss: 7.929444, mae: 48.436989, mean_q: 64.564980
 1052178/1100000: episode: 2363, duration: 4.656s, episode steps: 635, steps per second: 136, episode reward: 228.508, mean reward: 0.360 [-10.316, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.021 [-0.642, 1.522], loss: 5.951606, mae: 48.146839, mean_q: 63.962315
 1052409/1100000: episode: 2364, duration: 1.585s, episode steps: 231, steps per second: 146, episode reward: 220.718, mean reward: 0.955 [-9.663, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.145 [-0.703, 1.395], loss: 5.106265, mae: 47.841206, mean_q: 63.725445
 1052743/1100000: episode: 2365, duration: 2.356s, episode steps: 334, steps per second: 142, episode reward: -182.366, mean reward: -0.546 [-100.000, 17.950], mean action: 1.689 [0.000, 3.000], mean observation: 0.091 [-0.816, 1.401], loss: 5.446852, mae: 48.100529, mean_q: 64.071281
 1053573/1100000: episode: 2366, duration: 5.882s, episode steps: 830, steps per second: 141, episode reward: 255.566, mean reward: 0.308 [-20.956, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.174 [-0.908, 1.388], loss: 6.091758, mae: 48.037857, mean_q: 63.878510
 1053901/1100000: episode: 2367, duration: 2.243s, episode steps: 328, steps per second: 146, episode reward: 237.471, mean reward: 0.724 [-14.755, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.173 [-0.850, 1.509], loss: 6.466100, mae: 47.543118, mean_q: 63.726562
 1054901/1100000: episode: 2368, duration: 7.845s, episode steps: 1000, steps per second: 127, episode reward: -82.841, mean reward: -0.083 [-4.817, 4.745], mean action: 1.866 [0.000, 3.000], mean observation: -0.022 [-0.600, 1.397], loss: 9.492826, mae: 47.700119, mean_q: 63.499798
 1055234/1100000: episode: 2369, duration: 2.303s, episode steps: 333, steps per second: 145, episode reward: 247.580, mean reward: 0.743 [-13.494, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.093 [-0.701, 1.421], loss: 6.835012, mae: 47.411217, mean_q: 63.248077
 1055559/1100000: episode: 2370, duration: 2.203s, episode steps: 325, steps per second: 148, episode reward: 238.821, mean reward: 0.735 [-13.955, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.166 [-0.590, 1.436], loss: 7.580193, mae: 47.355591, mean_q: 62.848190
 1056044/1100000: episode: 2371, duration: 3.458s, episode steps: 485, steps per second: 140, episode reward: -116.243, mean reward: -0.240 [-100.000, 4.213], mean action: 1.887 [0.000, 3.000], mean observation: 0.084 [-0.600, 1.506], loss: 6.021376, mae: 47.038200, mean_q: 62.566929
 1056367/1100000: episode: 2372, duration: 2.233s, episode steps: 323, steps per second: 145, episode reward: 243.313, mean reward: 0.753 [-17.408, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.176 [-0.619, 1.488], loss: 7.907763, mae: 46.865730, mean_q: 62.421196
 1056725/1100000: episode: 2373, duration: 2.533s, episode steps: 358, steps per second: 141, episode reward: 194.724, mean reward: 0.544 [-13.187, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.120 [-0.838, 1.407], loss: 6.467956, mae: 47.142818, mean_q: 62.241909
 1057716/1100000: episode: 2374, duration: 7.396s, episode steps: 991, steps per second: 134, episode reward: -235.971, mean reward: -0.238 [-100.000, 4.689], mean action: 1.806 [0.000, 3.000], mean observation: 0.032 [-0.717, 1.494], loss: 7.610148, mae: 46.648991, mean_q: 61.938404
 1058065/1100000: episode: 2375, duration: 2.420s, episode steps: 349, steps per second: 144, episode reward: 260.559, mean reward: 0.747 [-17.881, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.167 [-0.634, 1.465], loss: 6.895134, mae: 46.498383, mean_q: 61.680981
 1058993/1100000: episode: 2376, duration: 7.249s, episode steps: 928, steps per second: 128, episode reward: 81.030, mean reward: 0.087 [-12.730, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: 0.107 [-0.896, 1.401], loss: 6.750104, mae: 46.216602, mean_q: 61.244370
 1059401/1100000: episode: 2377, duration: 2.838s, episode steps: 408, steps per second: 144, episode reward: 185.202, mean reward: 0.454 [-9.344, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.178 [-0.588, 1.396], loss: 6.120955, mae: 46.212070, mean_q: 61.316467
 1059716/1100000: episode: 2378, duration: 2.207s, episode steps: 315, steps per second: 143, episode reward: 281.846, mean reward: 0.895 [-17.822, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.084 [-0.801, 1.407], loss: 6.828956, mae: 46.100216, mean_q: 61.384468
 1060716/1100000: episode: 2379, duration: 7.772s, episode steps: 1000, steps per second: 129, episode reward: -31.091, mean reward: -0.031 [-5.862, 4.813], mean action: 1.855 [0.000, 3.000], mean observation: 0.000 [-0.847, 1.416], loss: 7.062696, mae: 46.337456, mean_q: 61.248268
 1060996/1100000: episode: 2380, duration: 1.928s, episode steps: 280, steps per second: 145, episode reward: 266.862, mean reward: 0.953 [-10.040, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.099 [-1.123, 1.390], loss: 5.482681, mae: 45.723152, mean_q: 60.500210
 1061251/1100000: episode: 2381, duration: 1.721s, episode steps: 255, steps per second: 148, episode reward: 289.111, mean reward: 1.134 [-9.385, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.041 [-0.623, 1.401], loss: 6.037798, mae: 45.819576, mean_q: 60.574841
 1061974/1100000: episode: 2382, duration: 5.229s, episode steps: 723, steps per second: 138, episode reward: 129.192, mean reward: 0.179 [-17.888, 100.000], mean action: 1.621 [0.000, 3.000], mean observation: -0.006 [-0.717, 1.456], loss: 6.573777, mae: 45.999893, mean_q: 60.865219
 1062974/1100000: episode: 2383, duration: 7.278s, episode steps: 1000, steps per second: 137, episode reward: -31.690, mean reward: -0.032 [-17.721, 24.585], mean action: 1.619 [0.000, 3.000], mean observation: -0.009 [-0.867, 1.414], loss: 6.081706, mae: 45.669384, mean_q: 60.502201
 1063518/1100000: episode: 2384, duration: 3.780s, episode steps: 544, steps per second: 144, episode reward: 241.089, mean reward: 0.443 [-20.763, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.035 [-0.661, 1.408], loss: 6.219578, mae: 45.451061, mean_q: 60.317513
 1064069/1100000: episode: 2385, duration: 3.944s, episode steps: 551, steps per second: 140, episode reward: 220.544, mean reward: 0.400 [-10.199, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.009 [-0.734, 1.405], loss: 6.528543, mae: 45.264740, mean_q: 59.866470
 1064621/1100000: episode: 2386, duration: 3.957s, episode steps: 552, steps per second: 139, episode reward: 260.044, mean reward: 0.471 [-18.629, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.234 [-0.706, 1.393], loss: 6.038188, mae: 45.249634, mean_q: 59.595688
 1064901/1100000: episode: 2387, duration: 1.907s, episode steps: 280, steps per second: 147, episode reward: 247.632, mean reward: 0.884 [-19.060, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.227 [-0.732, 1.397], loss: 6.150086, mae: 44.923851, mean_q: 59.198528
 1065168/1100000: episode: 2388, duration: 1.801s, episode steps: 267, steps per second: 148, episode reward: 283.571, mean reward: 1.062 [-18.405, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.067 [-0.988, 1.526], loss: 4.745022, mae: 44.948082, mean_q: 59.371834
 1065474/1100000: episode: 2389, duration: 2.087s, episode steps: 306, steps per second: 147, episode reward: 212.515, mean reward: 0.694 [-4.237, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.217 [-0.658, 1.465], loss: 8.590463, mae: 45.105625, mean_q: 59.673546
 1066050/1100000: episode: 2390, duration: 4.070s, episode steps: 576, steps per second: 142, episode reward: 276.455, mean reward: 0.480 [-17.816, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.016 [-1.181, 1.441], loss: 5.966024, mae: 45.225674, mean_q: 59.713039
 1066520/1100000: episode: 2391, duration: 3.268s, episode steps: 470, steps per second: 144, episode reward: 221.945, mean reward: 0.472 [-11.836, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.171 [-0.628, 1.515], loss: 6.803824, mae: 45.087746, mean_q: 59.569866
 1066960/1100000: episode: 2392, duration: 3.091s, episode steps: 440, steps per second: 142, episode reward: 250.763, mean reward: 0.570 [-19.370, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.141 [-0.921, 1.405], loss: 6.750574, mae: 44.860252, mean_q: 59.443226
 1067397/1100000: episode: 2393, duration: 3.120s, episode steps: 437, steps per second: 140, episode reward: 268.391, mean reward: 0.614 [-19.550, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.124 [-0.657, 1.395], loss: 5.754456, mae: 45.328793, mean_q: 60.151844
 1067726/1100000: episode: 2394, duration: 2.302s, episode steps: 329, steps per second: 143, episode reward: 233.434, mean reward: 0.710 [-19.145, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.145 [-0.751, 1.388], loss: 6.399981, mae: 45.282257, mean_q: 60.054924
 1068571/1100000: episode: 2395, duration: 6.154s, episode steps: 845, steps per second: 137, episode reward: 217.141, mean reward: 0.257 [-18.055, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.037 [-1.127, 1.520], loss: 5.716514, mae: 45.155888, mean_q: 59.754086
 1069253/1100000: episode: 2396, duration: 5.029s, episode steps: 682, steps per second: 136, episode reward: 258.358, mean reward: 0.379 [-20.374, 100.000], mean action: 0.823 [0.000, 3.000], mean observation: 0.106 [-0.715, 1.449], loss: 5.695832, mae: 45.448799, mean_q: 60.456470
 1069590/1100000: episode: 2397, duration: 2.358s, episode steps: 337, steps per second: 143, episode reward: 206.643, mean reward: 0.613 [-17.669, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: -0.037 [-0.663, 1.402], loss: 5.165442, mae: 45.335918, mean_q: 60.064373
 1069981/1100000: episode: 2398, duration: 2.716s, episode steps: 391, steps per second: 144, episode reward: 144.175, mean reward: 0.369 [-21.930, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.120 [-0.811, 1.504], loss: 4.636800, mae: 45.160839, mean_q: 60.097515
 1070946/1100000: episode: 2399, duration: 7.220s, episode steps: 965, steps per second: 134, episode reward: -556.374, mean reward: -0.577 [-100.000, 51.709], mean action: 1.588 [0.000, 3.000], mean observation: 0.037 [-3.384, 2.046], loss: 6.662713, mae: 45.192093, mean_q: 59.931427
 1071334/1100000: episode: 2400, duration: 2.790s, episode steps: 388, steps per second: 139, episode reward: 240.966, mean reward: 0.621 [-11.604, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.146 [-0.908, 1.455], loss: 5.857112, mae: 45.096870, mean_q: 59.642746
 1072334/1100000: episode: 2401, duration: 7.102s, episode steps: 1000, steps per second: 141, episode reward: 51.400, mean reward: 0.051 [-20.823, 21.872], mean action: 0.819 [0.000, 3.000], mean observation: 0.169 [-0.956, 1.417], loss: 6.489077, mae: 45.410187, mean_q: 60.091766
 1072608/1100000: episode: 2402, duration: 1.870s, episode steps: 274, steps per second: 147, episode reward: 265.284, mean reward: 0.968 [-19.184, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.043 [-0.716, 1.451], loss: 4.867635, mae: 45.021393, mean_q: 59.824417
 1072847/1100000: episode: 2403, duration: 1.622s, episode steps: 239, steps per second: 147, episode reward: 269.550, mean reward: 1.128 [-17.346, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.069 [-0.814, 1.407], loss: 6.429008, mae: 44.688198, mean_q: 59.039307
 1073478/1100000: episode: 2404, duration: 4.671s, episode steps: 631, steps per second: 135, episode reward: 219.693, mean reward: 0.348 [-18.576, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: -0.003 [-0.765, 1.395], loss: 4.977259, mae: 45.022594, mean_q: 59.614334
 1074173/1100000: episode: 2405, duration: 5.169s, episode steps: 695, steps per second: 134, episode reward: -518.108, mean reward: -0.745 [-100.000, 20.771], mean action: 1.626 [0.000, 3.000], mean observation: 0.063 [-1.338, 3.794], loss: 5.707163, mae: 44.823360, mean_q: 59.404240
 1074413/1100000: episode: 2406, duration: 1.608s, episode steps: 240, steps per second: 149, episode reward: -6.351, mean reward: -0.026 [-100.000, 10.936], mean action: 1.538 [0.000, 3.000], mean observation: 0.154 [-0.728, 1.507], loss: 4.502573, mae: 44.684483, mean_q: 59.099365
 1074926/1100000: episode: 2407, duration: 3.800s, episode steps: 513, steps per second: 135, episode reward: 189.716, mean reward: 0.370 [-19.636, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: -0.048 [-0.600, 1.407], loss: 9.429564, mae: 44.736713, mean_q: 59.180775
 1075148/1100000: episode: 2408, duration: 1.511s, episode steps: 222, steps per second: 147, episode reward: 283.233, mean reward: 1.276 [-2.746, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.073 [-0.784, 1.386], loss: 4.999343, mae: 45.172253, mean_q: 60.091282
 1076148/1100000: episode: 2409, duration: 7.539s, episode steps: 1000, steps per second: 133, episode reward: 77.568, mean reward: 0.078 [-17.768, 18.902], mean action: 1.386 [0.000, 3.000], mean observation: 0.017 [-0.890, 1.431], loss: 7.198022, mae: 44.361515, mean_q: 58.534180
 1076573/1100000: episode: 2410, duration: 3.041s, episode steps: 425, steps per second: 140, episode reward: 197.539, mean reward: 0.465 [-20.197, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: -0.074 [-0.673, 1.410], loss: 5.341358, mae: 44.427616, mean_q: 58.883759
 1077014/1100000: episode: 2411, duration: 3.209s, episode steps: 441, steps per second: 137, episode reward: 220.126, mean reward: 0.499 [-13.750, 100.000], mean action: 1.565 [0.000, 3.000], mean observation: 0.143 [-0.748, 1.622], loss: 5.127807, mae: 44.631840, mean_q: 58.819359
 1077247/1100000: episode: 2412, duration: 1.588s, episode steps: 233, steps per second: 147, episode reward: 243.488, mean reward: 1.045 [-19.279, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.055 [-0.707, 1.405], loss: 6.374682, mae: 44.759029, mean_q: 58.932285
 1077450/1100000: episode: 2413, duration: 1.380s, episode steps: 203, steps per second: 147, episode reward: 20.492, mean reward: 0.101 [-100.000, 13.183], mean action: 1.690 [0.000, 3.000], mean observation: 0.138 [-0.620, 1.402], loss: 5.952521, mae: 44.594578, mean_q: 58.747940
 1077675/1100000: episode: 2414, duration: 1.518s, episode steps: 225, steps per second: 148, episode reward: 268.655, mean reward: 1.194 [-9.441, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.093 [-0.655, 1.391], loss: 7.366210, mae: 44.816189, mean_q: 59.032482
 1077939/1100000: episode: 2415, duration: 1.810s, episode steps: 264, steps per second: 146, episode reward: 266.352, mean reward: 1.009 [-2.619, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.091 [-0.825, 1.389], loss: 7.162631, mae: 44.884022, mean_q: 58.946766
 1078157/1100000: episode: 2416, duration: 1.484s, episode steps: 218, steps per second: 147, episode reward: 281.314, mean reward: 1.290 [-5.509, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.085 [-0.858, 1.387], loss: 6.560685, mae: 45.182621, mean_q: 59.441845
 1078473/1100000: episode: 2417, duration: 2.173s, episode steps: 316, steps per second: 145, episode reward: 274.153, mean reward: 0.868 [-3.067, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.124 [-0.671, 1.392], loss: 9.708405, mae: 44.920483, mean_q: 58.905296
 1079473/1100000: episode: 2418, duration: 7.230s, episode steps: 1000, steps per second: 138, episode reward: -33.487, mean reward: -0.033 [-13.309, 14.367], mean action: 1.630 [0.000, 3.000], mean observation: -0.024 [-0.636, 1.432], loss: 5.548868, mae: 44.718929, mean_q: 58.843433
 1079706/1100000: episode: 2419, duration: 1.577s, episode steps: 233, steps per second: 148, episode reward: 280.964, mean reward: 1.206 [-11.883, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.177 [-0.681, 1.394], loss: 3.649803, mae: 45.294846, mean_q: 59.520077
 1080194/1100000: episode: 2420, duration: 3.478s, episode steps: 488, steps per second: 140, episode reward: 206.484, mean reward: 0.423 [-14.136, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: -0.025 [-0.600, 1.489], loss: 7.807021, mae: 44.985271, mean_q: 59.228767
 1080536/1100000: episode: 2421, duration: 2.366s, episode steps: 342, steps per second: 145, episode reward: 301.051, mean reward: 0.880 [-11.682, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.117 [-0.695, 1.730], loss: 7.826268, mae: 45.226517, mean_q: 59.810268
 1080891/1100000: episode: 2422, duration: 2.417s, episode steps: 355, steps per second: 147, episode reward: 251.360, mean reward: 0.708 [-11.727, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.119 [-0.835, 1.448], loss: 4.431268, mae: 44.827183, mean_q: 59.211002
 1081312/1100000: episode: 2423, duration: 2.963s, episode steps: 421, steps per second: 142, episode reward: 269.773, mean reward: 0.641 [-22.054, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.152 [-0.651, 1.431], loss: 4.679802, mae: 44.972824, mean_q: 59.329914
 1081824/1100000: episode: 2424, duration: 3.615s, episode steps: 512, steps per second: 142, episode reward: 206.549, mean reward: 0.403 [-12.538, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: -0.017 [-0.635, 1.404], loss: 5.902698, mae: 44.618622, mean_q: 59.036457
 1082114/1100000: episode: 2425, duration: 1.997s, episode steps: 290, steps per second: 145, episode reward: 257.232, mean reward: 0.887 [-2.914, 100.000], mean action: 1.893 [0.000, 3.000], mean observation: 0.231 [-0.717, 1.389], loss: 7.068993, mae: 44.810009, mean_q: 59.491661
 1082700/1100000: episode: 2426, duration: 4.228s, episode steps: 586, steps per second: 139, episode reward: 263.361, mean reward: 0.449 [-19.478, 100.000], mean action: 0.684 [0.000, 3.000], mean observation: 0.203 [-0.732, 1.387], loss: 5.444233, mae: 45.195282, mean_q: 59.979401
 1082805/1100000: episode: 2427, duration: 0.694s, episode steps: 105, steps per second: 151, episode reward: -208.382, mean reward: -1.985 [-100.000, 34.908], mean action: 0.981 [0.000, 3.000], mean observation: -0.190 [-4.428, 1.481], loss: 8.278206, mae: 45.502094, mean_q: 60.235916
 1083094/1100000: episode: 2428, duration: 1.963s, episode steps: 289, steps per second: 147, episode reward: 244.141, mean reward: 0.845 [-9.616, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.177 [-0.607, 1.390], loss: 6.279027, mae: 45.499142, mean_q: 60.236496
 1083705/1100000: episode: 2429, duration: 4.239s, episode steps: 611, steps per second: 144, episode reward: 257.009, mean reward: 0.421 [-17.810, 100.000], mean action: 0.591 [0.000, 3.000], mean observation: 0.260 [-0.893, 1.458], loss: 8.542139, mae: 45.695461, mean_q: 60.599899
 1084089/1100000: episode: 2430, duration: 2.616s, episode steps: 384, steps per second: 147, episode reward: 287.524, mean reward: 0.749 [-17.963, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.076 [-0.678, 1.438], loss: 10.348450, mae: 45.572338, mean_q: 60.212887
 1084438/1100000: episode: 2431, duration: 2.414s, episode steps: 349, steps per second: 145, episode reward: 257.137, mean reward: 0.737 [-17.741, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: -0.045 [-0.635, 1.444], loss: 9.803035, mae: 46.145229, mean_q: 60.941757
 1084691/1100000: episode: 2432, duration: 1.744s, episode steps: 253, steps per second: 145, episode reward: 294.427, mean reward: 1.164 [-9.476, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.119 [-0.814, 1.470], loss: 5.050779, mae: 45.458252, mean_q: 60.403267
 1085008/1100000: episode: 2433, duration: 2.232s, episode steps: 317, steps per second: 142, episode reward: 276.455, mean reward: 0.872 [-11.192, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.130 [-0.748, 1.392], loss: 5.231773, mae: 46.153122, mean_q: 60.921726
 1085230/1100000: episode: 2434, duration: 1.504s, episode steps: 222, steps per second: 148, episode reward: 263.864, mean reward: 1.189 [-8.869, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.027 [-0.659, 1.409], loss: 5.776669, mae: 45.549152, mean_q: 60.119755
 1085806/1100000: episode: 2435, duration: 4.223s, episode steps: 576, steps per second: 136, episode reward: 178.497, mean reward: 0.310 [-17.937, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: -0.028 [-0.600, 1.466], loss: 4.922697, mae: 46.190475, mean_q: 61.176861
 1086070/1100000: episode: 2436, duration: 1.879s, episode steps: 264, steps per second: 141, episode reward: 210.871, mean reward: 0.799 [-9.701, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.209 [-0.709, 1.422], loss: 6.027349, mae: 45.886944, mean_q: 60.728073
 1086604/1100000: episode: 2437, duration: 3.744s, episode steps: 534, steps per second: 143, episode reward: 251.442, mean reward: 0.471 [-19.144, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.206 [-0.743, 1.431], loss: 6.742289, mae: 46.298973, mean_q: 61.296558
 1087018/1100000: episode: 2438, duration: 2.927s, episode steps: 414, steps per second: 141, episode reward: 204.653, mean reward: 0.494 [-17.918, 100.000], mean action: 2.140 [0.000, 3.000], mean observation: 0.026 [-0.600, 1.420], loss: 5.228214, mae: 46.318565, mean_q: 61.281590
 1087439/1100000: episode: 2439, duration: 2.959s, episode steps: 421, steps per second: 142, episode reward: 218.362, mean reward: 0.519 [-18.049, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: -0.003 [-0.600, 1.465], loss: 7.068555, mae: 46.516003, mean_q: 61.362751
 1088156/1100000: episode: 2440, duration: 5.346s, episode steps: 717, steps per second: 134, episode reward: 206.620, mean reward: 0.288 [-22.523, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.017 [-0.600, 1.456], loss: 6.974581, mae: 46.750217, mean_q: 61.826664
 1088437/1100000: episode: 2441, duration: 1.911s, episode steps: 281, steps per second: 147, episode reward: 263.649, mean reward: 0.938 [-2.878, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.074 [-0.624, 1.408], loss: 5.682944, mae: 46.711159, mean_q: 61.282654
 1089227/1100000: episode: 2442, duration: 5.772s, episode steps: 790, steps per second: 137, episode reward: 176.757, mean reward: 0.224 [-20.906, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.149 [-0.910, 1.397], loss: 6.713364, mae: 47.209179, mean_q: 62.310928
 1089585/1100000: episode: 2443, duration: 2.454s, episode steps: 358, steps per second: 146, episode reward: 304.108, mean reward: 0.849 [-19.788, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.131 [-0.778, 1.397], loss: 5.698346, mae: 47.236317, mean_q: 62.278652
 1089897/1100000: episode: 2444, duration: 2.144s, episode steps: 312, steps per second: 146, episode reward: 280.731, mean reward: 0.900 [-11.473, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.051 [-0.604, 1.495], loss: 4.956769, mae: 47.235046, mean_q: 62.217434
 1090265/1100000: episode: 2445, duration: 2.550s, episode steps: 368, steps per second: 144, episode reward: 263.051, mean reward: 0.715 [-11.220, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.123 [-0.787, 1.393], loss: 4.868985, mae: 47.193195, mean_q: 62.558380
 1090664/1100000: episode: 2446, duration: 2.844s, episode steps: 399, steps per second: 140, episode reward: 251.044, mean reward: 0.629 [-17.161, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: -0.001 [-0.604, 1.393], loss: 4.872666, mae: 47.273815, mean_q: 62.664997
 1091130/1100000: episode: 2447, duration: 3.296s, episode steps: 466, steps per second: 141, episode reward: 250.117, mean reward: 0.537 [-18.496, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.150 [-0.693, 1.401], loss: 6.626998, mae: 47.216518, mean_q: 62.187973
 1091450/1100000: episode: 2448, duration: 2.236s, episode steps: 320, steps per second: 143, episode reward: 236.047, mean reward: 0.738 [-10.453, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: -0.042 [-0.600, 1.386], loss: 6.505840, mae: 47.131081, mean_q: 62.108196
 1091922/1100000: episode: 2449, duration: 3.263s, episode steps: 472, steps per second: 145, episode reward: 221.158, mean reward: 0.469 [-17.381, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.143 [-0.583, 1.409], loss: 4.483158, mae: 47.244720, mean_q: 62.505470
 1092922/1100000: episode: 2450, duration: 7.252s, episode steps: 1000, steps per second: 138, episode reward: 116.921, mean reward: 0.117 [-19.419, 22.650], mean action: 2.504 [0.000, 3.000], mean observation: 0.134 [-0.600, 1.393], loss: 5.487239, mae: 47.189529, mean_q: 62.362190
 1093386/1100000: episode: 2451, duration: 3.216s, episode steps: 464, steps per second: 144, episode reward: 199.164, mean reward: 0.429 [-20.261, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.136 [-0.629, 1.393], loss: 7.328693, mae: 46.495174, mean_q: 61.539635
 1093704/1100000: episode: 2452, duration: 2.181s, episode steps: 318, steps per second: 146, episode reward: 289.737, mean reward: 0.911 [-19.551, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.124 [-1.369, 1.459], loss: 5.046782, mae: 45.976448, mean_q: 60.914856
 1094023/1100000: episode: 2453, duration: 2.302s, episode steps: 319, steps per second: 139, episode reward: 215.462, mean reward: 0.675 [-9.374, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.103 [-0.729, 1.402], loss: 6.134163, mae: 45.857376, mean_q: 60.928192
 1094430/1100000: episode: 2454, duration: 2.920s, episode steps: 407, steps per second: 139, episode reward: 201.967, mean reward: 0.496 [-23.446, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: -0.022 [-0.600, 1.404], loss: 5.835091, mae: 46.488770, mean_q: 61.697510
 1095114/1100000: episode: 2455, duration: 4.826s, episode steps: 684, steps per second: 142, episode reward: 246.983, mean reward: 0.361 [-24.774, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.237 [-0.734, 1.386], loss: 4.680713, mae: 45.930607, mean_q: 60.858269
 1095440/1100000: episode: 2456, duration: 2.251s, episode steps: 326, steps per second: 145, episode reward: 264.711, mean reward: 0.812 [-9.399, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.216 [-0.772, 1.481], loss: 5.015730, mae: 45.732037, mean_q: 60.731831
 1095710/1100000: episode: 2457, duration: 1.829s, episode steps: 270, steps per second: 148, episode reward: 233.148, mean reward: 0.864 [-18.127, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.227 [-0.749, 1.417], loss: 6.006050, mae: 46.544483, mean_q: 61.866196
 1095961/1100000: episode: 2458, duration: 1.686s, episode steps: 251, steps per second: 149, episode reward: 53.731, mean reward: 0.214 [-100.000, 15.071], mean action: 1.187 [0.000, 3.000], mean observation: 0.174 [-0.697, 1.456], loss: 4.682305, mae: 46.202747, mean_q: 61.593834
 1096188/1100000: episode: 2459, duration: 1.538s, episode steps: 227, steps per second: 148, episode reward: 251.353, mean reward: 1.107 [-10.764, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.054 [-0.764, 1.406], loss: 4.102380, mae: 45.879292, mean_q: 61.051205
 1096473/1100000: episode: 2460, duration: 1.951s, episode steps: 285, steps per second: 146, episode reward: 289.507, mean reward: 1.016 [-17.442, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.072 [-0.931, 1.512], loss: 4.138930, mae: 46.339699, mean_q: 61.538364
 1096962/1100000: episode: 2461, duration: 3.407s, episode steps: 489, steps per second: 144, episode reward: 232.708, mean reward: 0.476 [-10.545, 100.000], mean action: 1.935 [0.000, 3.000], mean observation: 0.148 [-0.726, 1.413], loss: 4.453833, mae: 46.576038, mean_q: 62.022472
 1097421/1100000: episode: 2462, duration: 3.122s, episode steps: 459, steps per second: 147, episode reward: 285.335, mean reward: 0.622 [-19.158, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.144 [-0.893, 1.420], loss: 5.851255, mae: 46.453690, mean_q: 61.901806
 1098180/1100000: episode: 2463, duration: 5.697s, episode steps: 759, steps per second: 133, episode reward: 192.496, mean reward: 0.254 [-20.564, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.016 [-0.753, 1.554], loss: 6.018899, mae: 46.134232, mean_q: 61.162407
 1098921/1100000: episode: 2464, duration: 5.632s, episode steps: 741, steps per second: 132, episode reward: 208.109, mean reward: 0.281 [-20.500, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: -0.018 [-0.744, 1.487], loss: 5.649771, mae: 46.228111, mean_q: 61.446117
 1099546/1100000: episode: 2465, duration: 4.653s, episode steps: 625, steps per second: 134, episode reward: 164.841, mean reward: 0.264 [-26.258, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: -0.027 [-0.680, 1.391], loss: 5.142578, mae: 45.985909, mean_q: 61.266338
 1099840/1100000: episode: 2466, duration: 2.054s, episode steps: 294, steps per second: 143, episode reward: 223.883, mean reward: 0.762 [-7.840, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: -0.084 [-0.693, 1.400], loss: 4.138505, mae: 45.937866, mean_q: 61.188610
done, took 7899.246 seconds
Testing for 1000 episodes ...
Episode 1: reward: 217.032, steps: 467
Episode 2: reward: -63.727, steps: 227
Episode 3: reward: 293.234, steps: 242
Episode 4: reward: -74.009, steps: 1000
Episode 5: reward: 266.232, steps: 217
Episode 6: reward: 229.700, steps: 402
Episode 7: reward: 229.439, steps: 252
Episode 8: reward: -104.756, steps: 305
Episode 9: reward: -361.641, steps: 316
Episode 10: reward: 218.350, steps: 222
Episode 11: reward: -11.868, steps: 240
Episode 12: reward: 44.931, steps: 336
Episode 13: reward: -258.771, steps: 396
Episode 14: reward: 191.215, steps: 602
Episode 15: reward: 223.388, steps: 221
Episode 16: reward: -27.876, steps: 1000
Episode 17: reward: 232.573, steps: 203
Episode 18: reward: 220.404, steps: 234
Episode 19: reward: 218.151, steps: 272
Episode 20: reward: -203.630, steps: 732
Episode 21: reward: 147.398, steps: 421
Episode 22: reward: 272.775, steps: 245
Episode 23: reward: 161.005, steps: 798
Episode 24: reward: 264.575, steps: 210
Episode 25: reward: 245.688, steps: 212
Episode 26: reward: 233.268, steps: 219
Episode 27: reward: -460.738, steps: 324
Episode 28: reward: 230.441, steps: 258
Episode 29: reward: 192.369, steps: 391
Episode 30: reward: -284.972, steps: 245
Episode 31: reward: 274.586, steps: 231
Episode 32: reward: 212.524, steps: 732
Episode 33: reward: 260.917, steps: 206
Episode 34: reward: -52.752, steps: 164
Episode 35: reward: 240.663, steps: 210
Episode 36: reward: -36.136, steps: 156
Episode 37: reward: 293.044, steps: 227
Episode 38: reward: -88.458, steps: 227
Episode 39: reward: 287.962, steps: 235
Episode 40: reward: 299.646, steps: 260
Episode 41: reward: 123.518, steps: 933
Episode 42: reward: 15.977, steps: 192
Episode 43: reward: 223.070, steps: 309
Episode 44: reward: 219.957, steps: 414
Episode 45: reward: 263.826, steps: 204
Episode 46: reward: 255.894, steps: 236
Episode 47: reward: -55.132, steps: 210
Episode 48: reward: 248.853, steps: 254
Episode 49: reward: -361.147, steps: 416
Episode 50: reward: 292.333, steps: 223
Episode 51: reward: 9.891, steps: 290
Episode 52: reward: 232.181, steps: 366
Episode 53: reward: -316.647, steps: 538
Episode 54: reward: 114.745, steps: 1000
Episode 55: reward: -17.515, steps: 194
Episode 56: reward: 224.380, steps: 218
Episode 57: reward: 224.952, steps: 259
Episode 58: reward: 284.227, steps: 221
Episode 59: reward: -511.273, steps: 347
Episode 60: reward: 226.348, steps: 227
Episode 61: reward: 214.980, steps: 411
Episode 62: reward: 275.794, steps: 238
Episode 63: reward: 193.222, steps: 668
Episode 64: reward: -49.596, steps: 214
Episode 65: reward: 231.549, steps: 361
Episode 66: reward: 259.118, steps: 230
Episode 67: reward: -57.987, steps: 169
Episode 68: reward: 265.068, steps: 199
Episode 69: reward: 260.975, steps: 244
Episode 70: reward: 267.403, steps: 266
Episode 71: reward: 112.560, steps: 1000
Episode 72: reward: -133.887, steps: 1000
Episode 73: reward: 285.181, steps: 200
Episode 74: reward: 275.149, steps: 264
Episode 75: reward: 142.473, steps: 760
Episode 76: reward: -90.987, steps: 261
Episode 77: reward: 217.320, steps: 349
Episode 78: reward: 238.455, steps: 245
Episode 79: reward: 231.436, steps: 384
Episode 80: reward: 236.786, steps: 224
Episode 81: reward: 256.274, steps: 258
Episode 82: reward: 287.097, steps: 288
Episode 83: reward: 183.172, steps: 300
Episode 84: reward: -360.619, steps: 273
Episode 85: reward: -410.331, steps: 651
Episode 86: reward: 266.897, steps: 214
Episode 87: reward: 246.112, steps: 256
Episode 88: reward: 290.645, steps: 225
Episode 89: reward: 216.342, steps: 386
Episode 90: reward: 239.492, steps: 338
Episode 91: reward: -266.015, steps: 242
Episode 92: reward: -309.810, steps: 451
Episode 93: reward: 201.899, steps: 359
Episode 94: reward: 216.268, steps: 358
Episode 95: reward: 255.411, steps: 186
Episode 96: reward: 231.022, steps: 235
Episode 97: reward: 299.045, steps: 215
Episode 98: reward: 214.387, steps: 482
Episode 99: reward: 270.300, steps: 237
Episode 100: reward: 148.411, steps: 536
Episode 101: reward: 262.918, steps: 337
Episode 102: reward: -262.981, steps: 382
Episode 103: reward: 223.373, steps: 272
Episode 104: reward: -119.355, steps: 946
Episode 105: reward: 180.191, steps: 482
Episode 106: reward: -139.405, steps: 165
Episode 107: reward: 202.670, steps: 352
Episode 108: reward: 8.468, steps: 210
Episode 109: reward: -30.349, steps: 152
Episode 110: reward: 16.587, steps: 464
Episode 111: reward: -131.546, steps: 400
Episode 112: reward: 230.957, steps: 228
Episode 113: reward: 239.667, steps: 216
Episode 114: reward: 184.345, steps: 540
Episode 115: reward: 257.354, steps: 232
Episode 116: reward: 236.387, steps: 220
Episode 117: reward: 201.861, steps: 273
Episode 118: reward: -163.458, steps: 141
Episode 119: reward: -67.363, steps: 201
Episode 120: reward: 191.166, steps: 383
Episode 121: reward: 175.752, steps: 504
Episode 122: reward: -118.855, steps: 595
Episode 123: reward: 149.938, steps: 529
Episode 124: reward: 178.551, steps: 359
Episode 125: reward: 168.979, steps: 464
Episode 126: reward: 307.335, steps: 256
Episode 127: reward: -175.856, steps: 255
Episode 128: reward: 219.729, steps: 465
Episode 129: reward: 270.606, steps: 243
Episode 130: reward: -311.564, steps: 701
Episode 131: reward: 219.367, steps: 432
Episode 132: reward: 240.204, steps: 233
Episode 133: reward: 251.638, steps: 216
Episode 134: reward: 247.362, steps: 684
Episode 135: reward: 208.206, steps: 451
Episode 136: reward: 243.414, steps: 214
Episode 137: reward: 266.408, steps: 243
Episode 138: reward: 173.441, steps: 378
Episode 139: reward: -155.912, steps: 772
Episode 140: reward: 273.167, steps: 230
Episode 141: reward: 189.036, steps: 400
Episode 142: reward: 28.759, steps: 253
Episode 143: reward: 265.852, steps: 248
Episode 144: reward: 128.998, steps: 813
Episode 145: reward: -160.177, steps: 527
Episode 146: reward: 266.390, steps: 359
Episode 147: reward: 229.521, steps: 370
Episode 148: reward: 249.285, steps: 222
Episode 149: reward: 262.750, steps: 613
Episode 150: reward: 201.740, steps: 322
Episode 151: reward: -46.517, steps: 210
Episode 152: reward: 201.488, steps: 533
Episode 153: reward: -49.693, steps: 172
Episode 154: reward: -36.576, steps: 187
Episode 155: reward: 214.029, steps: 392
Episode 156: reward: 115.808, steps: 816
Episode 157: reward: 185.979, steps: 389
Episode 158: reward: 225.189, steps: 325
Episode 159: reward: 195.378, steps: 402
Episode 160: reward: 246.189, steps: 238
Episode 161: reward: -143.705, steps: 355
Episode 162: reward: -12.848, steps: 118
Episode 163: reward: 272.502, steps: 225
Episode 164: reward: 216.103, steps: 369
Episode 165: reward: -62.841, steps: 200
Episode 166: reward: 265.354, steps: 412
Episode 167: reward: -341.129, steps: 610
Episode 168: reward: -40.387, steps: 387
Episode 169: reward: -61.230, steps: 199
Episode 170: reward: 256.891, steps: 237
Episode 171: reward: -298.819, steps: 497
Episode 172: reward: 201.843, steps: 299
Episode 173: reward: -84.448, steps: 1000
Episode 174: reward: 219.745, steps: 380
Episode 175: reward: 250.549, steps: 248
Episode 176: reward: 203.671, steps: 454
Episode 177: reward: 209.476, steps: 552
Episode 178: reward: 270.403, steps: 203
Episode 179: reward: 264.039, steps: 217
Episode 180: reward: -86.355, steps: 206
Episode 181: reward: -24.462, steps: 1000
Episode 182: reward: 202.245, steps: 256
Episode 183: reward: -60.274, steps: 181
Episode 184: reward: 248.962, steps: 456
Episode 185: reward: 105.996, steps: 968
Episode 186: reward: -345.643, steps: 287
Episode 187: reward: -162.405, steps: 919
Episode 188: reward: 253.835, steps: 200
Episode 189: reward: 232.025, steps: 244
Episode 190: reward: 185.148, steps: 601
Episode 191: reward: -41.990, steps: 190
Episode 192: reward: -49.486, steps: 298
Episode 193: reward: -156.543, steps: 748
Episode 194: reward: -98.275, steps: 192
Episode 195: reward: 253.950, steps: 215
Episode 196: reward: 265.568, steps: 284
Episode 197: reward: -78.898, steps: 186
Episode 198: reward: 233.060, steps: 238
Episode 199: reward: 215.691, steps: 426
Episode 200: reward: 269.399, steps: 324
Episode 201: reward: 186.153, steps: 408
Episode 202: reward: 265.679, steps: 224
Episode 203: reward: 262.847, steps: 247
Episode 204: reward: -251.628, steps: 289
Episode 205: reward: 240.519, steps: 201
Episode 206: reward: 261.262, steps: 203
Episode 207: reward: 186.419, steps: 317
Episode 208: reward: -124.148, steps: 455
Episode 209: reward: -59.325, steps: 196
Episode 210: reward: -165.578, steps: 520
Episode 211: reward: -242.739, steps: 395
Episode 212: reward: 184.214, steps: 721
Episode 213: reward: 233.702, steps: 250
Episode 214: reward: 273.219, steps: 208
Episode 215: reward: 206.883, steps: 333
Episode 216: reward: 211.631, steps: 383
Episode 217: reward: -242.746, steps: 442
Episode 218: reward: 253.303, steps: 210
Episode 219: reward: 223.818, steps: 652
Episode 220: reward: 211.105, steps: 225
Episode 221: reward: 218.880, steps: 202
Episode 222: reward: 212.971, steps: 264
Episode 223: reward: 281.328, steps: 236
Episode 224: reward: 292.757, steps: 249
Episode 225: reward: 214.662, steps: 673
Episode 226: reward: -478.249, steps: 474
Episode 227: reward: 253.006, steps: 237
Episode 228: reward: -48.469, steps: 182
Episode 229: reward: -56.457, steps: 180
Episode 230: reward: -360.560, steps: 827
Episode 231: reward: 271.030, steps: 196
Episode 232: reward: 275.881, steps: 251
Episode 233: reward: 259.481, steps: 270
Episode 234: reward: -180.272, steps: 346
Episode 235: reward: 2.959, steps: 193
Episode 236: reward: 244.307, steps: 219
Episode 237: reward: 87.231, steps: 675
Episode 238: reward: 278.720, steps: 191
Episode 239: reward: -77.856, steps: 220
Episode 240: reward: -71.557, steps: 539
Episode 241: reward: -263.067, steps: 411
Episode 242: reward: 308.250, steps: 211
Episode 243: reward: 1.331, steps: 116
Episode 244: reward: -4.900, steps: 1000
Episode 245: reward: 279.011, steps: 205
Episode 246: reward: 200.099, steps: 402
Episode 247: reward: -286.521, steps: 434
Episode 248: reward: 18.019, steps: 109
Episode 249: reward: 210.638, steps: 302
Episode 250: reward: 245.562, steps: 421
Episode 251: reward: 282.574, steps: 206
Episode 252: reward: 264.767, steps: 450
Episode 253: reward: 212.471, steps: 268
Episode 254: reward: 259.568, steps: 192
Episode 255: reward: 242.003, steps: 277
Episode 256: reward: 213.735, steps: 619
Episode 257: reward: 162.759, steps: 700
Episode 258: reward: 203.333, steps: 443
Episode 259: reward: -218.582, steps: 297
Episode 260: reward: 285.239, steps: 224
Episode 261: reward: -175.378, steps: 338
Episode 262: reward: 263.453, steps: 233
Episode 263: reward: -145.192, steps: 61
Episode 264: reward: -80.100, steps: 228
Episode 265: reward: 214.475, steps: 223
Episode 266: reward: 290.448, steps: 238
Episode 267: reward: 227.784, steps: 250
Episode 268: reward: 226.944, steps: 299
Episode 269: reward: 225.709, steps: 247
Episode 270: reward: -56.598, steps: 202
Episode 271: reward: 297.410, steps: 229
Episode 272: reward: 286.600, steps: 222
Episode 273: reward: 191.145, steps: 560
Episode 274: reward: 231.007, steps: 330
Episode 275: reward: -240.841, steps: 403
Episode 276: reward: 273.513, steps: 342
Episode 277: reward: 256.165, steps: 262
Episode 278: reward: -52.639, steps: 182
Episode 279: reward: 234.316, steps: 364
Episode 280: reward: -33.244, steps: 1000
Episode 281: reward: 197.680, steps: 684
Episode 282: reward: 230.223, steps: 235
Episode 283: reward: 268.624, steps: 363
Episode 284: reward: 254.638, steps: 244
Episode 285: reward: -45.981, steps: 183
Episode 286: reward: -167.659, steps: 459
Episode 287: reward: 185.608, steps: 1000
Episode 288: reward: -76.549, steps: 183
Episode 289: reward: 273.244, steps: 223
Episode 290: reward: -839.505, steps: 339
Episode 291: reward: 197.786, steps: 283
Episode 292: reward: -172.795, steps: 350
Episode 293: reward: 206.147, steps: 293
Episode 294: reward: 174.593, steps: 430
Episode 295: reward: 239.859, steps: 234
Episode 296: reward: 244.670, steps: 368
Episode 297: reward: 246.389, steps: 361
Episode 298: reward: 237.091, steps: 276
Episode 299: reward: 209.646, steps: 581
Episode 300: reward: -3.954, steps: 192
Episode 301: reward: 238.833, steps: 242
Episode 302: reward: 183.399, steps: 1000
Episode 303: reward: 270.932, steps: 237
Episode 304: reward: 201.354, steps: 225
Episode 305: reward: -198.188, steps: 266
Episode 306: reward: -361.782, steps: 385
Episode 307: reward: 235.005, steps: 223
Episode 308: reward: -235.213, steps: 311
Episode 309: reward: 238.537, steps: 388
Episode 310: reward: 250.708, steps: 338
Episode 311: reward: -252.058, steps: 356
Episode 312: reward: -15.291, steps: 605
Episode 313: reward: 238.470, steps: 476
Episode 314: reward: -304.486, steps: 450
Episode 315: reward: 276.885, steps: 232
Episode 316: reward: 233.178, steps: 594
Episode 317: reward: 265.490, steps: 282
Episode 318: reward: 250.117, steps: 211
Episode 319: reward: 296.667, steps: 258
Episode 320: reward: -298.146, steps: 483
Episode 321: reward: -228.104, steps: 76
Episode 322: reward: 141.753, steps: 470
Episode 323: reward: 175.589, steps: 487
Episode 324: reward: 277.416, steps: 228
Episode 325: reward: 264.030, steps: 268
Episode 326: reward: -974.448, steps: 352
Episode 327: reward: -284.034, steps: 389
Episode 328: reward: 238.174, steps: 227
Episode 329: reward: 234.797, steps: 260
Episode 330: reward: -29.207, steps: 69
Episode 331: reward: 206.599, steps: 340
Episode 332: reward: 273.987, steps: 232
Episode 333: reward: -188.267, steps: 119
Episode 334: reward: -242.085, steps: 396
Episode 335: reward: -164.166, steps: 553
Episode 336: reward: -189.352, steps: 231
Episode 337: reward: 262.991, steps: 211
Episode 338: reward: 259.059, steps: 253
Episode 339: reward: 249.642, steps: 248
Episode 340: reward: 272.948, steps: 243
Episode 341: reward: 268.958, steps: 219
Episode 342: reward: 250.244, steps: 500
Episode 343: reward: 242.830, steps: 204
Episode 344: reward: 273.248, steps: 203
Episode 345: reward: 265.666, steps: 189
Episode 346: reward: 189.895, steps: 658
Episode 347: reward: 238.229, steps: 206
Episode 348: reward: 211.058, steps: 622
Episode 349: reward: 199.355, steps: 332
Episode 350: reward: 278.311, steps: 201
Episode 351: reward: 271.341, steps: 204
Episode 352: reward: 119.585, steps: 1000
Episode 353: reward: 260.641, steps: 486
Episode 354: reward: 248.797, steps: 225
Episode 355: reward: 142.796, steps: 1000
Episode 356: reward: 238.394, steps: 258
Episode 357: reward: 260.180, steps: 383
Episode 358: reward: 245.233, steps: 454
Episode 359: reward: 160.658, steps: 487
Episode 360: reward: 264.004, steps: 198
Episode 361: reward: 275.311, steps: 208
Episode 362: reward: 199.718, steps: 392
Episode 363: reward: 134.714, steps: 741
Episode 364: reward: 260.140, steps: 224
Episode 365: reward: 178.614, steps: 423
Episode 366: reward: -74.348, steps: 223
Episode 367: reward: -51.575, steps: 170
Episode 368: reward: 210.223, steps: 440
Episode 369: reward: 169.349, steps: 451
Episode 370: reward: 196.939, steps: 429
Episode 371: reward: 267.091, steps: 250
Episode 372: reward: 273.300, steps: 235
Episode 373: reward: 248.072, steps: 393
Episode 374: reward: -228.773, steps: 427
Episode 375: reward: 192.332, steps: 381
Episode 376: reward: -160.284, steps: 218
Episode 377: reward: 253.509, steps: 357
Episode 378: reward: 173.499, steps: 583
Episode 379: reward: 251.314, steps: 229
Episode 380: reward: 207.218, steps: 387
Episode 381: reward: -289.171, steps: 431
Episode 382: reward: -249.810, steps: 319
Episode 383: reward: -253.439, steps: 332
Episode 384: reward: -266.902, steps: 263
Episode 385: reward: -61.767, steps: 1000
Episode 386: reward: 205.001, steps: 316
Episode 387: reward: 238.816, steps: 179
Episode 388: reward: 213.295, steps: 628
Episode 389: reward: 241.385, steps: 413
Episode 390: reward: 259.636, steps: 243
Episode 391: reward: 284.404, steps: 240
Episode 392: reward: 30.574, steps: 117
Episode 393: reward: 266.056, steps: 274
Episode 394: reward: 242.799, steps: 203
Episode 395: reward: 232.818, steps: 234
Episode 396: reward: -305.557, steps: 263
Episode 397: reward: -64.242, steps: 178
Episode 398: reward: -253.676, steps: 359
Episode 399: reward: 270.222, steps: 230
Episode 400: reward: 204.333, steps: 371
Episode 401: reward: 229.629, steps: 374
Episode 402: reward: 280.924, steps: 194
Episode 403: reward: 286.966, steps: 207
Episode 404: reward: 138.649, steps: 435
Episode 405: reward: 265.728, steps: 269
Episode 406: reward: 277.323, steps: 256
Episode 407: reward: 217.332, steps: 435
Episode 408: reward: 253.978, steps: 284
Episode 409: reward: -84.998, steps: 1000
Episode 410: reward: 225.772, steps: 263
Episode 411: reward: -96.044, steps: 220
Episode 412: reward: 214.828, steps: 386
Episode 413: reward: 235.586, steps: 219
Episode 414: reward: -28.575, steps: 1000
Episode 415: reward: 285.902, steps: 206
Episode 416: reward: 261.586, steps: 183
Episode 417: reward: 223.745, steps: 216
Episode 418: reward: -173.443, steps: 375
Episode 419: reward: 287.446, steps: 247
Episode 420: reward: 262.018, steps: 195
Episode 421: reward: 253.793, steps: 241
Episode 422: reward: -1067.556, steps: 322
Episode 423: reward: -69.028, steps: 208
Episode 424: reward: 230.165, steps: 248
Episode 425: reward: 222.199, steps: 315
Episode 426: reward: 216.742, steps: 457
Episode 427: reward: 223.145, steps: 374
Episode 428: reward: 215.689, steps: 610
Episode 429: reward: 247.256, steps: 314
Episode 430: reward: 236.737, steps: 242
Episode 431: reward: -22.088, steps: 219
Episode 432: reward: 213.951, steps: 425
Episode 433: reward: 187.274, steps: 465
Episode 434: reward: 274.373, steps: 238
Episode 435: reward: 134.750, steps: 1000
Episode 436: reward: 198.806, steps: 500
Episode 437: reward: 260.256, steps: 239
Episode 438: reward: 233.512, steps: 200
Episode 439: reward: 198.895, steps: 424
Episode 440: reward: -253.692, steps: 264
Episode 441: reward: 261.518, steps: 212
Episode 442: reward: 256.619, steps: 197
Episode 443: reward: 222.603, steps: 254
Episode 444: reward: -273.249, steps: 441
Episode 445: reward: -44.577, steps: 217
Episode 446: reward: 227.444, steps: 185
Episode 447: reward: 267.619, steps: 203
Episode 448: reward: -118.240, steps: 352
Episode 449: reward: 176.827, steps: 454
Episode 450: reward: 262.295, steps: 200
Episode 451: reward: -211.711, steps: 654
Episode 452: reward: 197.366, steps: 470
Episode 453: reward: 228.331, steps: 190
Episode 454: reward: 174.481, steps: 435
Episode 455: reward: 240.482, steps: 365
Episode 456: reward: -325.657, steps: 668
Episode 457: reward: 285.621, steps: 238
Episode 458: reward: 179.693, steps: 445
Episode 459: reward: 271.108, steps: 272
Episode 460: reward: 289.396, steps: 222
Episode 461: reward: 225.323, steps: 344
Episode 462: reward: -74.697, steps: 108
Episode 463: reward: 295.182, steps: 218
Episode 464: reward: 232.417, steps: 246
Episode 465: reward: -295.852, steps: 454
Episode 466: reward: 285.739, steps: 201
Episode 467: reward: -459.668, steps: 312
Episode 468: reward: 176.821, steps: 396
Episode 469: reward: 173.161, steps: 437
Episode 470: reward: 234.136, steps: 366
Episode 471: reward: 279.571, steps: 241
Episode 472: reward: 241.327, steps: 366
Episode 473: reward: -155.914, steps: 227
Episode 474: reward: 257.852, steps: 347
Episode 475: reward: 209.377, steps: 487
Episode 476: reward: 285.871, steps: 229
Episode 477: reward: 217.419, steps: 440
Episode 478: reward: -78.643, steps: 171
Episode 479: reward: 225.526, steps: 465
Episode 480: reward: 265.721, steps: 368
Episode 481: reward: 163.922, steps: 403
Episode 482: reward: -64.067, steps: 1000
Episode 483: reward: -129.900, steps: 586
Episode 484: reward: 187.261, steps: 448
Episode 485: reward: 275.300, steps: 213
Episode 486: reward: -38.727, steps: 162
Episode 487: reward: 260.262, steps: 278
Episode 488: reward: 273.007, steps: 216
Episode 489: reward: -248.751, steps: 242
Episode 490: reward: -576.136, steps: 326
Episode 491: reward: 233.101, steps: 222
Episode 492: reward: 298.476, steps: 207
Episode 493: reward: 183.201, steps: 457
Episode 494: reward: -98.122, steps: 112
Episode 495: reward: 223.082, steps: 328
Episode 496: reward: 213.394, steps: 573
Episode 497: reward: 223.891, steps: 547
Episode 498: reward: 194.270, steps: 430
Episode 499: reward: 210.809, steps: 376
Episode 500: reward: 261.207, steps: 229
Episode 501: reward: 239.414, steps: 348
Episode 502: reward: -453.847, steps: 324
Episode 503: reward: -71.203, steps: 126
Episode 504: reward: 260.575, steps: 240
Episode 505: reward: 228.431, steps: 250
Episode 506: reward: 240.184, steps: 285
Episode 507: reward: -301.558, steps: 240
Episode 508: reward: 264.937, steps: 332
Episode 509: reward: 255.707, steps: 218
Episode 510: reward: 223.375, steps: 246
Episode 511: reward: 251.154, steps: 273
Episode 512: reward: 133.314, steps: 890
Episode 513: reward: 228.959, steps: 196
Episode 514: reward: 240.578, steps: 225
Episode 515: reward: 264.525, steps: 315
Episode 516: reward: 256.783, steps: 261
Episode 517: reward: 286.624, steps: 224
Episode 518: reward: -68.024, steps: 139
Episode 519: reward: 178.131, steps: 665
Episode 520: reward: 190.042, steps: 334
Episode 521: reward: -150.076, steps: 136
Episode 522: reward: 191.758, steps: 478
Episode 523: reward: 224.953, steps: 355
Episode 524: reward: 224.110, steps: 613
Episode 525: reward: 219.578, steps: 259
Episode 526: reward: 187.691, steps: 345
Episode 527: reward: 249.413, steps: 444
Episode 528: reward: -352.514, steps: 279
Episode 529: reward: -728.030, steps: 313
Episode 530: reward: 147.053, steps: 435
Episode 531: reward: 184.430, steps: 630
Episode 532: reward: -306.319, steps: 761
Episode 533: reward: 258.271, steps: 198
Episode 534: reward: -46.031, steps: 174
Episode 535: reward: 240.652, steps: 510
Episode 536: reward: 174.615, steps: 399
Episode 537: reward: 220.021, steps: 381
Episode 538: reward: 230.286, steps: 232
Episode 539: reward: 240.093, steps: 223
Episode 540: reward: 277.873, steps: 329
Episode 541: reward: 241.693, steps: 302
Episode 542: reward: -66.775, steps: 234
Episode 543: reward: 219.890, steps: 254
Episode 544: reward: -247.161, steps: 244
Episode 545: reward: -357.975, steps: 315
Episode 546: reward: 290.961, steps: 208
Episode 547: reward: 185.528, steps: 372
Episode 548: reward: -171.613, steps: 575
Episode 549: reward: 275.428, steps: 341
Episode 550: reward: 243.249, steps: 222
Episode 551: reward: 287.842, steps: 320
Episode 552: reward: 245.568, steps: 281
Episode 553: reward: -283.441, steps: 250
Episode 554: reward: 252.778, steps: 237
Episode 555: reward: 263.635, steps: 254
Episode 556: reward: 217.360, steps: 445
Episode 557: reward: -107.719, steps: 1000
Episode 558: reward: 228.209, steps: 395
Episode 559: reward: 234.229, steps: 211
Episode 560: reward: 239.016, steps: 220
Episode 561: reward: 211.133, steps: 449
Episode 562: reward: -271.440, steps: 406
Episode 563: reward: 220.960, steps: 219
Episode 564: reward: 239.185, steps: 202
Episode 565: reward: -20.587, steps: 170
Episode 566: reward: 208.906, steps: 594
Episode 567: reward: 239.177, steps: 393
Episode 568: reward: -266.480, steps: 591
Episode 569: reward: 241.353, steps: 352
Episode 570: reward: -187.297, steps: 635
Episode 571: reward: 183.572, steps: 352
Episode 572: reward: 188.288, steps: 435
Episode 573: reward: -15.605, steps: 165
Episode 574: reward: 196.358, steps: 375
Episode 575: reward: 267.254, steps: 261
Episode 576: reward: 214.141, steps: 298
Episode 577: reward: -43.977, steps: 204
Episode 578: reward: 243.880, steps: 251
Episode 579: reward: -355.545, steps: 360
Episode 580: reward: 228.501, steps: 206
Episode 581: reward: 224.860, steps: 247
Episode 582: reward: 275.162, steps: 201
Episode 583: reward: 229.948, steps: 213
Episode 584: reward: -177.283, steps: 465
Episode 585: reward: -251.712, steps: 254
Episode 586: reward: 244.207, steps: 594
Episode 587: reward: -381.043, steps: 269
Episode 588: reward: 216.656, steps: 237
Episode 589: reward: 219.139, steps: 529
Episode 590: reward: -89.652, steps: 185
Episode 591: reward: 169.223, steps: 350
Episode 592: reward: 114.663, steps: 518
Episode 593: reward: 287.305, steps: 250
Episode 594: reward: 294.985, steps: 206
Episode 595: reward: 223.892, steps: 476
Episode 596: reward: -241.306, steps: 278
Episode 597: reward: 258.036, steps: 224
Episode 598: reward: 279.881, steps: 234
Episode 599: reward: 35.127, steps: 126
Episode 600: reward: -102.761, steps: 123
Episode 601: reward: -366.023, steps: 731
Episode 602: reward: 245.645, steps: 226
Episode 603: reward: 243.294, steps: 196
Episode 604: reward: 232.965, steps: 250
Episode 605: reward: 226.721, steps: 222
Episode 606: reward: 248.757, steps: 289
Episode 607: reward: 256.174, steps: 194
Episode 608: reward: 193.630, steps: 400
Episode 609: reward: 243.572, steps: 237
Episode 610: reward: 224.141, steps: 443
Episode 611: reward: 233.866, steps: 379
Episode 612: reward: -165.355, steps: 103
Episode 613: reward: 219.477, steps: 274
Episode 614: reward: 207.371, steps: 337
Episode 615: reward: -24.346, steps: 472
Episode 616: reward: 275.902, steps: 269
Episode 617: reward: -178.662, steps: 494
Episode 618: reward: 260.121, steps: 260
Episode 619: reward: 188.384, steps: 670
Episode 620: reward: 200.816, steps: 357
Episode 621: reward: 234.805, steps: 206
Episode 622: reward: 192.946, steps: 372
Episode 623: reward: 233.486, steps: 225
Episode 624: reward: 273.391, steps: 302
Episode 625: reward: 37.300, steps: 221
Episode 626: reward: 222.484, steps: 460
Episode 627: reward: 235.208, steps: 225
Episode 628: reward: 2.931, steps: 166
Episode 629: reward: -311.118, steps: 795
Episode 630: reward: 213.565, steps: 544
Episode 631: reward: 263.268, steps: 258
Episode 632: reward: 286.682, steps: 215
Episode 633: reward: 211.064, steps: 452
Episode 634: reward: -17.471, steps: 195
Episode 635: reward: -143.565, steps: 474
Episode 636: reward: 252.135, steps: 372
Episode 637: reward: 259.153, steps: 256
Episode 638: reward: 280.311, steps: 278
Episode 639: reward: -140.754, steps: 124
Episode 640: reward: 298.761, steps: 231
Episode 641: reward: 241.727, steps: 436
Episode 642: reward: 204.579, steps: 371
Episode 643: reward: -20.904, steps: 199
Episode 644: reward: 247.777, steps: 212
Episode 645: reward: 265.090, steps: 198
Episode 646: reward: 226.216, steps: 261
Episode 647: reward: 207.667, steps: 421
Episode 648: reward: 259.353, steps: 195
Episode 649: reward: -243.633, steps: 274
Episode 650: reward: -146.880, steps: 797
Episode 651: reward: 193.352, steps: 385
Episode 652: reward: -96.238, steps: 1000
Episode 653: reward: 241.017, steps: 221
Episode 654: reward: -39.678, steps: 1000
Episode 655: reward: 179.009, steps: 427
Episode 656: reward: 219.692, steps: 250
Episode 657: reward: 236.492, steps: 227
Episode 658: reward: 221.752, steps: 245
Episode 659: reward: 244.916, steps: 362
Episode 660: reward: -279.082, steps: 435
Episode 661: reward: 208.297, steps: 244
Episode 662: reward: -583.041, steps: 332
Episode 663: reward: 212.902, steps: 369
Episode 664: reward: 23.693, steps: 221
Episode 665: reward: 212.850, steps: 240
Episode 666: reward: -301.714, steps: 446
Episode 667: reward: 178.585, steps: 373
Episode 668: reward: -258.130, steps: 454
Episode 669: reward: 288.364, steps: 196
Episode 670: reward: 263.678, steps: 197
Episode 671: reward: 297.965, steps: 238
Episode 672: reward: 228.264, steps: 679
Episode 673: reward: 256.579, steps: 209
Episode 674: reward: 20.818, steps: 149
Episode 675: reward: 248.875, steps: 229
Episode 676: reward: 237.553, steps: 374
Episode 677: reward: -304.899, steps: 873
Episode 678: reward: 178.140, steps: 401
Episode 679: reward: -258.454, steps: 229
Episode 680: reward: 188.762, steps: 404
Episode 681: reward: 276.867, steps: 313
Episode 682: reward: -154.428, steps: 542
Episode 683: reward: 217.197, steps: 360
Episode 684: reward: 293.242, steps: 236
Episode 685: reward: 223.496, steps: 222
Episode 686: reward: -34.225, steps: 196
Episode 687: reward: 277.017, steps: 229
Episode 688: reward: 265.108, steps: 245
Episode 689: reward: -44.685, steps: 200
Episode 690: reward: 286.600, steps: 213
Episode 691: reward: 286.723, steps: 226
Episode 692: reward: 244.518, steps: 233
Episode 693: reward: 239.707, steps: 406
Episode 694: reward: 244.016, steps: 468
Episode 695: reward: -1588.512, steps: 351
Episode 696: reward: 240.232, steps: 455
Episode 697: reward: 288.699, steps: 189
Episode 698: reward: -244.533, steps: 332
Episode 699: reward: 293.622, steps: 216
Episode 700: reward: 240.176, steps: 494
Episode 701: reward: 237.800, steps: 210
Episode 702: reward: -279.550, steps: 614
Episode 703: reward: 223.994, steps: 414
Episode 704: reward: -65.392, steps: 413
Episode 705: reward: 258.101, steps: 220
Episode 706: reward: -24.188, steps: 191
Episode 707: reward: -32.329, steps: 157
Episode 708: reward: -36.796, steps: 151
Episode 709: reward: 212.058, steps: 415
Episode 710: reward: 188.561, steps: 343
Episode 711: reward: 209.941, steps: 280
Episode 712: reward: 270.354, steps: 286
Episode 713: reward: 184.034, steps: 760
Episode 714: reward: -47.689, steps: 188
Episode 715: reward: 215.765, steps: 517
Episode 716: reward: 165.991, steps: 782
Episode 717: reward: 162.741, steps: 391
Episode 718: reward: 247.584, steps: 287
Episode 719: reward: 84.488, steps: 640
Episode 720: reward: 222.715, steps: 218
Episode 721: reward: 260.980, steps: 191
Episode 722: reward: -303.073, steps: 232
Episode 723: reward: -326.343, steps: 233
Episode 724: reward: 260.152, steps: 269
Episode 725: reward: 101.161, steps: 789
Episode 726: reward: 211.278, steps: 351
Episode 727: reward: 50.718, steps: 263
Episode 728: reward: 221.042, steps: 256
Episode 729: reward: 235.104, steps: 498
Episode 730: reward: -128.589, steps: 290
Episode 731: reward: 248.546, steps: 268
Episode 732: reward: 207.472, steps: 290
Episode 733: reward: 227.955, steps: 525
Episode 734: reward: 192.600, steps: 389
Episode 735: reward: -22.559, steps: 190
Episode 736: reward: 257.455, steps: 430
Episode 737: reward: 234.766, steps: 261
Episode 738: reward: 234.227, steps: 527
Episode 739: reward: 214.017, steps: 291
Episode 740: reward: 278.205, steps: 197
Episode 741: reward: 303.092, steps: 223
Episode 742: reward: -145.417, steps: 502
Episode 743: reward: -80.749, steps: 200
Episode 744: reward: 271.807, steps: 233
Episode 745: reward: 296.367, steps: 199
Episode 746: reward: 277.893, steps: 244
Episode 747: reward: -217.394, steps: 393
Episode 748: reward: 224.361, steps: 226
Episode 749: reward: -148.899, steps: 101
Episode 750: reward: 271.506, steps: 242
Episode 751: reward: 163.154, steps: 503
Episode 752: reward: -296.997, steps: 300
Episode 753: reward: 119.608, steps: 891
Episode 754: reward: 165.230, steps: 556
Episode 755: reward: -256.597, steps: 416
Episode 756: reward: 218.113, steps: 508
Episode 757: reward: 267.299, steps: 202
Episode 758: reward: 165.143, steps: 429
Episode 759: reward: -159.654, steps: 327
Episode 760: reward: 197.038, steps: 335
Episode 761: reward: 212.784, steps: 293
Episode 762: reward: 220.880, steps: 418
Episode 763: reward: -296.648, steps: 440
Episode 764: reward: 265.867, steps: 377
Episode 765: reward: 282.368, steps: 202
Episode 766: reward: -258.398, steps: 1000
Episode 767: reward: -243.040, steps: 417
Episode 768: reward: -258.669, steps: 423
Episode 769: reward: 224.932, steps: 310
Episode 770: reward: 279.811, steps: 288
Episode 771: reward: -68.898, steps: 174
Episode 772: reward: -499.938, steps: 680
Episode 773: reward: -257.594, steps: 437
Episode 774: reward: 267.837, steps: 213
Episode 775: reward: 220.821, steps: 409
Episode 776: reward: 221.141, steps: 209
Episode 777: reward: 224.641, steps: 238
Episode 778: reward: 218.098, steps: 252
Episode 779: reward: 266.696, steps: 238
Episode 780: reward: 220.914, steps: 378
Episode 781: reward: 298.738, steps: 228
Episode 782: reward: 229.791, steps: 540
Episode 783: reward: 254.387, steps: 410
Episode 784: reward: 209.590, steps: 377
Episode 785: reward: 297.912, steps: 234
Episode 786: reward: 252.455, steps: 239
Episode 787: reward: -295.849, steps: 502
Episode 788: reward: 280.882, steps: 226
Episode 789: reward: 165.146, steps: 433
Episode 790: reward: -158.471, steps: 477
Episode 791: reward: 209.236, steps: 587
Episode 792: reward: 284.195, steps: 190
Episode 793: reward: 305.196, steps: 191
Episode 794: reward: -191.469, steps: 281
Episode 795: reward: 245.979, steps: 557
Episode 796: reward: 270.491, steps: 265
Episode 797: reward: -253.940, steps: 410
Episode 798: reward: 250.275, steps: 203
Episode 799: reward: 101.581, steps: 1000
Episode 800: reward: -274.963, steps: 215
Episode 801: reward: 218.027, steps: 375
Episode 802: reward: 261.384, steps: 245
Episode 803: reward: 213.297, steps: 359
Episode 804: reward: -227.445, steps: 745
Episode 805: reward: 289.370, steps: 237
Episode 806: reward: 281.256, steps: 274
Episode 807: reward: 275.306, steps: 245
Episode 808: reward: 213.698, steps: 373
Episode 809: reward: 229.704, steps: 224
Episode 810: reward: -225.484, steps: 600
Episode 811: reward: 246.279, steps: 493
Episode 812: reward: 252.890, steps: 235
Episode 813: reward: 210.365, steps: 517
Episode 814: reward: 219.602, steps: 430
Episode 815: reward: 150.061, steps: 1000
Episode 816: reward: 234.107, steps: 165
Episode 817: reward: -37.340, steps: 165
Episode 818: reward: 216.067, steps: 239
Episode 819: reward: 233.561, steps: 363
Episode 820: reward: -313.609, steps: 292
Episode 821: reward: 222.480, steps: 214
Episode 822: reward: -181.723, steps: 684
Episode 823: reward: 212.450, steps: 468
Episode 824: reward: -158.538, steps: 1000
Episode 825: reward: 156.941, steps: 389
Episode 826: reward: 252.755, steps: 226
Episode 827: reward: 314.424, steps: 228
Episode 828: reward: 161.911, steps: 1000
Episode 829: reward: 219.249, steps: 224
Episode 830: reward: 271.890, steps: 273
Episode 831: reward: -44.789, steps: 335
Episode 832: reward: 223.219, steps: 383
Episode 833: reward: 283.010, steps: 257
Episode 834: reward: 282.812, steps: 197
Episode 835: reward: -319.145, steps: 460
Episode 836: reward: 203.859, steps: 376
Episode 837: reward: 205.618, steps: 223
Episode 838: reward: -41.275, steps: 168
Episode 839: reward: 187.276, steps: 664
Episode 840: reward: 180.382, steps: 398
Episode 841: reward: -160.585, steps: 291
Episode 842: reward: -233.435, steps: 298
Episode 843: reward: -181.316, steps: 267
Episode 844: reward: 202.634, steps: 444
Episode 845: reward: 152.528, steps: 529
Episode 846: reward: -298.094, steps: 276
Episode 847: reward: 264.290, steps: 238
Episode 848: reward: 178.890, steps: 712
Episode 849: reward: 209.074, steps: 415
Episode 850: reward: -270.145, steps: 329
Episode 851: reward: -175.390, steps: 287
Episode 852: reward: 223.414, steps: 254
Episode 853: reward: 216.511, steps: 285
Episode 854: reward: 221.156, steps: 346
Episode 855: reward: 231.812, steps: 225
Episode 856: reward: 248.946, steps: 260
Episode 857: reward: 212.166, steps: 243
Episode 858: reward: -190.568, steps: 288
Episode 859: reward: 175.744, steps: 930
Episode 860: reward: -29.297, steps: 166
Episode 861: reward: 217.923, steps: 255
Episode 862: reward: 268.766, steps: 237
Episode 863: reward: 249.869, steps: 286
Episode 864: reward: 182.674, steps: 1000
Episode 865: reward: 270.107, steps: 276
Episode 866: reward: -154.499, steps: 145
Episode 867: reward: 245.431, steps: 243
Episode 868: reward: 225.174, steps: 169
Episode 869: reward: 255.670, steps: 253
Episode 870: reward: 242.461, steps: 221
Episode 871: reward: 246.853, steps: 444
Episode 872: reward: 287.299, steps: 220
Episode 873: reward: 267.888, steps: 249
Episode 874: reward: 278.228, steps: 209
Episode 875: reward: 236.906, steps: 318
Episode 876: reward: 180.394, steps: 843
Episode 877: reward: -148.695, steps: 153
Episode 878: reward: 236.618, steps: 262
Episode 879: reward: 270.660, steps: 240
Episode 880: reward: -41.987, steps: 188
Episode 881: reward: 242.469, steps: 371
Episode 882: reward: 272.730, steps: 195
Episode 883: reward: -0.036, steps: 123
Episode 884: reward: -45.976, steps: 204
Episode 885: reward: -496.214, steps: 311
Episode 886: reward: 137.797, steps: 580
Episode 887: reward: 225.232, steps: 275
Episode 888: reward: 206.647, steps: 384
Episode 889: reward: 197.348, steps: 343
Episode 890: reward: 247.093, steps: 205
Episode 891: reward: 306.242, steps: 246
Episode 892: reward: -70.174, steps: 227
Episode 893: reward: 262.292, steps: 257
Episode 894: reward: -313.483, steps: 441
Episode 895: reward: 13.134, steps: 206
Episode 896: reward: -590.370, steps: 289
Episode 897: reward: 256.607, steps: 257
Episode 898: reward: 175.594, steps: 712
Episode 899: reward: 305.865, steps: 237
Episode 900: reward: 232.900, steps: 251
Episode 901: reward: -73.800, steps: 187
Episode 902: reward: 184.859, steps: 461
Episode 903: reward: -286.785, steps: 466
Episode 904: reward: 21.933, steps: 329
Episode 905: reward: 245.527, steps: 218
Episode 906: reward: -172.835, steps: 122
Episode 907: reward: 262.920, steps: 202
Episode 908: reward: 205.239, steps: 574
Episode 909: reward: 204.646, steps: 681
Episode 910: reward: 150.779, steps: 396
Episode 911: reward: 280.666, steps: 274
Episode 912: reward: -244.487, steps: 230
Episode 913: reward: 25.199, steps: 100
Episode 914: reward: 162.942, steps: 1000
Episode 915: reward: 154.626, steps: 489
Episode 916: reward: 244.789, steps: 309
Episode 917: reward: 290.883, steps: 228
Episode 918: reward: -39.107, steps: 280
Episode 919: reward: 213.648, steps: 403
Episode 920: reward: 255.880, steps: 247
Episode 921: reward: 167.063, steps: 1000
Episode 922: reward: 237.494, steps: 539
Episode 923: reward: -156.961, steps: 1000
Episode 924: reward: 273.914, steps: 199
Episode 925: reward: -434.174, steps: 361
Episode 926: reward: 250.583, steps: 232
Episode 927: reward: 17.025, steps: 111
Episode 928: reward: 294.038, steps: 304
Episode 929: reward: -53.843, steps: 231
Episode 930: reward: -897.696, steps: 364
Episode 931: reward: 186.152, steps: 338
Episode 932: reward: 11.679, steps: 200
Episode 933: reward: 208.780, steps: 491
Episode 934: reward: 249.267, steps: 189
Episode 935: reward: -145.300, steps: 156
Episode 936: reward: 283.988, steps: 194
Episode 937: reward: 257.456, steps: 257
Episode 938: reward: 238.990, steps: 313
Episode 939: reward: -48.441, steps: 234
Episode 940: reward: -314.638, steps: 476
Episode 941: reward: 221.790, steps: 349
Episode 942: reward: 264.236, steps: 213
Episode 943: reward: -84.878, steps: 232
Episode 944: reward: 253.861, steps: 209
Episode 945: reward: 178.140, steps: 669
Episode 946: reward: 239.548, steps: 462
Episode 947: reward: -234.334, steps: 390
Episode 948: reward: 191.340, steps: 316
Episode 949: reward: -301.702, steps: 706
Episode 950: reward: 194.275, steps: 359
Episode 951: reward: -49.559, steps: 158
Episode 952: reward: 15.706, steps: 195
Episode 953: reward: -475.786, steps: 317
Episode 954: reward: -266.170, steps: 342
Episode 955: reward: 238.927, steps: 243
Episode 956: reward: 230.625, steps: 467
Episode 957: reward: 240.474, steps: 258
Episode 958: reward: 174.108, steps: 711
Episode 959: reward: 188.250, steps: 485
Episode 960: reward: 222.235, steps: 215
Episode 961: reward: 257.537, steps: 220
Episode 962: reward: 200.999, steps: 470
Episode 963: reward: -27.310, steps: 1000
Episode 964: reward: 207.488, steps: 313
Episode 965: reward: -135.263, steps: 564
Episode 966: reward: -149.398, steps: 400
Episode 967: reward: 199.094, steps: 292
Episode 968: reward: 277.670, steps: 251
Episode 969: reward: 259.399, steps: 262
Episode 970: reward: 225.267, steps: 229
Episode 971: reward: 130.854, steps: 1000
Episode 972: reward: 272.283, steps: 219
Episode 973: reward: 305.015, steps: 258
Episode 974: reward: 231.476, steps: 418
Episode 975: reward: -244.206, steps: 279
Episode 976: reward: -64.823, steps: 178
Episode 977: reward: -53.254, steps: 208
Episode 978: reward: 225.325, steps: 249
Episode 979: reward: 4.483, steps: 180
Episode 980: reward: -28.614, steps: 396
Episode 981: reward: 279.460, steps: 200
Episode 982: reward: 230.966, steps: 253
Episode 983: reward: 199.187, steps: 402
Episode 984: reward: 168.729, steps: 319
Episode 985: reward: -49.422, steps: 182
Episode 986: reward: -94.246, steps: 112
Episode 987: reward: 260.622, steps: 244
Episode 988: reward: 214.951, steps: 513
Episode 989: reward: 256.192, steps: 270
Episode 990: reward: -115.083, steps: 399
Episode 991: reward: 261.459, steps: 273
Episode 992: reward: 266.911, steps: 206
Episode 993: reward: 210.201, steps: 394
Episode 994: reward: 250.776, steps: 349
Episode 995: reward: 184.050, steps: 701
Episode 996: reward: 245.632, steps: 209
Episode 997: reward: 151.615, steps: 505
Episode 998: reward: -205.155, steps: 1000
Episode 999: reward: 220.935, steps: 230
Episode 1000: reward: 282.133, steps: 218

Process finished with exit code 0
