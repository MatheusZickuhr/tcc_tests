/home/matheus/tcc_implementation/venv/bin/python /home/matheus/tcc_implementation/agents/dqn/keras_rl_train_dqn_agent.py
Using TensorFlow backend.
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2019-10-31 20:43:35.046444: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-31 20:43:35.067245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2019-10-31 20:43:35.067493: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x33187c0 executing computations on platform Host. Devices:
2019-10-31 20:43:35.067515: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-31 20:43:35.093000: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Training for 1100000 steps ...
WARNING:tensorflow:From /home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

     160/1100000: episode: 1, duration: 0.231s, episode steps: 160, steps per second: 691, episode reward: -987.103, mean reward: -6.169 [-100.000, 2.138], mean action: 1.956 [0.000, 3.000], mean observation: 0.449 [-3.266, 5.195], loss: --, mae: --, mean_q: --
     229/1100000: episode: 2, duration: 0.047s, episode steps: 69, steps per second: 1473, episode reward: -439.530, mean reward: -6.370 [-100.000, 1.325], mean action: 1.623 [0.000, 3.000], mean observation: -0.034 [-3.681, 1.953], loss: --, mae: --, mean_q: --
     311/1100000: episode: 3, duration: 0.058s, episode steps: 82, steps per second: 1417, episode reward: -354.634, mean reward: -4.325 [-100.000, 1.303], mean action: 2.012 [0.000, 3.000], mean observation: 0.056 [-1.752, 2.126], loss: --, mae: --, mean_q: --
     460/1100000: episode: 4, duration: 0.117s, episode steps: 149, steps per second: 1272, episode reward: -947.817, mean reward: -6.361 [-100.000, 3.881], mean action: 1.926 [0.000, 3.000], mean observation: 0.388 [-0.674, 5.726], loss: --, mae: --, mean_q: --
     646/1100000: episode: 5, duration: 0.147s, episode steps: 186, steps per second: 1265, episode reward: -1489.699, mean reward: -8.009 [-100.000, 1.567], mean action: 1.957 [0.000, 3.000], mean observation: 0.612 [-3.165, 9.604], loss: --, mae: --, mean_q: --
     736/1100000: episode: 6, duration: 0.062s, episode steps: 90, steps per second: 1455, episode reward: -302.455, mean reward: -3.361 [-100.000, 3.091], mean action: 1.544 [0.000, 3.000], mean observation: 0.354 [-0.516, 2.242], loss: --, mae: --, mean_q: --
     814/1100000: episode: 7, duration: 0.052s, episode steps: 78, steps per second: 1507, episode reward: -462.326, mean reward: -5.927 [-100.000, 2.386], mean action: 1.641 [0.000, 3.000], mean observation: 0.155 [-2.331, 7.394], loss: --, mae: --, mean_q: --
     925/1100000: episode: 8, duration: 0.079s, episode steps: 111, steps per second: 1410, episode reward: -754.147, mean reward: -6.794 [-100.000, 0.653], mean action: 1.955 [0.000, 3.000], mean observation: 0.370 [-2.360, 4.295], loss: --, mae: --, mean_q: --
2019-10-31 20:43:36.633340: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 20:43:36.634821: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 20:43:36.649861: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-10-31 20:43:36.650975: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-10-31 20:43:36.652269: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-10-31 20:43:36.655430: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
    1059/1100000: episode: 9, duration: 0.987s, episode steps: 134, steps per second: 136, episode reward: -487.882, mean reward: -3.641 [-100.000, 1.415], mean action: 1.209 [0.000, 3.000], mean observation: 0.301 [-1.007, 4.457], loss: 54.757170, mae: 1.632514, mean_q: -0.176690
    1133/1100000: episode: 10, duration: 0.344s, episode steps: 74, steps per second: 215, episode reward: -133.435, mean reward: -1.803 [-100.000, 17.469], mean action: 0.149 [0.000, 3.000], mean observation: 0.064 [-1.750, 1.423], loss: 33.005699, mae: 2.105048, mean_q: -1.088786
    1216/1100000: episode: 11, duration: 0.374s, episode steps: 83, steps per second: 222, episode reward: -132.928, mean reward: -1.602 [-100.000, 8.533], mean action: 0.157 [0.000, 3.000], mean observation: -0.016 [-1.807, 5.203], loss: 39.678577, mae: 2.684763, mean_q: -1.443285
    1285/1100000: episode: 12, duration: 0.312s, episode steps: 69, steps per second: 221, episode reward: -153.165, mean reward: -2.220 [-100.000, 9.359], mean action: 0.203 [0.000, 3.000], mean observation: -0.034 [-1.666, 1.418], loss: 44.497570, mae: 3.640730, mean_q: -2.283903
    1369/1100000: episode: 13, duration: 0.388s, episode steps: 84, steps per second: 217, episode reward: -287.905, mean reward: -3.427 [-100.000, 99.095], mean action: 1.202 [0.000, 3.000], mean observation: -0.108 [-3.288, 1.410], loss: 37.674477, mae: 4.527802, mean_q: -3.510205
    1420/1100000: episode: 14, duration: 0.234s, episode steps: 51, steps per second: 218, episode reward: -106.772, mean reward: -2.094 [-100.000, 1.090], mean action: 0.020 [0.000, 1.000], mean observation: 0.054 [-1.881, 5.924], loss: 38.389305, mae: 5.476851, mean_q: -4.071359
    1480/1100000: episode: 15, duration: 0.272s, episode steps: 60, steps per second: 220, episode reward: -137.446, mean reward: -2.291 [-100.000, 35.753], mean action: 0.167 [0.000, 3.000], mean observation: 0.125 [-1.720, 2.991], loss: 53.386829, mae: 6.821793, mean_q: -4.945495
    1540/1100000: episode: 16, duration: 0.274s, episode steps: 60, steps per second: 219, episode reward: -124.771, mean reward: -2.080 [-100.000, 4.698], mean action: 0.250 [0.000, 3.000], mean observation: 0.051 [-1.842, 5.591], loss: 42.279556, mae: 7.975266, mean_q: -5.750696
    1679/1100000: episode: 17, duration: 0.636s, episode steps: 139, steps per second: 219, episode reward: -356.325, mean reward: -2.563 [-100.000, 118.363], mean action: 0.791 [0.000, 3.000], mean observation: 0.275 [-1.572, 3.767], loss: 40.383347, mae: 8.087513, mean_q: -6.524898
    1743/1100000: episode: 18, duration: 0.291s, episode steps: 64, steps per second: 220, episode reward: -124.156, mean reward: -1.940 [-100.000, 7.059], mean action: 0.438 [0.000, 3.000], mean observation: -0.088 [-4.589, 1.395], loss: 37.722328, mae: 9.046492, mean_q: -7.540151
    1865/1100000: episode: 19, duration: 0.569s, episode steps: 122, steps per second: 214, episode reward: -67.190, mean reward: -0.551 [-100.000, 22.258], mean action: 1.492 [0.000, 3.000], mean observation: 0.165 [-4.617, 1.407], loss: 33.059055, mae: 9.571897, mean_q: -8.588768
    2108/1100000: episode: 20, duration: 1.146s, episode steps: 243, steps per second: 212, episode reward: -269.317, mean reward: -1.108 [-100.000, 5.591], mean action: 1.333 [0.000, 3.000], mean observation: 0.025 [-1.016, 3.052], loss: 22.977894, mae: 9.234282, mean_q: -8.543489
    2248/1100000: episode: 21, duration: 0.654s, episode steps: 140, steps per second: 214, episode reward: -238.253, mean reward: -1.702 [-100.000, 79.260], mean action: 1.850 [0.000, 3.000], mean observation: 0.248 [-1.040, 2.630], loss: 21.812244, mae: 9.449449, mean_q: -9.063726
    2415/1100000: episode: 22, duration: 0.790s, episode steps: 167, steps per second: 211, episode reward: -261.005, mean reward: -1.563 [-100.000, 86.249], mean action: 1.269 [0.000, 3.000], mean observation: 0.166 [-1.452, 3.086], loss: 15.501204, mae: 10.224874, mean_q: -10.068928
    2805/1100000: episode: 23, duration: 1.913s, episode steps: 390, steps per second: 204, episode reward: -230.341, mean reward: -0.591 [-100.000, 9.412], mean action: 1.474 [0.000, 3.000], mean observation: 0.009 [-1.183, 1.408], loss: 19.765305, mae: 11.442245, mean_q: -11.189548
    2961/1100000: episode: 24, duration: 0.847s, episode steps: 156, steps per second: 184, episode reward: -567.806, mean reward: -3.640 [-100.000, 4.767], mean action: 1.327 [0.000, 3.000], mean observation: 0.060 [-2.148, 2.987], loss: 16.880333, mae: 12.265105, mean_q: -11.550800
    3410/1100000: episode: 25, duration: 2.361s, episode steps: 449, steps per second: 190, episode reward: -319.042, mean reward: -0.711 [-100.000, 35.082], mean action: 1.461 [0.000, 3.000], mean observation: 0.180 [-1.102, 3.203], loss: 22.250401, mae: 12.123356, mean_q: -11.109434
    3635/1100000: episode: 26, duration: 1.072s, episode steps: 225, steps per second: 210, episode reward: -11.809, mean reward: -0.052 [-100.000, 14.747], mean action: 1.573 [0.000, 3.000], mean observation: 0.142 [-0.739, 1.580], loss: 17.824076, mae: 11.970747, mean_q: -10.570957
    3726/1100000: episode: 27, duration: 0.420s, episode steps: 91, steps per second: 216, episode reward: -311.423, mean reward: -3.422 [-100.000, 5.174], mean action: 0.802 [0.000, 3.000], mean observation: 0.063 [-5.916, 1.407], loss: 22.884945, mae: 12.269867, mean_q: -10.758507
    3918/1100000: episode: 28, duration: 0.914s, episode steps: 192, steps per second: 210, episode reward: -306.478, mean reward: -1.596 [-100.000, 68.248], mean action: 1.599 [0.000, 3.000], mean observation: 0.246 [-1.072, 2.590], loss: 16.881744, mae: 12.005123, mean_q: -9.677814
    4747/1100000: episode: 29, duration: 4.891s, episode steps: 829, steps per second: 170, episode reward: -317.916, mean reward: -0.383 [-100.000, 4.837], mean action: 1.834 [0.000, 3.000], mean observation: 0.107 [-1.217, 4.603], loss: 17.018396, mae: 13.186239, mean_q: -9.906200
    4828/1100000: episode: 30, duration: 0.377s, episode steps: 81, steps per second: 215, episode reward: -205.933, mean reward: -2.542 [-100.000, 31.230], mean action: 0.469 [0.000, 3.000], mean observation: 0.005 [-4.619, 1.457], loss: 29.683792, mae: 13.832860, mean_q: -9.447862
    5063/1100000: episode: 31, duration: 1.143s, episode steps: 235, steps per second: 206, episode reward: -95.286, mean reward: -0.405 [-100.000, 28.531], mean action: 1.489 [0.000, 3.000], mean observation: 0.022 [-2.361, 1.401], loss: 17.452120, mae: 14.246813, mean_q: -9.100821
    5293/1100000: episode: 32, duration: 1.089s, episode steps: 230, steps per second: 211, episode reward: -219.192, mean reward: -0.953 [-100.000, 19.707], mean action: 1.361 [0.000, 3.000], mean observation: 0.065 [-0.903, 3.270], loss: 21.723843, mae: 13.877265, mean_q: -7.429729
    5380/1100000: episode: 33, duration: 0.416s, episode steps: 87, steps per second: 209, episode reward: -186.785, mean reward: -2.147 [-100.000, 32.509], mean action: 1.368 [0.000, 3.000], mean observation: -0.016 [-3.100, 1.391], loss: 16.175768, mae: 15.045737, mean_q: -8.119211
    5781/1100000: episode: 34, duration: 2.104s, episode steps: 401, steps per second: 191, episode reward: -243.691, mean reward: -0.608 [-100.000, 5.109], mean action: 1.638 [0.000, 3.000], mean observation: 0.138 [-1.215, 3.513], loss: 19.656504, mae: 15.056628, mean_q: -6.863519
    6080/1100000: episode: 35, duration: 1.447s, episode steps: 299, steps per second: 207, episode reward: -166.587, mean reward: -0.557 [-100.000, 4.303], mean action: 1.582 [0.000, 3.000], mean observation: 0.069 [-0.600, 1.449], loss: 20.199488, mae: 15.641293, mean_q: -6.291790
    6644/1100000: episode: 36, duration: 3.045s, episode steps: 564, steps per second: 185, episode reward: -81.322, mean reward: -0.144 [-100.000, 9.442], mean action: 1.816 [0.000, 3.000], mean observation: 0.122 [-0.689, 1.468], loss: 17.171621, mae: 16.322819, mean_q: -5.026237
    7644/1100000: episode: 37, duration: 5.923s, episode steps: 1000, steps per second: 169, episode reward: -74.219, mean reward: -0.074 [-4.927, 5.280], mean action: 1.911 [0.000, 3.000], mean observation: 0.098 [-0.467, 1.407], loss: 11.616033, mae: 17.607746, mean_q: -2.875894
    7856/1100000: episode: 38, duration: 1.027s, episode steps: 212, steps per second: 207, episode reward: -174.507, mean reward: -0.823 [-100.000, 3.832], mean action: 1.632 [0.000, 3.000], mean observation: 0.050 [-0.649, 1.399], loss: 8.668003, mae: 17.677160, mean_q: 0.235591
    8169/1100000: episode: 39, duration: 1.567s, episode steps: 313, steps per second: 200, episode reward: -141.632, mean reward: -0.452 [-100.000, 4.749], mean action: 1.696 [0.000, 3.000], mean observation: 0.174 [-0.458, 1.416], loss: 9.550656, mae: 18.162413, mean_q: 0.589978
    8439/1100000: episode: 40, duration: 1.311s, episode steps: 270, steps per second: 206, episode reward: -200.370, mean reward: -0.742 [-100.000, 4.448], mean action: 1.600 [0.000, 3.000], mean observation: 0.024 [-0.600, 1.396], loss: 6.816046, mae: 18.564854, mean_q: 2.306555
    8665/1100000: episode: 41, duration: 1.077s, episode steps: 226, steps per second: 210, episode reward: -162.798, mean reward: -0.720 [-100.000, 4.381], mean action: 1.513 [0.000, 3.000], mean observation: 0.085 [-0.600, 1.477], loss: 8.860877, mae: 19.415104, mean_q: 2.008250
    9054/1100000: episode: 42, duration: 1.952s, episode steps: 389, steps per second: 199, episode reward: -135.352, mean reward: -0.348 [-100.000, 3.248], mean action: 1.769 [0.000, 3.000], mean observation: 0.085 [-1.000, 1.411], loss: 7.575470, mae: 19.699060, mean_q: 4.135419
    9506/1100000: episode: 43, duration: 2.269s, episode steps: 452, steps per second: 199, episode reward: -223.704, mean reward: -0.495 [-100.000, 4.352], mean action: 1.684 [0.000, 3.000], mean observation: 0.102 [-1.000, 1.477], loss: 9.398928, mae: 20.412083, mean_q: 6.235913
    9915/1100000: episode: 44, duration: 2.092s, episode steps: 409, steps per second: 196, episode reward: -261.365, mean reward: -0.639 [-100.000, 12.425], mean action: 1.623 [0.000, 3.000], mean observation: 0.082 [-2.950, 1.415], loss: 9.059471, mae: 21.159628, mean_q: 7.647729
   10915/1100000: episode: 45, duration: 6.782s, episode steps: 1000, steps per second: 147, episode reward: -42.800, mean reward: -0.043 [-10.096, 12.283], mean action: 1.590 [0.000, 3.000], mean observation: 0.091 [-1.252, 1.396], loss: 7.152163, mae: 22.779867, mean_q: 7.921476
   11915/1100000: episode: 46, duration: 5.642s, episode steps: 1000, steps per second: 177, episode reward: -100.292, mean reward: -0.100 [-4.730, 4.687], mean action: 1.763 [0.000, 3.000], mean observation: 0.116 [-0.334, 1.446], loss: 8.368043, mae: 24.492491, mean_q: 8.508666
   12529/1100000: episode: 47, duration: 3.469s, episode steps: 614, steps per second: 177, episode reward: -210.400, mean reward: -0.343 [-100.000, 4.872], mean action: 1.555 [0.000, 3.000], mean observation: 0.072 [-1.004, 1.403], loss: 7.263663, mae: 24.533136, mean_q: 10.818541
   13529/1100000: episode: 48, duration: 5.271s, episode steps: 1000, steps per second: 190, episode reward: -88.816, mean reward: -0.089 [-5.030, 4.947], mean action: 1.759 [0.000, 3.000], mean observation: 0.031 [-0.600, 1.540], loss: 6.548626, mae: 24.621172, mean_q: 12.681666
   14529/1100000: episode: 49, duration: 6.225s, episode steps: 1000, steps per second: 161, episode reward: 37.555, mean reward: 0.038 [-24.420, 26.233], mean action: 1.552 [0.000, 3.000], mean observation: 0.039 [-0.658, 1.407], loss: 8.645965, mae: 24.989973, mean_q: 15.039370
   15529/1100000: episode: 50, duration: 5.660s, episode steps: 1000, steps per second: 177, episode reward: -80.826, mean reward: -0.081 [-4.497, 5.282], mean action: 1.656 [0.000, 3.000], mean observation: 0.098 [-0.744, 1.443], loss: 7.254463, mae: 25.223330, mean_q: 17.403440
   16529/1100000: episode: 51, duration: 5.711s, episode steps: 1000, steps per second: 175, episode reward: -122.032, mean reward: -0.122 [-4.385, 4.651], mean action: 1.658 [0.000, 3.000], mean observation: 0.025 [-0.600, 1.406], loss: 5.943643, mae: 25.202023, mean_q: 19.260700
   17529/1100000: episode: 52, duration: 6.273s, episode steps: 1000, steps per second: 159, episode reward: -9.582, mean reward: -0.010 [-4.025, 5.490], mean action: 1.857 [0.000, 3.000], mean observation: 0.092 [-0.628, 1.402], loss: 5.605013, mae: 25.542730, mean_q: 20.006083
   18529/1100000: episode: 53, duration: 5.554s, episode steps: 1000, steps per second: 180, episode reward: -58.438, mean reward: -0.058 [-4.048, 4.701], mean action: 1.899 [0.000, 3.000], mean observation: 0.030 [-0.600, 1.403], loss: 4.958378, mae: 25.539864, mean_q: 21.418541
   19529/1100000: episode: 54, duration: 5.829s, episode steps: 1000, steps per second: 172, episode reward: -87.916, mean reward: -0.088 [-4.827, 4.145], mean action: 1.662 [0.000, 3.000], mean observation: 0.096 [-0.665, 1.399], loss: 5.541970, mae: 25.826195, mean_q: 23.467918
   19726/1100000: episode: 55, duration: 0.966s, episode steps: 197, steps per second: 204, episode reward: -11.464, mean reward: -0.058 [-100.000, 17.043], mean action: 1.228 [0.000, 3.000], mean observation: -0.098 [-1.008, 1.402], loss: 6.752889, mae: 25.937351, mean_q: 23.707405
   20726/1100000: episode: 56, duration: 6.080s, episode steps: 1000, steps per second: 164, episode reward: -8.098, mean reward: -0.008 [-19.625, 19.633], mean action: 1.219 [0.000, 3.000], mean observation: 0.065 [-0.710, 1.398], loss: 4.958496, mae: 26.027777, mean_q: 24.855940
   21726/1100000: episode: 57, duration: 6.680s, episode steps: 1000, steps per second: 150, episode reward: 9.502, mean reward: 0.010 [-4.033, 4.769], mean action: 1.759 [0.000, 3.000], mean observation: 0.026 [-0.438, 1.392], loss: 6.315664, mae: 26.375593, mean_q: 26.608322
   21892/1100000: episode: 58, duration: 0.833s, episode steps: 166, steps per second: 199, episode reward: 36.171, mean reward: 0.218 [-100.000, 22.159], mean action: 1.590 [0.000, 3.000], mean observation: 0.197 [-0.615, 1.390], loss: 7.542269, mae: 26.190149, mean_q: 26.465300
   22361/1100000: episode: 59, duration: 2.606s, episode steps: 469, steps per second: 180, episode reward: -447.564, mean reward: -0.954 [-100.000, 34.220], mean action: 1.719 [0.000, 3.000], mean observation: 0.096 [-0.916, 3.360], loss: 6.903172, mae: 26.907537, mean_q: 28.494003
   23361/1100000: episode: 60, duration: 6.181s, episode steps: 1000, steps per second: 162, episode reward: 25.507, mean reward: 0.026 [-4.393, 5.035], mean action: 1.642 [0.000, 3.000], mean observation: 0.032 [-0.499, 1.438], loss: 6.615273, mae: 27.605036, mean_q: 29.154610
   24361/1100000: episode: 61, duration: 6.658s, episode steps: 1000, steps per second: 150, episode reward: -37.024, mean reward: -0.037 [-4.823, 5.038], mean action: 1.718 [0.000, 3.000], mean observation: 0.088 [-0.864, 1.499], loss: 5.971454, mae: 27.365618, mean_q: 30.008898
   25361/1100000: episode: 62, duration: 6.417s, episode steps: 1000, steps per second: 156, episode reward: -19.656, mean reward: -0.020 [-4.444, 5.147], mean action: 1.606 [0.000, 3.000], mean observation: 0.021 [-0.685, 1.393], loss: 4.595033, mae: 27.545670, mean_q: 30.765442
   26361/1100000: episode: 63, duration: 6.500s, episode steps: 1000, steps per second: 154, episode reward: -36.793, mean reward: -0.037 [-4.010, 4.438], mean action: 1.697 [0.000, 3.000], mean observation: 0.053 [-0.349, 1.509], loss: 5.403973, mae: 27.951332, mean_q: 31.747227
   27361/1100000: episode: 64, duration: 5.874s, episode steps: 1000, steps per second: 170, episode reward: -54.585, mean reward: -0.055 [-4.030, 4.361], mean action: 1.835 [0.000, 3.000], mean observation: -0.007 [-0.600, 1.489], loss: 4.554042, mae: 28.247040, mean_q: 32.064941
   27707/1100000: episode: 65, duration: 1.805s, episode steps: 346, steps per second: 192, episode reward: -69.177, mean reward: -0.200 [-100.000, 11.357], mean action: 1.506 [0.000, 3.000], mean observation: -0.003 [-0.884, 1.464], loss: 3.618001, mae: 28.435642, mean_q: 32.651577
   27779/1100000: episode: 66, duration: 0.359s, episode steps: 72, steps per second: 200, episode reward: -74.671, mean reward: -1.037 [-100.000, 12.106], mean action: 1.208 [0.000, 3.000], mean observation: 0.026 [-0.975, 2.083], loss: 4.893078, mae: 28.690151, mean_q: 33.856270
   28403/1100000: episode: 67, duration: 3.417s, episode steps: 624, steps per second: 183, episode reward: -162.150, mean reward: -0.260 [-100.000, 15.670], mean action: 1.699 [0.000, 3.000], mean observation: -0.013 [-0.989, 1.495], loss: 5.051949, mae: 28.523994, mean_q: 33.282627
   29403/1100000: episode: 68, duration: 6.465s, episode steps: 1000, steps per second: 155, episode reward: -22.822, mean reward: -0.023 [-4.413, 4.828], mean action: 1.733 [0.000, 3.000], mean observation: 0.059 [-0.488, 1.412], loss: 5.119684, mae: 29.057846, mean_q: 33.098434
   30403/1100000: episode: 69, duration: 6.317s, episode steps: 1000, steps per second: 158, episode reward: -31.715, mean reward: -0.032 [-5.059, 4.693], mean action: 1.766 [0.000, 3.000], mean observation: 0.087 [-0.720, 1.399], loss: 4.378336, mae: 28.855368, mean_q: 33.327358
   31403/1100000: episode: 70, duration: 5.625s, episode steps: 1000, steps per second: 178, episode reward: -73.146, mean reward: -0.073 [-5.240, 4.702], mean action: 1.718 [0.000, 3.000], mean observation: 0.100 [-0.756, 1.425], loss: 4.372113, mae: 28.633699, mean_q: 33.428715
   32403/1100000: episode: 71, duration: 6.689s, episode steps: 1000, steps per second: 150, episode reward: -27.038, mean reward: -0.027 [-4.647, 4.815], mean action: 1.675 [0.000, 3.000], mean observation: 0.047 [-0.708, 1.428], loss: 3.792741, mae: 28.559906, mean_q: 33.744080
   33403/1100000: episode: 72, duration: 5.923s, episode steps: 1000, steps per second: 169, episode reward: -75.085, mean reward: -0.075 [-4.982, 4.508], mean action: 1.733 [0.000, 3.000], mean observation: 0.112 [-0.560, 1.424], loss: 3.673094, mae: 28.851286, mean_q: 33.862507
   34403/1100000: episode: 73, duration: 6.739s, episode steps: 1000, steps per second: 148, episode reward: -35.081, mean reward: -0.035 [-4.754, 4.585], mean action: 1.684 [0.000, 3.000], mean observation: 0.050 [-0.822, 1.401], loss: 3.685521, mae: 28.984774, mean_q: 34.356773
   35403/1100000: episode: 74, duration: 6.194s, episode steps: 1000, steps per second: 161, episode reward: -82.167, mean reward: -0.082 [-4.758, 5.780], mean action: 1.779 [0.000, 3.000], mean observation: -0.011 [-0.696, 1.396], loss: 3.987120, mae: 28.969969, mean_q: 34.221672
   36403/1100000: episode: 75, duration: 6.191s, episode steps: 1000, steps per second: 162, episode reward: -60.576, mean reward: -0.061 [-4.461, 4.808], mean action: 1.746 [0.000, 3.000], mean observation: 0.049 [-0.525, 1.413], loss: 3.938108, mae: 29.434090, mean_q: 34.785339
   37403/1100000: episode: 76, duration: 6.007s, episode steps: 1000, steps per second: 166, episode reward: -32.531, mean reward: -0.033 [-4.505, 5.058], mean action: 1.740 [0.000, 3.000], mean observation: 0.110 [-0.964, 1.386], loss: 3.135371, mae: 29.730883, mean_q: 35.615047
   38403/1100000: episode: 77, duration: 6.685s, episode steps: 1000, steps per second: 150, episode reward: -13.271, mean reward: -0.013 [-4.411, 4.998], mean action: 1.631 [0.000, 3.000], mean observation: 0.041 [-0.592, 1.433], loss: 3.261762, mae: 30.102339, mean_q: 35.907787
   39403/1100000: episode: 78, duration: 6.948s, episode steps: 1000, steps per second: 144, episode reward: 17.501, mean reward: 0.018 [-4.784, 5.723], mean action: 1.687 [0.000, 3.000], mean observation: 0.013 [-1.173, 1.394], loss: 3.690536, mae: 30.534128, mean_q: 36.537788
   39511/1100000: episode: 79, duration: 0.567s, episode steps: 108, steps per second: 191, episode reward: -48.521, mean reward: -0.449 [-100.000, 11.356], mean action: 1.648 [0.000, 3.000], mean observation: 0.090 [-0.930, 1.409], loss: 4.076147, mae: 31.201321, mean_q: 36.900742
   40102/1100000: episode: 80, duration: 3.367s, episode steps: 591, steps per second: 176, episode reward: 221.827, mean reward: 0.375 [-22.520, 100.000], mean action: 1.575 [0.000, 3.000], mean observation: 0.035 [-0.640, 1.389], loss: 4.045840, mae: 30.518896, mean_q: 36.583035
   40564/1100000: episode: 81, duration: 2.651s, episode steps: 462, steps per second: 174, episode reward: 286.504, mean reward: 0.620 [-13.917, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.091 [-0.669, 1.387], loss: 6.117091, mae: 31.029371, mean_q: 37.193851
   41564/1100000: episode: 82, duration: 6.409s, episode steps: 1000, steps per second: 156, episode reward: -63.321, mean reward: -0.063 [-5.675, 4.733], mean action: 1.702 [0.000, 3.000], mean observation: 0.050 [-0.652, 1.430], loss: 4.174055, mae: 30.702679, mean_q: 37.023838
   42055/1100000: episode: 83, duration: 2.914s, episode steps: 491, steps per second: 169, episode reward: -39.925, mean reward: -0.081 [-100.000, 16.593], mean action: 1.731 [0.000, 3.000], mean observation: -0.027 [-0.681, 1.387], loss: 3.416828, mae: 30.638357, mean_q: 37.034382
   43055/1100000: episode: 84, duration: 6.199s, episode steps: 1000, steps per second: 161, episode reward: 29.767, mean reward: 0.030 [-24.208, 20.499], mean action: 1.287 [0.000, 3.000], mean observation: 0.062 [-0.868, 1.433], loss: 3.767407, mae: 30.553530, mean_q: 36.938553
   44055/1100000: episode: 85, duration: 6.308s, episode steps: 1000, steps per second: 159, episode reward: -35.526, mean reward: -0.036 [-6.406, 5.395], mean action: 1.750 [0.000, 3.000], mean observation: 0.021 [-0.583, 1.404], loss: 4.254876, mae: 30.265491, mean_q: 36.755543
   44140/1100000: episode: 86, duration: 0.450s, episode steps: 85, steps per second: 189, episode reward: -68.359, mean reward: -0.804 [-100.000, 10.933], mean action: 1.518 [0.000, 3.000], mean observation: 0.072 [-1.030, 1.407], loss: 2.940946, mae: 30.442041, mean_q: 37.083168
   44795/1100000: episode: 87, duration: 4.048s, episode steps: 655, steps per second: 162, episode reward: 219.148, mean reward: 0.335 [-19.521, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.093 [-0.399, 1.394], loss: 3.898652, mae: 30.394484, mean_q: 36.897263
   45795/1100000: episode: 88, duration: 5.855s, episode steps: 1000, steps per second: 171, episode reward: 5.185, mean reward: 0.005 [-18.033, 21.025], mean action: 1.489 [0.000, 3.000], mean observation: -0.028 [-0.643, 1.423], loss: 4.225571, mae: 30.283594, mean_q: 36.695545
   46241/1100000: episode: 89, duration: 2.481s, episode steps: 446, steps per second: 180, episode reward: -235.217, mean reward: -0.527 [-100.000, 23.150], mean action: 1.574 [0.000, 3.000], mean observation: -0.030 [-1.654, 1.396], loss: 4.957825, mae: 29.852840, mean_q: 36.284397
   47015/1100000: episode: 90, duration: 4.764s, episode steps: 774, steps per second: 162, episode reward: 158.309, mean reward: 0.205 [-17.795, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.108 [-0.682, 1.439], loss: 4.506768, mae: 29.927685, mean_q: 36.602245
   48015/1100000: episode: 91, duration: 7.111s, episode steps: 1000, steps per second: 141, episode reward: -44.563, mean reward: -0.045 [-5.068, 5.398], mean action: 1.754 [0.000, 3.000], mean observation: 0.031 [-0.521, 1.387], loss: 3.583571, mae: 29.873316, mean_q: 36.567356
   48745/1100000: episode: 92, duration: 4.404s, episode steps: 730, steps per second: 166, episode reward: 206.371, mean reward: 0.283 [-18.290, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.216 [-0.652, 1.542], loss: 4.102578, mae: 29.825787, mean_q: 36.331463
   49745/1100000: episode: 93, duration: 6.325s, episode steps: 1000, steps per second: 158, episode reward: -16.608, mean reward: -0.017 [-5.147, 4.945], mean action: 1.806 [0.000, 3.000], mean observation: 0.018 [-0.654, 1.406], loss: 4.641029, mae: 29.283205, mean_q: 36.190250
   50711/1100000: episode: 94, duration: 6.283s, episode steps: 966, steps per second: 154, episode reward: 177.591, mean reward: 0.184 [-23.593, 100.000], mean action: 1.616 [0.000, 3.000], mean observation: 0.044 [-0.600, 1.388], loss: 4.406178, mae: 29.405914, mean_q: 36.259605
   51489/1100000: episode: 95, duration: 4.816s, episode steps: 778, steps per second: 162, episode reward: -223.864, mean reward: -0.288 [-100.000, 12.683], mean action: 1.512 [0.000, 3.000], mean observation: -0.054 [-1.003, 1.404], loss: 4.025307, mae: 29.302795, mean_q: 36.038456
   52264/1100000: episode: 96, duration: 5.081s, episode steps: 775, steps per second: 153, episode reward: 184.316, mean reward: 0.238 [-12.748, 100.000], mean action: 1.590 [0.000, 3.000], mean observation: 0.008 [-0.600, 1.441], loss: 3.861299, mae: 29.135077, mean_q: 36.042213
   52383/1100000: episode: 97, duration: 0.715s, episode steps: 119, steps per second: 166, episode reward: 39.861, mean reward: 0.335 [-100.000, 14.639], mean action: 1.664 [0.000, 3.000], mean observation: 0.056 [-0.881, 1.393], loss: 4.582250, mae: 28.984955, mean_q: 35.917168
   53035/1100000: episode: 98, duration: 4.086s, episode steps: 652, steps per second: 160, episode reward: 263.147, mean reward: 0.404 [-17.404, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.113 [-0.658, 1.390], loss: 5.834527, mae: 29.000526, mean_q: 35.558334
   53207/1100000: episode: 99, duration: 0.941s, episode steps: 172, steps per second: 183, episode reward: 19.334, mean reward: 0.112 [-100.000, 18.417], mean action: 1.523 [0.000, 3.000], mean observation: 0.207 [-0.947, 1.388], loss: 11.020737, mae: 29.538023, mean_q: 36.166935
   53819/1100000: episode: 100, duration: 3.863s, episode steps: 612, steps per second: 158, episode reward: 134.145, mean reward: 0.219 [-11.967, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: 0.001 [-0.600, 1.390], loss: 5.437077, mae: 29.272177, mean_q: 36.230022
   54819/1100000: episode: 101, duration: 6.201s, episode steps: 1000, steps per second: 161, episode reward: -56.319, mean reward: -0.056 [-17.564, 15.685], mean action: 1.834 [0.000, 3.000], mean observation: 0.013 [-0.748, 1.398], loss: 4.121853, mae: 29.053097, mean_q: 36.183876
   55819/1100000: episode: 102, duration: 6.543s, episode steps: 1000, steps per second: 153, episode reward: -32.926, mean reward: -0.033 [-5.185, 4.746], mean action: 1.817 [0.000, 3.000], mean observation: 0.023 [-0.483, 1.509], loss: 4.745323, mae: 28.915789, mean_q: 36.241039
   56268/1100000: episode: 103, duration: 2.610s, episode steps: 449, steps per second: 172, episode reward: 249.864, mean reward: 0.556 [-11.566, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: -0.025 [-0.799, 1.387], loss: 4.153103, mae: 28.832487, mean_q: 36.222469
   57268/1100000: episode: 104, duration: 6.501s, episode steps: 1000, steps per second: 154, episode reward: -37.573, mean reward: -0.038 [-5.511, 6.028], mean action: 1.772 [0.000, 3.000], mean observation: 0.105 [-0.605, 1.389], loss: 4.101832, mae: 29.034395, mean_q: 36.374149
   57547/1100000: episode: 105, duration: 1.571s, episode steps: 279, steps per second: 178, episode reward: -10.591, mean reward: -0.038 [-100.000, 16.938], mean action: 1.681 [0.000, 3.000], mean observation: -0.030 [-0.650, 1.508], loss: 5.159379, mae: 28.899342, mean_q: 36.092319
   57909/1100000: episode: 106, duration: 2.099s, episode steps: 362, steps per second: 172, episode reward: 144.841, mean reward: 0.400 [-11.997, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.015 [-0.778, 1.414], loss: 5.345343, mae: 29.573175, mean_q: 36.921082
   58909/1100000: episode: 107, duration: 6.487s, episode steps: 1000, steps per second: 154, episode reward: -59.676, mean reward: -0.060 [-5.317, 5.938], mean action: 1.847 [0.000, 3.000], mean observation: 0.146 [-0.471, 1.403], loss: 3.994259, mae: 29.636707, mean_q: 37.283554
   59909/1100000: episode: 108, duration: 6.574s, episode steps: 1000, steps per second: 152, episode reward: 14.940, mean reward: 0.015 [-20.556, 23.337], mean action: 1.554 [0.000, 3.000], mean observation: 0.026 [-0.916, 1.395], loss: 5.290658, mae: 29.812969, mean_q: 37.512890
   60690/1100000: episode: 109, duration: 4.893s, episode steps: 781, steps per second: 160, episode reward: -44.143, mean reward: -0.057 [-100.000, 23.776], mean action: 1.722 [0.000, 3.000], mean observation: 0.102 [-0.701, 1.420], loss: 5.019364, mae: 30.017765, mean_q: 37.971012
   61690/1100000: episode: 110, duration: 6.415s, episode steps: 1000, steps per second: 156, episode reward: 130.402, mean reward: 0.130 [-20.588, 23.248], mean action: 1.386 [0.000, 3.000], mean observation: 0.056 [-0.668, 1.397], loss: 5.216852, mae: 30.503223, mean_q: 38.435135
   61960/1100000: episode: 111, duration: 1.546s, episode steps: 270, steps per second: 175, episode reward: 5.447, mean reward: 0.020 [-100.000, 13.025], mean action: 1.763 [0.000, 3.000], mean observation: 0.091 [-0.782, 1.397], loss: 4.775239, mae: 30.649443, mean_q: 38.503033
   62369/1100000: episode: 112, duration: 2.412s, episode steps: 409, steps per second: 170, episode reward: 218.132, mean reward: 0.533 [-3.122, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: 0.030 [-0.806, 1.446], loss: 3.740846, mae: 31.000647, mean_q: 39.262035
   62983/1100000: episode: 113, duration: 3.652s, episode steps: 614, steps per second: 168, episode reward: -201.774, mean reward: -0.329 [-100.000, 5.821], mean action: 1.731 [0.000, 3.000], mean observation: 0.071 [-1.002, 1.422], loss: 5.819541, mae: 31.344221, mean_q: 39.510403
   63983/1100000: episode: 114, duration: 6.922s, episode steps: 1000, steps per second: 144, episode reward: -27.586, mean reward: -0.028 [-13.095, 18.708], mean action: 1.732 [0.000, 3.000], mean observation: -0.010 [-0.665, 1.416], loss: 3.648968, mae: 31.671957, mean_q: 39.924110
   64983/1100000: episode: 115, duration: 6.450s, episode steps: 1000, steps per second: 155, episode reward: 34.100, mean reward: 0.034 [-23.262, 14.031], mean action: 1.785 [0.000, 3.000], mean observation: 0.041 [-0.933, 1.453], loss: 6.160897, mae: 31.560946, mean_q: 39.743294
   65460/1100000: episode: 116, duration: 2.869s, episode steps: 477, steps per second: 166, episode reward: 227.616, mean reward: 0.477 [-19.359, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.053 [-1.084, 1.431], loss: 5.159842, mae: 31.647839, mean_q: 40.276295
   66460/1100000: episode: 117, duration: 6.547s, episode steps: 1000, steps per second: 153, episode reward: -38.142, mean reward: -0.038 [-5.122, 5.979], mean action: 1.768 [0.000, 3.000], mean observation: 0.080 [-0.718, 1.500], loss: 3.304449, mae: 31.722380, mean_q: 40.833263
   66639/1100000: episode: 118, duration: 1.023s, episode steps: 179, steps per second: 175, episode reward: -5.863, mean reward: -0.033 [-100.000, 16.514], mean action: 1.654 [0.000, 3.000], mean observation: 0.052 [-0.763, 1.406], loss: 5.371411, mae: 31.604446, mean_q: 41.195957
   67639/1100000: episode: 119, duration: 8.429s, episode steps: 1000, steps per second: 119, episode reward: -22.549, mean reward: -0.023 [-5.179, 5.639], mean action: 1.841 [0.000, 3.000], mean observation: 0.002 [-0.830, 1.393], loss: 3.807613, mae: 31.469784, mean_q: 41.162170
   68147/1100000: episode: 120, duration: 3.169s, episode steps: 508, steps per second: 160, episode reward: -24.870, mean reward: -0.049 [-100.000, 12.129], mean action: 1.695 [0.000, 3.000], mean observation: 0.021 [-0.682, 1.461], loss: 4.694873, mae: 30.891356, mean_q: 40.690468
   68649/1100000: episode: 121, duration: 3.021s, episode steps: 502, steps per second: 166, episode reward: 229.070, mean reward: 0.456 [-17.652, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.050 [-0.600, 1.388], loss: 4.061341, mae: 30.894999, mean_q: 40.652615
   69248/1100000: episode: 122, duration: 3.763s, episode steps: 599, steps per second: 159, episode reward: 177.921, mean reward: 0.297 [-10.254, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.124 [-0.859, 1.407], loss: 4.577860, mae: 30.823528, mean_q: 40.817749
   70248/1100000: episode: 123, duration: 6.425s, episode steps: 1000, steps per second: 156, episode reward: -47.773, mean reward: -0.048 [-4.316, 4.866], mean action: 1.598 [0.000, 3.000], mean observation: 0.099 [-0.579, 1.401], loss: 3.310356, mae: 30.702826, mean_q: 40.769585
   70761/1100000: episode: 124, duration: 2.987s, episode steps: 513, steps per second: 172, episode reward: 188.213, mean reward: 0.367 [-20.454, 100.000], mean action: 0.805 [0.000, 3.000], mean observation: 0.032 [-0.600, 1.405], loss: 4.294985, mae: 30.761894, mean_q: 40.749317
   70999/1100000: episode: 125, duration: 1.362s, episode steps: 238, steps per second: 175, episode reward: -50.801, mean reward: -0.213 [-100.000, 21.047], mean action: 1.466 [0.000, 3.000], mean observation: 0.020 [-1.750, 1.439], loss: 2.750967, mae: 30.664818, mean_q: 40.769154
   71329/1100000: episode: 126, duration: 1.915s, episode steps: 330, steps per second: 172, episode reward: -181.027, mean reward: -0.549 [-100.000, 15.120], mean action: 1.612 [0.000, 3.000], mean observation: 0.092 [-1.229, 1.407], loss: 3.836062, mae: 30.448219, mean_q: 40.484589
   71577/1100000: episode: 127, duration: 1.440s, episode steps: 248, steps per second: 172, episode reward: 14.112, mean reward: 0.057 [-100.000, 12.656], mean action: 1.581 [0.000, 3.000], mean observation: 0.016 [-1.393, 1.398], loss: 3.646349, mae: 30.572701, mean_q: 40.271553
   71739/1100000: episode: 128, duration: 0.932s, episode steps: 162, steps per second: 174, episode reward: -16.834, mean reward: -0.104 [-100.000, 19.938], mean action: 1.772 [0.000, 3.000], mean observation: 0.025 [-1.997, 1.392], loss: 3.220516, mae: 30.781858, mean_q: 40.713493
   72558/1100000: episode: 129, duration: 5.171s, episode steps: 819, steps per second: 158, episode reward: -283.719, mean reward: -0.346 [-100.000, 14.081], mean action: 1.643 [0.000, 3.000], mean observation: -0.002 [-0.738, 1.396], loss: 3.311905, mae: 31.151232, mean_q: 41.252747
   73558/1100000: episode: 130, duration: 7.038s, episode steps: 1000, steps per second: 142, episode reward: -47.944, mean reward: -0.048 [-21.849, 17.750], mean action: 1.691 [0.000, 3.000], mean observation: 0.018 [-0.889, 1.406], loss: 3.138147, mae: 31.066132, mean_q: 41.112938
   74233/1100000: episode: 131, duration: 4.264s, episode steps: 675, steps per second: 158, episode reward: 201.413, mean reward: 0.298 [-20.735, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.050 [-0.748, 1.391], loss: 4.903345, mae: 31.055174, mean_q: 41.261013
   75233/1100000: episode: 132, duration: 6.516s, episode steps: 1000, steps per second: 153, episode reward: -31.900, mean reward: -0.032 [-4.867, 4.950], mean action: 1.704 [0.000, 3.000], mean observation: 0.082 [-0.543, 1.409], loss: 4.278553, mae: 30.844051, mean_q: 41.018009
   75951/1100000: episode: 133, duration: 4.722s, episode steps: 718, steps per second: 152, episode reward: 195.687, mean reward: 0.273 [-21.132, 100.000], mean action: 1.631 [0.000, 3.000], mean observation: 0.002 [-0.860, 1.398], loss: 3.709276, mae: 29.984430, mean_q: 39.953568
   76316/1100000: episode: 134, duration: 2.169s, episode steps: 365, steps per second: 168, episode reward: 240.499, mean reward: 0.659 [-14.039, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.144 [-0.798, 1.437], loss: 4.810023, mae: 30.423756, mean_q: 40.336533
   76431/1100000: episode: 135, duration: 0.656s, episode steps: 115, steps per second: 175, episode reward: -1.829, mean reward: -0.016 [-100.000, 16.945], mean action: 1.774 [0.000, 3.000], mean observation: 0.039 [-0.944, 1.407], loss: 4.822654, mae: 30.291866, mean_q: 40.197655
   76984/1100000: episode: 136, duration: 3.444s, episode steps: 553, steps per second: 161, episode reward: 171.330, mean reward: 0.310 [-21.400, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: -0.027 [-0.721, 1.447], loss: 4.286084, mae: 30.020679, mean_q: 40.008461
   77984/1100000: episode: 137, duration: 7.653s, episode steps: 1000, steps per second: 131, episode reward: -17.015, mean reward: -0.017 [-5.352, 5.215], mean action: 1.695 [0.000, 3.000], mean observation: 0.091 [-0.626, 1.426], loss: 5.409465, mae: 29.790972, mean_q: 39.695839
   78829/1100000: episode: 138, duration: 5.479s, episode steps: 845, steps per second: 154, episode reward: -217.583, mean reward: -0.257 [-100.000, 27.366], mean action: 1.469 [0.000, 3.000], mean observation: 0.026 [-0.787, 1.594], loss: 4.138180, mae: 29.951927, mean_q: 39.988056
   79154/1100000: episode: 139, duration: 1.985s, episode steps: 325, steps per second: 164, episode reward: 270.545, mean reward: 0.832 [-19.872, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.100 [-0.627, 1.433], loss: 4.003766, mae: 29.520151, mean_q: 39.410519
   79778/1100000: episode: 140, duration: 3.831s, episode steps: 624, steps per second: 163, episode reward: 256.874, mean reward: 0.412 [-20.876, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.077 [-0.811, 1.422], loss: 5.582598, mae: 29.749308, mean_q: 39.712059
   80778/1100000: episode: 141, duration: 6.549s, episode steps: 1000, steps per second: 153, episode reward: -56.257, mean reward: -0.056 [-12.303, 11.667], mean action: 1.687 [0.000, 3.000], mean observation: 0.084 [-0.685, 1.401], loss: 5.042895, mae: 29.732067, mean_q: 39.604507
   81033/1100000: episode: 142, duration: 1.528s, episode steps: 255, steps per second: 167, episode reward: 220.879, mean reward: 0.866 [-14.729, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.018 [-0.922, 1.390], loss: 3.316852, mae: 29.890823, mean_q: 39.822170
   81923/1100000: episode: 143, duration: 5.768s, episode steps: 890, steps per second: 154, episode reward: 116.928, mean reward: 0.131 [-14.501, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: -0.013 [-1.293, 1.404], loss: 5.694329, mae: 29.723736, mean_q: 39.556648
   82339/1100000: episode: 144, duration: 2.541s, episode steps: 416, steps per second: 164, episode reward: 245.671, mean reward: 0.591 [-20.794, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.035 [-0.888, 1.402], loss: 2.841927, mae: 30.232788, mean_q: 40.366146
   83339/1100000: episode: 145, duration: 7.096s, episode steps: 1000, steps per second: 141, episode reward: 0.802, mean reward: 0.001 [-9.892, 14.450], mean action: 1.733 [0.000, 3.000], mean observation: 0.082 [-0.556, 1.402], loss: 3.235780, mae: 30.319246, mean_q: 40.537907
   83661/1100000: episode: 146, duration: 1.942s, episode steps: 322, steps per second: 166, episode reward: 228.679, mean reward: 0.710 [-3.082, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.036 [-0.908, 1.413], loss: 2.752461, mae: 30.405104, mean_q: 40.726147
   83952/1100000: episode: 147, duration: 1.692s, episode steps: 291, steps per second: 172, episode reward: 225.252, mean reward: 0.774 [-11.222, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.056 [-0.783, 1.431], loss: 2.994832, mae: 30.102308, mean_q: 40.219154
   84181/1100000: episode: 148, duration: 1.354s, episode steps: 229, steps per second: 169, episode reward: 253.285, mean reward: 1.106 [-11.365, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.068 [-0.625, 1.406], loss: 3.240629, mae: 30.302727, mean_q: 40.518372
   85181/1100000: episode: 149, duration: 6.169s, episode steps: 1000, steps per second: 162, episode reward: 105.887, mean reward: 0.106 [-23.615, 23.077], mean action: 0.954 [0.000, 3.000], mean observation: 0.198 [-1.239, 1.518], loss: 3.957240, mae: 30.790886, mean_q: 40.941967
   86181/1100000: episode: 150, duration: 6.979s, episode steps: 1000, steps per second: 143, episode reward: -30.899, mean reward: -0.031 [-5.372, 5.074], mean action: 1.622 [0.000, 3.000], mean observation: 0.107 [-0.615, 1.436], loss: 5.164120, mae: 30.915087, mean_q: 40.989761
   86816/1100000: episode: 151, duration: 4.314s, episode steps: 635, steps per second: 147, episode reward: 201.743, mean reward: 0.318 [-11.314, 100.000], mean action: 1.557 [0.000, 3.000], mean observation: 0.120 [-0.610, 1.402], loss: 3.634483, mae: 31.223259, mean_q: 41.628853
   87294/1100000: episode: 152, duration: 2.910s, episode steps: 478, steps per second: 164, episode reward: 238.973, mean reward: 0.500 [-20.392, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: 0.081 [-1.020, 1.387], loss: 4.255127, mae: 31.149330, mean_q: 41.633499
   87995/1100000: episode: 153, duration: 4.302s, episode steps: 701, steps per second: 163, episode reward: -331.700, mean reward: -0.473 [-100.000, 19.778], mean action: 1.485 [0.000, 3.000], mean observation: 0.084 [-2.199, 1.405], loss: 4.485983, mae: 31.289856, mean_q: 41.829483
   88995/1100000: episode: 154, duration: 6.848s, episode steps: 1000, steps per second: 146, episode reward: -65.989, mean reward: -0.066 [-5.527, 5.023], mean action: 1.836 [0.000, 3.000], mean observation: 0.084 [-0.541, 1.412], loss: 5.288262, mae: 31.087713, mean_q: 41.585697
   89852/1100000: episode: 155, duration: 5.604s, episode steps: 857, steps per second: 153, episode reward: 214.808, mean reward: 0.251 [-19.573, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.132 [-0.648, 1.404], loss: 5.655969, mae: 31.294930, mean_q: 41.889839
   90852/1100000: episode: 156, duration: 7.249s, episode steps: 1000, steps per second: 138, episode reward: -36.052, mean reward: -0.036 [-6.017, 5.915], mean action: 1.836 [0.000, 3.000], mean observation: 0.077 [-0.929, 1.397], loss: 5.541584, mae: 31.870871, mean_q: 42.676239
   91099/1100000: episode: 157, duration: 1.419s, episode steps: 247, steps per second: 174, episode reward: 258.155, mean reward: 1.045 [-13.157, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.154 [-0.603, 1.419], loss: 5.243552, mae: 31.652672, mean_q: 42.459263
   91639/1100000: episode: 158, duration: 3.379s, episode steps: 540, steps per second: 160, episode reward: 217.519, mean reward: 0.403 [-12.735, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.004 [-1.112, 1.408], loss: 4.464267, mae: 32.237976, mean_q: 43.201931
   91957/1100000: episode: 159, duration: 1.911s, episode steps: 318, steps per second: 166, episode reward: -197.647, mean reward: -0.622 [-100.000, 16.452], mean action: 1.912 [0.000, 3.000], mean observation: 0.084 [-0.734, 1.561], loss: 4.763212, mae: 32.540203, mean_q: 43.618393
   92863/1100000: episode: 160, duration: 5.978s, episode steps: 906, steps per second: 152, episode reward: 97.594, mean reward: 0.108 [-13.178, 100.000], mean action: 1.757 [0.000, 3.000], mean observation: 0.095 [-0.983, 1.450], loss: 5.008492, mae: 32.937420, mean_q: 44.085644
   93168/1100000: episode: 161, duration: 1.782s, episode steps: 305, steps per second: 171, episode reward: 253.987, mean reward: 0.833 [-17.429, 100.000], mean action: 1.839 [0.000, 3.000], mean observation: 0.012 [-0.717, 1.486], loss: 4.295631, mae: 33.121864, mean_q: 44.377182
   94168/1100000: episode: 162, duration: 6.291s, episode steps: 1000, steps per second: 159, episode reward: -6.234, mean reward: -0.006 [-23.200, 29.737], mean action: 1.909 [0.000, 3.000], mean observation: 0.056 [-1.625, 1.393], loss: 4.879233, mae: 33.450630, mean_q: 44.841324
   95168/1100000: episode: 163, duration: 6.521s, episode steps: 1000, steps per second: 153, episode reward: -73.085, mean reward: -0.073 [-5.452, 5.668], mean action: 1.762 [0.000, 3.000], mean observation: 0.081 [-0.803, 1.403], loss: 5.908361, mae: 33.961998, mean_q: 45.534023
   96040/1100000: episode: 164, duration: 5.515s, episode steps: 872, steps per second: 158, episode reward: 163.751, mean reward: 0.188 [-20.471, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.184 [-0.658, 1.403], loss: 4.568362, mae: 34.389568, mean_q: 46.173855
   96493/1100000: episode: 165, duration: 2.698s, episode steps: 453, steps per second: 168, episode reward: -205.989, mean reward: -0.455 [-100.000, 16.356], mean action: 1.912 [0.000, 3.000], mean observation: 0.100 [-0.661, 1.492], loss: 5.397130, mae: 34.769154, mean_q: 46.559010
   97181/1100000: episode: 166, duration: 4.057s, episode steps: 688, steps per second: 170, episode reward: 204.142, mean reward: 0.297 [-19.684, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.205 [-0.610, 1.408], loss: 5.372126, mae: 35.355740, mean_q: 47.397213
   97581/1100000: episode: 167, duration: 2.308s, episode steps: 400, steps per second: 173, episode reward: 221.636, mean reward: 0.554 [-18.068, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.207 [-0.995, 1.431], loss: 7.400521, mae: 35.609631, mean_q: 47.779751
   98581/1100000: episode: 168, duration: 7.069s, episode steps: 1000, steps per second: 141, episode reward: 7.464, mean reward: 0.007 [-21.060, 24.051], mean action: 1.689 [0.000, 3.000], mean observation: 0.013 [-1.049, 1.426], loss: 6.341353, mae: 35.959152, mean_q: 48.224659
   98721/1100000: episode: 169, duration: 0.802s, episode steps: 140, steps per second: 175, episode reward: 66.142, mean reward: 0.472 [-100.000, 21.445], mean action: 1.693 [0.000, 3.000], mean observation: 0.020 [-1.199, 1.401], loss: 5.087385, mae: 36.858574, mean_q: 49.575012
   98890/1100000: episode: 170, duration: 0.965s, episode steps: 169, steps per second: 175, episode reward: 59.148, mean reward: 0.350 [-100.000, 25.161], mean action: 1.905 [0.000, 3.000], mean observation: 0.006 [-1.700, 1.456], loss: 6.946115, mae: 36.858631, mean_q: 49.471649
   99441/1100000: episode: 171, duration: 3.417s, episode steps: 551, steps per second: 161, episode reward: 222.165, mean reward: 0.403 [-10.557, 100.000], mean action: 1.722 [0.000, 3.000], mean observation: -0.044 [-0.792, 1.443], loss: 6.114717, mae: 36.575504, mean_q: 49.027218
   99922/1100000: episode: 172, duration: 2.915s, episode steps: 481, steps per second: 165, episode reward: 267.207, mean reward: 0.556 [-17.470, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.126 [-0.800, 1.512], loss: 5.544539, mae: 37.161076, mean_q: 49.685585
  100228/1100000: episode: 173, duration: 1.751s, episode steps: 306, steps per second: 175, episode reward: 229.564, mean reward: 0.750 [-19.492, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.184 [-0.687, 1.460], loss: 5.495120, mae: 37.407677, mean_q: 50.206139
  100533/1100000: episode: 174, duration: 1.784s, episode steps: 305, steps per second: 171, episode reward: 245.325, mean reward: 0.804 [-7.953, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: -0.029 [-0.944, 1.408], loss: 6.535048, mae: 37.463810, mean_q: 50.186378
  100820/1100000: episode: 175, duration: 1.666s, episode steps: 287, steps per second: 172, episode reward: 231.364, mean reward: 0.806 [-11.921, 100.000], mean action: 1.976 [0.000, 3.000], mean observation: 0.022 [-1.040, 1.396], loss: 7.232263, mae: 37.808418, mean_q: 50.591244
  101173/1100000: episode: 176, duration: 2.055s, episode steps: 353, steps per second: 172, episode reward: 246.255, mean reward: 0.698 [-18.883, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.196 [-0.765, 1.402], loss: 6.158986, mae: 37.945221, mean_q: 50.945889
  101726/1100000: episode: 177, duration: 3.431s, episode steps: 553, steps per second: 161, episode reward: 223.888, mean reward: 0.405 [-19.736, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.133 [-0.868, 1.407], loss: 4.143603, mae: 38.004482, mean_q: 50.897068
  102621/1100000: episode: 178, duration: 5.463s, episode steps: 895, steps per second: 164, episode reward: -274.886, mean reward: -0.307 [-100.000, 12.431], mean action: 1.790 [0.000, 3.000], mean observation: 0.075 [-0.932, 2.338], loss: 7.116177, mae: 38.601788, mean_q: 51.698959
  103082/1100000: episode: 179, duration: 2.782s, episode steps: 461, steps per second: 166, episode reward: 249.805, mean reward: 0.542 [-20.714, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.113 [-0.554, 1.429], loss: 5.508883, mae: 38.612129, mean_q: 51.617229
  103188/1100000: episode: 180, duration: 0.602s, episode steps: 106, steps per second: 176, episode reward: 3.499, mean reward: 0.033 [-100.000, 9.994], mean action: 1.613 [0.000, 3.000], mean observation: 0.134 [-1.357, 1.405], loss: 2.023201, mae: 38.430882, mean_q: 51.527069
  103456/1100000: episode: 181, duration: 1.551s, episode steps: 268, steps per second: 173, episode reward: 229.056, mean reward: 0.855 [-14.385, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.063 [-0.812, 1.391], loss: 5.653752, mae: 39.230064, mean_q: 52.573872
  103548/1100000: episode: 182, duration: 0.521s, episode steps: 92, steps per second: 176, episode reward: -30.300, mean reward: -0.329 [-100.000, 16.161], mean action: 1.293 [0.000, 3.000], mean observation: 0.019 [-2.593, 1.461], loss: 4.667727, mae: 39.324783, mean_q: 53.013657
  103825/1100000: episode: 183, duration: 1.617s, episode steps: 277, steps per second: 171, episode reward: 264.063, mean reward: 0.953 [-17.797, 100.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.071 [-0.703, 1.513], loss: 4.936214, mae: 38.891010, mean_q: 52.188091
  104433/1100000: episode: 184, duration: 3.776s, episode steps: 608, steps per second: 161, episode reward: 222.201, mean reward: 0.365 [-19.417, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: -0.006 [-0.862, 1.473], loss: 4.200879, mae: 39.213821, mean_q: 52.533543
  105433/1100000: episode: 185, duration: 7.438s, episode steps: 1000, steps per second: 134, episode reward: 31.200, mean reward: 0.031 [-20.339, 22.874], mean action: 1.514 [0.000, 3.000], mean observation: -0.037 [-0.600, 1.396], loss: 6.143474, mae: 39.536087, mean_q: 52.944710
  105778/1100000: episode: 186, duration: 2.017s, episode steps: 345, steps per second: 171, episode reward: 233.617, mean reward: 0.677 [-11.312, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: -0.024 [-0.779, 1.398], loss: 5.644356, mae: 39.430870, mean_q: 52.922577
  106113/1100000: episode: 187, duration: 1.963s, episode steps: 335, steps per second: 171, episode reward: 262.725, mean reward: 0.784 [-17.437, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.115 [-0.831, 1.433], loss: 6.750996, mae: 39.944675, mean_q: 53.645679
  107113/1100000: episode: 188, duration: 6.775s, episode steps: 1000, steps per second: 148, episode reward: 118.130, mean reward: 0.118 [-24.256, 32.992], mean action: 1.822 [0.000, 3.000], mean observation: 0.053 [-0.802, 1.390], loss: 5.050287, mae: 39.908051, mean_q: 53.446815
  107429/1100000: episode: 189, duration: 1.870s, episode steps: 316, steps per second: 169, episode reward: 261.213, mean reward: 0.827 [-9.843, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.117 [-0.982, 1.397], loss: 4.632313, mae: 40.356846, mean_q: 54.154892
  107891/1100000: episode: 190, duration: 2.780s, episode steps: 462, steps per second: 166, episode reward: 152.326, mean reward: 0.330 [-21.910, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: -0.019 [-0.876, 1.399], loss: 4.896165, mae: 40.156277, mean_q: 53.815941
  108694/1100000: episode: 191, duration: 4.986s, episode steps: 803, steps per second: 161, episode reward: 210.106, mean reward: 0.262 [-23.290, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.185 [-0.549, 1.400], loss: 4.378458, mae: 40.233372, mean_q: 53.977036
  109024/1100000: episode: 192, duration: 1.941s, episode steps: 330, steps per second: 170, episode reward: 199.067, mean reward: 0.603 [-17.125, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.181 [-0.564, 1.411], loss: 4.975563, mae: 40.117867, mean_q: 53.949463
  109505/1100000: episode: 193, duration: 2.992s, episode steps: 481, steps per second: 161, episode reward: 257.492, mean reward: 0.535 [-10.167, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: -0.014 [-0.700, 1.404], loss: 7.623587, mae: 40.373871, mean_q: 54.247589
  109603/1100000: episode: 194, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: -30.473, mean reward: -0.311 [-100.000, 7.751], mean action: 1.827 [0.000, 3.000], mean observation: 0.021 [-1.630, 1.409], loss: 6.074964, mae: 39.993118, mean_q: 53.765354
  109696/1100000: episode: 195, duration: 0.528s, episode steps: 93, steps per second: 176, episode reward: -35.527, mean reward: -0.382 [-100.000, 9.893], mean action: 1.387 [0.000, 3.000], mean observation: -0.000 [-1.418, 1.391], loss: 5.388967, mae: 40.110416, mean_q: 54.217144
  110587/1100000: episode: 196, duration: 5.598s, episode steps: 891, steps per second: 159, episode reward: 185.341, mean reward: 0.208 [-20.234, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: -0.007 [-0.653, 1.446], loss: 5.966715, mae: 40.163185, mean_q: 54.001026
  110849/1100000: episode: 197, duration: 1.562s, episode steps: 262, steps per second: 168, episode reward: 221.507, mean reward: 0.845 [-12.567, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.167 [-0.745, 1.397], loss: 7.237476, mae: 40.059563, mean_q: 53.783180
  110924/1100000: episode: 198, duration: 0.431s, episode steps: 75, steps per second: 174, episode reward: -4.194, mean reward: -0.056 [-100.000, 11.515], mean action: 1.480 [0.000, 3.000], mean observation: -0.004 [-1.230, 1.386], loss: 7.488706, mae: 40.208626, mean_q: 54.180038
  111386/1100000: episode: 199, duration: 2.843s, episode steps: 462, steps per second: 162, episode reward: 218.944, mean reward: 0.474 [-18.815, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.069 [-0.787, 1.436], loss: 7.600249, mae: 39.885117, mean_q: 53.612732
  111441/1100000: episode: 200, duration: 0.318s, episode steps: 55, steps per second: 173, episode reward: -173.295, mean reward: -3.151 [-100.000, 11.019], mean action: 1.927 [0.000, 3.000], mean observation: -0.191 [-1.745, 1.386], loss: 5.335810, mae: 40.045467, mean_q: 53.683861
  112175/1100000: episode: 201, duration: 4.745s, episode steps: 734, steps per second: 155, episode reward: 212.459, mean reward: 0.289 [-19.360, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.137 [-0.852, 1.487], loss: 6.551967, mae: 39.883064, mean_q: 53.753532
  112355/1100000: episode: 202, duration: 1.032s, episode steps: 180, steps per second: 174, episode reward: 4.807, mean reward: 0.027 [-100.000, 12.818], mean action: 1.750 [0.000, 3.000], mean observation: -0.037 [-1.359, 1.401], loss: 4.780040, mae: 39.685440, mean_q: 53.487587
  112547/1100000: episode: 203, duration: 1.085s, episode steps: 192, steps per second: 177, episode reward: 10.738, mean reward: 0.056 [-100.000, 27.003], mean action: 1.453 [0.000, 3.000], mean observation: 0.070 [-2.454, 1.524], loss: 7.369160, mae: 40.235748, mean_q: 54.200195
  112631/1100000: episode: 204, duration: 0.479s, episode steps: 84, steps per second: 176, episode reward: -32.913, mean reward: -0.392 [-100.000, 8.803], mean action: 1.274 [0.000, 3.000], mean observation: 0.028 [-1.388, 1.395], loss: 3.387391, mae: 39.534519, mean_q: 53.170555
  113631/1100000: episode: 205, duration: 6.364s, episode steps: 1000, steps per second: 157, episode reward: 31.311, mean reward: 0.031 [-19.738, 23.301], mean action: 1.365 [0.000, 3.000], mean observation: 0.066 [-1.039, 1.454], loss: 7.786483, mae: 39.843578, mean_q: 53.648197
  114631/1100000: episode: 206, duration: 7.000s, episode steps: 1000, steps per second: 143, episode reward: 96.013, mean reward: 0.096 [-19.573, 13.802], mean action: 1.167 [0.000, 3.000], mean observation: 0.130 [-0.632, 1.398], loss: 5.701956, mae: 39.987007, mean_q: 53.849155
  115140/1100000: episode: 207, duration: 3.066s, episode steps: 509, steps per second: 166, episode reward: 263.722, mean reward: 0.518 [-9.512, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.029 [-0.808, 1.526], loss: 5.806457, mae: 39.727028, mean_q: 53.539913
  115431/1100000: episode: 208, duration: 1.694s, episode steps: 291, steps per second: 172, episode reward: -44.218, mean reward: -0.152 [-100.000, 16.393], mean action: 1.725 [0.000, 3.000], mean observation: -0.015 [-0.830, 1.457], loss: 6.989874, mae: 39.809399, mean_q: 53.501698
  116000/1100000: episode: 209, duration: 3.577s, episode steps: 569, steps per second: 159, episode reward: 183.321, mean reward: 0.322 [-18.434, 100.000], mean action: 2.007 [0.000, 3.000], mean observation: 0.083 [-0.600, 1.474], loss: 7.318763, mae: 39.798515, mean_q: 53.510258
  116583/1100000: episode: 210, duration: 3.681s, episode steps: 583, steps per second: 158, episode reward: 119.100, mean reward: 0.204 [-10.076, 100.000], mean action: 1.930 [0.000, 3.000], mean observation: 0.041 [-0.728, 1.414], loss: 5.112746, mae: 39.683884, mean_q: 53.465359
  116876/1100000: episode: 211, duration: 1.777s, episode steps: 293, steps per second: 165, episode reward: -36.874, mean reward: -0.126 [-100.000, 9.039], mean action: 1.700 [0.000, 3.000], mean observation: 0.088 [-0.580, 1.968], loss: 6.924892, mae: 39.690693, mean_q: 53.400585
  117695/1100000: episode: 212, duration: 5.226s, episode steps: 819, steps per second: 157, episode reward: 164.752, mean reward: 0.201 [-20.535, 100.000], mean action: 2.170 [0.000, 3.000], mean observation: 0.088 [-0.974, 1.393], loss: 7.464302, mae: 39.833767, mean_q: 53.570301
  117914/1100000: episode: 213, duration: 1.260s, episode steps: 219, steps per second: 174, episode reward: -10.580, mean reward: -0.048 [-100.000, 10.587], mean action: 1.763 [0.000, 3.000], mean observation: 0.144 [-0.693, 1.447], loss: 7.806946, mae: 39.798119, mean_q: 53.667282
  118414/1100000: episode: 214, duration: 3.098s, episode steps: 500, steps per second: 161, episode reward: 197.842, mean reward: 0.396 [-22.719, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: -0.053 [-0.867, 1.404], loss: 6.705060, mae: 39.501511, mean_q: 53.189274
  118805/1100000: episode: 215, duration: 2.307s, episode steps: 391, steps per second: 169, episode reward: 180.523, mean reward: 0.462 [-8.658, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: -0.040 [-0.600, 1.406], loss: 5.683526, mae: 39.521805, mean_q: 53.154881
  119339/1100000: episode: 216, duration: 3.364s, episode steps: 534, steps per second: 159, episode reward: 161.136, mean reward: 0.302 [-3.387, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: -0.056 [-0.600, 1.428], loss: 3.560927, mae: 39.389309, mean_q: 53.007587
  120059/1100000: episode: 217, duration: 4.814s, episode steps: 720, steps per second: 150, episode reward: 89.682, mean reward: 0.125 [-18.990, 100.000], mean action: 1.753 [0.000, 3.000], mean observation: 0.221 [-0.583, 1.470], loss: 5.653174, mae: 39.327934, mean_q: 52.832413
  121059/1100000: episode: 218, duration: 6.155s, episode steps: 1000, steps per second: 162, episode reward: 121.964, mean reward: 0.122 [-21.037, 25.376], mean action: 1.557 [0.000, 3.000], mean observation: 0.201 [-1.170, 1.412], loss: 7.523125, mae: 39.586407, mean_q: 53.258640
  121261/1100000: episode: 219, duration: 1.157s, episode steps: 202, steps per second: 175, episode reward: 23.674, mean reward: 0.117 [-100.000, 16.415], mean action: 1.658 [0.000, 3.000], mean observation: -0.035 [-0.600, 1.463], loss: 5.337895, mae: 39.746349, mean_q: 53.396976
  121855/1100000: episode: 220, duration: 3.778s, episode steps: 594, steps per second: 157, episode reward: 233.530, mean reward: 0.393 [-14.808, 100.000], mean action: 1.697 [0.000, 3.000], mean observation: 0.108 [-0.722, 1.458], loss: 5.362164, mae: 39.913082, mean_q: 53.767059
  122318/1100000: episode: 221, duration: 2.779s, episode steps: 463, steps per second: 167, episode reward: 219.531, mean reward: 0.474 [-12.747, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.067 [-0.586, 1.409], loss: 6.893744, mae: 40.008209, mean_q: 53.793644
  123078/1100000: episode: 222, duration: 4.610s, episode steps: 760, steps per second: 165, episode reward: 206.862, mean reward: 0.272 [-20.137, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.146 [-0.795, 1.388], loss: 6.183810, mae: 40.219460, mean_q: 54.132320
  123670/1100000: episode: 223, duration: 3.959s, episode steps: 592, steps per second: 150, episode reward: 227.670, mean reward: 0.385 [-17.970, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.111 [-0.700, 1.397], loss: 4.614455, mae: 39.852623, mean_q: 53.576561
  123898/1100000: episode: 224, duration: 1.337s, episode steps: 228, steps per second: 171, episode reward: 40.011, mean reward: 0.175 [-100.000, 15.690], mean action: 1.868 [0.000, 3.000], mean observation: -0.037 [-0.660, 1.387], loss: 7.528953, mae: 40.020962, mean_q: 53.834679
  124860/1100000: episode: 225, duration: 5.992s, episode steps: 962, steps per second: 161, episode reward: 150.177, mean reward: 0.156 [-23.127, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: -0.049 [-0.600, 1.441], loss: 6.874791, mae: 39.969486, mean_q: 53.584042
  125557/1100000: episode: 226, duration: 4.539s, episode steps: 697, steps per second: 154, episode reward: 192.185, mean reward: 0.276 [-20.167, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.257 [-0.897, 1.397], loss: 5.671273, mae: 40.133347, mean_q: 53.850189
  125934/1100000: episode: 227, duration: 2.231s, episode steps: 377, steps per second: 169, episode reward: 223.287, mean reward: 0.592 [-12.026, 100.000], mean action: 1.947 [0.000, 3.000], mean observation: 0.064 [-0.658, 1.445], loss: 6.315537, mae: 40.015186, mean_q: 53.676979
  126460/1100000: episode: 228, duration: 3.390s, episode steps: 526, steps per second: 155, episode reward: 133.051, mean reward: 0.253 [-12.399, 100.000], mean action: 1.700 [0.000, 3.000], mean observation: 0.181 [-0.587, 1.414], loss: 5.023149, mae: 39.946522, mean_q: 53.384678
  126845/1100000: episode: 229, duration: 2.362s, episode steps: 385, steps per second: 163, episode reward: 206.276, mean reward: 0.536 [-8.201, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.159 [-0.726, 1.411], loss: 9.386783, mae: 39.918659, mean_q: 53.506245
  127845/1100000: episode: 230, duration: 6.805s, episode steps: 1000, steps per second: 147, episode reward: 50.984, mean reward: 0.051 [-23.193, 12.644], mean action: 1.371 [0.000, 3.000], mean observation: -0.028 [-0.726, 1.457], loss: 6.986436, mae: 39.742886, mean_q: 53.313175
  128318/1100000: episode: 231, duration: 2.778s, episode steps: 473, steps per second: 170, episode reward: 210.478, mean reward: 0.445 [-9.698, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.189 [-0.772, 1.453], loss: 4.695039, mae: 40.090401, mean_q: 53.759270
  128878/1100000: episode: 232, duration: 3.462s, episode steps: 560, steps per second: 162, episode reward: 219.113, mean reward: 0.391 [-18.504, 100.000], mean action: 0.936 [0.000, 3.000], mean observation: 0.106 [-0.692, 1.405], loss: 6.521497, mae: 40.468323, mean_q: 54.314941
  129335/1100000: episode: 233, duration: 2.723s, episode steps: 457, steps per second: 168, episode reward: 226.967, mean reward: 0.497 [-9.129, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: -0.036 [-0.600, 1.407], loss: 4.384163, mae: 40.390789, mean_q: 54.261051
  129583/1100000: episode: 234, duration: 1.444s, episode steps: 248, steps per second: 172, episode reward: 302.648, mean reward: 1.220 [-3.293, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.150 [-1.018, 1.390], loss: 5.446070, mae: 40.224026, mean_q: 54.033592
  129919/1100000: episode: 235, duration: 1.962s, episode steps: 336, steps per second: 171, episode reward: 272.730, mean reward: 0.812 [-17.357, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.085 [-0.730, 1.415], loss: 3.773254, mae: 40.053814, mean_q: 53.843559
  130235/1100000: episode: 236, duration: 1.838s, episode steps: 316, steps per second: 172, episode reward: 262.172, mean reward: 0.830 [-21.551, 100.000], mean action: 1.937 [0.000, 3.000], mean observation: 0.051 [-0.916, 1.456], loss: 9.123086, mae: 40.397308, mean_q: 54.437622
  131131/1100000: episode: 237, duration: 5.592s, episode steps: 896, steps per second: 160, episode reward: 258.692, mean reward: 0.289 [-20.319, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.166 [-0.677, 1.402], loss: 6.076931, mae: 40.749931, mean_q: 54.716545
  131564/1100000: episode: 238, duration: 2.656s, episode steps: 433, steps per second: 163, episode reward: 213.011, mean reward: 0.492 [-10.426, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.093 [-0.620, 1.416], loss: 4.476775, mae: 41.047218, mean_q: 55.097549
  131942/1100000: episode: 239, duration: 2.256s, episode steps: 378, steps per second: 168, episode reward: 258.265, mean reward: 0.683 [-17.540, 100.000], mean action: 1.569 [0.000, 3.000], mean observation: 0.087 [-0.950, 1.407], loss: 6.136436, mae: 41.190468, mean_q: 55.402924
  132059/1100000: episode: 240, duration: 0.669s, episode steps: 117, steps per second: 175, episode reward: 38.095, mean reward: 0.326 [-100.000, 15.540], mean action: 1.906 [0.000, 3.000], mean observation: 0.166 [-1.034, 1.400], loss: 6.608504, mae: 41.245056, mean_q: 55.514164
  132966/1100000: episode: 241, duration: 5.705s, episode steps: 907, steps per second: 159, episode reward: 177.461, mean reward: 0.196 [-18.780, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.108 [-0.600, 1.480], loss: 6.170603, mae: 41.294266, mean_q: 55.466076
  133158/1100000: episode: 242, duration: 1.095s, episode steps: 192, steps per second: 175, episode reward: -56.185, mean reward: -0.293 [-100.000, 16.525], mean action: 1.578 [0.000, 3.000], mean observation: 0.038 [-1.452, 1.402], loss: 8.243880, mae: 41.094589, mean_q: 55.209396
  133463/1100000: episode: 243, duration: 1.854s, episode steps: 305, steps per second: 164, episode reward: 182.214, mean reward: 0.597 [-17.787, 100.000], mean action: 2.144 [0.000, 3.000], mean observation: 0.126 [-0.649, 1.403], loss: 6.698113, mae: 40.985283, mean_q: 55.057018
  134282/1100000: episode: 244, duration: 5.372s, episode steps: 819, steps per second: 152, episode reward: 130.872, mean reward: 0.160 [-17.005, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: -0.035 [-1.113, 1.400], loss: 6.649812, mae: 40.842537, mean_q: 54.618179
  134532/1100000: episode: 245, duration: 1.434s, episode steps: 250, steps per second: 174, episode reward: 230.126, mean reward: 0.921 [-8.287, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.078 [-1.204, 1.407], loss: 6.676569, mae: 40.321392, mean_q: 54.052128
  135132/1100000: episode: 246, duration: 3.678s, episode steps: 600, steps per second: 163, episode reward: 144.772, mean reward: 0.241 [-18.135, 100.000], mean action: 1.995 [0.000, 3.000], mean observation: 0.037 [-0.781, 1.430], loss: 6.022475, mae: 41.069328, mean_q: 55.139454
  135439/1100000: episode: 247, duration: 1.811s, episode steps: 307, steps per second: 170, episode reward: -76.991, mean reward: -0.251 [-100.000, 4.850], mean action: 1.860 [0.000, 3.000], mean observation: 0.212 [-0.745, 1.396], loss: 6.214839, mae: 41.142487, mean_q: 55.063271
  135842/1100000: episode: 248, duration: 2.377s, episode steps: 403, steps per second: 170, episode reward: 207.190, mean reward: 0.514 [-13.844, 100.000], mean action: 1.911 [0.000, 3.000], mean observation: -0.036 [-0.608, 1.408], loss: 5.590762, mae: 41.093609, mean_q: 55.086445
  136037/1100000: episode: 249, duration: 1.132s, episode steps: 195, steps per second: 172, episode reward: 58.089, mean reward: 0.298 [-100.000, 15.375], mean action: 1.856 [0.000, 3.000], mean observation: 0.038 [-0.901, 1.410], loss: 13.533464, mae: 40.377323, mean_q: 54.255600
  136827/1100000: episode: 250, duration: 4.834s, episode steps: 790, steps per second: 163, episode reward: 169.365, mean reward: 0.214 [-17.998, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: -0.013 [-0.600, 1.407], loss: 5.325252, mae: 40.591911, mean_q: 54.558464
  137227/1100000: episode: 251, duration: 2.396s, episode steps: 400, steps per second: 167, episode reward: 211.472, mean reward: 0.529 [-13.093, 100.000], mean action: 1.915 [0.000, 3.000], mean observation: 0.018 [-0.600, 1.412], loss: 7.054986, mae: 40.658005, mean_q: 54.660458
  137837/1100000: episode: 252, duration: 3.871s, episode steps: 610, steps per second: 158, episode reward: 240.861, mean reward: 0.395 [-20.553, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.092 [-0.550, 1.432], loss: 6.168856, mae: 40.252998, mean_q: 54.140919
  138244/1100000: episode: 253, duration: 2.390s, episode steps: 407, steps per second: 170, episode reward: 227.630, mean reward: 0.559 [-10.654, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.203 [-0.682, 1.501], loss: 4.650548, mae: 40.492821, mean_q: 54.434593
  139010/1100000: episode: 254, duration: 4.852s, episode steps: 766, steps per second: 158, episode reward: 195.171, mean reward: 0.255 [-11.164, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.145 [-0.511, 1.388], loss: 6.645656, mae: 40.359592, mean_q: 54.213074
  139569/1100000: episode: 255, duration: 3.333s, episode steps: 559, steps per second: 168, episode reward: 128.641, mean reward: 0.230 [-13.975, 100.000], mean action: 1.852 [0.000, 3.000], mean observation: -0.046 [-0.600, 1.415], loss: 5.662874, mae: 40.052963, mean_q: 53.789589
  139994/1100000: episode: 256, duration: 2.583s, episode steps: 425, steps per second: 165, episode reward: 187.672, mean reward: 0.442 [-19.573, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.033 [-0.665, 1.424], loss: 4.967109, mae: 40.260284, mean_q: 54.131866
  140284/1100000: episode: 257, duration: 1.697s, episode steps: 290, steps per second: 171, episode reward: 1.893, mean reward: 0.007 [-100.000, 12.070], mean action: 1.652 [0.000, 3.000], mean observation: 0.126 [-0.698, 1.395], loss: 8.159826, mae: 40.597992, mean_q: 54.428185
  140650/1100000: episode: 258, duration: 2.183s, episode steps: 366, steps per second: 168, episode reward: -44.933, mean reward: -0.123 [-100.000, 16.014], mean action: 1.735 [0.000, 3.000], mean observation: 0.107 [-0.830, 1.471], loss: 6.316489, mae: 39.950951, mean_q: 53.823494
  141047/1100000: episode: 259, duration: 2.440s, episode steps: 397, steps per second: 163, episode reward: 277.870, mean reward: 0.700 [-8.256, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: -0.058 [-0.863, 1.522], loss: 5.806575, mae: 40.602531, mean_q: 54.499695
  141394/1100000: episode: 260, duration: 2.081s, episode steps: 347, steps per second: 167, episode reward: 0.141, mean reward: 0.000 [-100.000, 17.034], mean action: 1.827 [0.000, 3.000], mean observation: -0.084 [-1.339, 1.389], loss: 5.264676, mae: 40.672523, mean_q: 54.687561
  141752/1100000: episode: 261, duration: 2.148s, episode steps: 358, steps per second: 167, episode reward: 217.778, mean reward: 0.608 [-7.999, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: -0.002 [-0.941, 1.526], loss: 8.187090, mae: 40.268177, mean_q: 54.096855
  142151/1100000: episode: 262, duration: 2.436s, episode steps: 399, steps per second: 164, episode reward: -20.532, mean reward: -0.051 [-100.000, 14.376], mean action: 1.594 [0.000, 3.000], mean observation: 0.120 [-0.725, 1.405], loss: 4.693684, mae: 40.411301, mean_q: 54.416748
  143151/1100000: episode: 263, duration: 7.302s, episode steps: 1000, steps per second: 137, episode reward: -66.833, mean reward: -0.067 [-5.441, 4.890], mean action: 1.722 [0.000, 3.000], mean observation: 0.051 [-0.685, 1.387], loss: 6.741212, mae: 40.589233, mean_q: 54.573101
  143794/1100000: episode: 264, duration: 4.106s, episode steps: 643, steps per second: 157, episode reward: 183.879, mean reward: 0.286 [-19.515, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: -0.030 [-0.707, 1.479], loss: 6.325726, mae: 40.650127, mean_q: 54.698788
  144569/1100000: episode: 265, duration: 5.001s, episode steps: 775, steps per second: 155, episode reward: 135.344, mean reward: 0.175 [-11.770, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: -0.060 [-0.744, 1.412], loss: 7.185199, mae: 40.443707, mean_q: 54.461502
  145019/1100000: episode: 266, duration: 2.792s, episode steps: 450, steps per second: 161, episode reward: 204.126, mean reward: 0.454 [-18.444, 100.000], mean action: 2.200 [0.000, 3.000], mean observation: 0.071 [-0.666, 1.386], loss: 6.360765, mae: 40.279877, mean_q: 54.300217
  145233/1100000: episode: 267, duration: 1.233s, episode steps: 214, steps per second: 174, episode reward: -98.128, mean reward: -0.459 [-100.000, 18.064], mean action: 1.738 [0.000, 3.000], mean observation: -0.174 [-1.394, 1.390], loss: 5.521251, mae: 40.654060, mean_q: 54.731533
  145577/1100000: episode: 268, duration: 2.034s, episode steps: 344, steps per second: 169, episode reward: 227.690, mean reward: 0.662 [-3.919, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.157 [-0.768, 1.394], loss: 6.545700, mae: 40.729359, mean_q: 54.741631
  145908/1100000: episode: 269, duration: 2.018s, episode steps: 331, steps per second: 164, episode reward: 203.907, mean reward: 0.616 [-12.930, 100.000], mean action: 2.060 [0.000, 3.000], mean observation: 0.047 [-0.675, 1.423], loss: 6.017940, mae: 41.012108, mean_q: 55.147411
  146370/1100000: episode: 270, duration: 2.831s, episode steps: 462, steps per second: 163, episode reward: 228.726, mean reward: 0.495 [-9.950, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.161 [-0.657, 1.413], loss: 9.099685, mae: 40.883842, mean_q: 54.956726
  146478/1100000: episode: 271, duration: 0.626s, episode steps: 108, steps per second: 172, episode reward: -27.817, mean reward: -0.258 [-100.000, 20.739], mean action: 1.880 [0.000, 3.000], mean observation: 0.126 [-1.558, 1.408], loss: 14.149964, mae: 40.739609, mean_q: 54.814911
  146885/1100000: episode: 272, duration: 2.455s, episode steps: 407, steps per second: 166, episode reward: 254.804, mean reward: 0.626 [-13.256, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.170 [-0.689, 1.394], loss: 6.754517, mae: 40.707569, mean_q: 54.812553
  147267/1100000: episode: 273, duration: 2.268s, episode steps: 382, steps per second: 168, episode reward: 300.024, mean reward: 0.785 [-10.202, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.068 [-1.173, 1.467], loss: 5.648111, mae: 40.895844, mean_q: 55.170147
  147503/1100000: episode: 274, duration: 1.360s, episode steps: 236, steps per second: 174, episode reward: 301.684, mean reward: 1.278 [-13.130, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.052 [-0.916, 1.392], loss: 7.266547, mae: 41.057808, mean_q: 55.104179
  148503/1100000: episode: 275, duration: 7.352s, episode steps: 1000, steps per second: 136, episode reward: 58.877, mean reward: 0.059 [-10.937, 12.895], mean action: 1.867 [0.000, 3.000], mean observation: 0.109 [-0.702, 1.386], loss: 6.595346, mae: 41.647781, mean_q: 55.995216
  148603/1100000: episode: 276, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 15.046, mean reward: 0.150 [-100.000, 18.619], mean action: 1.620 [0.000, 3.000], mean observation: -0.019 [-1.063, 1.849], loss: 3.811323, mae: 41.246750, mean_q: 55.517799
  149052/1100000: episode: 277, duration: 2.654s, episode steps: 449, steps per second: 169, episode reward: 222.524, mean reward: 0.496 [-19.850, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.004 [-0.903, 1.398], loss: 6.082551, mae: 42.196270, mean_q: 56.739712
  149338/1100000: episode: 278, duration: 1.693s, episode steps: 286, steps per second: 169, episode reward: 251.827, mean reward: 0.881 [-3.349, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.052 [-0.875, 1.445], loss: 8.179137, mae: 42.379448, mean_q: 57.018806
  149653/1100000: episode: 279, duration: 1.842s, episode steps: 315, steps per second: 171, episode reward: 220.209, mean reward: 0.699 [-13.001, 100.000], mean action: 2.013 [0.000, 3.000], mean observation: 0.006 [-0.600, 1.414], loss: 6.398206, mae: 42.280380, mean_q: 56.748451
  149731/1100000: episode: 280, duration: 0.449s, episode steps: 78, steps per second: 174, episode reward: -31.956, mean reward: -0.410 [-100.000, 13.328], mean action: 1.603 [0.000, 3.000], mean observation: 0.041 [-2.751, 1.399], loss: 12.546393, mae: 42.627983, mean_q: 57.165195
  149925/1100000: episode: 281, duration: 1.115s, episode steps: 194, steps per second: 174, episode reward: 4.668, mean reward: 0.024 [-100.000, 37.118], mean action: 1.701 [0.000, 3.000], mean observation: 0.179 [-1.158, 2.838], loss: 5.044800, mae: 42.702713, mean_q: 57.462318
  150188/1100000: episode: 282, duration: 1.508s, episode steps: 263, steps per second: 174, episode reward: 269.511, mean reward: 1.025 [-3.010, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.079 [-0.870, 1.487], loss: 9.402803, mae: 42.257885, mean_q: 56.746891
  150682/1100000: episode: 283, duration: 3.064s, episode steps: 494, steps per second: 161, episode reward: 186.737, mean reward: 0.378 [-17.370, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: -0.040 [-0.617, 1.397], loss: 7.274609, mae: 42.470097, mean_q: 57.073654
  151027/1100000: episode: 284, duration: 2.049s, episode steps: 345, steps per second: 168, episode reward: 253.555, mean reward: 0.735 [-17.729, 100.000], mean action: 1.974 [0.000, 3.000], mean observation: 0.095 [-0.915, 1.474], loss: 6.459860, mae: 42.671055, mean_q: 57.346977
  151575/1100000: episode: 285, duration: 3.419s, episode steps: 548, steps per second: 160, episode reward: 217.315, mean reward: 0.397 [-18.136, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.070 [-0.688, 1.387], loss: 5.691344, mae: 42.800732, mean_q: 57.578739
  152283/1100000: episode: 286, duration: 4.342s, episode steps: 708, steps per second: 163, episode reward: 189.453, mean reward: 0.268 [-17.856, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.203 [-0.622, 1.476], loss: 6.897442, mae: 42.819431, mean_q: 57.575050
  152907/1100000: episode: 287, duration: 3.797s, episode steps: 624, steps per second: 164, episode reward: 228.846, mean reward: 0.367 [-18.697, 100.000], mean action: 0.870 [0.000, 3.000], mean observation: 0.111 [-0.732, 1.423], loss: 8.209970, mae: 42.661339, mean_q: 57.349716
  153445/1100000: episode: 288, duration: 3.241s, episode steps: 538, steps per second: 166, episode reward: 227.792, mean reward: 0.423 [-19.814, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.006 [-0.895, 1.460], loss: 6.693061, mae: 42.298798, mean_q: 56.886971
  153899/1100000: episode: 289, duration: 2.775s, episode steps: 454, steps per second: 164, episode reward: 213.368, mean reward: 0.470 [-17.129, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: -0.026 [-0.621, 1.408], loss: 7.507914, mae: 42.764954, mean_q: 57.646923
  154422/1100000: episode: 290, duration: 3.250s, episode steps: 523, steps per second: 161, episode reward: 200.991, mean reward: 0.384 [-18.181, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: -0.020 [-0.600, 1.405], loss: 7.007217, mae: 42.749035, mean_q: 57.493652
  155069/1100000: episode: 291, duration: 4.085s, episode steps: 647, steps per second: 158, episode reward: 198.311, mean reward: 0.307 [-15.851, 100.000], mean action: 1.765 [0.000, 3.000], mean observation: 0.119 [-0.668, 1.526], loss: 8.726818, mae: 42.732243, mean_q: 57.444309
  156069/1100000: episode: 292, duration: 6.648s, episode steps: 1000, steps per second: 150, episode reward: 50.398, mean reward: 0.050 [-20.377, 18.950], mean action: 1.510 [0.000, 3.000], mean observation: 0.123 [-0.648, 1.396], loss: 7.044129, mae: 42.445267, mean_q: 57.117142
  156536/1100000: episode: 293, duration: 2.741s, episode steps: 467, steps per second: 170, episode reward: 219.943, mean reward: 0.471 [-14.318, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: 0.003 [-0.600, 1.480], loss: 6.045672, mae: 42.556919, mean_q: 57.168243
  157218/1100000: episode: 294, duration: 4.131s, episode steps: 682, steps per second: 165, episode reward: -200.493, mean reward: -0.294 [-100.000, 16.204], mean action: 0.931 [0.000, 3.000], mean observation: -0.093 [-1.704, 1.403], loss: 9.159982, mae: 42.596870, mean_q: 57.209808
  157540/1100000: episode: 295, duration: 1.887s, episode steps: 322, steps per second: 171, episode reward: 226.631, mean reward: 0.704 [-8.494, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: -0.021 [-0.974, 1.425], loss: 8.950037, mae: 42.682884, mean_q: 57.471996
  158029/1100000: episode: 296, duration: 2.999s, episode steps: 489, steps per second: 163, episode reward: 214.113, mean reward: 0.438 [-20.363, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.152 [-0.710, 1.431], loss: 7.350494, mae: 42.758347, mean_q: 57.618134
  158130/1100000: episode: 297, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: -349.944, mean reward: -3.465 [-100.000, 2.180], mean action: 0.881 [0.000, 3.000], mean observation: -0.170 [-2.552, 1.502], loss: 8.375316, mae: 42.500690, mean_q: 57.290840
  158379/1100000: episode: 298, duration: 1.434s, episode steps: 249, steps per second: 174, episode reward: 22.026, mean reward: 0.088 [-100.000, 34.454], mean action: 1.643 [0.000, 3.000], mean observation: -0.009 [-0.646, 1.404], loss: 5.646774, mae: 43.170181, mean_q: 58.090622
  158634/1100000: episode: 299, duration: 1.477s, episode steps: 255, steps per second: 173, episode reward: 272.791, mean reward: 1.070 [-9.116, 100.000], mean action: 1.710 [0.000, 3.000], mean observation: 0.075 [-1.447, 1.467], loss: 5.280204, mae: 43.191818, mean_q: 58.198891
  158870/1100000: episode: 300, duration: 1.380s, episode steps: 236, steps per second: 171, episode reward: 241.177, mean reward: 1.022 [-11.032, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.147 [-0.914, 1.386], loss: 6.919369, mae: 42.593086, mean_q: 57.306305
  159260/1100000: episode: 301, duration: 2.328s, episode steps: 390, steps per second: 168, episode reward: 198.794, mean reward: 0.510 [-17.497, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.105 [-0.581, 1.460], loss: 7.418839, mae: 43.026241, mean_q: 57.996269
  159397/1100000: episode: 302, duration: 0.786s, episode steps: 137, steps per second: 174, episode reward: -35.859, mean reward: -0.262 [-100.000, 16.332], mean action: 1.854 [0.000, 3.000], mean observation: 0.037 [-0.720, 1.407], loss: 9.176321, mae: 42.951477, mean_q: 57.878002
  160350/1100000: episode: 303, duration: 5.889s, episode steps: 953, steps per second: 162, episode reward: 176.990, mean reward: 0.186 [-21.505, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.077 [-0.765, 1.423], loss: 8.006554, mae: 43.888687, mean_q: 59.054287
  161350/1100000: episode: 304, duration: 6.507s, episode steps: 1000, steps per second: 154, episode reward: 20.242, mean reward: 0.020 [-17.832, 13.820], mean action: 2.068 [0.000, 3.000], mean observation: 0.149 [-0.654, 1.393], loss: 9.161403, mae: 44.051697, mean_q: 59.257725
  161685/1100000: episode: 305, duration: 2.002s, episode steps: 335, steps per second: 167, episode reward: 244.041, mean reward: 0.728 [-17.708, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.080 [-0.722, 1.399], loss: 6.489003, mae: 44.279438, mean_q: 59.614536
  162014/1100000: episode: 306, duration: 1.959s, episode steps: 329, steps per second: 168, episode reward: 227.183, mean reward: 0.691 [-17.469, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.040 [-0.599, 1.405], loss: 5.530984, mae: 44.500366, mean_q: 59.720402
  162459/1100000: episode: 307, duration: 2.776s, episode steps: 445, steps per second: 160, episode reward: 255.110, mean reward: 0.573 [-20.118, 100.000], mean action: 0.870 [0.000, 3.000], mean observation: 0.130 [-0.676, 1.407], loss: 6.846889, mae: 44.624203, mean_q: 59.852615
  163063/1100000: episode: 308, duration: 3.719s, episode steps: 604, steps per second: 162, episode reward: 188.839, mean reward: 0.313 [-19.205, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: 0.014 [-0.600, 1.448], loss: 7.353798, mae: 44.719948, mean_q: 60.037861
  163193/1100000: episode: 309, duration: 0.738s, episode steps: 130, steps per second: 176, episode reward: 16.824, mean reward: 0.129 [-100.000, 26.171], mean action: 1.546 [0.000, 3.000], mean observation: 0.018 [-1.002, 1.410], loss: 6.635997, mae: 44.371620, mean_q: 59.714039
  163878/1100000: episode: 310, duration: 4.468s, episode steps: 685, steps per second: 153, episode reward: 202.577, mean reward: 0.296 [-17.469, 100.000], mean action: 0.861 [0.000, 3.000], mean observation: 0.022 [-0.600, 1.409], loss: 9.236941, mae: 44.332027, mean_q: 59.580418
  164522/1100000: episode: 311, duration: 4.025s, episode steps: 644, steps per second: 160, episode reward: 192.525, mean reward: 0.299 [-23.739, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.234 [-1.001, 1.401], loss: 6.976890, mae: 44.528461, mean_q: 59.692024
  165438/1100000: episode: 312, duration: 6.132s, episode steps: 916, steps per second: 149, episode reward: 171.783, mean reward: 0.188 [-17.841, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.194 [-0.493, 1.412], loss: 6.288665, mae: 43.925095, mean_q: 59.069881
  165799/1100000: episode: 313, duration: 2.146s, episode steps: 361, steps per second: 168, episode reward: 290.792, mean reward: 0.806 [-18.368, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.178 [-0.932, 1.399], loss: 5.308355, mae: 43.968670, mean_q: 58.925095
  166684/1100000: episode: 314, duration: 5.933s, episode steps: 885, steps per second: 149, episode reward: 144.949, mean reward: 0.164 [-17.662, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.213 [-0.716, 1.395], loss: 7.905388, mae: 43.692944, mean_q: 58.688461
  167550/1100000: episode: 315, duration: 5.461s, episode steps: 866, steps per second: 159, episode reward: 180.480, mean reward: 0.208 [-18.087, 100.000], mean action: 2.141 [0.000, 3.000], mean observation: 0.179 [-0.629, 1.388], loss: 7.291353, mae: 43.472626, mean_q: 58.462406
  167947/1100000: episode: 316, duration: 2.445s, episode steps: 397, steps per second: 162, episode reward: 229.447, mean reward: 0.578 [-18.743, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: -0.044 [-0.855, 1.386], loss: 5.885046, mae: 43.553043, mean_q: 58.544617
  168126/1100000: episode: 317, duration: 1.019s, episode steps: 179, steps per second: 176, episode reward: -43.196, mean reward: -0.241 [-100.000, 12.102], mean action: 1.709 [0.000, 3.000], mean observation: -0.078 [-0.741, 1.612], loss: 7.529267, mae: 43.693268, mean_q: 58.778233
  169126/1100000: episode: 318, duration: 6.189s, episode steps: 1000, steps per second: 162, episode reward: 112.849, mean reward: 0.113 [-18.532, 11.975], mean action: 0.887 [0.000, 3.000], mean observation: 0.170 [-0.664, 1.391], loss: 7.380388, mae: 43.330849, mean_q: 58.291092
  169227/1100000: episode: 319, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: -40.226, mean reward: -0.398 [-100.000, 8.832], mean action: 1.703 [0.000, 3.000], mean observation: 0.095 [-0.954, 1.385], loss: 6.210213, mae: 43.729034, mean_q: 58.876232
  169578/1100000: episode: 320, duration: 2.067s, episode steps: 351, steps per second: 170, episode reward: 270.047, mean reward: 0.769 [-10.891, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.090 [-0.621, 1.409], loss: 5.669210, mae: 43.362900, mean_q: 58.362759
  169831/1100000: episode: 321, duration: 1.466s, episode steps: 253, steps per second: 173, episode reward: 205.216, mean reward: 0.811 [-9.941, 100.000], mean action: 1.711 [0.000, 3.000], mean observation: 0.222 [-0.697, 1.413], loss: 4.013766, mae: 43.579731, mean_q: 58.784702
  170250/1100000: episode: 322, duration: 2.488s, episode steps: 419, steps per second: 168, episode reward: 205.713, mean reward: 0.491 [-17.857, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: -0.058 [-0.659, 1.402], loss: 7.789039, mae: 43.629364, mean_q: 58.704514
  170403/1100000: episode: 323, duration: 0.868s, episode steps: 153, steps per second: 176, episode reward: -12.924, mean reward: -0.084 [-100.000, 18.110], mean action: 1.438 [0.000, 3.000], mean observation: 0.197 [-0.990, 1.480], loss: 5.706223, mae: 43.544014, mean_q: 58.663853
  170604/1100000: episode: 324, duration: 1.149s, episode steps: 201, steps per second: 175, episode reward: 41.045, mean reward: 0.204 [-100.000, 11.527], mean action: 1.746 [0.000, 3.000], mean observation: -0.041 [-0.771, 1.717], loss: 8.144169, mae: 43.873142, mean_q: 58.980503
  171079/1100000: episode: 325, duration: 2.923s, episode steps: 475, steps per second: 163, episode reward: 162.101, mean reward: 0.341 [-11.356, 100.000], mean action: 1.686 [0.000, 3.000], mean observation: 0.205 [-0.893, 1.404], loss: 7.173478, mae: 43.564892, mean_q: 58.689198
  171695/1100000: episode: 326, duration: 3.980s, episode steps: 616, steps per second: 155, episode reward: 203.396, mean reward: 0.330 [-18.214, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.191 [-0.743, 1.456], loss: 6.859174, mae: 43.853600, mean_q: 59.013775
  172695/1100000: episode: 327, duration: 6.441s, episode steps: 1000, steps per second: 155, episode reward: 43.311, mean reward: 0.043 [-22.000, 24.065], mean action: 2.337 [0.000, 3.000], mean observation: 0.129 [-0.749, 1.408], loss: 6.469901, mae: 44.003700, mean_q: 59.126610
  173061/1100000: episode: 328, duration: 2.167s, episode steps: 366, steps per second: 169, episode reward: 209.227, mean reward: 0.572 [-12.327, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.109 [-0.619, 1.456], loss: 7.582743, mae: 44.036999, mean_q: 59.079105
  173570/1100000: episode: 329, duration: 3.221s, episode steps: 509, steps per second: 158, episode reward: 170.973, mean reward: 0.336 [-18.597, 100.000], mean action: 2.194 [0.000, 3.000], mean observation: 0.060 [-0.637, 1.433], loss: 5.941604, mae: 44.170639, mean_q: 59.380440
  173948/1100000: episode: 330, duration: 2.248s, episode steps: 378, steps per second: 168, episode reward: 260.292, mean reward: 0.689 [-13.735, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.052 [-0.843, 1.539], loss: 8.711452, mae: 44.459332, mean_q: 59.727505
  174488/1100000: episode: 331, duration: 3.139s, episode steps: 540, steps per second: 172, episode reward: 222.541, mean reward: 0.412 [-18.414, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.189 [-0.742, 1.433], loss: 5.503018, mae: 44.065472, mean_q: 59.387280
  174916/1100000: episode: 332, duration: 2.653s, episode steps: 428, steps per second: 161, episode reward: -124.215, mean reward: -0.290 [-100.000, 5.225], mean action: 1.907 [0.000, 3.000], mean observation: 0.014 [-0.692, 1.430], loss: 6.143092, mae: 44.056679, mean_q: 59.216999
  175916/1100000: episode: 333, duration: 6.318s, episode steps: 1000, steps per second: 158, episode reward: 78.132, mean reward: 0.078 [-19.326, 16.268], mean action: 0.957 [0.000, 3.000], mean observation: 0.159 [-0.598, 1.393], loss: 6.825135, mae: 44.317719, mean_q: 59.619682
  176241/1100000: episode: 334, duration: 1.913s, episode steps: 325, steps per second: 170, episode reward: 230.743, mean reward: 0.710 [-13.052, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: -0.016 [-0.697, 1.435], loss: 7.590271, mae: 44.112827, mean_q: 59.354290
  176495/1100000: episode: 335, duration: 1.467s, episode steps: 254, steps per second: 173, episode reward: -101.727, mean reward: -0.400 [-100.000, 14.187], mean action: 1.543 [0.000, 3.000], mean observation: 0.054 [-0.931, 1.414], loss: 5.781222, mae: 44.130543, mean_q: 59.421551
  176864/1100000: episode: 336, duration: 2.146s, episode steps: 369, steps per second: 172, episode reward: -102.378, mean reward: -0.277 [-100.000, 12.838], mean action: 1.705 [0.000, 3.000], mean observation: -0.005 [-0.600, 1.396], loss: 6.311125, mae: 43.865208, mean_q: 59.015774
  177271/1100000: episode: 337, duration: 2.370s, episode steps: 407, steps per second: 172, episode reward: 226.265, mean reward: 0.556 [-17.448, 100.000], mean action: 0.749 [0.000, 3.000], mean observation: -0.008 [-0.975, 1.396], loss: 6.965844, mae: 43.834698, mean_q: 58.968384
  177623/1100000: episode: 338, duration: 2.123s, episode steps: 352, steps per second: 166, episode reward: 216.108, mean reward: 0.614 [-18.036, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.075 [-0.948, 1.419], loss: 10.912839, mae: 43.942730, mean_q: 59.193974
  177968/1100000: episode: 339, duration: 2.030s, episode steps: 345, steps per second: 170, episode reward: 225.124, mean reward: 0.653 [-10.618, 100.000], mean action: 2.157 [0.000, 3.000], mean observation: -0.047 [-1.026, 1.401], loss: 6.969602, mae: 43.500237, mean_q: 58.739792
  178387/1100000: episode: 340, duration: 2.572s, episode steps: 419, steps per second: 163, episode reward: 181.807, mean reward: 0.434 [-17.108, 100.000], mean action: 1.561 [0.000, 3.000], mean observation: 0.086 [-0.727, 1.401], loss: 5.635553, mae: 44.026783, mean_q: 59.285988
  178873/1100000: episode: 341, duration: 2.949s, episode steps: 486, steps per second: 165, episode reward: 248.976, mean reward: 0.512 [-20.431, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.080 [-0.510, 1.409], loss: 7.458675, mae: 43.881607, mean_q: 59.164127
  179258/1100000: episode: 342, duration: 2.292s, episode steps: 385, steps per second: 168, episode reward: 228.870, mean reward: 0.594 [-17.359, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: -0.012 [-0.951, 1.485], loss: 6.201664, mae: 43.868538, mean_q: 59.019566
  180258/1100000: episode: 343, duration: 6.524s, episode steps: 1000, steps per second: 153, episode reward: -140.269, mean reward: -0.140 [-6.687, 5.419], mean action: 1.645 [0.000, 3.000], mean observation: 0.085 [-0.803, 1.512], loss: 7.325240, mae: 44.205513, mean_q: 59.484138
  181258/1100000: episode: 344, duration: 6.336s, episode steps: 1000, steps per second: 158, episode reward: -78.890, mean reward: -0.079 [-5.634, 4.840], mean action: 1.706 [0.000, 3.000], mean observation: 0.091 [-0.731, 1.386], loss: 6.668190, mae: 44.202587, mean_q: 59.562038
  181525/1100000: episode: 345, duration: 1.539s, episode steps: 267, steps per second: 173, episode reward: 190.361, mean reward: 0.713 [-10.319, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.007 [-1.184, 1.428], loss: 8.663928, mae: 44.105564, mean_q: 59.251221
  182251/1100000: episode: 346, duration: 4.835s, episode steps: 726, steps per second: 150, episode reward: 219.841, mean reward: 0.303 [-11.433, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.173 [-0.600, 1.408], loss: 5.421148, mae: 44.485844, mean_q: 59.935612
  183251/1100000: episode: 347, duration: 6.454s, episode steps: 1000, steps per second: 155, episode reward: 64.543, mean reward: 0.065 [-18.822, 21.970], mean action: 1.391 [0.000, 3.000], mean observation: 0.196 [-0.599, 1.409], loss: 7.591086, mae: 44.315926, mean_q: 59.722530
  183720/1100000: episode: 348, duration: 2.835s, episode steps: 469, steps per second: 165, episode reward: 192.760, mean reward: 0.411 [-11.365, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.182 [-0.558, 1.414], loss: 6.235067, mae: 44.510456, mean_q: 59.936508
  184194/1100000: episode: 349, duration: 2.836s, episode steps: 474, steps per second: 167, episode reward: 253.936, mean reward: 0.536 [-17.756, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.236 [-0.834, 1.434], loss: 4.882511, mae: 44.555367, mean_q: 60.118561
  184638/1100000: episode: 350, duration: 2.882s, episode steps: 444, steps per second: 154, episode reward: 173.129, mean reward: 0.390 [-20.965, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: 0.079 [-0.533, 1.411], loss: 9.684235, mae: 44.918659, mean_q: 60.591103
  184958/1100000: episode: 351, duration: 1.877s, episode steps: 320, steps per second: 171, episode reward: 190.134, mean reward: 0.594 [-13.937, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.169 [-0.883, 1.447], loss: 8.494891, mae: 44.590656, mean_q: 60.096230
  185873/1100000: episode: 352, duration: 5.844s, episode steps: 915, steps per second: 157, episode reward: 174.438, mean reward: 0.191 [-20.952, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: 0.248 [-0.509, 1.394], loss: 5.546285, mae: 45.232624, mean_q: 60.924061
  186738/1100000: episode: 353, duration: 5.339s, episode steps: 865, steps per second: 162, episode reward: 187.709, mean reward: 0.217 [-18.942, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.148 [-0.625, 1.438], loss: 7.829332, mae: 45.688019, mean_q: 61.277561
  187112/1100000: episode: 354, duration: 2.226s, episode steps: 374, steps per second: 168, episode reward: 255.850, mean reward: 0.684 [-18.640, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.016 [-0.810, 1.442], loss: 7.012209, mae: 45.339996, mean_q: 60.947620
  187232/1100000: episode: 355, duration: 0.681s, episode steps: 120, steps per second: 176, episode reward: 8.032, mean reward: 0.067 [-100.000, 17.456], mean action: 1.692 [0.000, 3.000], mean observation: 0.071 [-0.894, 2.003], loss: 2.992101, mae: 45.353603, mean_q: 61.013687
  187514/1100000: episode: 356, duration: 1.669s, episode steps: 282, steps per second: 169, episode reward: 181.501, mean reward: 0.644 [-14.756, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: -0.020 [-1.100, 1.410], loss: 5.599620, mae: 45.865078, mean_q: 61.751884
  187854/1100000: episode: 357, duration: 1.975s, episode steps: 340, steps per second: 172, episode reward: 236.253, mean reward: 0.695 [-10.541, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: -0.029 [-1.200, 1.399], loss: 6.752615, mae: 45.623352, mean_q: 61.461491
  188325/1100000: episode: 358, duration: 3.003s, episode steps: 471, steps per second: 157, episode reward: 226.254, mean reward: 0.480 [-20.002, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.125 [-0.835, 1.402], loss: 10.085819, mae: 46.042782, mean_q: 61.725216
  189070/1100000: episode: 359, duration: 4.640s, episode steps: 745, steps per second: 161, episode reward: 189.689, mean reward: 0.255 [-19.276, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.013 [-0.729, 1.386], loss: 5.098310, mae: 46.122818, mean_q: 62.100529
  189472/1100000: episode: 360, duration: 2.403s, episode steps: 402, steps per second: 167, episode reward: 176.318, mean reward: 0.439 [-18.260, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.181 [-0.828, 1.411], loss: 4.843887, mae: 46.325550, mean_q: 62.284222
  189774/1100000: episode: 361, duration: 1.772s, episode steps: 302, steps per second: 170, episode reward: -48.580, mean reward: -0.161 [-100.000, 12.035], mean action: 1.911 [0.000, 3.000], mean observation: 0.024 [-0.808, 1.403], loss: 4.275249, mae: 46.365902, mean_q: 62.371933
  190774/1100000: episode: 362, duration: 6.377s, episode steps: 1000, steps per second: 157, episode reward: -125.209, mean reward: -0.125 [-21.109, 26.594], mean action: 1.779 [0.000, 3.000], mean observation: 0.077 [-0.757, 1.412], loss: 7.661997, mae: 46.458099, mean_q: 62.450035
  191474/1100000: episode: 363, duration: 4.325s, episode steps: 700, steps per second: 162, episode reward: 244.408, mean reward: 0.349 [-21.237, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.049 [-0.949, 1.392], loss: 6.061447, mae: 46.515251, mean_q: 62.460178
  191872/1100000: episode: 364, duration: 2.429s, episode steps: 398, steps per second: 164, episode reward: 272.685, mean reward: 0.685 [-13.983, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.074 [-0.620, 1.527], loss: 6.464360, mae: 46.473106, mean_q: 62.478378
  192195/1100000: episode: 365, duration: 1.879s, episode steps: 323, steps per second: 172, episode reward: 229.616, mean reward: 0.711 [-14.724, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.188 [-1.004, 1.640], loss: 6.157852, mae: 46.517921, mean_q: 62.604290
  192647/1100000: episode: 366, duration: 2.697s, episode steps: 452, steps per second: 168, episode reward: 184.183, mean reward: 0.407 [-19.695, 100.000], mean action: 1.878 [0.000, 3.000], mean observation: 0.140 [-0.765, 1.489], loss: 6.937731, mae: 46.307453, mean_q: 62.233051
  192845/1100000: episode: 367, duration: 1.127s, episode steps: 198, steps per second: 176, episode reward: -19.702, mean reward: -0.100 [-100.000, 13.716], mean action: 1.601 [0.000, 3.000], mean observation: -0.098 [-0.913, 1.412], loss: 4.034361, mae: 46.078804, mean_q: 61.846680
  193141/1100000: episode: 368, duration: 1.733s, episode steps: 296, steps per second: 171, episode reward: 266.105, mean reward: 0.899 [-12.421, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.015 [-0.882, 1.402], loss: 5.219067, mae: 46.008537, mean_q: 61.830856
  193670/1100000: episode: 369, duration: 3.285s, episode steps: 529, steps per second: 161, episode reward: 192.071, mean reward: 0.363 [-21.537, 100.000], mean action: 2.301 [0.000, 3.000], mean observation: 0.224 [-1.211, 1.396], loss: 6.126157, mae: 46.373398, mean_q: 62.296551
  194248/1100000: episode: 370, duration: 3.625s, episode steps: 578, steps per second: 159, episode reward: 165.618, mean reward: 0.287 [-19.160, 100.000], mean action: 2.159 [0.000, 3.000], mean observation: 0.102 [-0.691, 1.418], loss: 5.691677, mae: 46.088436, mean_q: 62.015030
  194516/1100000: episode: 371, duration: 1.564s, episode steps: 268, steps per second: 171, episode reward: 206.309, mean reward: 0.770 [-10.273, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: -0.024 [-1.283, 1.413], loss: 6.186520, mae: 46.341316, mean_q: 62.174171
  194800/1100000: episode: 372, duration: 1.668s, episode steps: 284, steps per second: 170, episode reward: 230.802, mean reward: 0.813 [-8.904, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.065 [-0.657, 1.402], loss: 7.439835, mae: 45.798710, mean_q: 61.467003
  195004/1100000: episode: 373, duration: 1.173s, episode steps: 204, steps per second: 174, episode reward: 49.122, mean reward: 0.241 [-100.000, 17.657], mean action: 1.691 [0.000, 3.000], mean observation: -0.112 [-0.786, 1.388], loss: 6.706563, mae: 45.539589, mean_q: 61.143105
  195322/1100000: episode: 374, duration: 1.855s, episode steps: 318, steps per second: 171, episode reward: 223.595, mean reward: 0.703 [-17.337, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.199 [-0.849, 1.401], loss: 5.508281, mae: 45.458038, mean_q: 61.099842
  195603/1100000: episode: 375, duration: 1.661s, episode steps: 281, steps per second: 169, episode reward: 215.097, mean reward: 0.765 [-10.900, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.006 [-0.758, 1.403], loss: 6.190807, mae: 45.496380, mean_q: 61.201958
  195943/1100000: episode: 376, duration: 1.959s, episode steps: 340, steps per second: 174, episode reward: 259.831, mean reward: 0.764 [-9.703, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.144 [-0.764, 1.523], loss: 7.320633, mae: 45.431171, mean_q: 60.944389
  196726/1100000: episode: 377, duration: 5.544s, episode steps: 783, steps per second: 141, episode reward: 110.052, mean reward: 0.141 [-14.395, 100.000], mean action: 1.633 [0.000, 3.000], mean observation: -0.032 [-0.694, 1.397], loss: 5.650270, mae: 45.290760, mean_q: 60.952450
  197597/1100000: episode: 378, duration: 5.464s, episode steps: 871, steps per second: 159, episode reward: 151.392, mean reward: 0.174 [-12.400, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: -0.009 [-0.949, 1.450], loss: 6.986475, mae: 45.239429, mean_q: 60.880024
  198120/1100000: episode: 379, duration: 3.146s, episode steps: 523, steps per second: 166, episode reward: 220.427, mean reward: 0.421 [-18.576, 100.000], mean action: 1.774 [0.000, 3.000], mean observation: 0.243 [-0.827, 1.418], loss: 5.423777, mae: 45.304241, mean_q: 60.933361
  198628/1100000: episode: 380, duration: 3.291s, episode steps: 508, steps per second: 154, episode reward: -293.637, mean reward: -0.578 [-100.000, 15.090], mean action: 1.823 [0.000, 3.000], mean observation: 0.074 [-1.410, 1.708], loss: 5.458619, mae: 45.634167, mean_q: 61.355839
  199009/1100000: episode: 381, duration: 2.250s, episode steps: 381, steps per second: 169, episode reward: 284.132, mean reward: 0.746 [-17.704, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.088 [-0.703, 1.504], loss: 7.166014, mae: 45.948307, mean_q: 61.779659
  199362/1100000: episode: 382, duration: 2.100s, episode steps: 353, steps per second: 168, episode reward: 259.316, mean reward: 0.735 [-18.823, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: -0.028 [-0.840, 1.395], loss: 7.239330, mae: 46.090778, mean_q: 61.783695
  199607/1100000: episode: 383, duration: 1.425s, episode steps: 245, steps per second: 172, episode reward: 279.450, mean reward: 1.141 [-4.229, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.048 [-0.761, 1.455], loss: 4.168879, mae: 46.095928, mean_q: 61.924091
  199731/1100000: episode: 384, duration: 0.716s, episode steps: 124, steps per second: 173, episode reward: -40.344, mean reward: -0.325 [-100.000, 11.432], mean action: 1.742 [0.000, 3.000], mean observation: 0.074 [-1.095, 1.396], loss: 3.717364, mae: 45.865181, mean_q: 61.759319
  199919/1100000: episode: 385, duration: 1.086s, episode steps: 188, steps per second: 173, episode reward: -18.084, mean reward: -0.096 [-100.000, 14.562], mean action: 1.862 [0.000, 3.000], mean observation: 0.144 [-0.733, 1.520], loss: 5.103034, mae: 46.137882, mean_q: 62.095734
  200180/1100000: episode: 386, duration: 1.539s, episode steps: 261, steps per second: 170, episode reward: 244.889, mean reward: 0.938 [-17.480, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: -0.031 [-0.757, 1.396], loss: 6.423053, mae: 45.993938, mean_q: 61.986057
  200468/1100000: episode: 387, duration: 1.697s, episode steps: 288, steps per second: 170, episode reward: 290.033, mean reward: 1.007 [-7.814, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: -0.016 [-0.798, 1.389], loss: 6.839602, mae: 46.424942, mean_q: 62.406509
  201054/1100000: episode: 388, duration: 3.585s, episode steps: 586, steps per second: 163, episode reward: 250.323, mean reward: 0.427 [-19.471, 100.000], mean action: 0.911 [0.000, 3.000], mean observation: 0.209 [-0.889, 1.399], loss: 6.536532, mae: 45.893105, mean_q: 61.726784
  201157/1100000: episode: 389, duration: 0.605s, episode steps: 103, steps per second: 170, episode reward: -167.679, mean reward: -1.628 [-100.000, 3.493], mean action: 1.146 [0.000, 3.000], mean observation: -0.139 [-1.530, 1.466], loss: 3.794037, mae: 45.871948, mean_q: 61.534161
  201469/1100000: episode: 390, duration: 1.803s, episode steps: 312, steps per second: 173, episode reward: 229.344, mean reward: 0.735 [-14.128, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.035 [-0.926, 1.452], loss: 12.244517, mae: 46.404915, mean_q: 62.366055
  201718/1100000: episode: 391, duration: 1.462s, episode steps: 249, steps per second: 170, episode reward: 275.512, mean reward: 1.106 [-3.550, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.092 [-1.606, 1.400], loss: 12.893061, mae: 46.622223, mean_q: 62.703949
  202205/1100000: episode: 392, duration: 2.953s, episode steps: 487, steps per second: 165, episode reward: 216.485, mean reward: 0.445 [-19.307, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.037 [-1.044, 1.427], loss: 9.728934, mae: 46.369946, mean_q: 62.262989
  202327/1100000: episode: 393, duration: 0.692s, episode steps: 122, steps per second: 176, episode reward: 5.489, mean reward: 0.045 [-100.000, 8.930], mean action: 1.434 [0.000, 3.000], mean observation: 0.071 [-1.704, 1.433], loss: 6.854403, mae: 46.593925, mean_q: 62.803249
  202459/1100000: episode: 394, duration: 0.759s, episode steps: 132, steps per second: 174, episode reward: 4.788, mean reward: 0.036 [-100.000, 16.340], mean action: 1.879 [0.000, 3.000], mean observation: 0.102 [-1.443, 1.414], loss: 6.324606, mae: 46.997009, mean_q: 63.350750
  202657/1100000: episode: 395, duration: 1.148s, episode steps: 198, steps per second: 172, episode reward: 4.259, mean reward: 0.022 [-100.000, 13.749], mean action: 1.884 [0.000, 3.000], mean observation: -0.045 [-0.689, 1.400], loss: 5.601153, mae: 46.864258, mean_q: 63.115353
  202815/1100000: episode: 396, duration: 0.898s, episode steps: 158, steps per second: 176, episode reward: 15.539, mean reward: 0.098 [-100.000, 9.746], mean action: 1.462 [0.000, 3.000], mean observation: -0.057 [-0.840, 1.422], loss: 5.452255, mae: 47.293667, mean_q: 63.572403
  203081/1100000: episode: 397, duration: 1.586s, episode steps: 266, steps per second: 168, episode reward: 247.330, mean reward: 0.930 [-15.546, 100.000], mean action: 1.737 [0.000, 3.000], mean observation: -0.042 [-0.778, 1.390], loss: 7.193551, mae: 47.353443, mean_q: 63.725109
  203270/1100000: episode: 398, duration: 1.096s, episode steps: 189, steps per second: 172, episode reward: 41.624, mean reward: 0.220 [-100.000, 11.610], mean action: 1.831 [0.000, 3.000], mean observation: 0.121 [-0.758, 1.394], loss: 4.911051, mae: 47.590321, mean_q: 64.141479
  203676/1100000: episode: 399, duration: 2.430s, episode steps: 406, steps per second: 167, episode reward: 237.191, mean reward: 0.584 [-13.825, 100.000], mean action: 1.892 [0.000, 3.000], mean observation: 0.216 [-0.541, 1.446], loss: 10.586240, mae: 47.455631, mean_q: 63.806984
  204057/1100000: episode: 400, duration: 2.355s, episode steps: 381, steps per second: 162, episode reward: -155.118, mean reward: -0.407 [-100.000, 12.520], mean action: 1.963 [0.000, 3.000], mean observation: -0.023 [-1.380, 1.387], loss: 8.451921, mae: 47.794430, mean_q: 64.255119
  204402/1100000: episode: 401, duration: 2.083s, episode steps: 345, steps per second: 166, episode reward: 165.038, mean reward: 0.478 [-17.942, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: -0.033 [-1.282, 1.472], loss: 7.807575, mae: 47.850647, mean_q: 64.317024
  204643/1100000: episode: 402, duration: 1.394s, episode steps: 241, steps per second: 173, episode reward: 269.021, mean reward: 1.116 [-19.685, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.069 [-0.837, 1.393], loss: 7.932260, mae: 48.214527, mean_q: 64.879364
  205511/1100000: episode: 403, duration: 5.600s, episode steps: 868, steps per second: 155, episode reward: 174.795, mean reward: 0.201 [-18.990, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.121 [-0.633, 1.498], loss: 11.236097, mae: 48.202648, mean_q: 64.884285
  206034/1100000: episode: 404, duration: 3.331s, episode steps: 523, steps per second: 157, episode reward: 234.066, mean reward: 0.448 [-19.843, 100.000], mean action: 0.851 [0.000, 3.000], mean observation: 0.116 [-0.771, 1.398], loss: 7.566089, mae: 48.146320, mean_q: 64.889191
  206459/1100000: episode: 405, duration: 2.526s, episode steps: 425, steps per second: 168, episode reward: 210.012, mean reward: 0.494 [-10.666, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: -0.072 [-0.704, 1.392], loss: 5.748718, mae: 47.978943, mean_q: 64.520699
  206892/1100000: episode: 406, duration: 2.627s, episode steps: 433, steps per second: 165, episode reward: 202.381, mean reward: 0.467 [-11.119, 100.000], mean action: 1.734 [0.000, 3.000], mean observation: 0.140 [-0.787, 1.412], loss: 6.652176, mae: 48.515659, mean_q: 65.194199
  207249/1100000: episode: 407, duration: 2.129s, episode steps: 357, steps per second: 168, episode reward: 215.467, mean reward: 0.604 [-18.336, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: -0.015 [-1.320, 1.413], loss: 7.534876, mae: 47.976376, mean_q: 64.477867
  207663/1100000: episode: 408, duration: 2.470s, episode steps: 414, steps per second: 168, episode reward: -28.451, mean reward: -0.069 [-100.000, 18.333], mean action: 2.213 [0.000, 3.000], mean observation: 0.162 [-0.571, 1.400], loss: 6.218936, mae: 47.719543, mean_q: 63.883354
  208534/1100000: episode: 409, duration: 5.539s, episode steps: 871, steps per second: 157, episode reward: -339.826, mean reward: -0.390 [-100.000, 31.368], mean action: 1.770 [0.000, 3.000], mean observation: 0.070 [-2.315, 1.390], loss: 7.578508, mae: 47.446247, mean_q: 63.680931
  208941/1100000: episode: 410, duration: 2.441s, episode steps: 407, steps per second: 167, episode reward: 247.424, mean reward: 0.608 [-18.125, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.093 [-0.516, 1.394], loss: 5.888958, mae: 47.580860, mean_q: 63.781487
  209245/1100000: episode: 411, duration: 1.845s, episode steps: 304, steps per second: 165, episode reward: 226.554, mean reward: 0.745 [-17.673, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.096 [-0.709, 1.400], loss: 6.711938, mae: 47.047558, mean_q: 63.209988
  209431/1100000: episode: 412, duration: 1.076s, episode steps: 186, steps per second: 173, episode reward: -52.150, mean reward: -0.280 [-100.000, 13.034], mean action: 1.968 [0.000, 3.000], mean observation: -0.089 [-1.154, 1.452], loss: 6.846563, mae: 47.149078, mean_q: 63.452705
  209731/1100000: episode: 413, duration: 1.758s, episode steps: 300, steps per second: 171, episode reward: 253.618, mean reward: 0.845 [-17.779, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.116 [-0.710, 1.415], loss: 7.293623, mae: 47.508389, mean_q: 63.696262
  210217/1100000: episode: 414, duration: 2.906s, episode steps: 486, steps per second: 167, episode reward: 184.915, mean reward: 0.380 [-14.230, 100.000], mean action: 1.825 [0.000, 3.000], mean observation: 0.147 [-0.688, 1.412], loss: 7.622567, mae: 47.335861, mean_q: 63.731552
  210615/1100000: episode: 415, duration: 2.410s, episode steps: 398, steps per second: 165, episode reward: 234.985, mean reward: 0.590 [-12.992, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.007 [-0.638, 1.392], loss: 8.376198, mae: 47.542538, mean_q: 63.743690
  211043/1100000: episode: 416, duration: 2.573s, episode steps: 428, steps per second: 166, episode reward: 249.954, mean reward: 0.584 [-14.576, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.199 [-0.789, 1.418], loss: 8.557462, mae: 47.794582, mean_q: 64.005447
  211510/1100000: episode: 417, duration: 2.760s, episode steps: 467, steps per second: 169, episode reward: 205.668, mean reward: 0.440 [-17.467, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.226 [-1.037, 1.404], loss: 6.411399, mae: 47.888855, mean_q: 64.194389
  211753/1100000: episode: 418, duration: 1.389s, episode steps: 243, steps per second: 175, episode reward: 254.541, mean reward: 1.047 [-11.219, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: -0.079 [-0.948, 1.417], loss: 7.781946, mae: 47.711170, mean_q: 64.001266
  211906/1100000: episode: 419, duration: 0.875s, episode steps: 153, steps per second: 175, episode reward: -21.277, mean reward: -0.139 [-100.000, 9.437], mean action: 1.595 [0.000, 3.000], mean observation: 0.079 [-0.799, 1.408], loss: 9.884791, mae: 48.153877, mean_q: 64.535347
  212376/1100000: episode: 420, duration: 2.769s, episode steps: 470, steps per second: 170, episode reward: 290.164, mean reward: 0.617 [-17.402, 100.000], mean action: 0.809 [0.000, 3.000], mean observation: 0.132 [-0.860, 1.389], loss: 7.785710, mae: 47.857338, mean_q: 64.316986
  212741/1100000: episode: 421, duration: 2.139s, episode steps: 365, steps per second: 171, episode reward: 238.870, mean reward: 0.654 [-17.669, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.100 [-0.727, 1.419], loss: 7.106283, mae: 47.502422, mean_q: 63.775513
  213154/1100000: episode: 422, duration: 2.560s, episode steps: 413, steps per second: 161, episode reward: 230.981, mean reward: 0.559 [-3.550, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.048 [-0.794, 1.464], loss: 5.774685, mae: 47.747631, mean_q: 64.157532
  213376/1100000: episode: 423, duration: 1.293s, episode steps: 222, steps per second: 172, episode reward: 256.173, mean reward: 1.154 [-3.002, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: -0.033 [-0.761, 1.393], loss: 9.421508, mae: 47.621559, mean_q: 63.663868
  213552/1100000: episode: 424, duration: 1.018s, episode steps: 176, steps per second: 173, episode reward: -20.139, mean reward: -0.114 [-100.000, 14.881], mean action: 1.835 [0.000, 3.000], mean observation: 0.146 [-1.037, 1.520], loss: 5.503268, mae: 47.589962, mean_q: 63.896240
  213923/1100000: episode: 425, duration: 2.232s, episode steps: 371, steps per second: 166, episode reward: 301.440, mean reward: 0.813 [-20.739, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.146 [-0.776, 1.421], loss: 7.642443, mae: 47.961578, mean_q: 64.454582
  214070/1100000: episode: 426, duration: 0.883s, episode steps: 147, steps per second: 166, episode reward: -60.062, mean reward: -0.409 [-100.000, 12.281], mean action: 1.748 [0.000, 3.000], mean observation: 0.112 [-1.337, 1.522], loss: 6.263059, mae: 47.348930, mean_q: 63.436279
  214339/1100000: episode: 427, duration: 1.567s, episode steps: 269, steps per second: 172, episode reward: 254.665, mean reward: 0.947 [-7.656, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.027 [-1.012, 1.394], loss: 9.928664, mae: 47.469345, mean_q: 63.575321
  214610/1100000: episode: 428, duration: 1.568s, episode steps: 271, steps per second: 173, episode reward: 295.161, mean reward: 1.089 [-6.969, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.126 [-0.828, 1.491], loss: 4.287764, mae: 48.090927, mean_q: 64.785248
  214821/1100000: episode: 429, duration: 1.219s, episode steps: 211, steps per second: 173, episode reward: 249.230, mean reward: 1.181 [-19.791, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.068 [-0.868, 1.405], loss: 8.634396, mae: 48.055706, mean_q: 64.441399
  215821/1100000: episode: 430, duration: 6.226s, episode steps: 1000, steps per second: 161, episode reward: 58.384, mean reward: 0.058 [-18.865, 23.010], mean action: 1.248 [0.000, 3.000], mean observation: 0.183 [-0.810, 1.410], loss: 7.073849, mae: 47.810150, mean_q: 64.189850
  216253/1100000: episode: 431, duration: 2.612s, episode steps: 432, steps per second: 165, episode reward: 199.999, mean reward: 0.463 [-12.583, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.147 [-0.769, 1.438], loss: 5.917379, mae: 47.571201, mean_q: 63.987129
  217034/1100000: episode: 432, duration: 4.719s, episode steps: 781, steps per second: 166, episode reward: 177.019, mean reward: 0.227 [-18.560, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.071 [-0.628, 1.433], loss: 7.704488, mae: 47.901619, mean_q: 64.194336
  217469/1100000: episode: 433, duration: 2.632s, episode steps: 435, steps per second: 165, episode reward: 234.483, mean reward: 0.539 [-19.003, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.092 [-1.015, 1.423], loss: 5.845009, mae: 48.154926, mean_q: 64.528305
  217771/1100000: episode: 434, duration: 1.801s, episode steps: 302, steps per second: 168, episode reward: 237.599, mean reward: 0.787 [-10.530, 100.000], mean action: 2.288 [0.000, 3.000], mean observation: 0.096 [-1.014, 1.425], loss: 5.251011, mae: 48.031342, mean_q: 64.622795
  218094/1100000: episode: 435, duration: 1.869s, episode steps: 323, steps per second: 173, episode reward: 219.735, mean reward: 0.680 [-9.600, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: -0.024 [-0.675, 1.409], loss: 6.355009, mae: 48.163048, mean_q: 64.602242
  219094/1100000: episode: 436, duration: 7.468s, episode steps: 1000, steps per second: 134, episode reward: 47.741, mean reward: 0.048 [-17.447, 12.103], mean action: 1.791 [0.000, 3.000], mean observation: 0.105 [-0.536, 1.501], loss: 9.815679, mae: 48.396202, mean_q: 65.040771
  219390/1100000: episode: 437, duration: 1.719s, episode steps: 296, steps per second: 172, episode reward: 19.504, mean reward: 0.066 [-100.000, 13.663], mean action: 1.524 [0.000, 3.000], mean observation: -0.075 [-0.910, 1.536], loss: 4.253886, mae: 48.945393, mean_q: 65.842339
  219815/1100000: episode: 438, duration: 2.478s, episode steps: 425, steps per second: 171, episode reward: 259.426, mean reward: 0.610 [-18.420, 100.000], mean action: 0.727 [0.000, 3.000], mean observation: 0.006 [-0.961, 1.402], loss: 8.332170, mae: 48.530731, mean_q: 65.109718
  220595/1100000: episode: 439, duration: 4.862s, episode steps: 780, steps per second: 160, episode reward: 160.137, mean reward: 0.205 [-6.088, 100.000], mean action: 1.581 [0.000, 3.000], mean observation: 0.125 [-0.720, 1.406], loss: 6.107542, mae: 48.544678, mean_q: 65.212753
  221169/1100000: episode: 440, duration: 3.620s, episode steps: 574, steps per second: 159, episode reward: 240.141, mean reward: 0.418 [-19.568, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.163 [-0.868, 1.388], loss: 7.363222, mae: 48.467930, mean_q: 65.085770
  221402/1100000: episode: 441, duration: 1.353s, episode steps: 233, steps per second: 172, episode reward: 265.701, mean reward: 1.140 [-8.440, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.066 [-0.959, 1.395], loss: 4.655886, mae: 48.914944, mean_q: 65.298149
  221945/1100000: episode: 442, duration: 3.210s, episode steps: 543, steps per second: 169, episode reward: 206.991, mean reward: 0.381 [-18.245, 100.000], mean action: 1.696 [0.000, 3.000], mean observation: 0.124 [-0.937, 1.423], loss: 7.092279, mae: 48.469288, mean_q: 65.035164
  222319/1100000: episode: 443, duration: 2.229s, episode steps: 374, steps per second: 168, episode reward: 272.061, mean reward: 0.727 [-17.364, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.118 [-0.906, 1.486], loss: 8.138448, mae: 48.422947, mean_q: 64.697327
  222613/1100000: episode: 444, duration: 1.696s, episode steps: 294, steps per second: 173, episode reward: 259.714, mean reward: 0.883 [-19.520, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: -0.016 [-0.990, 1.414], loss: 7.634424, mae: 47.819366, mean_q: 64.183884
  223232/1100000: episode: 445, duration: 3.835s, episode steps: 619, steps per second: 161, episode reward: 173.162, mean reward: 0.280 [-20.002, 100.000], mean action: 2.058 [0.000, 3.000], mean observation: 0.186 [-0.689, 1.443], loss: 6.645172, mae: 48.122540, mean_q: 64.338173
  223483/1100000: episode: 446, duration: 1.488s, episode steps: 251, steps per second: 169, episode reward: 228.189, mean reward: 0.909 [-14.604, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.058 [-0.994, 1.417], loss: 14.596848, mae: 47.729836, mean_q: 63.818336
  224128/1100000: episode: 447, duration: 4.064s, episode steps: 645, steps per second: 159, episode reward: 238.498, mean reward: 0.370 [-22.579, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.190 [-0.723, 1.404], loss: 8.055237, mae: 48.014706, mean_q: 64.402481
  224332/1100000: episode: 448, duration: 1.181s, episode steps: 204, steps per second: 173, episode reward: 272.058, mean reward: 1.334 [-7.400, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.041 [-0.991, 1.423], loss: 8.066356, mae: 48.210716, mean_q: 64.749855
  224911/1100000: episode: 449, duration: 3.651s, episode steps: 579, steps per second: 159, episode reward: 234.881, mean reward: 0.406 [-19.467, 100.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.196 [-0.644, 1.416], loss: 10.102767, mae: 47.645996, mean_q: 63.978748
  225180/1100000: episode: 450, duration: 1.573s, episode steps: 269, steps per second: 171, episode reward: -32.946, mean reward: -0.122 [-100.000, 15.926], mean action: 2.004 [0.000, 3.000], mean observation: -0.041 [-0.611, 1.442], loss: 6.674928, mae: 47.174866, mean_q: 63.492523
  225286/1100000: episode: 451, duration: 0.617s, episode steps: 106, steps per second: 172, episode reward: -30.437, mean reward: -0.287 [-100.000, 13.398], mean action: 1.670 [0.000, 3.000], mean observation: -0.062 [-0.965, 1.443], loss: 4.925273, mae: 46.345875, mean_q: 62.210724
  226286/1100000: episode: 452, duration: 6.597s, episode steps: 1000, steps per second: 152, episode reward: 38.634, mean reward: 0.039 [-17.083, 12.172], mean action: 1.689 [0.000, 3.000], mean observation: 0.111 [-0.654, 1.411], loss: 6.495014, mae: 47.183685, mean_q: 63.490414
  226811/1100000: episode: 453, duration: 3.127s, episode steps: 525, steps per second: 168, episode reward: 205.289, mean reward: 0.391 [-13.789, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.139 [-0.854, 1.419], loss: 7.924753, mae: 46.566132, mean_q: 62.590046
  227062/1100000: episode: 454, duration: 1.464s, episode steps: 251, steps per second: 171, episode reward: -105.591, mean reward: -0.421 [-100.000, 13.287], mean action: 2.120 [0.000, 3.000], mean observation: -0.062 [-1.041, 1.424], loss: 8.705897, mae: 46.628689, mean_q: 62.504082
  227442/1100000: episode: 455, duration: 2.262s, episode steps: 380, steps per second: 168, episode reward: 288.262, mean reward: 0.759 [-8.958, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: 0.129 [-1.298, 1.388], loss: 8.084685, mae: 45.958630, mean_q: 61.570347
  227793/1100000: episode: 456, duration: 2.163s, episode steps: 351, steps per second: 162, episode reward: 240.499, mean reward: 0.685 [-17.917, 100.000], mean action: 2.202 [0.000, 3.000], mean observation: 0.179 [-0.696, 1.395], loss: 5.555849, mae: 46.597401, mean_q: 62.776443
  228224/1100000: episode: 457, duration: 2.544s, episode steps: 431, steps per second: 169, episode reward: 252.310, mean reward: 0.585 [-19.772, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.081 [-1.213, 1.424], loss: 8.695333, mae: 45.989311, mean_q: 61.931095
  228314/1100000: episode: 458, duration: 0.519s, episode steps: 90, steps per second: 173, episode reward: 51.088, mean reward: 0.568 [-100.000, 13.728], mean action: 1.922 [0.000, 3.000], mean observation: -0.092 [-1.001, 1.387], loss: 10.268798, mae: 45.579380, mean_q: 61.376808
  228433/1100000: episode: 459, duration: 0.685s, episode steps: 119, steps per second: 174, episode reward: -66.114, mean reward: -0.556 [-100.000, 18.723], mean action: 2.042 [0.000, 3.000], mean observation: -0.057 [-1.037, 1.779], loss: 5.954940, mae: 45.989902, mean_q: 62.062244
  228770/1100000: episode: 460, duration: 1.983s, episode steps: 337, steps per second: 170, episode reward: 256.318, mean reward: 0.761 [-8.536, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.082 [-0.665, 1.486], loss: 7.105679, mae: 45.702782, mean_q: 61.508503
  229031/1100000: episode: 461, duration: 1.501s, episode steps: 261, steps per second: 174, episode reward: 269.346, mean reward: 1.032 [-18.245, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.092 [-0.741, 1.418], loss: 4.509161, mae: 45.763927, mean_q: 61.722927
  229824/1100000: episode: 462, duration: 5.027s, episode steps: 793, steps per second: 158, episode reward: 135.223, mean reward: 0.171 [-23.806, 100.000], mean action: 1.666 [0.000, 3.000], mean observation: 0.124 [-0.687, 1.406], loss: 7.571905, mae: 45.702641, mean_q: 61.601948
  230281/1100000: episode: 463, duration: 2.734s, episode steps: 457, steps per second: 167, episode reward: 167.087, mean reward: 0.366 [-18.848, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.174 [-0.696, 1.410], loss: 5.258860, mae: 45.695679, mean_q: 61.458694
  230673/1100000: episode: 464, duration: 2.319s, episode steps: 392, steps per second: 169, episode reward: 222.872, mean reward: 0.569 [-18.018, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.173 [-0.609, 1.470], loss: 8.494576, mae: 46.133575, mean_q: 62.033962
  231673/1100000: episode: 465, duration: 6.261s, episode steps: 1000, steps per second: 160, episode reward: 56.552, mean reward: 0.057 [-20.465, 20.036], mean action: 1.347 [0.000, 3.000], mean observation: 0.204 [-0.548, 1.412], loss: 6.456151, mae: 45.802444, mean_q: 61.821396
  231833/1100000: episode: 466, duration: 0.914s, episode steps: 160, steps per second: 175, episode reward: -107.783, mean reward: -0.674 [-100.000, 11.369], mean action: 1.925 [0.000, 3.000], mean observation: -0.078 [-1.117, 1.392], loss: 6.922886, mae: 45.656582, mean_q: 61.749828
  232230/1100000: episode: 467, duration: 2.352s, episode steps: 397, steps per second: 169, episode reward: 206.776, mean reward: 0.521 [-20.839, 100.000], mean action: 1.952 [0.000, 3.000], mean observation: 0.156 [-0.614, 1.402], loss: 6.359786, mae: 45.465935, mean_q: 61.259499
  232642/1100000: episode: 468, duration: 2.452s, episode steps: 412, steps per second: 168, episode reward: 243.836, mean reward: 0.592 [-17.969, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.081 [-0.737, 1.411], loss: 7.704334, mae: 45.669357, mean_q: 61.430874
  233018/1100000: episode: 469, duration: 2.180s, episode steps: 376, steps per second: 172, episode reward: 244.470, mean reward: 0.650 [-9.662, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: -0.029 [-0.922, 1.393], loss: 6.629818, mae: 45.823833, mean_q: 61.652218
  233315/1100000: episode: 470, duration: 1.761s, episode steps: 297, steps per second: 169, episode reward: 8.999, mean reward: 0.030 [-100.000, 16.778], mean action: 1.785 [0.000, 3.000], mean observation: 0.118 [-0.674, 1.411], loss: 8.705742, mae: 45.497391, mean_q: 61.184345
  233871/1100000: episode: 471, duration: 3.416s, episode steps: 556, steps per second: 163, episode reward: -161.563, mean reward: -0.291 [-100.000, 21.440], mean action: 2.007 [0.000, 3.000], mean observation: 0.097 [-1.004, 1.405], loss: 6.084807, mae: 45.571415, mean_q: 61.453995
  234371/1100000: episode: 472, duration: 3.049s, episode steps: 500, steps per second: 164, episode reward: 211.498, mean reward: 0.423 [-12.119, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: -0.018 [-1.017, 1.391], loss: 6.435073, mae: 45.671436, mean_q: 61.567726
  234529/1100000: episode: 473, duration: 0.906s, episode steps: 158, steps per second: 174, episode reward: -84.455, mean reward: -0.535 [-100.000, 17.315], mean action: 1.918 [0.000, 3.000], mean observation: -0.068 [-0.856, 1.395], loss: 4.967919, mae: 45.187019, mean_q: 60.997997
  235068/1100000: episode: 474, duration: 3.194s, episode steps: 539, steps per second: 169, episode reward: 188.400, mean reward: 0.350 [-10.027, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.176 [-0.699, 1.416], loss: 4.969158, mae: 45.337585, mean_q: 61.128929
  235195/1100000: episode: 475, duration: 0.712s, episode steps: 127, steps per second: 178, episode reward: -28.446, mean reward: -0.224 [-100.000, 10.447], mean action: 1.323 [0.000, 3.000], mean observation: 0.097 [-0.911, 1.747], loss: 14.031497, mae: 45.027973, mean_q: 60.522137
  235499/1100000: episode: 476, duration: 1.779s, episode steps: 304, steps per second: 171, episode reward: 254.918, mean reward: 0.839 [-20.191, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.001 [-0.987, 1.390], loss: 5.156581, mae: 45.402203, mean_q: 61.110596
  235788/1100000: episode: 477, duration: 1.648s, episode steps: 289, steps per second: 175, episode reward: 199.309, mean reward: 0.690 [-11.724, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: -0.033 [-0.665, 1.431], loss: 7.472795, mae: 44.980347, mean_q: 60.711006
  236234/1100000: episode: 478, duration: 2.733s, episode steps: 446, steps per second: 163, episode reward: 220.009, mean reward: 0.493 [-10.348, 100.000], mean action: 1.908 [0.000, 3.000], mean observation: 0.208 [-0.641, 1.393], loss: 7.680454, mae: 45.285099, mean_q: 60.919426
  236539/1100000: episode: 479, duration: 1.749s, episode steps: 305, steps per second: 174, episode reward: 236.309, mean reward: 0.775 [-17.329, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.113 [-0.740, 1.409], loss: 8.854552, mae: 45.337749, mean_q: 60.975346
  236847/1100000: episode: 480, duration: 1.790s, episode steps: 308, steps per second: 172, episode reward: 239.021, mean reward: 0.776 [-10.840, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: -0.055 [-1.093, 1.398], loss: 6.734440, mae: 45.331165, mean_q: 60.901772
  237350/1100000: episode: 481, duration: 2.961s, episode steps: 503, steps per second: 170, episode reward: 268.070, mean reward: 0.533 [-18.533, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.230 [-0.819, 1.438], loss: 6.406594, mae: 45.207108, mean_q: 60.758549
  237831/1100000: episode: 482, duration: 2.886s, episode steps: 481, steps per second: 167, episode reward: 245.658, mean reward: 0.511 [-20.267, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.059 [-0.712, 1.388], loss: 10.119205, mae: 45.382202, mean_q: 61.124020
  238502/1100000: episode: 483, duration: 4.211s, episode steps: 671, steps per second: 159, episode reward: 227.904, mean reward: 0.340 [-19.023, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.190 [-0.767, 1.403], loss: 7.860825, mae: 45.080986, mean_q: 60.605919
  238759/1100000: episode: 484, duration: 1.487s, episode steps: 257, steps per second: 173, episode reward: 271.476, mean reward: 1.056 [-9.231, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.019 [-1.034, 1.425], loss: 11.250725, mae: 45.254425, mean_q: 60.923340
  239649/1100000: episode: 485, duration: 5.656s, episode steps: 890, steps per second: 157, episode reward: -85.369, mean reward: -0.096 [-100.000, 14.647], mean action: 1.893 [0.000, 3.000], mean observation: 0.100 [-0.783, 1.429], loss: 6.189472, mae: 45.069176, mean_q: 60.563904
  240238/1100000: episode: 486, duration: 3.778s, episode steps: 589, steps per second: 156, episode reward: 158.014, mean reward: 0.268 [-14.700, 100.000], mean action: 1.888 [0.000, 3.000], mean observation: 0.114 [-0.678, 1.462], loss: 6.255987, mae: 44.890339, mean_q: 60.319630
  240431/1100000: episode: 487, duration: 1.112s, episode steps: 193, steps per second: 174, episode reward: -160.683, mean reward: -0.833 [-100.000, 5.352], mean action: 1.751 [0.000, 3.000], mean observation: 0.008 [-1.020, 1.428], loss: 5.610394, mae: 44.639938, mean_q: 60.096382
  241029/1100000: episode: 488, duration: 3.790s, episode steps: 598, steps per second: 158, episode reward: 231.617, mean reward: 0.387 [-17.567, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.138 [-0.800, 1.392], loss: 6.743268, mae: 44.790436, mean_q: 60.051792
  241172/1100000: episode: 489, duration: 0.819s, episode steps: 143, steps per second: 175, episode reward: -66.742, mean reward: -0.467 [-100.000, 22.197], mean action: 1.839 [0.000, 3.000], mean observation: 0.089 [-2.143, 1.395], loss: 5.677037, mae: 44.824848, mean_q: 60.338425
  241441/1100000: episode: 490, duration: 1.575s, episode steps: 269, steps per second: 171, episode reward: 243.196, mean reward: 0.904 [-10.556, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: -0.045 [-0.916, 1.401], loss: 8.049401, mae: 44.882030, mean_q: 60.352978
  241760/1100000: episode: 491, duration: 1.897s, episode steps: 319, steps per second: 168, episode reward: 229.814, mean reward: 0.720 [-9.262, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: -0.044 [-0.889, 1.469], loss: 7.420350, mae: 44.829937, mean_q: 60.431755
  242017/1100000: episode: 492, duration: 1.503s, episode steps: 257, steps per second: 171, episode reward: -105.281, mean reward: -0.410 [-100.000, 9.494], mean action: 1.770 [0.000, 3.000], mean observation: 0.035 [-0.718, 2.774], loss: 6.970891, mae: 45.320572, mean_q: 60.852398
  242667/1100000: episode: 493, duration: 4.127s, episode steps: 650, steps per second: 157, episode reward: 210.493, mean reward: 0.324 [-20.271, 100.000], mean action: 1.717 [0.000, 3.000], mean observation: 0.156 [-0.829, 1.403], loss: 5.869879, mae: 44.878410, mean_q: 60.515011
  243015/1100000: episode: 494, duration: 2.052s, episode steps: 348, steps per second: 170, episode reward: 246.021, mean reward: 0.707 [-3.233, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: -0.026 [-0.706, 1.493], loss: 6.441385, mae: 44.955368, mean_q: 60.370281
  243928/1100000: episode: 495, duration: 6.075s, episode steps: 913, steps per second: 150, episode reward: 235.158, mean reward: 0.258 [-19.723, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.219 [-0.780, 1.431], loss: 6.282914, mae: 44.510311, mean_q: 60.062115
  244514/1100000: episode: 496, duration: 3.508s, episode steps: 586, steps per second: 167, episode reward: 257.070, mean reward: 0.439 [-23.072, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.109 [-0.877, 1.398], loss: 7.893510, mae: 44.411949, mean_q: 59.403530
  244849/1100000: episode: 497, duration: 2.005s, episode steps: 335, steps per second: 167, episode reward: -5.540, mean reward: -0.017 [-100.000, 22.115], mean action: 1.776 [0.000, 3.000], mean observation: -0.023 [-0.894, 1.456], loss: 5.750700, mae: 44.771503, mean_q: 59.932709
  245240/1100000: episode: 498, duration: 2.317s, episode steps: 391, steps per second: 169, episode reward: 220.519, mean reward: 0.564 [-18.941, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.180 [-0.780, 1.386], loss: 4.298676, mae: 44.793636, mean_q: 60.117027
  246240/1100000: episode: 499, duration: 6.881s, episode steps: 1000, steps per second: 145, episode reward: -25.756, mean reward: -0.026 [-6.084, 5.211], mean action: 1.786 [0.000, 3.000], mean observation: 0.093 [-0.764, 1.427], loss: 6.468557, mae: 44.860680, mean_q: 60.345592
  246524/1100000: episode: 500, duration: 1.661s, episode steps: 284, steps per second: 171, episode reward: 212.289, mean reward: 0.747 [-3.600, 100.000], mean action: 1.708 [0.000, 3.000], mean observation: -0.044 [-0.778, 1.422], loss: 6.909235, mae: 44.785934, mean_q: 60.218452
  247524/1100000: episode: 501, duration: 6.634s, episode steps: 1000, steps per second: 151, episode reward: -58.789, mean reward: -0.059 [-5.122, 5.076], mean action: 1.872 [0.000, 3.000], mean observation: 0.081 [-0.784, 1.409], loss: 7.999825, mae: 44.796398, mean_q: 60.348892
  248189/1100000: episode: 502, duration: 3.990s, episode steps: 665, steps per second: 167, episode reward: 194.884, mean reward: 0.293 [-18.901, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.047 [-0.871, 1.455], loss: 5.687404, mae: 44.641972, mean_q: 60.010174
  249189/1100000: episode: 503, duration: 6.431s, episode steps: 1000, steps per second: 155, episode reward: -26.975, mean reward: -0.027 [-5.525, 5.006], mean action: 1.902 [0.000, 3.000], mean observation: 0.079 [-0.837, 1.398], loss: 8.044648, mae: 44.967178, mean_q: 60.440937
  250124/1100000: episode: 504, duration: 6.423s, episode steps: 935, steps per second: 146, episode reward: 137.005, mean reward: 0.147 [-18.189, 100.000], mean action: 1.791 [0.000, 3.000], mean observation: 0.141 [-0.580, 1.447], loss: 6.282386, mae: 44.348503, mean_q: 59.688286
  250461/1100000: episode: 505, duration: 2.036s, episode steps: 337, steps per second: 165, episode reward: 256.671, mean reward: 0.762 [-10.452, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.105 [-0.667, 1.407], loss: 5.567865, mae: 44.293808, mean_q: 59.581615
  250692/1100000: episode: 506, duration: 1.335s, episode steps: 231, steps per second: 173, episode reward: -45.316, mean reward: -0.196 [-100.000, 23.564], mean action: 1.355 [0.000, 3.000], mean observation: -0.086 [-0.600, 1.798], loss: 7.669706, mae: 45.035149, mean_q: 60.724770
  250858/1100000: episode: 507, duration: 0.945s, episode steps: 166, steps per second: 176, episode reward: -9.567, mean reward: -0.058 [-100.000, 17.345], mean action: 1.512 [0.000, 3.000], mean observation: -0.098 [-0.772, 1.540], loss: 6.653033, mae: 43.971462, mean_q: 59.149509
  250933/1100000: episode: 508, duration: 0.425s, episode steps: 75, steps per second: 176, episode reward: -43.360, mean reward: -0.578 [-100.000, 13.849], mean action: 1.187 [0.000, 3.000], mean observation: -0.116 [-1.175, 1.395], loss: 2.932680, mae: 44.507511, mean_q: 59.742352
  251329/1100000: episode: 509, duration: 2.359s, episode steps: 396, steps per second: 168, episode reward: 218.291, mean reward: 0.551 [-11.433, 100.000], mean action: 1.644 [0.000, 3.000], mean observation: 0.013 [-0.600, 1.406], loss: 5.483007, mae: 44.595844, mean_q: 59.837318
  252329/1100000: episode: 510, duration: 6.247s, episode steps: 1000, steps per second: 160, episode reward: 112.143, mean reward: 0.112 [-20.020, 22.651], mean action: 1.713 [0.000, 3.000], mean observation: 0.156 [-0.922, 1.425], loss: 9.109959, mae: 44.627636, mean_q: 59.926022
  252691/1100000: episode: 511, duration: 2.202s, episode steps: 362, steps per second: 164, episode reward: 221.837, mean reward: 0.613 [-18.947, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.154 [-0.866, 1.412], loss: 6.503223, mae: 44.202343, mean_q: 59.534180
  253019/1100000: episode: 512, duration: 2.020s, episode steps: 328, steps per second: 162, episode reward: 287.722, mean reward: 0.877 [-2.473, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.132 [-0.817, 1.464], loss: 6.320611, mae: 44.504894, mean_q: 59.986126
  253665/1100000: episode: 513, duration: 3.938s, episode steps: 646, steps per second: 164, episode reward: 229.265, mean reward: 0.355 [-10.122, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.008 [-0.600, 1.453], loss: 7.663143, mae: 44.741432, mean_q: 60.259335
  253821/1100000: episode: 514, duration: 0.900s, episode steps: 156, steps per second: 173, episode reward: -106.163, mean reward: -0.681 [-100.000, 4.238], mean action: 1.865 [0.000, 3.000], mean observation: 0.152 [-0.600, 1.523], loss: 9.079415, mae: 44.536251, mean_q: 60.004093
  254359/1100000: episode: 515, duration: 3.263s, episode steps: 538, steps per second: 165, episode reward: 223.767, mean reward: 0.416 [-11.513, 100.000], mean action: 1.623 [0.000, 3.000], mean observation: 0.021 [-0.744, 1.392], loss: 6.395095, mae: 44.323154, mean_q: 59.700764
  254743/1100000: episode: 516, duration: 2.269s, episode steps: 384, steps per second: 169, episode reward: 236.180, mean reward: 0.615 [-8.301, 100.000], mean action: 1.641 [0.000, 3.000], mean observation: 0.159 [-0.770, 1.419], loss: 7.644058, mae: 44.577087, mean_q: 59.980316
  255054/1100000: episode: 517, duration: 1.857s, episode steps: 311, steps per second: 168, episode reward: 243.767, mean reward: 0.784 [-18.833, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.087 [-0.815, 1.410], loss: 6.564599, mae: 44.669510, mean_q: 60.146652
  255458/1100000: episode: 518, duration: 2.430s, episode steps: 404, steps per second: 166, episode reward: 198.555, mean reward: 0.491 [-18.467, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.188 [-0.757, 1.419], loss: 9.357409, mae: 44.819298, mean_q: 60.282459
  255788/1100000: episode: 519, duration: 1.931s, episode steps: 330, steps per second: 171, episode reward: -5.370, mean reward: -0.016 [-100.000, 10.329], mean action: 1.464 [0.000, 3.000], mean observation: -0.060 [-0.753, 1.515], loss: 8.168576, mae: 45.015671, mean_q: 60.443047
  256038/1100000: episode: 520, duration: 1.457s, episode steps: 250, steps per second: 172, episode reward: 277.188, mean reward: 1.109 [-18.434, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.082 [-0.691, 1.402], loss: 10.146613, mae: 44.449104, mean_q: 59.887081
  256549/1100000: episode: 521, duration: 3.177s, episode steps: 511, steps per second: 161, episode reward: 212.024, mean reward: 0.415 [-11.432, 100.000], mean action: 1.906 [0.000, 3.000], mean observation: -0.074 [-0.736, 1.449], loss: 6.114018, mae: 45.158440, mean_q: 60.740627
  256818/1100000: episode: 522, duration: 1.576s, episode steps: 269, steps per second: 171, episode reward: 271.043, mean reward: 1.008 [-9.862, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.064 [-0.588, 1.512], loss: 8.949878, mae: 45.318592, mean_q: 60.932652
  257246/1100000: episode: 523, duration: 2.539s, episode steps: 428, steps per second: 169, episode reward: 296.903, mean reward: 0.694 [-19.449, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.071 [-0.855, 1.395], loss: 4.811059, mae: 44.874184, mean_q: 60.485207
  257591/1100000: episode: 524, duration: 2.049s, episode steps: 345, steps per second: 168, episode reward: 249.415, mean reward: 0.723 [-18.453, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.093 [-0.659, 1.439], loss: 8.017359, mae: 44.982201, mean_q: 60.427219
  257928/1100000: episode: 525, duration: 1.990s, episode steps: 337, steps per second: 169, episode reward: 264.048, mean reward: 0.784 [-18.695, 100.000], mean action: 2.228 [0.000, 3.000], mean observation: 0.201 [-0.714, 1.396], loss: 5.224433, mae: 44.365719, mean_q: 59.788898
  258158/1100000: episode: 526, duration: 1.311s, episode steps: 230, steps per second: 175, episode reward: 238.160, mean reward: 1.035 [-12.465, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.097 [-1.271, 1.409], loss: 8.104776, mae: 45.791313, mean_q: 61.445473
  258523/1100000: episode: 527, duration: 2.183s, episode steps: 365, steps per second: 167, episode reward: 212.879, mean reward: 0.583 [-11.372, 100.000], mean action: 1.940 [0.000, 3.000], mean observation: -0.019 [-0.600, 1.427], loss: 7.766402, mae: 45.041386, mean_q: 60.508266
  258675/1100000: episode: 528, duration: 0.865s, episode steps: 152, steps per second: 176, episode reward: 15.973, mean reward: 0.105 [-100.000, 12.337], mean action: 1.572 [0.000, 3.000], mean observation: 0.019 [-0.687, 1.392], loss: 5.740764, mae: 45.031979, mean_q: 60.584942
  259675/1100000: episode: 529, duration: 6.551s, episode steps: 1000, steps per second: 153, episode reward: 143.713, mean reward: 0.144 [-19.198, 22.072], mean action: 1.768 [0.000, 3.000], mean observation: 0.261 [-0.734, 1.391], loss: 7.692685, mae: 45.182064, mean_q: 60.735424
  260640/1100000: episode: 530, duration: 5.972s, episode steps: 965, steps per second: 162, episode reward: 223.015, mean reward: 0.231 [-20.418, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.199 [-0.795, 1.496], loss: 6.621648, mae: 45.070499, mean_q: 60.524181
  260892/1100000: episode: 531, duration: 1.455s, episode steps: 252, steps per second: 173, episode reward: 240.404, mean reward: 0.954 [-9.817, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.068 [-0.845, 1.423], loss: 10.052070, mae: 45.283020, mean_q: 60.845478
  261892/1100000: episode: 532, duration: 7.919s, episode steps: 1000, steps per second: 126, episode reward: -68.191, mean reward: -0.068 [-4.094, 4.771], mean action: 1.622 [0.000, 3.000], mean observation: 0.009 [-0.600, 1.415], loss: 6.031703, mae: 45.034760, mean_q: 60.565910
  262738/1100000: episode: 533, duration: 5.745s, episode steps: 846, steps per second: 147, episode reward: 219.939, mean reward: 0.260 [-19.192, 100.000], mean action: 0.881 [0.000, 3.000], mean observation: 0.165 [-0.677, 1.398], loss: 6.415102, mae: 45.145344, mean_q: 60.843555
  262919/1100000: episode: 534, duration: 1.038s, episode steps: 181, steps per second: 174, episode reward: 17.041, mean reward: 0.094 [-100.000, 13.178], mean action: 1.680 [0.000, 3.000], mean observation: 0.171 [-0.604, 1.398], loss: 6.388603, mae: 45.135803, mean_q: 60.762634
  263038/1100000: episode: 535, duration: 0.674s, episode steps: 119, steps per second: 176, episode reward: 8.914, mean reward: 0.075 [-100.000, 19.772], mean action: 1.487 [0.000, 3.000], mean observation: 0.069 [-1.476, 1.412], loss: 12.463957, mae: 45.780727, mean_q: 61.599930
  264038/1100000: episode: 536, duration: 6.454s, episode steps: 1000, steps per second: 155, episode reward: 27.748, mean reward: 0.028 [-19.993, 18.032], mean action: 2.001 [0.000, 3.000], mean observation: 0.178 [-0.781, 1.390], loss: 6.158883, mae: 45.506897, mean_q: 61.148617
  264516/1100000: episode: 537, duration: 2.881s, episode steps: 478, steps per second: 166, episode reward: 235.658, mean reward: 0.493 [-9.384, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.009 [-1.120, 1.513], loss: 6.620633, mae: 45.110882, mean_q: 60.680790
  264664/1100000: episode: 538, duration: 0.849s, episode steps: 148, steps per second: 174, episode reward: 40.446, mean reward: 0.273 [-100.000, 21.583], mean action: 1.682 [0.000, 3.000], mean observation: 0.177 [-1.758, 1.439], loss: 3.846049, mae: 45.455578, mean_q: 61.056404
  264910/1100000: episode: 539, duration: 1.434s, episode steps: 246, steps per second: 171, episode reward: 257.815, mean reward: 1.048 [-20.547, 100.000], mean action: 1.740 [0.000, 3.000], mean observation: 0.054 [-0.925, 1.399], loss: 7.225642, mae: 44.747025, mean_q: 60.142860
  265106/1100000: episode: 540, duration: 1.124s, episode steps: 196, steps per second: 174, episode reward: -62.548, mean reward: -0.319 [-100.000, 17.690], mean action: 1.658 [0.000, 3.000], mean observation: 0.105 [-1.450, 1.459], loss: 4.281209, mae: 45.117565, mean_q: 60.862156
  265355/1100000: episode: 541, duration: 1.465s, episode steps: 249, steps per second: 170, episode reward: 218.517, mean reward: 0.878 [-9.373, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.030 [-0.853, 1.404], loss: 6.791816, mae: 45.110542, mean_q: 60.774506
  265820/1100000: episode: 542, duration: 2.861s, episode steps: 465, steps per second: 163, episode reward: 230.339, mean reward: 0.495 [-15.334, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: -0.028 [-0.652, 1.415], loss: 5.764920, mae: 45.420052, mean_q: 61.276379
  266217/1100000: episode: 543, duration: 2.395s, episode steps: 397, steps per second: 166, episode reward: 261.400, mean reward: 0.658 [-19.999, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.095 [-1.002, 1.404], loss: 6.662122, mae: 45.342815, mean_q: 60.990841
  267217/1100000: episode: 544, duration: 6.349s, episode steps: 1000, steps per second: 158, episode reward: 0.857, mean reward: 0.001 [-5.119, 5.740], mean action: 1.816 [0.000, 3.000], mean observation: 0.079 [-0.615, 1.526], loss: 7.701089, mae: 45.401196, mean_q: 61.195572
  267951/1100000: episode: 545, duration: 4.310s, episode steps: 734, steps per second: 170, episode reward: 292.288, mean reward: 0.398 [-23.502, 100.000], mean action: 0.853 [0.000, 3.000], mean observation: 0.167 [-0.866, 1.456], loss: 8.323983, mae: 45.357182, mean_q: 61.114437
  268236/1100000: episode: 546, duration: 1.671s, episode steps: 285, steps per second: 171, episode reward: -55.445, mean reward: -0.195 [-100.000, 17.347], mean action: 1.740 [0.000, 3.000], mean observation: 0.021 [-0.600, 1.407], loss: 2.863405, mae: 45.196758, mean_q: 60.909218
  268336/1100000: episode: 547, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 11.765, mean reward: 0.118 [-100.000, 11.259], mean action: 1.550 [0.000, 3.000], mean observation: 0.069 [-1.283, 1.411], loss: 3.543815, mae: 45.477070, mean_q: 61.351929
  268672/1100000: episode: 548, duration: 1.998s, episode steps: 336, steps per second: 168, episode reward: 202.499, mean reward: 0.603 [-3.385, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.011 [-0.600, 1.394], loss: 4.402929, mae: 45.319744, mean_q: 60.999355
  269023/1100000: episode: 549, duration: 2.060s, episode steps: 351, steps per second: 170, episode reward: 304.426, mean reward: 0.867 [-17.339, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.116 [-0.726, 1.527], loss: 5.924762, mae: 45.549675, mean_q: 61.450966
  269438/1100000: episode: 550, duration: 2.518s, episode steps: 415, steps per second: 165, episode reward: 259.954, mean reward: 0.626 [-12.951, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: -0.022 [-0.781, 1.520], loss: 7.132983, mae: 45.221405, mean_q: 60.950848
  269654/1100000: episode: 551, duration: 1.253s, episode steps: 216, steps per second: 172, episode reward: 231.071, mean reward: 1.070 [-19.637, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.089 [-0.816, 1.405], loss: 4.930426, mae: 44.962627, mean_q: 60.507622
  269959/1100000: episode: 552, duration: 1.798s, episode steps: 305, steps per second: 170, episode reward: -209.279, mean reward: -0.686 [-100.000, 18.206], mean action: 1.787 [0.000, 3.000], mean observation: 0.120 [-1.009, 1.814], loss: 7.674272, mae: 45.768806, mean_q: 61.565933
  270273/1100000: episode: 553, duration: 1.839s, episode steps: 314, steps per second: 171, episode reward: 271.845, mean reward: 0.866 [-17.544, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.039 [-0.818, 1.464], loss: 4.556457, mae: 45.345291, mean_q: 61.004795
  270563/1100000: episode: 554, duration: 1.691s, episode steps: 290, steps per second: 172, episode reward: 274.549, mean reward: 0.947 [-11.028, 100.000], mean action: 1.952 [0.000, 3.000], mean observation: 0.216 [-0.780, 1.408], loss: 6.209952, mae: 45.539688, mean_q: 61.262627
  270892/1100000: episode: 555, duration: 1.959s, episode steps: 329, steps per second: 168, episode reward: 211.276, mean reward: 0.642 [-9.823, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.042 [-0.945, 1.411], loss: 7.297445, mae: 46.129971, mean_q: 62.064835
  271180/1100000: episode: 556, duration: 1.695s, episode steps: 288, steps per second: 170, episode reward: 229.898, mean reward: 0.798 [-8.937, 100.000], mean action: 1.670 [0.000, 3.000], mean observation: 0.148 [-1.046, 1.520], loss: 8.546513, mae: 45.534992, mean_q: 61.262966
  271280/1100000: episode: 557, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -7.616, mean reward: -0.076 [-100.000, 23.744], mean action: 1.460 [0.000, 3.000], mean observation: -0.070 [-0.868, 1.392], loss: 4.214540, mae: 45.996586, mean_q: 61.526615
  271395/1100000: episode: 558, duration: 0.662s, episode steps: 115, steps per second: 174, episode reward: 26.660, mean reward: 0.232 [-100.000, 15.224], mean action: 1.991 [0.000, 3.000], mean observation: 0.030 [-0.931, 1.396], loss: 3.808288, mae: 46.028973, mean_q: 61.753582
  271794/1100000: episode: 559, duration: 2.363s, episode steps: 399, steps per second: 169, episode reward: -3.298, mean reward: -0.008 [-100.000, 13.430], mean action: 1.481 [0.000, 3.000], mean observation: -0.053 [-0.663, 1.487], loss: 9.007274, mae: 45.417313, mean_q: 61.056210
  272100/1100000: episode: 560, duration: 1.816s, episode steps: 306, steps per second: 169, episode reward: -0.438, mean reward: -0.001 [-100.000, 13.868], mean action: 1.680 [0.000, 3.000], mean observation: -0.073 [-1.298, 1.390], loss: 7.499595, mae: 45.514862, mean_q: 61.238338
  272533/1100000: episode: 561, duration: 2.656s, episode steps: 433, steps per second: 163, episode reward: 242.437, mean reward: 0.560 [-15.140, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: -0.051 [-0.789, 1.422], loss: 8.848260, mae: 45.823372, mean_q: 61.779972
  272862/1100000: episode: 562, duration: 1.943s, episode steps: 329, steps per second: 169, episode reward: 243.428, mean reward: 0.740 [-3.803, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.020 [-0.861, 1.430], loss: 6.612378, mae: 45.761913, mean_q: 61.471066
  273532/1100000: episode: 563, duration: 4.244s, episode steps: 670, steps per second: 158, episode reward: 161.480, mean reward: 0.241 [-17.968, 100.000], mean action: 1.731 [0.000, 3.000], mean observation: 0.027 [-0.706, 1.391], loss: 8.240421, mae: 45.525372, mean_q: 61.251156
  273691/1100000: episode: 564, duration: 0.904s, episode steps: 159, steps per second: 176, episode reward: -203.161, mean reward: -1.278 [-100.000, 44.541], mean action: 1.421 [0.000, 3.000], mean observation: 0.212 [-1.349, 3.036], loss: 6.607461, mae: 45.157589, mean_q: 60.724709
  274101/1100000: episode: 565, duration: 2.467s, episode steps: 410, steps per second: 166, episode reward: 219.503, mean reward: 0.535 [-5.471, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.177 [-0.652, 1.433], loss: 6.816026, mae: 45.196903, mean_q: 60.685127
  274235/1100000: episode: 566, duration: 0.758s, episode steps: 134, steps per second: 177, episode reward: -28.521, mean reward: -0.213 [-100.000, 9.119], mean action: 1.321 [0.000, 3.000], mean observation: -0.096 [-3.205, 1.461], loss: 10.077796, mae: 45.427933, mean_q: 61.141987
  275096/1100000: episode: 567, duration: 6.046s, episode steps: 861, steps per second: 142, episode reward: 174.347, mean reward: 0.202 [-19.824, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: -0.051 [-0.738, 1.474], loss: 7.280150, mae: 45.185574, mean_q: 60.664654
  276096/1100000: episode: 568, duration: 6.631s, episode steps: 1000, steps per second: 151, episode reward: -12.386, mean reward: -0.012 [-11.565, 11.980], mean action: 1.559 [0.000, 3.000], mean observation: -0.068 [-1.048, 1.444], loss: 6.761880, mae: 45.185772, mean_q: 60.807663
  276333/1100000: episode: 569, duration: 1.371s, episode steps: 237, steps per second: 173, episode reward: -98.748, mean reward: -0.417 [-100.000, 3.861], mean action: 1.848 [0.000, 3.000], mean observation: 0.094 [-0.600, 1.411], loss: 7.715649, mae: 45.589638, mean_q: 61.304615
  277333/1100000: episode: 570, duration: 6.453s, episode steps: 1000, steps per second: 155, episode reward: 65.207, mean reward: 0.065 [-18.988, 17.377], mean action: 1.756 [0.000, 3.000], mean observation: 0.249 [-0.578, 1.394], loss: 7.195406, mae: 44.893673, mean_q: 60.416237
  278333/1100000: episode: 571, duration: 6.923s, episode steps: 1000, steps per second: 144, episode reward: 89.565, mean reward: 0.090 [-20.795, 22.437], mean action: 1.779 [0.000, 3.000], mean observation: 0.156 [-0.766, 1.468], loss: 7.857401, mae: 45.083797, mean_q: 60.662567
  279025/1100000: episode: 572, duration: 4.543s, episode steps: 692, steps per second: 152, episode reward: 211.536, mean reward: 0.306 [-17.449, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.168 [-0.882, 1.398], loss: 6.075787, mae: 44.476631, mean_q: 59.839470
  279299/1100000: episode: 573, duration: 1.596s, episode steps: 274, steps per second: 172, episode reward: -197.666, mean reward: -0.721 [-100.000, 5.527], mean action: 1.577 [0.000, 3.000], mean observation: 0.026 [-1.283, 4.424], loss: 11.949663, mae: 44.151470, mean_q: 59.580406
  279581/1100000: episode: 574, duration: 1.685s, episode steps: 282, steps per second: 167, episode reward: 248.418, mean reward: 0.881 [-2.882, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.076 [-0.500, 1.403], loss: 7.205752, mae: 44.079716, mean_q: 59.327682
  279931/1100000: episode: 575, duration: 2.102s, episode steps: 350, steps per second: 167, episode reward: 224.946, mean reward: 0.643 [-17.846, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.062 [-0.800, 1.415], loss: 8.021068, mae: 43.907871, mean_q: 59.082878
  280323/1100000: episode: 576, duration: 2.290s, episode steps: 392, steps per second: 171, episode reward: 188.709, mean reward: 0.481 [-12.865, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.116 [-0.882, 1.419], loss: 7.852798, mae: 44.384857, mean_q: 59.656467
  280714/1100000: episode: 577, duration: 2.333s, episode steps: 391, steps per second: 168, episode reward: 241.834, mean reward: 0.619 [-3.781, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.181 [-0.701, 1.547], loss: 8.541783, mae: 44.659283, mean_q: 59.980648
  281212/1100000: episode: 578, duration: 3.010s, episode steps: 498, steps per second: 165, episode reward: 205.714, mean reward: 0.413 [-18.712, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.196 [-0.609, 1.395], loss: 5.918679, mae: 44.765411, mean_q: 60.202538
  282212/1100000: episode: 579, duration: 6.381s, episode steps: 1000, steps per second: 157, episode reward: 67.581, mean reward: 0.068 [-18.476, 12.399], mean action: 1.768 [0.000, 3.000], mean observation: 0.265 [-0.556, 1.406], loss: 8.463662, mae: 44.338634, mean_q: 59.670399
  282759/1100000: episode: 580, duration: 3.381s, episode steps: 547, steps per second: 162, episode reward: 221.478, mean reward: 0.405 [-18.946, 100.000], mean action: 2.452 [0.000, 3.000], mean observation: 0.069 [-1.209, 1.441], loss: 5.845434, mae: 44.723370, mean_q: 60.150352
  283152/1100000: episode: 581, duration: 2.352s, episode steps: 393, steps per second: 167, episode reward: 174.618, mean reward: 0.444 [-12.982, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: -0.016 [-0.600, 1.411], loss: 7.077803, mae: 44.683922, mean_q: 60.071705
  283275/1100000: episode: 582, duration: 0.704s, episode steps: 123, steps per second: 175, episode reward: -47.690, mean reward: -0.388 [-100.000, 16.290], mean action: 1.789 [0.000, 3.000], mean observation: -0.006 [-1.512, 1.475], loss: 9.199417, mae: 44.966267, mean_q: 60.542522
  283698/1100000: episode: 583, duration: 2.608s, episode steps: 423, steps per second: 162, episode reward: 251.946, mean reward: 0.596 [-10.584, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.208 [-0.746, 1.388], loss: 7.803023, mae: 45.115055, mean_q: 60.691650
  284536/1100000: episode: 584, duration: 5.288s, episode steps: 838, steps per second: 158, episode reward: 230.945, mean reward: 0.276 [-19.695, 100.000], mean action: 0.893 [0.000, 3.000], mean observation: 0.008 [-0.663, 1.402], loss: 6.625241, mae: 45.173393, mean_q: 60.810226
  284907/1100000: episode: 585, duration: 2.263s, episode steps: 371, steps per second: 164, episode reward: 243.430, mean reward: 0.656 [-10.911, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.079 [-0.921, 1.388], loss: 7.350695, mae: 44.818798, mean_q: 60.362072
  285347/1100000: episode: 586, duration: 2.677s, episode steps: 440, steps per second: 164, episode reward: 225.279, mean reward: 0.512 [-17.679, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.091 [-0.553, 1.401], loss: 5.948860, mae: 45.215973, mean_q: 60.918156
  285738/1100000: episode: 587, duration: 2.326s, episode steps: 391, steps per second: 168, episode reward: 231.668, mean reward: 0.593 [-8.188, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: -0.036 [-0.676, 1.441], loss: 8.563705, mae: 44.822899, mean_q: 60.391815
  285915/1100000: episode: 588, duration: 1.014s, episode steps: 177, steps per second: 175, episode reward: 44.117, mean reward: 0.249 [-100.000, 14.222], mean action: 1.503 [0.000, 3.000], mean observation: 0.072 [-0.846, 1.500], loss: 6.521748, mae: 45.249546, mean_q: 60.905224
  286078/1100000: episode: 589, duration: 0.928s, episode steps: 163, steps per second: 176, episode reward: 10.065, mean reward: 0.062 [-100.000, 20.749], mean action: 1.442 [0.000, 3.000], mean observation: 0.026 [-1.803, 1.430], loss: 9.436371, mae: 45.019142, mean_q: 60.540981
  286567/1100000: episode: 590, duration: 2.974s, episode steps: 489, steps per second: 164, episode reward: 269.478, mean reward: 0.551 [-17.178, 100.000], mean action: 1.920 [0.000, 3.000], mean observation: 0.138 [-0.782, 1.401], loss: 7.755128, mae: 45.010017, mean_q: 60.693539
  286828/1100000: episode: 591, duration: 1.513s, episode steps: 261, steps per second: 172, episode reward: 259.268, mean reward: 0.993 [-3.042, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.047 [-0.850, 1.423], loss: 8.356138, mae: 45.139931, mean_q: 60.659309
  287828/1100000: episode: 592, duration: 6.109s, episode steps: 1000, steps per second: 164, episode reward: 33.901, mean reward: 0.034 [-20.216, 16.018], mean action: 1.001 [0.000, 3.000], mean observation: 0.023 [-0.722, 1.397], loss: 6.023676, mae: 45.042995, mean_q: 60.657772
  288131/1100000: episode: 593, duration: 1.745s, episode steps: 303, steps per second: 174, episode reward: 273.918, mean reward: 0.904 [-7.837, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.154 [-0.993, 1.523], loss: 5.423204, mae: 44.905823, mean_q: 60.500504
  288439/1100000: episode: 594, duration: 1.821s, episode steps: 308, steps per second: 169, episode reward: 251.329, mean reward: 0.816 [-17.426, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.193 [-0.841, 1.389], loss: 9.856215, mae: 44.828003, mean_q: 60.362278
  288773/1100000: episode: 595, duration: 1.962s, episode steps: 334, steps per second: 170, episode reward: 280.047, mean reward: 0.838 [-10.246, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.130 [-0.850, 1.455], loss: 7.779380, mae: 44.700825, mean_q: 60.158848
  289073/1100000: episode: 596, duration: 1.766s, episode steps: 300, steps per second: 170, episode reward: 203.565, mean reward: 0.679 [-13.697, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.156 [-0.552, 1.425], loss: 4.745893, mae: 45.103882, mean_q: 60.687557
  289615/1100000: episode: 597, duration: 3.344s, episode steps: 542, steps per second: 162, episode reward: 264.717, mean reward: 0.488 [-18.435, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.094 [-1.043, 1.387], loss: 5.217456, mae: 44.509644, mean_q: 59.853916
  289900/1100000: episode: 598, duration: 1.683s, episode steps: 285, steps per second: 169, episode reward: 255.256, mean reward: 0.896 [-8.917, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.071 [-0.918, 1.449], loss: 5.816496, mae: 44.645737, mean_q: 60.126194
  290322/1100000: episode: 599, duration: 2.555s, episode steps: 422, steps per second: 165, episode reward: 209.973, mean reward: 0.498 [-4.281, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: -0.026 [-1.030, 1.469], loss: 6.817060, mae: 44.765430, mean_q: 60.240074
  290997/1100000: episode: 600, duration: 4.036s, episode steps: 675, steps per second: 167, episode reward: 185.449, mean reward: 0.275 [-18.077, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.186 [-0.765, 1.437], loss: 6.507906, mae: 44.815994, mean_q: 60.230068
  291417/1100000: episode: 601, duration: 2.555s, episode steps: 420, steps per second: 164, episode reward: 170.806, mean reward: 0.407 [-3.471, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: -0.057 [-0.600, 1.413], loss: 5.080794, mae: 44.610275, mean_q: 60.087925
  291967/1100000: episode: 602, duration: 3.335s, episode steps: 550, steps per second: 165, episode reward: 245.769, mean reward: 0.447 [-17.780, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.104 [-0.706, 1.493], loss: 6.459195, mae: 44.634983, mean_q: 59.998188
  292240/1100000: episode: 603, duration: 1.583s, episode steps: 273, steps per second: 173, episode reward: 283.170, mean reward: 1.037 [-17.737, 100.000], mean action: 1.729 [0.000, 3.000], mean observation: 0.084 [-0.776, 1.529], loss: 6.268157, mae: 44.011559, mean_q: 59.275764
  292617/1100000: episode: 604, duration: 2.273s, episode steps: 377, steps per second: 166, episode reward: 241.443, mean reward: 0.640 [-11.023, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.049 [-1.111, 1.509], loss: 6.532024, mae: 44.286339, mean_q: 59.642681
  293356/1100000: episode: 605, duration: 4.467s, episode steps: 739, steps per second: 165, episode reward: 157.700, mean reward: 0.213 [-16.043, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: -0.029 [-0.741, 1.513], loss: 6.375891, mae: 44.419231, mean_q: 59.642170
  293679/1100000: episode: 606, duration: 1.878s, episode steps: 323, steps per second: 172, episode reward: 251.566, mean reward: 0.779 [-7.412, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.071 [-1.157, 1.513], loss: 5.854992, mae: 44.013340, mean_q: 59.300129
  294017/1100000: episode: 607, duration: 2.037s, episode steps: 338, steps per second: 166, episode reward: 248.504, mean reward: 0.735 [-10.167, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.088 [-0.791, 1.398], loss: 5.436384, mae: 44.119335, mean_q: 59.271519
  294402/1100000: episode: 608, duration: 2.275s, episode steps: 385, steps per second: 169, episode reward: 218.321, mean reward: 0.567 [-8.547, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: -0.021 [-0.600, 1.444], loss: 8.084793, mae: 44.120029, mean_q: 59.385918
  294617/1100000: episode: 609, duration: 1.251s, episode steps: 215, steps per second: 172, episode reward: -92.355, mean reward: -0.430 [-100.000, 25.594], mean action: 1.493 [0.000, 3.000], mean observation: 0.029 [-2.483, 1.405], loss: 6.928840, mae: 44.374447, mean_q: 59.715870
  294779/1100000: episode: 610, duration: 0.922s, episode steps: 162, steps per second: 176, episode reward: 30.109, mean reward: 0.186 [-100.000, 16.975], mean action: 1.512 [0.000, 3.000], mean observation: 0.061 [-1.557, 1.514], loss: 6.619242, mae: 44.284767, mean_q: 59.566673
  295450/1100000: episode: 611, duration: 4.043s, episode steps: 671, steps per second: 166, episode reward: -211.833, mean reward: -0.316 [-100.000, 14.747], mean action: 1.599 [0.000, 3.000], mean observation: -0.030 [-1.637, 1.496], loss: 6.246701, mae: 44.459076, mean_q: 59.679497
  295690/1100000: episode: 612, duration: 1.377s, episode steps: 240, steps per second: 174, episode reward: 261.830, mean reward: 1.091 [-18.763, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.061 [-0.799, 1.397], loss: 7.443567, mae: 44.016037, mean_q: 59.086853
  295939/1100000: episode: 613, duration: 1.447s, episode steps: 249, steps per second: 172, episode reward: -9.516, mean reward: -0.038 [-100.000, 13.559], mean action: 1.627 [0.000, 3.000], mean observation: -0.100 [-1.251, 1.397], loss: 9.361987, mae: 44.178944, mean_q: 59.181026
  296058/1100000: episode: 614, duration: 0.684s, episode steps: 119, steps per second: 174, episode reward: -144.665, mean reward: -1.216 [-100.000, 3.072], mean action: 2.025 [0.000, 3.000], mean observation: 0.061 [-0.871, 1.406], loss: 5.679747, mae: 44.106567, mean_q: 59.253166
  297010/1100000: episode: 615, duration: 6.191s, episode steps: 952, steps per second: 154, episode reward: 199.692, mean reward: 0.210 [-19.445, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: 0.236 [-0.527, 1.433], loss: 6.952448, mae: 44.426018, mean_q: 59.672459
  297345/1100000: episode: 616, duration: 1.976s, episode steps: 335, steps per second: 170, episode reward: 302.971, mean reward: 0.904 [-9.750, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.108 [-0.627, 1.413], loss: 8.684626, mae: 44.044899, mean_q: 59.318588
  297790/1100000: episode: 617, duration: 2.711s, episode steps: 445, steps per second: 164, episode reward: 194.520, mean reward: 0.437 [-23.312, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.048 [-1.570, 1.450], loss: 6.013176, mae: 43.956547, mean_q: 59.137428
  298437/1100000: episode: 618, duration: 3.947s, episode steps: 647, steps per second: 164, episode reward: 179.629, mean reward: 0.278 [-13.358, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.110 [-0.880, 1.392], loss: 7.428717, mae: 43.762474, mean_q: 58.820107
  298879/1100000: episode: 619, duration: 2.712s, episode steps: 442, steps per second: 163, episode reward: 273.162, mean reward: 0.618 [-15.365, 100.000], mean action: 1.950 [0.000, 3.000], mean observation: 0.011 [-0.817, 1.391], loss: 8.909382, mae: 43.645054, mean_q: 58.562939
  299350/1100000: episode: 620, duration: 2.821s, episode steps: 471, steps per second: 167, episode reward: 250.246, mean reward: 0.531 [-20.393, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.087 [-0.852, 1.413], loss: 6.051672, mae: 43.784245, mean_q: 58.874733
  299782/1100000: episode: 621, duration: 2.610s, episode steps: 432, steps per second: 165, episode reward: 245.976, mean reward: 0.569 [-18.041, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.119 [-0.582, 1.440], loss: 8.377231, mae: 43.979908, mean_q: 59.130791
  300341/1100000: episode: 622, duration: 3.383s, episode steps: 559, steps per second: 165, episode reward: 166.505, mean reward: 0.298 [-8.741, 100.000], mean action: 1.757 [0.000, 3.000], mean observation: 0.004 [-0.600, 1.459], loss: 5.166662, mae: 43.949322, mean_q: 59.111919
  301341/1100000: episode: 623, duration: 6.251s, episode steps: 1000, steps per second: 160, episode reward: 89.672, mean reward: 0.090 [-19.394, 20.888], mean action: 0.957 [0.000, 3.000], mean observation: 0.209 [-0.707, 1.393], loss: 5.371665, mae: 43.645245, mean_q: 58.781837
  302145/1100000: episode: 624, duration: 5.149s, episode steps: 804, steps per second: 156, episode reward: 132.599, mean reward: 0.165 [-20.428, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: -0.014 [-0.600, 1.402], loss: 4.394923, mae: 43.462494, mean_q: 58.519135
  302607/1100000: episode: 625, duration: 2.836s, episode steps: 462, steps per second: 163, episode reward: 214.382, mean reward: 0.464 [-17.755, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.216 [-0.535, 1.409], loss: 4.761693, mae: 43.593597, mean_q: 58.617542
  303284/1100000: episode: 626, duration: 4.456s, episode steps: 677, steps per second: 152, episode reward: 194.104, mean reward: 0.287 [-14.043, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: -0.060 [-0.606, 1.448], loss: 6.929641, mae: 43.568588, mean_q: 58.483891
  303585/1100000: episode: 627, duration: 1.745s, episode steps: 301, steps per second: 172, episode reward: 212.018, mean reward: 0.704 [-17.296, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.174 [-0.949, 1.405], loss: 5.721393, mae: 43.251911, mean_q: 58.114502
  303905/1100000: episode: 628, duration: 1.930s, episode steps: 320, steps per second: 166, episode reward: 255.302, mean reward: 0.798 [-7.327, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.082 [-0.756, 1.395], loss: 6.023194, mae: 43.030842, mean_q: 57.880859
  304122/1100000: episode: 629, duration: 1.253s, episode steps: 217, steps per second: 173, episode reward: -10.559, mean reward: -0.049 [-100.000, 20.362], mean action: 1.562 [0.000, 3.000], mean observation: 0.137 [-0.926, 1.487], loss: 4.462980, mae: 43.483574, mean_q: 58.662434
  304246/1100000: episode: 630, duration: 0.702s, episode steps: 124, steps per second: 177, episode reward: -7.885, mean reward: -0.064 [-100.000, 12.844], mean action: 1.355 [0.000, 3.000], mean observation: 0.000 [-0.884, 1.401], loss: 11.536846, mae: 43.378033, mean_q: 58.364212
  304667/1100000: episode: 631, duration: 2.540s, episode steps: 421, steps per second: 166, episode reward: 221.565, mean reward: 0.526 [-17.956, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.099 [-0.870, 1.408], loss: 4.612787, mae: 43.640812, mean_q: 58.767277
  305077/1100000: episode: 632, duration: 2.550s, episode steps: 410, steps per second: 161, episode reward: 216.110, mean reward: 0.527 [-18.456, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.088 [-0.568, 1.422], loss: 6.306841, mae: 43.241562, mean_q: 58.182652
  305371/1100000: episode: 633, duration: 1.720s, episode steps: 294, steps per second: 171, episode reward: 265.366, mean reward: 0.903 [-19.934, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.084 [-0.962, 1.498], loss: 7.683537, mae: 43.342815, mean_q: 58.231339
  305606/1100000: episode: 634, duration: 1.354s, episode steps: 235, steps per second: 174, episode reward: -347.292, mean reward: -1.478 [-100.000, 4.652], mean action: 1.489 [0.000, 3.000], mean observation: 0.179 [-1.131, 4.327], loss: 10.289353, mae: 43.386528, mean_q: 58.331585
  306065/1100000: episode: 635, duration: 2.876s, episode steps: 459, steps per second: 160, episode reward: 197.064, mean reward: 0.429 [-19.163, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: -0.000 [-0.600, 1.417], loss: 5.974817, mae: 43.458927, mean_q: 58.386837
  306330/1100000: episode: 636, duration: 1.560s, episode steps: 265, steps per second: 170, episode reward: 270.939, mean reward: 1.022 [-2.966, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.095 [-0.757, 1.396], loss: 3.625449, mae: 43.171520, mean_q: 57.880219
  307330/1100000: episode: 637, duration: 6.235s, episode steps: 1000, steps per second: 160, episode reward: 130.257, mean reward: 0.130 [-21.492, 21.888], mean action: 1.934 [0.000, 3.000], mean observation: 0.257 [-0.750, 1.440], loss: 4.881018, mae: 43.316391, mean_q: 58.253929
  307979/1100000: episode: 638, duration: 3.928s, episode steps: 649, steps per second: 165, episode reward: 249.871, mean reward: 0.385 [-19.424, 100.000], mean action: 0.684 [0.000, 3.000], mean observation: 0.256 [-0.706, 1.386], loss: 5.138624, mae: 43.157753, mean_q: 58.064831
  308211/1100000: episode: 639, duration: 1.342s, episode steps: 232, steps per second: 173, episode reward: 303.392, mean reward: 1.308 [-10.231, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: 0.084 [-0.796, 1.387], loss: 5.812578, mae: 43.269279, mean_q: 58.224125
  308533/1100000: episode: 640, duration: 1.976s, episode steps: 322, steps per second: 163, episode reward: 243.360, mean reward: 0.756 [-17.682, 100.000], mean action: 2.335 [0.000, 3.000], mean observation: 0.191 [-1.086, 1.395], loss: 4.806457, mae: 43.434711, mean_q: 58.534561
  308876/1100000: episode: 641, duration: 2.057s, episode steps: 343, steps per second: 167, episode reward: 221.905, mean reward: 0.647 [-10.589, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.228 [-0.625, 1.398], loss: 10.385372, mae: 43.703671, mean_q: 58.707401
  309772/1100000: episode: 642, duration: 5.707s, episode steps: 896, steps per second: 157, episode reward: 172.083, mean reward: 0.192 [-9.475, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.011 [-0.771, 1.408], loss: 6.308568, mae: 43.350777, mean_q: 58.348946
  310175/1100000: episode: 643, duration: 2.407s, episode steps: 403, steps per second: 167, episode reward: 267.776, mean reward: 0.664 [-21.448, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.104 [-0.768, 1.510], loss: 4.815800, mae: 43.251465, mean_q: 58.161755
  310285/1100000: episode: 644, duration: 0.616s, episode steps: 110, steps per second: 179, episode reward: -229.251, mean reward: -2.084 [-100.000, 72.211], mean action: 1.082 [0.000, 3.000], mean observation: 0.057 [-1.305, 3.350], loss: 4.301841, mae: 43.623569, mean_q: 58.780769
  310604/1100000: episode: 645, duration: 1.852s, episode steps: 319, steps per second: 172, episode reward: -149.851, mean reward: -0.470 [-100.000, 10.241], mean action: 1.470 [0.000, 3.000], mean observation: 0.188 [-3.758, 1.447], loss: 4.839272, mae: 43.164772, mean_q: 58.136089
  310792/1100000: episode: 646, duration: 1.074s, episode steps: 188, steps per second: 175, episode reward: -250.605, mean reward: -1.333 [-100.000, 13.031], mean action: 1.473 [0.000, 3.000], mean observation: -0.111 [-2.314, 1.482], loss: 5.342090, mae: 43.504322, mean_q: 58.575436
  311057/1100000: episode: 647, duration: 1.540s, episode steps: 265, steps per second: 172, episode reward: 277.812, mean reward: 1.048 [-10.127, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.095 [-0.571, 1.441], loss: 8.414429, mae: 43.945778, mean_q: 59.241508
  311366/1100000: episode: 648, duration: 1.886s, episode steps: 309, steps per second: 164, episode reward: 227.494, mean reward: 0.736 [-10.174, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: -0.037 [-0.907, 1.527], loss: 6.578058, mae: 44.095192, mean_q: 59.303715
  312110/1100000: episode: 649, duration: 4.656s, episode steps: 744, steps per second: 160, episode reward: 194.300, mean reward: 0.261 [-20.082, 100.000], mean action: 1.724 [0.000, 3.000], mean observation: -0.029 [-0.766, 1.395], loss: 5.392344, mae: 44.598644, mean_q: 59.746990
  312677/1100000: episode: 650, duration: 3.539s, episode steps: 567, steps per second: 160, episode reward: 196.126, mean reward: 0.346 [-18.024, 100.000], mean action: 1.827 [0.000, 3.000], mean observation: 0.173 [-0.733, 1.410], loss: 9.833099, mae: 44.756752, mean_q: 59.948730
  312990/1100000: episode: 651, duration: 1.843s, episode steps: 313, steps per second: 170, episode reward: 234.070, mean reward: 0.748 [-7.644, 100.000], mean action: 1.946 [0.000, 3.000], mean observation: -0.045 [-0.631, 1.521], loss: 8.954707, mae: 44.516289, mean_q: 59.731930
  313258/1100000: episode: 652, duration: 1.554s, episode steps: 268, steps per second: 172, episode reward: 246.713, mean reward: 0.921 [-9.674, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.169 [-0.855, 1.400], loss: 6.315753, mae: 44.304153, mean_q: 59.643635
  313551/1100000: episode: 653, duration: 1.703s, episode steps: 293, steps per second: 172, episode reward: 259.224, mean reward: 0.885 [-8.699, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.090 [-0.833, 1.434], loss: 6.436297, mae: 44.582878, mean_q: 59.686901
  313808/1100000: episode: 654, duration: 1.492s, episode steps: 257, steps per second: 172, episode reward: 244.033, mean reward: 0.950 [-10.248, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.075 [-0.864, 1.410], loss: 8.070759, mae: 44.868259, mean_q: 59.930592
  314211/1100000: episode: 655, duration: 2.475s, episode steps: 403, steps per second: 163, episode reward: 235.718, mean reward: 0.585 [-7.807, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.100 [-0.533, 1.447], loss: 8.432742, mae: 44.730770, mean_q: 60.094345
  314738/1100000: episode: 656, duration: 3.126s, episode steps: 527, steps per second: 169, episode reward: 292.404, mean reward: 0.555 [-17.869, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.131 [-0.544, 1.470], loss: 6.090706, mae: 44.895294, mean_q: 60.175064
  315134/1100000: episode: 657, duration: 2.487s, episode steps: 396, steps per second: 159, episode reward: 196.001, mean reward: 0.495 [-12.463, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: -0.058 [-0.787, 1.406], loss: 4.885146, mae: 45.041794, mean_q: 60.427670
  315453/1100000: episode: 658, duration: 1.888s, episode steps: 319, steps per second: 169, episode reward: 292.876, mean reward: 0.918 [-9.607, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.090 [-0.726, 1.388], loss: 6.763624, mae: 45.130295, mean_q: 60.504860
  315852/1100000: episode: 659, duration: 2.424s, episode steps: 399, steps per second: 165, episode reward: 235.288, mean reward: 0.590 [-9.344, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.055 [-0.642, 1.440], loss: 4.948981, mae: 45.208275, mean_q: 60.620918
  316167/1100000: episode: 660, duration: 1.862s, episode steps: 315, steps per second: 169, episode reward: 261.946, mean reward: 0.832 [-8.286, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.069 [-0.680, 1.520], loss: 5.199160, mae: 45.206100, mean_q: 60.740849
  316824/1100000: episode: 661, duration: 4.103s, episode steps: 657, steps per second: 160, episode reward: 219.809, mean reward: 0.335 [-18.985, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.068 [-0.825, 1.408], loss: 5.986836, mae: 45.313000, mean_q: 60.730320
  317824/1100000: episode: 662, duration: 6.309s, episode steps: 1000, steps per second: 159, episode reward: 130.836, mean reward: 0.131 [-18.972, 15.536], mean action: 1.276 [0.000, 3.000], mean observation: 0.269 [-0.871, 1.385], loss: 6.048280, mae: 45.320641, mean_q: 60.818241
  317976/1100000: episode: 663, duration: 0.867s, episode steps: 152, steps per second: 175, episode reward: 5.474, mean reward: 0.036 [-100.000, 11.678], mean action: 1.645 [0.000, 3.000], mean observation: 0.163 [-1.000, 1.527], loss: 3.304132, mae: 45.271923, mean_q: 60.979362
  318171/1100000: episode: 664, duration: 1.138s, episode steps: 195, steps per second: 171, episode reward: 15.385, mean reward: 0.079 [-100.000, 18.217], mean action: 1.897 [0.000, 3.000], mean observation: -0.027 [-0.600, 1.388], loss: 5.317581, mae: 45.400257, mean_q: 61.046955
  318563/1100000: episode: 665, duration: 2.435s, episode steps: 392, steps per second: 161, episode reward: 255.898, mean reward: 0.653 [-11.028, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.087 [-0.741, 1.388], loss: 6.595529, mae: 45.543133, mean_q: 61.006332
  319130/1100000: episode: 666, duration: 3.600s, episode steps: 567, steps per second: 157, episode reward: 131.228, mean reward: 0.231 [-19.404, 100.000], mean action: 1.746 [0.000, 3.000], mean observation: 0.230 [-1.049, 1.513], loss: 5.164170, mae: 45.436733, mean_q: 61.067791
  319423/1100000: episode: 667, duration: 1.733s, episode steps: 293, steps per second: 169, episode reward: 239.213, mean reward: 0.816 [-9.773, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.202 [-0.899, 1.452], loss: 5.692082, mae: 45.262032, mean_q: 60.790676
  319652/1100000: episode: 668, duration: 1.341s, episode steps: 229, steps per second: 171, episode reward: 194.783, mean reward: 0.851 [-9.480, 100.000], mean action: 1.712 [0.000, 3.000], mean observation: 0.180 [-0.842, 1.427], loss: 4.180410, mae: 45.497852, mean_q: 61.128319
  320011/1100000: episode: 669, duration: 2.164s, episode steps: 359, steps per second: 166, episode reward: 252.751, mean reward: 0.704 [-18.607, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.223 [-0.895, 1.505], loss: 9.301408, mae: 45.755104, mean_q: 61.264797
  320319/1100000: episode: 670, duration: 1.810s, episode steps: 308, steps per second: 170, episode reward: 258.505, mean reward: 0.839 [-17.556, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.061 [-0.724, 1.397], loss: 6.992453, mae: 45.409840, mean_q: 61.011429
  320505/1100000: episode: 671, duration: 1.073s, episode steps: 186, steps per second: 173, episode reward: 42.582, mean reward: 0.229 [-100.000, 14.149], mean action: 1.763 [0.000, 3.000], mean observation: 0.101 [-0.855, 1.385], loss: 3.966609, mae: 45.057495, mean_q: 60.794910
  320829/1100000: episode: 672, duration: 1.935s, episode steps: 324, steps per second: 167, episode reward: 231.587, mean reward: 0.715 [-11.109, 100.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.187 [-0.641, 1.425], loss: 5.029041, mae: 45.754761, mean_q: 61.424911
  320999/1100000: episode: 673, duration: 1.011s, episode steps: 170, steps per second: 168, episode reward: -136.874, mean reward: -0.805 [-100.000, 87.057], mean action: 2.065 [0.000, 3.000], mean observation: 0.173 [-0.990, 1.700], loss: 5.040547, mae: 45.245552, mean_q: 60.819382
  321369/1100000: episode: 674, duration: 2.274s, episode steps: 370, steps per second: 163, episode reward: 257.923, mean reward: 0.697 [-9.785, 100.000], mean action: 1.862 [0.000, 3.000], mean observation: 0.133 [-0.824, 1.402], loss: 4.383136, mae: 45.397381, mean_q: 61.002838
  321807/1100000: episode: 675, duration: 2.609s, episode steps: 438, steps per second: 168, episode reward: 201.404, mean reward: 0.460 [-10.890, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.004 [-0.600, 1.421], loss: 6.814914, mae: 45.440380, mean_q: 61.131660
  322558/1100000: episode: 676, duration: 4.608s, episode steps: 751, steps per second: 163, episode reward: 135.718, mean reward: 0.181 [-21.023, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.002 [-0.617, 1.416], loss: 7.456870, mae: 45.001442, mean_q: 60.331612
  322738/1100000: episode: 677, duration: 1.038s, episode steps: 180, steps per second: 173, episode reward: 35.570, mean reward: 0.198 [-100.000, 15.384], mean action: 1.850 [0.000, 3.000], mean observation: 0.179 [-0.653, 1.433], loss: 7.998195, mae: 45.076244, mean_q: 60.601875
  322920/1100000: episode: 678, duration: 1.049s, episode steps: 182, steps per second: 174, episode reward: 17.573, mean reward: 0.097 [-100.000, 15.774], mean action: 1.714 [0.000, 3.000], mean observation: 0.156 [-0.771, 1.448], loss: 9.782170, mae: 44.853401, mean_q: 60.078564
  323469/1100000: episode: 679, duration: 3.519s, episode steps: 549, steps per second: 156, episode reward: 247.710, mean reward: 0.451 [-17.872, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.115 [-0.803, 1.403], loss: 7.483516, mae: 44.931332, mean_q: 60.331135
  323870/1100000: episode: 680, duration: 2.359s, episode steps: 401, steps per second: 170, episode reward: -41.877, mean reward: -0.104 [-100.000, 21.760], mean action: 1.289 [0.000, 3.000], mean observation: 0.006 [-1.145, 1.458], loss: 6.692292, mae: 45.042084, mean_q: 60.154369
  324151/1100000: episode: 681, duration: 1.646s, episode steps: 281, steps per second: 171, episode reward: 243.677, mean reward: 0.867 [-3.484, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.065 [-0.736, 1.439], loss: 6.337214, mae: 44.919483, mean_q: 60.280834
  324507/1100000: episode: 682, duration: 2.074s, episode steps: 356, steps per second: 172, episode reward: -315.542, mean reward: -0.886 [-100.000, 17.969], mean action: 1.385 [0.000, 3.000], mean observation: 0.123 [-0.805, 2.442], loss: 5.107716, mae: 45.281216, mean_q: 60.806793
  324679/1100000: episode: 683, duration: 0.966s, episode steps: 172, steps per second: 178, episode reward: -274.255, mean reward: -1.595 [-100.000, 5.220], mean action: 1.134 [0.000, 3.000], mean observation: -0.044 [-1.643, 1.621], loss: 8.663358, mae: 45.605480, mean_q: 60.968773
  325087/1100000: episode: 684, duration: 2.479s, episode steps: 408, steps per second: 165, episode reward: 259.784, mean reward: 0.637 [-20.444, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.119 [-0.740, 1.389], loss: 7.744201, mae: 45.680233, mean_q: 61.085430
  325377/1100000: episode: 685, duration: 1.766s, episode steps: 290, steps per second: 164, episode reward: 219.114, mean reward: 0.756 [-9.927, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.055 [-0.516, 1.400], loss: 6.105305, mae: 45.244118, mean_q: 60.678234
  325696/1100000: episode: 686, duration: 1.906s, episode steps: 319, steps per second: 167, episode reward: -9.708, mean reward: -0.030 [-100.000, 11.535], mean action: 1.774 [0.000, 3.000], mean observation: 0.028 [-0.600, 1.412], loss: 7.541988, mae: 45.288345, mean_q: 60.767933
  325934/1100000: episode: 687, duration: 1.387s, episode steps: 238, steps per second: 172, episode reward: 256.847, mean reward: 1.079 [-4.065, 100.000], mean action: 1.706 [0.000, 3.000], mean observation: 0.193 [-0.806, 1.396], loss: 8.444210, mae: 45.642220, mean_q: 61.145020
  326524/1100000: episode: 688, duration: 3.572s, episode steps: 590, steps per second: 165, episode reward: 222.551, mean reward: 0.377 [-18.183, 100.000], mean action: 1.734 [0.000, 3.000], mean observation: 0.154 [-0.760, 1.395], loss: 8.722767, mae: 45.626423, mean_q: 60.714745
  326741/1100000: episode: 689, duration: 1.272s, episode steps: 217, steps per second: 171, episode reward: 45.749, mean reward: 0.211 [-100.000, 15.304], mean action: 1.627 [0.000, 3.000], mean observation: 0.066 [-0.842, 1.386], loss: 6.333724, mae: 45.667553, mean_q: 60.402802
  327237/1100000: episode: 690, duration: 3.029s, episode steps: 496, steps per second: 164, episode reward: 244.576, mean reward: 0.493 [-19.400, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.173 [-0.892, 1.436], loss: 12.250542, mae: 45.676254, mean_q: 61.102036
  327744/1100000: episode: 691, duration: 3.018s, episode steps: 507, steps per second: 168, episode reward: 207.770, mean reward: 0.410 [-20.418, 100.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.196 [-0.692, 1.701], loss: 9.067352, mae: 45.589909, mean_q: 61.143681
  327947/1100000: episode: 692, duration: 1.170s, episode steps: 203, steps per second: 174, episode reward: -231.063, mean reward: -1.138 [-100.000, 4.718], mean action: 1.512 [0.000, 3.000], mean observation: 0.131 [-0.924, 2.066], loss: 8.978705, mae: 45.182491, mean_q: 60.480160
  328203/1100000: episode: 693, duration: 1.491s, episode steps: 256, steps per second: 172, episode reward: 237.153, mean reward: 0.926 [-10.379, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.198 [-0.770, 1.408], loss: 9.879649, mae: 45.389519, mean_q: 60.798027
  329203/1100000: episode: 694, duration: 6.374s, episode steps: 1000, steps per second: 157, episode reward: 121.786, mean reward: 0.122 [-20.129, 16.892], mean action: 1.492 [0.000, 3.000], mean observation: 0.256 [-1.225, 1.461], loss: 8.167439, mae: 45.189228, mean_q: 60.545406
  329713/1100000: episode: 695, duration: 3.220s, episode steps: 510, steps per second: 158, episode reward: 203.925, mean reward: 0.400 [-19.665, 100.000], mean action: 2.057 [0.000, 3.000], mean observation: 0.256 [-0.651, 1.407], loss: 7.655852, mae: 45.257858, mean_q: 60.613106
  330612/1100000: episode: 696, duration: 5.990s, episode steps: 899, steps per second: 150, episode reward: 242.822, mean reward: 0.270 [-20.176, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.033 [-0.681, 1.448], loss: 7.085661, mae: 45.506290, mean_q: 60.909794
  330818/1100000: episode: 697, duration: 1.210s, episode steps: 206, steps per second: 170, episode reward: 31.656, mean reward: 0.154 [-100.000, 13.054], mean action: 1.854 [0.000, 3.000], mean observation: 0.066 [-0.925, 1.395], loss: 4.760978, mae: 45.535545, mean_q: 60.686146
  331302/1100000: episode: 698, duration: 3.147s, episode steps: 484, steps per second: 154, episode reward: 188.293, mean reward: 0.389 [-20.063, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: -0.046 [-0.657, 1.397], loss: 7.244579, mae: 45.975872, mean_q: 61.487171
  331963/1100000: episode: 699, duration: 4.207s, episode steps: 661, steps per second: 157, episode reward: 225.349, mean reward: 0.341 [-19.840, 100.000], mean action: 0.840 [0.000, 3.000], mean observation: 0.145 [-0.748, 1.487], loss: 9.099978, mae: 46.311462, mean_q: 61.669434
  332233/1100000: episode: 700, duration: 1.628s, episode steps: 270, steps per second: 166, episode reward: 269.050, mean reward: 0.996 [-10.756, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.196 [-0.747, 1.390], loss: 6.103298, mae: 46.811378, mean_q: 62.319763
  333233/1100000: episode: 701, duration: 6.667s, episode steps: 1000, steps per second: 150, episode reward: 27.422, mean reward: 0.027 [-21.580, 18.421], mean action: 1.539 [0.000, 3.000], mean observation: 0.049 [-0.874, 1.400], loss: 7.552433, mae: 47.250801, mean_q: 62.976536
  333534/1100000: episode: 702, duration: 1.758s, episode steps: 301, steps per second: 171, episode reward: 258.830, mean reward: 0.860 [-4.453, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.035 [-0.725, 1.418], loss: 5.935346, mae: 47.157223, mean_q: 63.109680
  333664/1100000: episode: 703, duration: 0.742s, episode steps: 130, steps per second: 175, episode reward: -25.172, mean reward: -0.194 [-100.000, 15.660], mean action: 1.631 [0.000, 3.000], mean observation: -0.006 [-0.623, 1.410], loss: 6.348063, mae: 47.496815, mean_q: 63.462502
  333853/1100000: episode: 704, duration: 1.089s, episode steps: 189, steps per second: 174, episode reward: -7.758, mean reward: -0.041 [-100.000, 17.020], mean action: 1.571 [0.000, 3.000], mean observation: -0.033 [-1.075, 1.435], loss: 10.607720, mae: 47.315517, mean_q: 63.136925
  334087/1100000: episode: 705, duration: 1.345s, episode steps: 234, steps per second: 174, episode reward: 269.726, mean reward: 1.153 [-5.150, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.076 [-1.385, 1.410], loss: 6.867702, mae: 47.669128, mean_q: 63.619614
  334474/1100000: episode: 706, duration: 2.392s, episode steps: 387, steps per second: 162, episode reward: 265.460, mean reward: 0.686 [-9.835, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.222 [-0.890, 1.387], loss: 9.144882, mae: 47.842426, mean_q: 63.986897
  334908/1100000: episode: 707, duration: 2.644s, episode steps: 434, steps per second: 164, episode reward: 251.995, mean reward: 0.581 [-17.449, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: -0.016 [-0.689, 1.413], loss: 7.804989, mae: 47.599667, mean_q: 63.567326
  335183/1100000: episode: 708, duration: 1.602s, episode steps: 275, steps per second: 172, episode reward: 306.187, mean reward: 1.113 [-3.572, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.041 [-0.791, 1.396], loss: 8.373505, mae: 48.120480, mean_q: 64.541374
  335511/1100000: episode: 709, duration: 1.939s, episode steps: 328, steps per second: 169, episode reward: 243.426, mean reward: 0.742 [-19.832, 100.000], mean action: 0.817 [0.000, 3.000], mean observation: 0.115 [-0.753, 1.404], loss: 7.868587, mae: 47.936970, mean_q: 64.166290
  335950/1100000: episode: 710, duration: 2.596s, episode steps: 439, steps per second: 169, episode reward: 247.660, mean reward: 0.564 [-14.948, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.015 [-0.853, 1.446], loss: 6.487326, mae: 47.860157, mean_q: 64.005745
  336227/1100000: episode: 711, duration: 1.641s, episode steps: 277, steps per second: 169, episode reward: 280.411, mean reward: 1.012 [-19.617, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.100 [-1.214, 1.391], loss: 5.221766, mae: 47.537491, mean_q: 63.490692
  336541/1100000: episode: 712, duration: 1.850s, episode steps: 314, steps per second: 170, episode reward: 223.714, mean reward: 0.712 [-7.520, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.186 [-0.859, 1.402], loss: 7.232879, mae: 47.501102, mean_q: 63.488056
  336922/1100000: episode: 713, duration: 2.299s, episode steps: 381, steps per second: 166, episode reward: 269.186, mean reward: 0.707 [-17.344, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.076 [-0.788, 1.396], loss: 7.600928, mae: 47.918415, mean_q: 63.853695
  337178/1100000: episode: 714, duration: 1.485s, episode steps: 256, steps per second: 172, episode reward: 36.986, mean reward: 0.144 [-100.000, 13.320], mean action: 1.434 [0.000, 3.000], mean observation: -0.001 [-0.798, 1.398], loss: 7.552854, mae: 48.152283, mean_q: 64.069290
  337638/1100000: episode: 715, duration: 2.767s, episode steps: 460, steps per second: 166, episode reward: 226.326, mean reward: 0.492 [-8.397, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.141 [-0.738, 1.391], loss: 6.869010, mae: 47.785275, mean_q: 63.835629
  338012/1100000: episode: 716, duration: 2.253s, episode steps: 374, steps per second: 166, episode reward: 259.126, mean reward: 0.693 [-17.847, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.122 [-0.817, 1.395], loss: 4.875123, mae: 47.608341, mean_q: 63.744392
  338328/1100000: episode: 717, duration: 1.868s, episode steps: 316, steps per second: 169, episode reward: 245.056, mean reward: 0.775 [-10.942, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: 0.168 [-0.874, 1.387], loss: 10.124606, mae: 47.866013, mean_q: 63.787529
  338654/1100000: episode: 718, duration: 1.956s, episode steps: 326, steps per second: 167, episode reward: 260.336, mean reward: 0.799 [-17.815, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.083 [-0.828, 1.473], loss: 5.822232, mae: 47.952225, mean_q: 64.051872
  338992/1100000: episode: 719, duration: 1.982s, episode steps: 338, steps per second: 171, episode reward: 227.757, mean reward: 0.674 [-18.357, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: -0.018 [-0.869, 1.470], loss: 5.791521, mae: 47.912849, mean_q: 64.056091
  339580/1100000: episode: 720, duration: 3.747s, episode steps: 588, steps per second: 157, episode reward: 214.953, mean reward: 0.366 [-19.490, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: -0.018 [-0.600, 1.404], loss: 8.246452, mae: 47.905636, mean_q: 64.051476
  340301/1100000: episode: 721, duration: 4.350s, episode steps: 721, steps per second: 166, episode reward: 187.359, mean reward: 0.260 [-10.639, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.192 [-0.833, 1.408], loss: 6.997566, mae: 47.702915, mean_q: 63.713242
  340896/1100000: episode: 722, duration: 3.708s, episode steps: 595, steps per second: 160, episode reward: 272.809, mean reward: 0.459 [-20.387, 100.000], mean action: 0.795 [0.000, 3.000], mean observation: 0.136 [-0.695, 1.496], loss: 6.566164, mae: 48.116909, mean_q: 64.190140
  341896/1100000: episode: 723, duration: 6.318s, episode steps: 1000, steps per second: 158, episode reward: 132.117, mean reward: 0.132 [-21.345, 23.344], mean action: 1.669 [0.000, 3.000], mean observation: 0.068 [-0.847, 1.388], loss: 8.667222, mae: 48.325314, mean_q: 64.316116
  342896/1100000: episode: 724, duration: 6.475s, episode steps: 1000, steps per second: 154, episode reward: 43.506, mean reward: 0.044 [-9.942, 24.171], mean action: 1.480 [0.000, 3.000], mean observation: -0.028 [-0.669, 1.450], loss: 6.561944, mae: 48.667217, mean_q: 64.805641
  343896/1100000: episode: 725, duration: 6.207s, episode steps: 1000, steps per second: 161, episode reward: 137.092, mean reward: 0.137 [-18.289, 14.052], mean action: 1.955 [0.000, 3.000], mean observation: 0.273 [-1.036, 1.395], loss: 5.942097, mae: 48.473190, mean_q: 64.662048
  344095/1100000: episode: 726, duration: 1.164s, episode steps: 199, steps per second: 171, episode reward: 18.960, mean reward: 0.095 [-100.000, 20.899], mean action: 1.563 [0.000, 3.000], mean observation: -0.109 [-1.392, 1.389], loss: 4.334876, mae: 48.453362, mean_q: 64.613029
  344455/1100000: episode: 727, duration: 2.161s, episode steps: 360, steps per second: 167, episode reward: 254.011, mean reward: 0.706 [-3.628, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.218 [-0.996, 1.464], loss: 7.178349, mae: 48.564800, mean_q: 64.694672
  344822/1100000: episode: 728, duration: 2.158s, episode steps: 367, steps per second: 170, episode reward: 252.070, mean reward: 0.687 [-14.510, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: -0.025 [-0.659, 1.437], loss: 6.828971, mae: 48.662983, mean_q: 64.945663
  345822/1100000: episode: 729, duration: 6.380s, episode steps: 1000, steps per second: 157, episode reward: 103.977, mean reward: 0.104 [-21.728, 21.213], mean action: 1.099 [0.000, 3.000], mean observation: 0.271 [-0.946, 1.422], loss: 7.653326, mae: 48.747414, mean_q: 65.083000
  346822/1100000: episode: 730, duration: 7.882s, episode steps: 1000, steps per second: 127, episode reward: 21.109, mean reward: 0.021 [-18.031, 20.014], mean action: 1.377 [0.000, 3.000], mean observation: -0.054 [-0.840, 1.399], loss: 7.409299, mae: 49.251148, mean_q: 65.663628
  347269/1100000: episode: 731, duration: 2.714s, episode steps: 447, steps per second: 165, episode reward: 204.177, mean reward: 0.457 [-17.396, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.195 [-0.719, 1.415], loss: 7.987521, mae: 49.138817, mean_q: 65.385895
  347386/1100000: episode: 732, duration: 0.667s, episode steps: 117, steps per second: 176, episode reward: 14.281, mean reward: 0.122 [-100.000, 18.922], mean action: 1.299 [0.000, 3.000], mean observation: 0.060 [-0.852, 1.418], loss: 5.559703, mae: 49.287415, mean_q: 65.932854
  347923/1100000: episode: 733, duration: 3.225s, episode steps: 537, steps per second: 166, episode reward: 269.128, mean reward: 0.501 [-18.464, 100.000], mean action: 0.721 [0.000, 3.000], mean observation: 0.246 [-0.776, 1.410], loss: 7.031003, mae: 49.332371, mean_q: 65.834663
  348299/1100000: episode: 734, duration: 2.287s, episode steps: 376, steps per second: 164, episode reward: 241.286, mean reward: 0.642 [-18.357, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.144 [-1.365, 1.467], loss: 4.018220, mae: 49.481201, mean_q: 66.010979
  348578/1100000: episode: 735, duration: 1.625s, episode steps: 279, steps per second: 172, episode reward: 254.945, mean reward: 0.914 [-11.946, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.186 [-0.809, 1.413], loss: 7.634529, mae: 50.084038, mean_q: 66.911644
  348977/1100000: episode: 736, duration: 2.422s, episode steps: 399, steps per second: 165, episode reward: -115.965, mean reward: -0.291 [-100.000, 5.597], mean action: 1.910 [0.000, 3.000], mean observation: -0.111 [-1.001, 1.486], loss: 4.956254, mae: 49.894424, mean_q: 66.405876
  349399/1100000: episode: 737, duration: 2.474s, episode steps: 422, steps per second: 171, episode reward: 239.184, mean reward: 0.567 [-18.807, 100.000], mean action: 0.751 [0.000, 3.000], mean observation: 0.141 [-0.793, 1.414], loss: 7.761207, mae: 49.991901, mean_q: 66.793343
  349779/1100000: episode: 738, duration: 2.247s, episode steps: 380, steps per second: 169, episode reward: 290.237, mean reward: 0.764 [-9.950, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.103 [-1.028, 1.431], loss: 7.770392, mae: 50.111717, mean_q: 66.465248
  350454/1100000: episode: 739, duration: 4.253s, episode steps: 675, steps per second: 159, episode reward: 215.401, mean reward: 0.319 [-18.452, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.025 [-0.776, 1.492], loss: 6.545917, mae: 50.046146, mean_q: 66.780441
  350859/1100000: episode: 740, duration: 2.424s, episode steps: 405, steps per second: 167, episode reward: -12.459, mean reward: -0.031 [-100.000, 18.387], mean action: 1.600 [0.000, 3.000], mean observation: 0.106 [-0.668, 1.560], loss: 8.323477, mae: 49.586033, mean_q: 66.007195
  350958/1100000: episode: 741, duration: 0.573s, episode steps: 99, steps per second: 173, episode reward: -255.717, mean reward: -2.583 [-100.000, 72.409], mean action: 1.960 [0.000, 3.000], mean observation: -0.036 [-2.743, 1.388], loss: 3.949429, mae: 49.659515, mean_q: 66.269058
  351343/1100000: episode: 742, duration: 2.325s, episode steps: 385, steps per second: 166, episode reward: 268.931, mean reward: 0.699 [-19.371, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.061 [-0.817, 1.401], loss: 6.968036, mae: 49.752567, mean_q: 66.238251
  352056/1100000: episode: 743, duration: 4.554s, episode steps: 713, steps per second: 157, episode reward: 191.387, mean reward: 0.268 [-17.560, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: -0.025 [-0.666, 1.402], loss: 7.658242, mae: 49.560642, mean_q: 65.695877
  352174/1100000: episode: 744, duration: 0.680s, episode steps: 118, steps per second: 174, episode reward: 32.381, mean reward: 0.274 [-100.000, 14.208], mean action: 1.890 [0.000, 3.000], mean observation: 0.186 [-1.193, 1.397], loss: 3.455136, mae: 49.411976, mean_q: 66.030396
  352498/1100000: episode: 745, duration: 1.912s, episode steps: 324, steps per second: 169, episode reward: 264.355, mean reward: 0.816 [-8.722, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.039 [-0.960, 1.447], loss: 8.155543, mae: 49.685295, mean_q: 65.862053
  352792/1100000: episode: 746, duration: 1.734s, episode steps: 294, steps per second: 170, episode reward: 273.027, mean reward: 0.929 [-17.360, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.166 [-0.615, 1.388], loss: 7.630093, mae: 50.234035, mean_q: 66.789169
  353527/1100000: episode: 747, duration: 4.681s, episode steps: 735, steps per second: 157, episode reward: -346.344, mean reward: -0.471 [-100.000, 21.384], mean action: 1.761 [0.000, 3.000], mean observation: -0.042 [-0.703, 2.453], loss: 10.299480, mae: 50.542023, mean_q: 66.797371
  353681/1100000: episode: 748, duration: 0.898s, episode steps: 154, steps per second: 172, episode reward: 26.675, mean reward: 0.173 [-100.000, 14.058], mean action: 1.766 [0.000, 3.000], mean observation: -0.009 [-1.408, 1.405], loss: 8.167150, mae: 49.864269, mean_q: 65.858482
  354300/1100000: episode: 749, duration: 3.928s, episode steps: 619, steps per second: 158, episode reward: 180.870, mean reward: 0.292 [-20.305, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: 0.137 [-0.665, 1.392], loss: 5.967648, mae: 50.268444, mean_q: 66.663292
  354858/1100000: episode: 750, duration: 3.358s, episode steps: 558, steps per second: 166, episode reward: 310.452, mean reward: 0.556 [-19.187, 100.000], mean action: 0.649 [0.000, 3.000], mean observation: 0.143 [-1.098, 1.388], loss: 7.755171, mae: 50.251720, mean_q: 66.455826
  355012/1100000: episode: 751, duration: 0.877s, episode steps: 154, steps per second: 176, episode reward: 29.107, mean reward: 0.189 [-100.000, 18.004], mean action: 1.578 [0.000, 3.000], mean observation: 0.093 [-1.511, 1.481], loss: 6.141735, mae: 50.216385, mean_q: 66.743774
  355709/1100000: episode: 752, duration: 4.380s, episode steps: 697, steps per second: 159, episode reward: 234.932, mean reward: 0.337 [-19.285, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.102 [-0.710, 1.393], loss: 13.032202, mae: 49.981201, mean_q: 65.796005
  356055/1100000: episode: 753, duration: 2.039s, episode steps: 346, steps per second: 170, episode reward: 238.063, mean reward: 0.688 [-18.014, 100.000], mean action: 0.850 [0.000, 3.000], mean observation: 0.148 [-0.972, 1.410], loss: 7.702688, mae: 50.147270, mean_q: 66.197823
  356348/1100000: episode: 754, duration: 1.723s, episode steps: 293, steps per second: 170, episode reward: 264.821, mean reward: 0.904 [-4.531, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.132 [-0.619, 1.422], loss: 8.500912, mae: 50.454659, mean_q: 66.613472
  356723/1100000: episode: 755, duration: 2.231s, episode steps: 375, steps per second: 168, episode reward: 210.798, mean reward: 0.562 [-17.857, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.201 [-0.637, 1.403], loss: 11.891242, mae: 49.858929, mean_q: 66.182617
  356974/1100000: episode: 756, duration: 1.463s, episode steps: 251, steps per second: 172, episode reward: 235.794, mean reward: 0.939 [-16.003, 100.000], mean action: 1.821 [0.000, 3.000], mean observation: 0.081 [-1.049, 1.416], loss: 14.301360, mae: 50.346924, mean_q: 66.478813
  357974/1100000: episode: 757, duration: 6.522s, episode steps: 1000, steps per second: 153, episode reward: 45.213, mean reward: 0.045 [-17.769, 13.794], mean action: 1.208 [0.000, 3.000], mean observation: -0.032 [-0.669, 1.424], loss: 8.689623, mae: 49.951843, mean_q: 66.439362
  358256/1100000: episode: 758, duration: 1.642s, episode steps: 282, steps per second: 172, episode reward: 242.785, mean reward: 0.861 [-18.835, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.138 [-0.709, 1.439], loss: 12.649941, mae: 49.084270, mean_q: 65.344620
  359256/1100000: episode: 759, duration: 6.282s, episode steps: 1000, steps per second: 159, episode reward: 82.536, mean reward: 0.083 [-17.954, 16.004], mean action: 1.230 [0.000, 3.000], mean observation: 0.192 [-0.686, 1.398], loss: 7.683771, mae: 49.552071, mean_q: 65.955147
  360256/1100000: episode: 760, duration: 6.642s, episode steps: 1000, steps per second: 151, episode reward: -6.727, mean reward: -0.007 [-13.615, 13.281], mean action: 1.433 [0.000, 3.000], mean observation: -0.029 [-0.739, 1.404], loss: 5.971898, mae: 48.460030, mean_q: 64.416306
  360821/1100000: episode: 761, duration: 3.506s, episode steps: 565, steps per second: 161, episode reward: 302.627, mean reward: 0.536 [-19.875, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.145 [-1.460, 1.475], loss: 5.281691, mae: 47.926830, mean_q: 63.728970
  361037/1100000: episode: 762, duration: 1.231s, episode steps: 216, steps per second: 175, episode reward: 75.080, mean reward: 0.348 [-100.000, 13.885], mean action: 1.472 [0.000, 3.000], mean observation: -0.016 [-1.035, 1.504], loss: 7.468946, mae: 48.058628, mean_q: 63.942894
  361296/1100000: episode: 763, duration: 1.503s, episode steps: 259, steps per second: 172, episode reward: 283.979, mean reward: 1.096 [-8.348, 100.000], mean action: 0.950 [0.000, 3.000], mean observation: 0.099 [-1.214, 1.386], loss: 8.864686, mae: 48.301975, mean_q: 64.436272
  361597/1100000: episode: 764, duration: 1.762s, episode steps: 301, steps per second: 171, episode reward: 242.003, mean reward: 0.804 [-13.522, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.101 [-1.144, 1.412], loss: 8.441493, mae: 48.175301, mean_q: 64.214417
  361906/1100000: episode: 765, duration: 1.818s, episode steps: 309, steps per second: 170, episode reward: 260.260, mean reward: 0.842 [-9.665, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.072 [-0.912, 1.409], loss: 6.287260, mae: 48.683239, mean_q: 65.073395
  362198/1100000: episode: 766, duration: 1.689s, episode steps: 292, steps per second: 173, episode reward: 224.245, mean reward: 0.768 [-13.671, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.185 [-0.675, 1.505], loss: 8.043663, mae: 48.727192, mean_q: 65.081261
  363198/1100000: episode: 767, duration: 6.850s, episode steps: 1000, steps per second: 146, episode reward: 87.827, mean reward: 0.088 [-20.310, 21.069], mean action: 1.139 [0.000, 3.000], mean observation: 0.001 [-0.879, 1.410], loss: 7.031671, mae: 49.027016, mean_q: 65.291649
  363388/1100000: episode: 768, duration: 1.086s, episode steps: 190, steps per second: 175, episode reward: -23.526, mean reward: -0.124 [-100.000, 15.220], mean action: 1.684 [0.000, 3.000], mean observation: 0.185 [-0.912, 1.419], loss: 5.687630, mae: 48.482788, mean_q: 64.345108
  363644/1100000: episode: 769, duration: 1.501s, episode steps: 256, steps per second: 171, episode reward: 276.789, mean reward: 1.081 [-17.695, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.091 [-0.695, 1.414], loss: 9.398581, mae: 48.299896, mean_q: 64.043449
  364285/1100000: episode: 770, duration: 4.064s, episode steps: 641, steps per second: 158, episode reward: 192.834, mean reward: 0.301 [-20.793, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: -0.009 [-0.753, 1.412], loss: 8.286383, mae: 48.283451, mean_q: 64.369049
  364458/1100000: episode: 771, duration: 0.989s, episode steps: 173, steps per second: 175, episode reward: -2.178, mean reward: -0.013 [-100.000, 16.302], mean action: 1.514 [0.000, 3.000], mean observation: 0.065 [-1.417, 1.438], loss: 6.325204, mae: 47.664761, mean_q: 63.406364
  364739/1100000: episode: 772, duration: 1.630s, episode steps: 281, steps per second: 172, episode reward: -230.984, mean reward: -0.822 [-100.000, 14.520], mean action: 1.651 [0.000, 3.000], mean observation: -0.024 [-1.060, 1.734], loss: 5.510686, mae: 48.175449, mean_q: 63.680645
  365000/1100000: episode: 773, duration: 1.507s, episode steps: 261, steps per second: 173, episode reward: 206.201, mean reward: 0.790 [-13.373, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.157 [-0.725, 1.404], loss: 12.630204, mae: 48.405426, mean_q: 63.998367
  366000/1100000: episode: 774, duration: 6.768s, episode steps: 1000, steps per second: 148, episode reward: -33.614, mean reward: -0.034 [-4.605, 5.286], mean action: 1.790 [0.000, 3.000], mean observation: -0.042 [-0.600, 1.396], loss: 8.681388, mae: 47.553413, mean_q: 63.033997
  366367/1100000: episode: 775, duration: 2.173s, episode steps: 367, steps per second: 169, episode reward: 257.274, mean reward: 0.701 [-18.921, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.215 [-1.178, 1.411], loss: 8.624849, mae: 46.947334, mean_q: 62.571941
  366523/1100000: episode: 776, duration: 0.894s, episode steps: 156, steps per second: 175, episode reward: 11.823, mean reward: 0.076 [-100.000, 12.284], mean action: 1.654 [0.000, 3.000], mean observation: 0.085 [-1.072, 1.392], loss: 11.686811, mae: 47.044510, mean_q: 62.718449
  366936/1100000: episode: 777, duration: 2.520s, episode steps: 413, steps per second: 164, episode reward: 243.530, mean reward: 0.590 [-12.108, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.005 [-0.600, 1.535], loss: 8.209050, mae: 46.931126, mean_q: 62.033695
  367307/1100000: episode: 778, duration: 2.200s, episode steps: 371, steps per second: 169, episode reward: 247.940, mean reward: 0.668 [-10.412, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.130 [-0.980, 1.415], loss: 7.535383, mae: 46.978893, mean_q: 62.506645
  368151/1100000: episode: 779, duration: 5.330s, episode steps: 844, steps per second: 158, episode reward: 163.313, mean reward: 0.193 [-18.463, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: -0.017 [-0.732, 1.503], loss: 10.190762, mae: 46.888977, mean_q: 62.254730
  368589/1100000: episode: 780, duration: 2.668s, episode steps: 438, steps per second: 164, episode reward: 243.781, mean reward: 0.557 [-17.670, 100.000], mean action: 2.107 [0.000, 3.000], mean observation: 0.159 [-0.777, 1.389], loss: 7.585722, mae: 47.354168, mean_q: 62.744835
  368986/1100000: episode: 781, duration: 2.426s, episode steps: 397, steps per second: 164, episode reward: 260.402, mean reward: 0.656 [-23.021, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.185 [-1.071, 1.386], loss: 10.272354, mae: 46.524139, mean_q: 61.874367
  369389/1100000: episode: 782, duration: 2.430s, episode steps: 403, steps per second: 166, episode reward: 199.299, mean reward: 0.495 [-10.965, 100.000], mean action: 1.464 [0.000, 3.000], mean observation: -0.040 [-0.600, 1.405], loss: 10.995017, mae: 46.521194, mean_q: 61.531803
  370389/1100000: episode: 783, duration: 6.478s, episode steps: 1000, steps per second: 154, episode reward: -19.244, mean reward: -0.019 [-12.242, 12.245], mean action: 1.775 [0.000, 3.000], mean observation: -0.041 [-0.639, 1.395], loss: 7.328925, mae: 46.078743, mean_q: 61.081173
  370741/1100000: episode: 784, duration: 2.089s, episode steps: 352, steps per second: 169, episode reward: 249.085, mean reward: 0.708 [-19.297, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.055 [-0.661, 1.389], loss: 7.907167, mae: 45.425064, mean_q: 60.318825
  371234/1100000: episode: 785, duration: 3.111s, episode steps: 493, steps per second: 158, episode reward: 226.700, mean reward: 0.460 [-19.970, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.098 [-0.646, 1.408], loss: 7.618629, mae: 45.668728, mean_q: 60.909122
  371389/1100000: episode: 786, duration: 0.887s, episode steps: 155, steps per second: 175, episode reward: -99.700, mean reward: -0.643 [-100.000, 3.474], mean action: 1.813 [0.000, 3.000], mean observation: 0.084 [-0.772, 1.482], loss: 5.436840, mae: 46.281906, mean_q: 61.646408
  371553/1100000: episode: 787, duration: 0.948s, episode steps: 164, steps per second: 173, episode reward: 21.509, mean reward: 0.131 [-100.000, 7.177], mean action: 1.927 [0.000, 3.000], mean observation: -0.042 [-0.719, 1.385], loss: 7.245323, mae: 44.888569, mean_q: 59.688644
  371787/1100000: episode: 788, duration: 1.350s, episode steps: 234, steps per second: 173, episode reward: -0.922, mean reward: -0.004 [-100.000, 23.829], mean action: 1.735 [0.000, 3.000], mean observation: 0.044 [-0.675, 1.497], loss: 7.363455, mae: 45.673561, mean_q: 60.925480
  371881/1100000: episode: 789, duration: 0.540s, episode steps: 94, steps per second: 174, episode reward: 21.613, mean reward: 0.230 [-100.000, 15.052], mean action: 1.798 [0.000, 3.000], mean observation: 0.059 [-0.838, 1.399], loss: 5.924872, mae: 46.137619, mean_q: 61.699524
  372032/1100000: episode: 790, duration: 0.867s, episode steps: 151, steps per second: 174, episode reward: 50.357, mean reward: 0.333 [-100.000, 18.091], mean action: 1.762 [0.000, 3.000], mean observation: 0.018 [-0.971, 1.388], loss: 8.750654, mae: 45.708412, mean_q: 61.025566
  372443/1100000: episode: 791, duration: 2.456s, episode steps: 411, steps per second: 167, episode reward: 194.466, mean reward: 0.473 [-10.054, 100.000], mean action: 1.951 [0.000, 3.000], mean observation: 0.058 [-0.760, 1.408], loss: 7.844508, mae: 45.843182, mean_q: 61.023819
  372554/1100000: episode: 792, duration: 0.635s, episode steps: 111, steps per second: 175, episode reward: -167.879, mean reward: -1.512 [-100.000, 20.629], mean action: 1.423 [0.000, 3.000], mean observation: 0.099 [-1.166, 4.388], loss: 8.603822, mae: 46.010857, mean_q: 61.557426
  372853/1100000: episode: 793, duration: 1.756s, episode steps: 299, steps per second: 170, episode reward: -74.512, mean reward: -0.249 [-100.000, 7.090], mean action: 1.706 [0.000, 3.000], mean observation: 0.054 [-2.267, 1.506], loss: 8.766434, mae: 45.631371, mean_q: 60.529533
  372940/1100000: episode: 794, duration: 0.494s, episode steps: 87, steps per second: 176, episode reward: -72.047, mean reward: -0.828 [-100.000, 39.702], mean action: 1.011 [0.000, 3.000], mean observation: 0.052 [-1.690, 1.717], loss: 11.976501, mae: 45.352768, mean_q: 60.134647
  373118/1100000: episode: 795, duration: 1.025s, episode steps: 178, steps per second: 174, episode reward: -25.020, mean reward: -0.141 [-100.000, 14.830], mean action: 1.815 [0.000, 3.000], mean observation: 0.137 [-0.914, 1.444], loss: 13.341527, mae: 45.959259, mean_q: 61.208622
  373703/1100000: episode: 796, duration: 3.625s, episode steps: 585, steps per second: 161, episode reward: 250.365, mean reward: 0.428 [-20.352, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.092 [-0.586, 1.414], loss: 9.240742, mae: 45.585648, mean_q: 60.883015
  373948/1100000: episode: 797, duration: 1.392s, episode steps: 245, steps per second: 176, episode reward: 34.690, mean reward: 0.142 [-100.000, 4.926], mean action: 1.437 [0.000, 3.000], mean observation: 0.005 [-0.720, 1.525], loss: 17.589123, mae: 45.424953, mean_q: 60.736603
  374329/1100000: episode: 798, duration: 2.317s, episode steps: 381, steps per second: 164, episode reward: 243.405, mean reward: 0.639 [-10.094, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.070 [-0.633, 1.450], loss: 9.323235, mae: 45.793415, mean_q: 61.257900
  374776/1100000: episode: 799, duration: 2.812s, episode steps: 447, steps per second: 159, episode reward: 246.844, mean reward: 0.552 [-11.338, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.062 [-0.628, 1.429], loss: 8.413577, mae: 45.756203, mean_q: 60.987659
  374990/1100000: episode: 800, duration: 1.268s, episode steps: 214, steps per second: 169, episode reward: -0.094, mean reward: -0.000 [-100.000, 9.669], mean action: 1.654 [0.000, 3.000], mean observation: 0.111 [-0.765, 1.720], loss: 10.663659, mae: 45.055969, mean_q: 60.070633
  375171/1100000: episode: 801, duration: 1.038s, episode steps: 181, steps per second: 174, episode reward: 260.555, mean reward: 1.440 [-19.203, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.178 [-1.051, 1.393], loss: 7.917336, mae: 45.888863, mean_q: 60.997509
  375286/1100000: episode: 802, duration: 0.659s, episode steps: 115, steps per second: 174, episode reward: -175.587, mean reward: -1.527 [-100.000, 74.801], mean action: 1.557 [0.000, 3.000], mean observation: -0.058 [-1.171, 1.449], loss: 3.697209, mae: 45.681839, mean_q: 61.013607
  376286/1100000: episode: 803, duration: 6.327s, episode steps: 1000, steps per second: 158, episode reward: -0.722, mean reward: -0.001 [-20.597, 22.785], mean action: 1.618 [0.000, 3.000], mean observation: -0.034 [-0.752, 1.392], loss: 9.232706, mae: 44.881657, mean_q: 59.956398
  377286/1100000: episode: 804, duration: 6.431s, episode steps: 1000, steps per second: 156, episode reward: 103.607, mean reward: 0.104 [-18.291, 21.403], mean action: 2.484 [0.000, 3.000], mean observation: 0.250 [-0.560, 1.462], loss: 8.334867, mae: 44.131702, mean_q: 59.040226
  377607/1100000: episode: 805, duration: 1.919s, episode steps: 321, steps per second: 167, episode reward: -2.902, mean reward: -0.009 [-100.000, 20.040], mean action: 1.601 [0.000, 3.000], mean observation: 0.159 [-1.596, 1.397], loss: 11.664952, mae: 44.425945, mean_q: 59.084763
  378016/1100000: episode: 806, duration: 2.449s, episode steps: 409, steps per second: 167, episode reward: 280.502, mean reward: 0.686 [-13.287, 100.000], mean action: 2.152 [0.000, 3.000], mean observation: 0.097 [-0.793, 1.415], loss: 7.000528, mae: 43.871258, mean_q: 58.569874
  378438/1100000: episode: 807, duration: 2.583s, episode steps: 422, steps per second: 163, episode reward: 232.332, mean reward: 0.551 [-10.591, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.104 [-0.669, 1.409], loss: 7.221995, mae: 43.846554, mean_q: 58.712627
  378642/1100000: episode: 808, duration: 1.180s, episode steps: 204, steps per second: 173, episode reward: 279.042, mean reward: 1.368 [-3.960, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.055 [-0.815, 1.389], loss: 7.982328, mae: 43.489670, mean_q: 58.144829
  379539/1100000: episode: 809, duration: 6.116s, episode steps: 897, steps per second: 147, episode reward: 134.562, mean reward: 0.150 [-18.634, 100.000], mean action: 1.505 [0.000, 3.000], mean observation: -0.012 [-0.600, 1.413], loss: 7.184910, mae: 43.544071, mean_q: 58.438210
  379850/1100000: episode: 810, duration: 1.874s, episode steps: 311, steps per second: 166, episode reward: 238.512, mean reward: 0.767 [-17.663, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.100 [-0.726, 1.390], loss: 8.228076, mae: 43.481319, mean_q: 57.969578
  380192/1100000: episode: 811, duration: 2.019s, episode steps: 342, steps per second: 169, episode reward: -214.444, mean reward: -0.627 [-100.000, 27.364], mean action: 1.570 [0.000, 3.000], mean observation: 0.019 [-0.987, 2.055], loss: 8.277486, mae: 43.125713, mean_q: 57.682941
  380612/1100000: episode: 812, duration: 2.615s, episode steps: 420, steps per second: 161, episode reward: 245.949, mean reward: 0.586 [-18.096, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.072 [-0.746, 1.395], loss: 9.983049, mae: 43.269821, mean_q: 57.709320
  381612/1100000: episode: 813, duration: 6.965s, episode steps: 1000, steps per second: 144, episode reward: 42.064, mean reward: 0.042 [-19.553, 19.384], mean action: 1.689 [0.000, 3.000], mean observation: -0.006 [-0.652, 1.388], loss: 8.763023, mae: 43.073391, mean_q: 57.710213
  382612/1100000: episode: 814, duration: 7.106s, episode steps: 1000, steps per second: 141, episode reward: 76.651, mean reward: 0.077 [-19.140, 13.347], mean action: 1.415 [0.000, 3.000], mean observation: 0.008 [-0.876, 1.414], loss: 7.866220, mae: 42.481861, mean_q: 56.801983
  383009/1100000: episode: 815, duration: 2.398s, episode steps: 397, steps per second: 166, episode reward: 219.824, mean reward: 0.554 [-12.456, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.176 [-0.772, 1.391], loss: 7.677832, mae: 42.473316, mean_q: 57.014366
  383315/1100000: episode: 816, duration: 1.805s, episode steps: 306, steps per second: 170, episode reward: 244.375, mean reward: 0.799 [-12.478, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.072 [-0.740, 1.439], loss: 6.724308, mae: 42.367435, mean_q: 56.667343
  383583/1100000: episode: 817, duration: 1.629s, episode steps: 268, steps per second: 165, episode reward: 242.565, mean reward: 0.905 [-19.828, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.067 [-0.763, 1.404], loss: 7.236233, mae: 42.331413, mean_q: 56.847797
  384233/1100000: episode: 818, duration: 3.932s, episode steps: 650, steps per second: 165, episode reward: 196.515, mean reward: 0.302 [-20.269, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.030 [-0.600, 1.441], loss: 9.593285, mae: 42.319141, mean_q: 56.681847
  384800/1100000: episode: 819, duration: 3.506s, episode steps: 567, steps per second: 162, episode reward: 202.460, mean reward: 0.357 [-11.043, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: -0.017 [-0.626, 1.456], loss: 7.921021, mae: 41.467098, mean_q: 55.521198
  384954/1100000: episode: 820, duration: 0.877s, episode steps: 154, steps per second: 176, episode reward: -143.079, mean reward: -0.929 [-100.000, 2.841], mean action: 1.610 [0.000, 3.000], mean observation: -0.034 [-1.003, 1.447], loss: 4.791015, mae: 41.408329, mean_q: 55.334675
  385512/1100000: episode: 821, duration: 3.666s, episode steps: 558, steps per second: 152, episode reward: -177.824, mean reward: -0.319 [-100.000, 17.243], mean action: 1.776 [0.000, 3.000], mean observation: -0.032 [-0.773, 1.660], loss: 8.301637, mae: 41.254696, mean_q: 55.180401
  385734/1100000: episode: 822, duration: 1.288s, episode steps: 222, steps per second: 172, episode reward: 248.373, mean reward: 1.119 [-8.162, 100.000], mean action: 1.640 [0.000, 3.000], mean observation: 0.076 [-0.665, 1.401], loss: 8.361803, mae: 40.964878, mean_q: 54.901337
  385928/1100000: episode: 823, duration: 1.109s, episode steps: 194, steps per second: 175, episode reward: 22.335, mean reward: 0.115 [-100.000, 15.610], mean action: 1.510 [0.000, 3.000], mean observation: 0.082 [-0.530, 1.412], loss: 6.675380, mae: 40.656925, mean_q: 54.464542
  386928/1100000: episode: 824, duration: 6.922s, episode steps: 1000, steps per second: 144, episode reward: -29.092, mean reward: -0.029 [-19.776, 17.996], mean action: 1.614 [0.000, 3.000], mean observation: -0.063 [-0.600, 1.424], loss: 8.901213, mae: 41.249748, mean_q: 54.963383
  387215/1100000: episode: 825, duration: 1.685s, episode steps: 287, steps per second: 170, episode reward: 287.427, mean reward: 1.001 [-2.529, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.055 [-0.562, 1.473], loss: 8.114223, mae: 41.367126, mean_q: 55.054054
  388215/1100000: episode: 826, duration: 6.645s, episode steps: 1000, steps per second: 150, episode reward: 40.512, mean reward: 0.041 [-17.424, 15.509], mean action: 1.345 [0.000, 3.000], mean observation: -0.003 [-0.633, 1.488], loss: 10.844933, mae: 41.044960, mean_q: 54.917843
  388465/1100000: episode: 827, duration: 1.484s, episode steps: 250, steps per second: 168, episode reward: -55.502, mean reward: -0.222 [-100.000, 16.940], mean action: 1.544 [0.000, 3.000], mean observation: 0.087 [-0.887, 1.705], loss: 9.814420, mae: 40.900257, mean_q: 54.658024
  388814/1100000: episode: 828, duration: 2.094s, episode steps: 349, steps per second: 167, episode reward: -179.278, mean reward: -0.514 [-100.000, 24.282], mean action: 1.613 [0.000, 3.000], mean observation: -0.014 [-0.980, 1.747], loss: 6.486858, mae: 41.336552, mean_q: 55.248566
  389165/1100000: episode: 829, duration: 2.105s, episode steps: 351, steps per second: 167, episode reward: 229.476, mean reward: 0.654 [-9.442, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.060 [-0.799, 1.412], loss: 6.778896, mae: 41.040943, mean_q: 54.516258
  390165/1100000: episode: 830, duration: 6.605s, episode steps: 1000, steps per second: 151, episode reward: -16.459, mean reward: -0.016 [-5.003, 5.446], mean action: 1.727 [0.000, 3.000], mean observation: 0.132 [-0.969, 1.523], loss: 8.172923, mae: 41.180531, mean_q: 54.787098
  391165/1100000: episode: 831, duration: 6.514s, episode steps: 1000, steps per second: 154, episode reward: 27.104, mean reward: 0.027 [-20.023, 22.341], mean action: 1.248 [0.000, 3.000], mean observation: -0.028 [-0.600, 1.412], loss: 8.024906, mae: 40.737709, mean_q: 54.389679
  391384/1100000: episode: 832, duration: 1.269s, episode steps: 219, steps per second: 173, episode reward: 273.888, mean reward: 1.251 [-17.467, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.050 [-0.756, 1.389], loss: 9.126296, mae: 40.596706, mean_q: 54.243992
  392384/1100000: episode: 833, duration: 6.833s, episode steps: 1000, steps per second: 146, episode reward: -24.562, mean reward: -0.025 [-4.305, 5.204], mean action: 1.841 [0.000, 3.000], mean observation: -0.101 [-0.600, 1.403], loss: 7.618743, mae: 40.398727, mean_q: 53.992928
  393384/1100000: episode: 834, duration: 7.125s, episode steps: 1000, steps per second: 140, episode reward: 43.963, mean reward: 0.044 [-19.505, 21.890], mean action: 1.561 [0.000, 3.000], mean observation: 0.150 [-0.906, 1.497], loss: 6.540415, mae: 40.634304, mean_q: 54.158295
  393936/1100000: episode: 835, duration: 3.482s, episode steps: 552, steps per second: 159, episode reward: 277.849, mean reward: 0.503 [-17.728, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.060 [-0.687, 1.508], loss: 6.939176, mae: 40.547630, mean_q: 53.998631
  394614/1100000: episode: 836, duration: 4.225s, episode steps: 678, steps per second: 160, episode reward: 161.890, mean reward: 0.239 [-24.488, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.159 [-0.763, 1.395], loss: 7.519696, mae: 40.480755, mean_q: 53.917110
  395144/1100000: episode: 837, duration: 3.229s, episode steps: 530, steps per second: 164, episode reward: 198.404, mean reward: 0.374 [-17.978, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.402], loss: 5.915930, mae: 40.398510, mean_q: 53.870937
  395615/1100000: episode: 838, duration: 2.904s, episode steps: 471, steps per second: 162, episode reward: 277.127, mean reward: 0.588 [-19.114, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.095 [-0.459, 1.408], loss: 6.452976, mae: 40.963326, mean_q: 54.608524
  395978/1100000: episode: 839, duration: 2.212s, episode steps: 363, steps per second: 164, episode reward: 260.650, mean reward: 0.718 [-10.667, 100.000], mean action: 1.658 [0.000, 3.000], mean observation: 0.083 [-0.794, 1.463], loss: 5.463934, mae: 40.521004, mean_q: 53.900795
  396978/1100000: episode: 840, duration: 6.610s, episode steps: 1000, steps per second: 151, episode reward: 133.216, mean reward: 0.133 [-20.067, 22.881], mean action: 1.644 [0.000, 3.000], mean observation: 0.126 [-0.688, 1.478], loss: 9.286083, mae: 40.848663, mean_q: 54.179264
  397428/1100000: episode: 841, duration: 2.706s, episode steps: 450, steps per second: 166, episode reward: 193.425, mean reward: 0.430 [-10.396, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.157 [-0.685, 1.406], loss: 5.125986, mae: 40.812668, mean_q: 54.486328
  398063/1100000: episode: 842, duration: 4.338s, episode steps: 635, steps per second: 146, episode reward: 205.864, mean reward: 0.324 [-10.598, 100.000], mean action: 1.526 [0.000, 3.000], mean observation: -0.075 [-0.651, 1.444], loss: 7.055591, mae: 40.371609, mean_q: 53.756779
  398366/1100000: episode: 843, duration: 1.803s, episode steps: 303, steps per second: 168, episode reward: 257.213, mean reward: 0.849 [-9.952, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.075 [-0.757, 1.414], loss: 7.666433, mae: 40.147865, mean_q: 53.520149
  398615/1100000: episode: 844, duration: 1.447s, episode steps: 249, steps per second: 172, episode reward: 265.910, mean reward: 1.068 [-18.477, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.061 [-0.788, 1.393], loss: 6.753084, mae: 39.970860, mean_q: 52.987282
  399308/1100000: episode: 845, duration: 4.178s, episode steps: 693, steps per second: 166, episode reward: 200.143, mean reward: 0.289 [-13.030, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.152 [-0.883, 1.388], loss: 5.904968, mae: 40.292637, mean_q: 53.540947
  399647/1100000: episode: 846, duration: 2.017s, episode steps: 339, steps per second: 168, episode reward: 239.636, mean reward: 0.707 [-17.744, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.080 [-0.716, 1.419], loss: 4.017988, mae: 40.063919, mean_q: 53.336830
  399970/1100000: episode: 847, duration: 1.930s, episode steps: 323, steps per second: 167, episode reward: 225.346, mean reward: 0.698 [-17.676, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.049 [-0.738, 1.400], loss: 6.397741, mae: 40.183308, mean_q: 53.468277
  400970/1100000: episode: 848, duration: 6.891s, episode steps: 1000, steps per second: 145, episode reward: -13.535, mean reward: -0.014 [-5.158, 4.555], mean action: 1.725 [0.000, 3.000], mean observation: -0.062 [-0.736, 1.412], loss: 6.106838, mae: 40.012875, mean_q: 53.430134
  401460/1100000: episode: 849, duration: 3.106s, episode steps: 490, steps per second: 158, episode reward: 168.738, mean reward: 0.344 [-16.041, 100.000], mean action: 1.912 [0.000, 3.000], mean observation: 0.017 [-1.240, 1.399], loss: 6.171558, mae: 40.083447, mean_q: 53.610085
  402460/1100000: episode: 850, duration: 6.423s, episode steps: 1000, steps per second: 156, episode reward: 83.806, mean reward: 0.084 [-21.447, 24.260], mean action: 1.343 [0.000, 3.000], mean observation: 0.058 [-1.257, 1.422], loss: 7.635614, mae: 39.927170, mean_q: 53.256058
  402756/1100000: episode: 851, duration: 1.779s, episode steps: 296, steps per second: 166, episode reward: 273.713, mean reward: 0.925 [-17.395, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.092 [-0.644, 1.393], loss: 5.935609, mae: 39.907410, mean_q: 53.464798
  402965/1100000: episode: 852, duration: 1.193s, episode steps: 209, steps per second: 175, episode reward: -3.759, mean reward: -0.018 [-100.000, 16.672], mean action: 1.321 [0.000, 3.000], mean observation: -0.021 [-0.734, 1.460], loss: 6.489849, mae: 39.433483, mean_q: 52.506115
  403654/1100000: episode: 853, duration: 4.431s, episode steps: 689, steps per second: 155, episode reward: 230.771, mean reward: 0.335 [-19.161, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.227 [-0.634, 1.407], loss: 7.840189, mae: 39.617668, mean_q: 52.844044
  404477/1100000: episode: 854, duration: 5.719s, episode steps: 823, steps per second: 144, episode reward: 162.934, mean reward: 0.198 [-19.067, 100.000], mean action: 1.623 [0.000, 3.000], mean observation: 0.154 [-0.701, 1.388], loss: 7.112698, mae: 39.960957, mean_q: 53.315651
  405477/1100000: episode: 855, duration: 6.414s, episode steps: 1000, steps per second: 156, episode reward: 144.650, mean reward: 0.145 [-19.869, 21.707], mean action: 1.398 [0.000, 3.000], mean observation: 0.101 [-0.648, 1.393], loss: 5.038980, mae: 39.555977, mean_q: 52.900566
  406110/1100000: episode: 856, duration: 4.030s, episode steps: 633, steps per second: 157, episode reward: 160.540, mean reward: 0.254 [-20.513, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.005 [-0.600, 1.402], loss: 5.120481, mae: 39.219372, mean_q: 52.319023
  406828/1100000: episode: 857, duration: 4.754s, episode steps: 718, steps per second: 151, episode reward: 238.935, mean reward: 0.333 [-18.682, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: -0.004 [-0.756, 1.386], loss: 4.469706, mae: 39.387028, mean_q: 52.607395
  407025/1100000: episode: 858, duration: 1.144s, episode steps: 197, steps per second: 172, episode reward: 247.525, mean reward: 1.256 [-9.050, 100.000], mean action: 1.574 [0.000, 3.000], mean observation: 0.064 [-0.834, 1.411], loss: 8.895222, mae: 39.841831, mean_q: 53.107334
  407963/1100000: episode: 859, duration: 6.923s, episode steps: 938, steps per second: 135, episode reward: 135.870, mean reward: 0.145 [-9.728, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: -0.082 [-0.623, 1.393], loss: 5.131261, mae: 39.331837, mean_q: 52.415577
  408243/1100000: episode: 860, duration: 1.664s, episode steps: 280, steps per second: 168, episode reward: 282.507, mean reward: 1.009 [-13.722, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.111 [-0.753, 1.392], loss: 6.054517, mae: 39.298523, mean_q: 52.222462
  408594/1100000: episode: 861, duration: 2.094s, episode steps: 351, steps per second: 168, episode reward: 240.656, mean reward: 0.686 [-19.154, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.159 [-0.966, 1.387], loss: 4.568731, mae: 39.501507, mean_q: 52.594238
  408901/1100000: episode: 862, duration: 1.765s, episode steps: 307, steps per second: 174, episode reward: 285.532, mean reward: 0.930 [-10.833, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.090 [-0.808, 1.478], loss: 6.294220, mae: 39.476524, mean_q: 52.618385
  409649/1100000: episode: 863, duration: 4.650s, episode steps: 748, steps per second: 161, episode reward: 228.502, mean reward: 0.305 [-23.959, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.047 [-0.600, 1.509], loss: 6.573215, mae: 39.552143, mean_q: 52.568256
  410177/1100000: episode: 864, duration: 3.190s, episode steps: 528, steps per second: 166, episode reward: 188.179, mean reward: 0.356 [-14.462, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.040 [-0.671, 1.431], loss: 5.088840, mae: 39.459000, mean_q: 52.407364
  411059/1100000: episode: 865, duration: 5.529s, episode steps: 882, steps per second: 160, episode reward: 222.314, mean reward: 0.252 [-24.556, 100.000], mean action: 0.935 [0.000, 3.000], mean observation: 0.230 [-0.987, 1.487], loss: 7.077219, mae: 39.594688, mean_q: 52.544361
  411587/1100000: episode: 866, duration: 3.195s, episode steps: 528, steps per second: 165, episode reward: -236.420, mean reward: -0.448 [-100.000, 18.108], mean action: 1.432 [0.000, 3.000], mean observation: 0.009 [-1.792, 1.391], loss: 5.101413, mae: 39.985340, mean_q: 53.295647
  412106/1100000: episode: 867, duration: 3.240s, episode steps: 519, steps per second: 160, episode reward: 230.880, mean reward: 0.445 [-19.301, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.196 [-1.027, 1.480], loss: 3.687598, mae: 39.855000, mean_q: 53.139233
  413054/1100000: episode: 868, duration: 6.369s, episode steps: 948, steps per second: 149, episode reward: -159.776, mean reward: -0.169 [-100.000, 15.519], mean action: 1.667 [0.000, 3.000], mean observation: -0.022 [-0.969, 1.396], loss: 6.121728, mae: 39.767033, mean_q: 52.849972
  414054/1100000: episode: 869, duration: 6.989s, episode steps: 1000, steps per second: 143, episode reward: 45.780, mean reward: 0.046 [-23.769, 23.574], mean action: 1.152 [0.000, 3.000], mean observation: 0.149 [-0.782, 1.402], loss: 5.162254, mae: 39.297813, mean_q: 52.309727
  414335/1100000: episode: 870, duration: 1.674s, episode steps: 281, steps per second: 168, episode reward: 270.696, mean reward: 0.963 [-6.772, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.082 [-0.785, 1.415], loss: 4.107224, mae: 39.429729, mean_q: 52.515026
  414896/1100000: episode: 871, duration: 3.414s, episode steps: 561, steps per second: 164, episode reward: 168.834, mean reward: 0.301 [-20.093, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.171 [-0.832, 1.405], loss: 6.687580, mae: 39.201077, mean_q: 51.992035
  415806/1100000: episode: 872, duration: 5.780s, episode steps: 910, steps per second: 157, episode reward: 192.342, mean reward: 0.211 [-20.586, 100.000], mean action: 1.568 [0.000, 3.000], mean observation: 0.061 [-0.600, 1.410], loss: 5.539076, mae: 39.065769, mean_q: 52.106163
  416348/1100000: episode: 873, duration: 3.328s, episode steps: 542, steps per second: 163, episode reward: 199.777, mean reward: 0.369 [-10.783, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.138 [-0.587, 1.420], loss: 5.336934, mae: 38.739674, mean_q: 51.637993
  417081/1100000: episode: 874, duration: 4.682s, episode steps: 733, steps per second: 157, episode reward: 236.614, mean reward: 0.323 [-20.557, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.153 [-0.881, 1.389], loss: 5.628713, mae: 38.710426, mean_q: 51.706852
  417353/1100000: episode: 875, duration: 1.600s, episode steps: 272, steps per second: 170, episode reward: 237.326, mean reward: 0.873 [-6.147, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.164 [-0.995, 1.405], loss: 6.133278, mae: 38.534603, mean_q: 51.558586
  417829/1100000: episode: 876, duration: 2.839s, episode steps: 476, steps per second: 168, episode reward: 217.500, mean reward: 0.457 [-13.301, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.145 [-0.638, 1.417], loss: 6.807812, mae: 38.748371, mean_q: 51.837154
  418439/1100000: episode: 877, duration: 4.091s, episode steps: 610, steps per second: 149, episode reward: 189.933, mean reward: 0.311 [-19.166, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: -0.002 [-0.693, 1.425], loss: 5.479546, mae: 39.203754, mean_q: 52.509777
  419049/1100000: episode: 878, duration: 3.738s, episode steps: 610, steps per second: 163, episode reward: 245.321, mean reward: 0.402 [-19.324, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.018 [-0.600, 1.496], loss: 4.390906, mae: 38.708683, mean_q: 51.861012
  419734/1100000: episode: 879, duration: 4.429s, episode steps: 685, steps per second: 155, episode reward: 171.895, mean reward: 0.251 [-20.380, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: -0.031 [-0.600, 1.431], loss: 5.288757, mae: 38.566063, mean_q: 51.591080
  420015/1100000: episode: 880, duration: 1.645s, episode steps: 281, steps per second: 171, episode reward: 294.858, mean reward: 1.049 [-11.033, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.099 [-0.805, 1.421], loss: 4.485492, mae: 38.646366, mean_q: 51.853012
  420918/1100000: episode: 881, duration: 5.661s, episode steps: 903, steps per second: 160, episode reward: 172.781, mean reward: 0.191 [-20.055, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.188 [-0.716, 1.410], loss: 5.444892, mae: 38.660999, mean_q: 51.704601
  421192/1100000: episode: 882, duration: 1.592s, episode steps: 274, steps per second: 172, episode reward: 29.735, mean reward: 0.109 [-100.000, 10.842], mean action: 1.617 [0.000, 3.000], mean observation: -0.017 [-1.308, 1.410], loss: 5.166601, mae: 38.802357, mean_q: 51.851906
  421730/1100000: episode: 883, duration: 3.263s, episode steps: 538, steps per second: 165, episode reward: 221.384, mean reward: 0.411 [-23.407, 100.000], mean action: 1.933 [0.000, 3.000], mean observation: 0.013 [-0.600, 1.396], loss: 4.963459, mae: 38.678516, mean_q: 51.665028
  422441/1100000: episode: 884, duration: 4.414s, episode steps: 711, steps per second: 161, episode reward: 177.925, mean reward: 0.250 [-13.530, 100.000], mean action: 1.868 [0.000, 3.000], mean observation: 0.007 [-0.742, 1.405], loss: 5.952536, mae: 38.417614, mean_q: 51.312084
  422691/1100000: episode: 885, duration: 1.470s, episode steps: 250, steps per second: 170, episode reward: 268.801, mean reward: 1.075 [-8.476, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.040 [-0.824, 1.387], loss: 5.195726, mae: 38.909382, mean_q: 52.063133
  422977/1100000: episode: 886, duration: 1.645s, episode steps: 286, steps per second: 174, episode reward: 213.483, mean reward: 0.746 [-19.463, 100.000], mean action: 0.801 [0.000, 3.000], mean observation: 0.204 [-0.992, 1.412], loss: 3.017988, mae: 38.728931, mean_q: 51.802048
  423091/1100000: episode: 887, duration: 0.646s, episode steps: 114, steps per second: 176, episode reward: 13.135, mean reward: 0.115 [-100.000, 17.652], mean action: 1.588 [0.000, 3.000], mean observation: 0.233 [-1.181, 1.521], loss: 8.933988, mae: 38.298492, mean_q: 51.410530
  424043/1100000: episode: 888, duration: 5.952s, episode steps: 952, steps per second: 160, episode reward: 174.961, mean reward: 0.184 [-19.510, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.014 [-0.708, 1.459], loss: 6.495759, mae: 38.865902, mean_q: 51.967690
  424248/1100000: episode: 889, duration: 1.172s, episode steps: 205, steps per second: 175, episode reward: 264.908, mean reward: 1.292 [-13.769, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.042 [-0.827, 1.406], loss: 5.548009, mae: 38.781109, mean_q: 51.751381
  424717/1100000: episode: 890, duration: 2.869s, episode steps: 469, steps per second: 163, episode reward: 198.211, mean reward: 0.423 [-18.940, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.194 [-0.990, 1.410], loss: 4.505986, mae: 38.801598, mean_q: 51.924339
  424887/1100000: episode: 891, duration: 0.966s, episode steps: 170, steps per second: 176, episode reward: -64.132, mean reward: -0.377 [-100.000, 8.319], mean action: 1.406 [0.000, 3.000], mean observation: -0.038 [-1.047, 3.791], loss: 6.750022, mae: 38.870342, mean_q: 52.012993
  425203/1100000: episode: 892, duration: 1.823s, episode steps: 316, steps per second: 173, episode reward: 230.443, mean reward: 0.729 [-18.724, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: -0.001 [-0.606, 1.425], loss: 4.341382, mae: 39.294815, mean_q: 52.545860
  425719/1100000: episode: 893, duration: 3.036s, episode steps: 516, steps per second: 170, episode reward: 210.945, mean reward: 0.409 [-19.369, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.023 [-0.706, 1.448], loss: 4.119096, mae: 38.629520, mean_q: 51.643093
  426057/1100000: episode: 894, duration: 2.000s, episode steps: 338, steps per second: 169, episode reward: 272.836, mean reward: 0.807 [-7.507, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.084 [-0.803, 1.503], loss: 9.550988, mae: 38.841400, mean_q: 51.880085
  426745/1100000: episode: 895, duration: 4.280s, episode steps: 688, steps per second: 161, episode reward: 192.359, mean reward: 0.280 [-19.497, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.019 [-0.678, 1.403], loss: 7.108855, mae: 38.505520, mean_q: 51.365093
  427048/1100000: episode: 896, duration: 1.788s, episode steps: 303, steps per second: 169, episode reward: 243.112, mean reward: 0.802 [-8.800, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.088 [-0.702, 1.417], loss: 4.736728, mae: 37.939182, mean_q: 50.650780
  427869/1100000: episode: 897, duration: 5.283s, episode steps: 821, steps per second: 155, episode reward: 113.254, mean reward: 0.138 [-14.158, 100.000], mean action: 1.691 [0.000, 3.000], mean observation: -0.045 [-0.600, 1.395], loss: 7.023857, mae: 38.200405, mean_q: 51.088242
  428036/1100000: episode: 898, duration: 0.952s, episode steps: 167, steps per second: 175, episode reward: -29.246, mean reward: -0.175 [-100.000, 10.932], mean action: 1.605 [0.000, 3.000], mean observation: 0.098 [-1.411, 1.430], loss: 3.774013, mae: 38.756145, mean_q: 51.770630
  428417/1100000: episode: 899, duration: 2.208s, episode steps: 381, steps per second: 173, episode reward: 246.729, mean reward: 0.648 [-20.374, 100.000], mean action: 0.769 [0.000, 3.000], mean observation: 0.234 [-1.013, 1.396], loss: 7.124976, mae: 38.013805, mean_q: 50.812904
  428629/1100000: episode: 900, duration: 1.214s, episode steps: 212, steps per second: 175, episode reward: -1.305, mean reward: -0.006 [-100.000, 16.973], mean action: 1.528 [0.000, 3.000], mean observation: 0.108 [-0.857, 1.451], loss: 4.487698, mae: 38.037804, mean_q: 50.818386
  429545/1100000: episode: 901, duration: 5.954s, episode steps: 916, steps per second: 154, episode reward: 186.067, mean reward: 0.203 [-25.399, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: -0.002 [-0.607, 1.392], loss: 6.001085, mae: 37.746078, mean_q: 50.325802
  429899/1100000: episode: 902, duration: 2.069s, episode steps: 354, steps per second: 171, episode reward: 285.796, mean reward: 0.807 [-17.457, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.091 [-1.083, 1.399], loss: 6.077089, mae: 37.523994, mean_q: 49.963669
  430162/1100000: episode: 903, duration: 1.514s, episode steps: 263, steps per second: 174, episode reward: -65.796, mean reward: -0.250 [-100.000, 14.643], mean action: 1.251 [0.000, 3.000], mean observation: -0.052 [-1.981, 1.498], loss: 7.363990, mae: 37.678146, mean_q: 50.096333
  430575/1100000: episode: 904, duration: 2.440s, episode steps: 413, steps per second: 169, episode reward: 234.119, mean reward: 0.567 [-20.083, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.139 [-0.572, 1.414], loss: 6.193808, mae: 37.819958, mean_q: 50.381508
  431430/1100000: episode: 905, duration: 5.261s, episode steps: 855, steps per second: 163, episode reward: 174.752, mean reward: 0.204 [-18.868, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.168 [-0.754, 1.484], loss: 5.972933, mae: 37.766678, mean_q: 50.476509
  431722/1100000: episode: 906, duration: 1.703s, episode steps: 292, steps per second: 171, episode reward: 238.105, mean reward: 0.815 [-14.556, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.106 [-0.626, 1.406], loss: 3.811596, mae: 37.752769, mean_q: 50.496113
  432133/1100000: episode: 907, duration: 2.434s, episode steps: 411, steps per second: 169, episode reward: 251.839, mean reward: 0.613 [-13.667, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.123 [-0.563, 1.434], loss: 5.472900, mae: 37.877884, mean_q: 50.571514
  432644/1100000: episode: 908, duration: 3.178s, episode steps: 511, steps per second: 161, episode reward: 224.261, mean reward: 0.439 [-13.767, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.003 [-0.702, 1.392], loss: 5.049868, mae: 37.773884, mean_q: 50.513660
  432947/1100000: episode: 909, duration: 1.799s, episode steps: 303, steps per second: 168, episode reward: 253.077, mean reward: 0.835 [-3.481, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.135 [-0.607, 1.411], loss: 6.606167, mae: 37.967274, mean_q: 50.563862
  433682/1100000: episode: 910, duration: 4.627s, episode steps: 735, steps per second: 159, episode reward: -82.608, mean reward: -0.112 [-100.000, 42.024], mean action: 1.737 [0.000, 3.000], mean observation: 0.117 [-1.410, 1.917], loss: 4.751131, mae: 37.781929, mean_q: 50.604763
  433911/1100000: episode: 911, duration: 1.316s, episode steps: 229, steps per second: 174, episode reward: -182.355, mean reward: -0.796 [-100.000, 4.341], mean action: 1.467 [0.000, 3.000], mean observation: 0.068 [-1.005, 1.392], loss: 4.449937, mae: 37.292206, mean_q: 49.903343
  434199/1100000: episode: 912, duration: 1.727s, episode steps: 288, steps per second: 167, episode reward: 251.264, mean reward: 0.872 [-19.129, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.085 [-0.583, 1.399], loss: 4.219590, mae: 37.808418, mean_q: 50.489937
  434879/1100000: episode: 913, duration: 4.415s, episode steps: 680, steps per second: 154, episode reward: -67.905, mean reward: -0.100 [-100.000, 14.775], mean action: 1.671 [0.000, 3.000], mean observation: 0.168 [-1.034, 1.495], loss: 5.213522, mae: 37.763329, mean_q: 50.509377
  435553/1100000: episode: 914, duration: 4.206s, episode steps: 674, steps per second: 160, episode reward: -228.371, mean reward: -0.339 [-100.000, 10.537], mean action: 1.613 [0.000, 3.000], mean observation: 0.090 [-0.984, 1.436], loss: 5.399600, mae: 37.915611, mean_q: 50.562019
  436190/1100000: episode: 915, duration: 3.917s, episode steps: 637, steps per second: 163, episode reward: 165.695, mean reward: 0.260 [-17.305, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.003 [-0.632, 1.434], loss: 7.629232, mae: 37.436604, mean_q: 49.800217
  437190/1100000: episode: 916, duration: 6.734s, episode steps: 1000, steps per second: 149, episode reward: 56.090, mean reward: 0.056 [-18.310, 22.760], mean action: 1.200 [0.000, 3.000], mean observation: 0.012 [-0.662, 1.415], loss: 5.386260, mae: 37.164158, mean_q: 49.552135
  437708/1100000: episode: 917, duration: 3.151s, episode steps: 518, steps per second: 164, episode reward: 229.355, mean reward: 0.443 [-10.396, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.044 [-0.600, 1.416], loss: 5.371254, mae: 37.405899, mean_q: 49.850349
  437792/1100000: episode: 918, duration: 0.471s, episode steps: 84, steps per second: 178, episode reward: -54.776, mean reward: -0.652 [-100.000, 10.128], mean action: 1.036 [0.000, 3.000], mean observation: -0.005 [-3.631, 1.406], loss: 2.659829, mae: 37.148026, mean_q: 49.729317
  438110/1100000: episode: 919, duration: 1.861s, episode steps: 318, steps per second: 171, episode reward: 287.840, mean reward: 0.905 [-17.925, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.080 [-0.608, 1.499], loss: 5.431787, mae: 37.397011, mean_q: 49.914516
  438746/1100000: episode: 920, duration: 4.038s, episode steps: 636, steps per second: 157, episode reward: 161.409, mean reward: 0.254 [-13.473, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: -0.039 [-0.600, 1.414], loss: 5.187012, mae: 37.103466, mean_q: 49.443905
  439227/1100000: episode: 921, duration: 2.871s, episode steps: 481, steps per second: 168, episode reward: 202.824, mean reward: 0.422 [-14.591, 100.000], mean action: 1.784 [0.000, 3.000], mean observation: -0.014 [-0.894, 1.386], loss: 4.538203, mae: 37.094803, mean_q: 49.351585
  440227/1100000: episode: 922, duration: 6.313s, episode steps: 1000, steps per second: 158, episode reward: -4.543, mean reward: -0.005 [-20.763, 17.554], mean action: 1.572 [0.000, 3.000], mean observation: 0.097 [-0.873, 1.456], loss: 6.025662, mae: 36.905617, mean_q: 49.257565
  440631/1100000: episode: 923, duration: 2.460s, episode steps: 404, steps per second: 164, episode reward: 233.831, mean reward: 0.579 [-8.865, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.207 [-0.853, 1.537], loss: 6.686728, mae: 36.955498, mean_q: 49.359379
  441088/1100000: episode: 924, duration: 2.763s, episode steps: 457, steps per second: 165, episode reward: 269.838, mean reward: 0.590 [-19.231, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.121 [-0.777, 1.523], loss: 2.816138, mae: 37.515362, mean_q: 50.149567
  441585/1100000: episode: 925, duration: 3.034s, episode steps: 497, steps per second: 164, episode reward: 247.548, mean reward: 0.498 [-17.833, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.117 [-0.874, 1.441], loss: 5.452451, mae: 37.817909, mean_q: 50.436115
  441803/1100000: episode: 926, duration: 1.258s, episode steps: 218, steps per second: 173, episode reward: 249.022, mean reward: 1.142 [-19.175, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.055 [-0.889, 1.401], loss: 3.727130, mae: 37.571651, mean_q: 49.994610
  442492/1100000: episode: 927, duration: 4.616s, episode steps: 689, steps per second: 149, episode reward: 202.751, mean reward: 0.294 [-13.558, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: -0.044 [-0.600, 1.503], loss: 4.692062, mae: 38.006706, mean_q: 50.804317
  442626/1100000: episode: 928, duration: 0.764s, episode steps: 134, steps per second: 175, episode reward: 3.985, mean reward: 0.030 [-100.000, 10.014], mean action: 1.664 [0.000, 3.000], mean observation: 0.028 [-1.280, 1.495], loss: 6.711690, mae: 37.877266, mean_q: 50.741028
  442772/1100000: episode: 929, duration: 0.833s, episode steps: 146, steps per second: 175, episode reward: 9.286, mean reward: 0.064 [-100.000, 24.962], mean action: 1.459 [0.000, 3.000], mean observation: 0.095 [-1.793, 1.479], loss: 5.664894, mae: 37.965248, mean_q: 50.752235
  443355/1100000: episode: 930, duration: 3.788s, episode steps: 583, steps per second: 154, episode reward: 208.846, mean reward: 0.358 [-10.543, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: -0.055 [-0.640, 1.399], loss: 8.265906, mae: 38.150753, mean_q: 50.969749
  443867/1100000: episode: 931, duration: 3.027s, episode steps: 512, steps per second: 169, episode reward: 238.059, mean reward: 0.465 [-20.306, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.009 [-0.922, 1.517], loss: 5.248704, mae: 37.893837, mean_q: 50.739784
  444112/1100000: episode: 932, duration: 1.425s, episode steps: 245, steps per second: 172, episode reward: 268.832, mean reward: 1.097 [-17.368, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.084 [-1.123, 1.392], loss: 12.660453, mae: 38.049034, mean_q: 50.892994
  444839/1100000: episode: 933, duration: 5.023s, episode steps: 727, steps per second: 145, episode reward: 212.202, mean reward: 0.292 [-9.352, 100.000], mean action: 1.615 [0.000, 3.000], mean observation: -0.051 [-0.766, 1.395], loss: 5.744652, mae: 38.134594, mean_q: 51.055519
  445839/1100000: episode: 934, duration: 6.639s, episode steps: 1000, steps per second: 151, episode reward: -47.206, mean reward: -0.047 [-18.565, 19.421], mean action: 1.711 [0.000, 3.000], mean observation: 0.198 [-0.790, 1.407], loss: 5.816453, mae: 38.167881, mean_q: 51.102360
  446522/1100000: episode: 935, duration: 4.211s, episode steps: 683, steps per second: 162, episode reward: 252.970, mean reward: 0.370 [-21.170, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.134 [-1.005, 1.456], loss: 6.279125, mae: 38.561951, mean_q: 51.676353
  446823/1100000: episode: 936, duration: 1.770s, episode steps: 301, steps per second: 170, episode reward: 254.627, mean reward: 0.846 [-8.588, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.082 [-0.772, 1.487], loss: 4.223005, mae: 38.443192, mean_q: 51.651684
  447823/1100000: episode: 937, duration: 6.280s, episode steps: 1000, steps per second: 159, episode reward: -118.927, mean reward: -0.119 [-13.997, 12.640], mean action: 1.725 [0.000, 3.000], mean observation: -0.001 [-1.088, 1.402], loss: 5.433915, mae: 38.949390, mean_q: 52.348007
  448501/1100000: episode: 938, duration: 4.317s, episode steps: 678, steps per second: 157, episode reward: 183.899, mean reward: 0.271 [-12.502, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: 0.125 [-0.701, 1.409], loss: 6.367732, mae: 38.580631, mean_q: 51.826962
  448987/1100000: episode: 939, duration: 3.042s, episode steps: 486, steps per second: 160, episode reward: 240.102, mean reward: 0.494 [-9.055, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.247 [-0.839, 1.486], loss: 6.251626, mae: 38.562359, mean_q: 51.732407
  449578/1100000: episode: 940, duration: 3.737s, episode steps: 591, steps per second: 158, episode reward: 201.450, mean reward: 0.341 [-19.210, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.171 [-0.797, 1.392], loss: 7.154882, mae: 38.759392, mean_q: 51.976116
  450186/1100000: episode: 941, duration: 3.755s, episode steps: 608, steps per second: 162, episode reward: 192.411, mean reward: 0.316 [-17.844, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.016 [-0.646, 1.428], loss: 5.807532, mae: 38.796947, mean_q: 52.007385
  450402/1100000: episode: 942, duration: 1.256s, episode steps: 216, steps per second: 172, episode reward: 292.852, mean reward: 1.356 [-9.083, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.048 [-0.965, 1.391], loss: 6.697791, mae: 38.896072, mean_q: 52.134712
  450783/1100000: episode: 943, duration: 2.384s, episode steps: 381, steps per second: 160, episode reward: 200.006, mean reward: 0.525 [-17.195, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.140 [-0.833, 1.400], loss: 5.942251, mae: 39.172638, mean_q: 52.629669
  451107/1100000: episode: 944, duration: 1.880s, episode steps: 324, steps per second: 172, episode reward: 251.046, mean reward: 0.775 [-20.841, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.112 [-1.392, 1.425], loss: 5.568501, mae: 38.995655, mean_q: 52.431808
  451364/1100000: episode: 945, duration: 1.515s, episode steps: 257, steps per second: 170, episode reward: 284.692, mean reward: 1.108 [-12.584, 100.000], mean action: 1.696 [0.000, 3.000], mean observation: 0.074 [-0.622, 1.399], loss: 4.703386, mae: 39.076317, mean_q: 52.521111
  451955/1100000: episode: 946, duration: 3.566s, episode steps: 591, steps per second: 166, episode reward: 226.014, mean reward: 0.382 [-19.047, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: -0.003 [-0.779, 1.388], loss: 6.703907, mae: 39.381222, mean_q: 52.938469
  452097/1100000: episode: 947, duration: 0.809s, episode steps: 142, steps per second: 176, episode reward: 1.804, mean reward: 0.013 [-100.000, 29.456], mean action: 1.310 [0.000, 3.000], mean observation: -0.027 [-0.866, 1.547], loss: 2.949044, mae: 39.158173, mean_q: 52.739689
  452419/1100000: episode: 948, duration: 1.841s, episode steps: 322, steps per second: 175, episode reward: 258.459, mean reward: 0.803 [-10.844, 100.000], mean action: 0.773 [0.000, 3.000], mean observation: 0.043 [-0.903, 1.446], loss: 7.221487, mae: 39.092117, mean_q: 52.410847
  452996/1100000: episode: 949, duration: 3.745s, episode steps: 577, steps per second: 154, episode reward: 161.916, mean reward: 0.281 [-24.218, 100.000], mean action: 1.586 [0.000, 3.000], mean observation: -0.018 [-0.769, 1.426], loss: 7.201111, mae: 39.446171, mean_q: 52.926723
  453996/1100000: episode: 950, duration: 6.368s, episode steps: 1000, steps per second: 157, episode reward: -49.275, mean reward: -0.049 [-5.376, 5.462], mean action: 1.695 [0.000, 3.000], mean observation: 0.078 [-0.827, 1.403], loss: 4.685434, mae: 38.795937, mean_q: 52.189392
  454691/1100000: episode: 951, duration: 4.456s, episode steps: 695, steps per second: 156, episode reward: 171.292, mean reward: 0.246 [-13.421, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: -0.054 [-0.663, 1.399], loss: 6.502219, mae: 38.739941, mean_q: 52.105846
  455184/1100000: episode: 952, duration: 3.033s, episode steps: 493, steps per second: 163, episode reward: 279.328, mean reward: 0.567 [-17.842, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.112 [-1.344, 1.416], loss: 4.686841, mae: 38.698711, mean_q: 52.037048
  455858/1100000: episode: 953, duration: 4.300s, episode steps: 674, steps per second: 157, episode reward: 244.466, mean reward: 0.363 [-17.154, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.222 [-0.760, 1.395], loss: 5.558289, mae: 39.204300, mean_q: 52.717972
  456297/1100000: episode: 954, duration: 2.712s, episode steps: 439, steps per second: 162, episode reward: 227.205, mean reward: 0.518 [-19.094, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: -0.058 [-0.604, 1.393], loss: 4.960988, mae: 39.127918, mean_q: 52.644863
  456971/1100000: episode: 955, duration: 4.235s, episode steps: 674, steps per second: 159, episode reward: 184.698, mean reward: 0.274 [-20.164, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.217 [-0.910, 1.413], loss: 5.109351, mae: 39.268570, mean_q: 52.850716
  457179/1100000: episode: 956, duration: 1.181s, episode steps: 208, steps per second: 176, episode reward: -132.325, mean reward: -0.636 [-100.000, 11.987], mean action: 1.221 [0.000, 3.000], mean observation: 0.051 [-1.101, 1.494], loss: 4.883004, mae: 38.786156, mean_q: 52.224190
  457389/1100000: episode: 957, duration: 1.226s, episode steps: 210, steps per second: 171, episode reward: -371.011, mean reward: -1.767 [-100.000, 22.992], mean action: 1.762 [0.000, 3.000], mean observation: 0.219 [-1.999, 3.280], loss: 6.407710, mae: 39.699322, mean_q: 53.445713
  457657/1100000: episode: 958, duration: 1.567s, episode steps: 268, steps per second: 171, episode reward: 217.687, mean reward: 0.812 [-9.721, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: -0.030 [-0.812, 1.418], loss: 6.909865, mae: 39.566189, mean_q: 53.214378
  458051/1100000: episode: 959, duration: 2.380s, episode steps: 394, steps per second: 166, episode reward: 223.982, mean reward: 0.568 [-10.829, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: -0.056 [-0.608, 1.411], loss: 4.946887, mae: 39.781128, mean_q: 53.546093
  458201/1100000: episode: 960, duration: 0.848s, episode steps: 150, steps per second: 177, episode reward: -131.890, mean reward: -0.879 [-100.000, 3.604], mean action: 1.407 [0.000, 3.000], mean observation: 0.048 [-1.003, 1.430], loss: 5.887472, mae: 39.791817, mean_q: 53.544621
  458940/1100000: episode: 961, duration: 4.500s, episode steps: 739, steps per second: 164, episode reward: 217.202, mean reward: 0.294 [-18.356, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.206 [-0.493, 1.424], loss: 6.359404, mae: 39.615692, mean_q: 53.173779
  459416/1100000: episode: 962, duration: 2.889s, episode steps: 476, steps per second: 165, episode reward: 282.888, mean reward: 0.594 [-17.739, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.075 [-0.875, 1.395], loss: 6.850391, mae: 39.421825, mean_q: 52.964577
  459805/1100000: episode: 963, duration: 2.303s, episode steps: 389, steps per second: 169, episode reward: 200.178, mean reward: 0.515 [-17.377, 100.000], mean action: 2.010 [0.000, 3.000], mean observation: 0.036 [-0.621, 1.401], loss: 2.527922, mae: 39.502487, mean_q: 53.186424
  460041/1100000: episode: 964, duration: 1.381s, episode steps: 236, steps per second: 171, episode reward: 263.439, mean reward: 1.116 [-10.143, 100.000], mean action: 1.597 [0.000, 3.000], mean observation: 0.079 [-0.653, 1.404], loss: 3.954606, mae: 39.827106, mean_q: 53.634319
  460286/1100000: episode: 965, duration: 1.443s, episode steps: 245, steps per second: 170, episode reward: 284.315, mean reward: 1.160 [-6.508, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.096 [-0.758, 1.386], loss: 4.181499, mae: 40.037327, mean_q: 53.937603
  460631/1100000: episode: 966, duration: 2.006s, episode steps: 345, steps per second: 172, episode reward: 211.314, mean reward: 0.613 [-9.289, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.146 [-0.829, 1.436], loss: 4.618418, mae: 40.099415, mean_q: 53.909943
  460905/1100000: episode: 967, duration: 1.613s, episode steps: 274, steps per second: 170, episode reward: 274.211, mean reward: 1.001 [-8.844, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.045 [-0.673, 1.407], loss: 4.824719, mae: 39.873688, mean_q: 53.751789
  461151/1100000: episode: 968, duration: 1.453s, episode steps: 246, steps per second: 169, episode reward: 281.392, mean reward: 1.144 [-7.764, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: 0.073 [-0.823, 1.421], loss: 4.171193, mae: 39.990116, mean_q: 53.929897
  461538/1100000: episode: 969, duration: 2.298s, episode steps: 387, steps per second: 168, episode reward: 272.256, mean reward: 0.704 [-17.588, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.087 [-0.731, 1.397], loss: 9.762415, mae: 40.394447, mean_q: 54.396229
  461730/1100000: episode: 970, duration: 1.103s, episode steps: 192, steps per second: 174, episode reward: 228.127, mean reward: 1.188 [-9.223, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.063 [-0.766, 1.429], loss: 5.782324, mae: 40.751232, mean_q: 54.943935
  461858/1100000: episode: 971, duration: 0.736s, episode steps: 128, steps per second: 174, episode reward: -147.651, mean reward: -1.154 [-100.000, 15.909], mean action: 1.484 [0.000, 3.000], mean observation: 0.037 [-0.702, 1.522], loss: 9.600863, mae: 40.018188, mean_q: 53.890652
  462243/1100000: episode: 972, duration: 2.327s, episode steps: 385, steps per second: 165, episode reward: 184.745, mean reward: 0.480 [-10.042, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: -0.058 [-0.641, 1.421], loss: 5.564314, mae: 40.868813, mean_q: 55.125648
  462419/1100000: episode: 973, duration: 1.006s, episode steps: 176, steps per second: 175, episode reward: -154.314, mean reward: -0.877 [-100.000, 2.212], mean action: 1.341 [0.000, 3.000], mean observation: -0.071 [-1.006, 1.396], loss: 2.561154, mae: 40.809689, mean_q: 55.077816
  463008/1100000: episode: 974, duration: 3.549s, episode steps: 589, steps per second: 166, episode reward: 224.426, mean reward: 0.381 [-17.274, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.207 [-0.767, 1.514], loss: 5.511467, mae: 40.846138, mean_q: 55.032665
  463916/1100000: episode: 975, duration: 5.612s, episode steps: 908, steps per second: 162, episode reward: 166.527, mean reward: 0.183 [-19.196, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.123 [-0.689, 1.395], loss: 5.001623, mae: 40.588234, mean_q: 54.703293
  464613/1100000: episode: 976, duration: 4.334s, episode steps: 697, steps per second: 161, episode reward: 266.837, mean reward: 0.383 [-18.122, 100.000], mean action: 0.813 [0.000, 3.000], mean observation: 0.140 [-0.710, 1.408], loss: 6.758918, mae: 40.638515, mean_q: 54.686176
  465118/1100000: episode: 977, duration: 3.134s, episode steps: 505, steps per second: 161, episode reward: 248.624, mean reward: 0.492 [-20.376, 100.000], mean action: 0.784 [0.000, 3.000], mean observation: 0.128 [-0.867, 1.452], loss: 6.585042, mae: 40.584545, mean_q: 54.651539
  465738/1100000: episode: 978, duration: 3.832s, episode steps: 620, steps per second: 162, episode reward: 199.183, mean reward: 0.321 [-22.203, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.195 [-0.681, 1.408], loss: 6.835459, mae: 41.139465, mean_q: 55.285561
  466014/1100000: episode: 979, duration: 1.603s, episode steps: 276, steps per second: 172, episode reward: 260.567, mean reward: 0.944 [-12.217, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.105 [-0.713, 1.467], loss: 2.954956, mae: 41.072010, mean_q: 55.362915
  466682/1100000: episode: 980, duration: 4.597s, episode steps: 668, steps per second: 145, episode reward: 204.682, mean reward: 0.306 [-19.404, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: -0.056 [-0.697, 1.389], loss: 5.965435, mae: 40.961857, mean_q: 55.059265
  467045/1100000: episode: 981, duration: 2.141s, episode steps: 363, steps per second: 170, episode reward: 212.546, mean reward: 0.586 [-12.039, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.092 [-0.677, 1.405], loss: 4.526410, mae: 41.079071, mean_q: 55.296429
  467384/1100000: episode: 982, duration: 1.992s, episode steps: 339, steps per second: 170, episode reward: 251.482, mean reward: 0.742 [-7.294, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: -0.025 [-0.752, 1.501], loss: 7.907926, mae: 40.987331, mean_q: 55.101891
  467596/1100000: episode: 983, duration: 1.218s, episode steps: 212, steps per second: 174, episode reward: -0.055, mean reward: -0.000 [-100.000, 7.262], mean action: 1.670 [0.000, 3.000], mean observation: 0.090 [-0.616, 1.467], loss: 5.754095, mae: 40.840286, mean_q: 54.928684
  467908/1100000: episode: 984, duration: 1.826s, episode steps: 312, steps per second: 171, episode reward: -72.670, mean reward: -0.233 [-100.000, 14.310], mean action: 1.522 [0.000, 3.000], mean observation: 0.098 [-0.722, 1.511], loss: 5.626333, mae: 40.903923, mean_q: 55.080616
  468413/1100000: episode: 985, duration: 3.204s, episode steps: 505, steps per second: 158, episode reward: 171.769, mean reward: 0.340 [-12.603, 100.000], mean action: 1.707 [0.000, 3.000], mean observation: -0.050 [-0.748, 1.386], loss: 8.022401, mae: 41.200420, mean_q: 55.380756
  468916/1100000: episode: 986, duration: 3.084s, episode steps: 503, steps per second: 163, episode reward: 269.177, mean reward: 0.535 [-18.001, 100.000], mean action: 1.823 [0.000, 3.000], mean observation: 0.057 [-0.802, 1.388], loss: 4.665075, mae: 41.523697, mean_q: 55.883324
  469342/1100000: episode: 987, duration: 2.599s, episode steps: 426, steps per second: 164, episode reward: 232.112, mean reward: 0.545 [-18.135, 100.000], mean action: 1.850 [0.000, 3.000], mean observation: 0.129 [-1.048, 1.452], loss: 3.947750, mae: 41.319626, mean_q: 55.601398
  470342/1100000: episode: 988, duration: 6.133s, episode steps: 1000, steps per second: 163, episode reward: 109.599, mean reward: 0.110 [-19.625, 21.943], mean action: 0.831 [0.000, 3.000], mean observation: 0.037 [-0.600, 1.445], loss: 5.703772, mae: 41.777992, mean_q: 56.140984
  470450/1100000: episode: 989, duration: 0.638s, episode steps: 108, steps per second: 169, episode reward: -2.073, mean reward: -0.019 [-100.000, 24.559], mean action: 2.046 [0.000, 3.000], mean observation: 0.128 [-1.355, 1.394], loss: 3.403014, mae: 41.338108, mean_q: 55.685966
  470793/1100000: episode: 990, duration: 2.067s, episode steps: 343, steps per second: 166, episode reward: -47.487, mean reward: -0.138 [-100.000, 19.177], mean action: 1.242 [0.000, 3.000], mean observation: 0.179 [-0.988, 1.397], loss: 7.852637, mae: 42.019848, mean_q: 56.509129
  471323/1100000: episode: 991, duration: 3.223s, episode steps: 530, steps per second: 164, episode reward: 271.627, mean reward: 0.513 [-20.013, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.152 [-0.749, 1.468], loss: 6.727690, mae: 42.233913, mean_q: 56.915634
  471803/1100000: episode: 992, duration: 2.860s, episode steps: 480, steps per second: 168, episode reward: 203.499, mean reward: 0.424 [-15.783, 100.000], mean action: 1.792 [0.000, 3.000], mean observation: 0.173 [-0.723, 1.520], loss: 6.006178, mae: 42.542961, mean_q: 57.347462
  472160/1100000: episode: 993, duration: 2.141s, episode steps: 357, steps per second: 167, episode reward: -235.751, mean reward: -0.660 [-100.000, 12.269], mean action: 1.406 [0.000, 3.000], mean observation: 0.084 [-0.729, 1.718], loss: 9.483047, mae: 42.341534, mean_q: 56.900627
  473160/1100000: episode: 994, duration: 6.915s, episode steps: 1000, steps per second: 145, episode reward: -13.611, mean reward: -0.014 [-13.789, 11.814], mean action: 1.707 [0.000, 3.000], mean observation: -0.032 [-0.730, 1.398], loss: 5.094238, mae: 42.671719, mean_q: 57.288658
  473501/1100000: episode: 995, duration: 2.004s, episode steps: 341, steps per second: 170, episode reward: 227.236, mean reward: 0.666 [-10.693, 100.000], mean action: 1.613 [0.000, 3.000], mean observation: -0.078 [-0.699, 1.452], loss: 9.551082, mae: 42.680405, mean_q: 57.157909
  474007/1100000: episode: 996, duration: 3.095s, episode steps: 506, steps per second: 163, episode reward: 226.680, mean reward: 0.448 [-17.405, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.189 [-0.752, 1.467], loss: 7.308842, mae: 42.428841, mean_q: 56.893074
  474161/1100000: episode: 997, duration: 0.889s, episode steps: 154, steps per second: 173, episode reward: -133.335, mean reward: -0.866 [-100.000, 9.449], mean action: 1.851 [0.000, 3.000], mean observation: 0.079 [-0.923, 3.723], loss: 5.165050, mae: 42.528969, mean_q: 56.925640
  474567/1100000: episode: 998, duration: 2.428s, episode steps: 406, steps per second: 167, episode reward: 279.655, mean reward: 0.689 [-3.487, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.117 [-0.738, 1.481], loss: 6.349898, mae: 42.393787, mean_q: 56.668644
  474765/1100000: episode: 999, duration: 1.179s, episode steps: 198, steps per second: 168, episode reward: 251.477, mean reward: 1.270 [-17.614, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.064 [-0.686, 1.406], loss: 5.360506, mae: 42.148388, mean_q: 56.330303
  475264/1100000: episode: 1000, duration: 3.032s, episode steps: 499, steps per second: 165, episode reward: 245.310, mean reward: 0.492 [-17.382, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.220 [-0.870, 1.486], loss: 6.110102, mae: 42.300694, mean_q: 56.633377
  475571/1100000: episode: 1001, duration: 1.821s, episode steps: 307, steps per second: 169, episode reward: 249.103, mean reward: 0.811 [-19.527, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: -0.004 [-0.618, 1.393], loss: 6.110343, mae: 42.644459, mean_q: 57.246971
  475975/1100000: episode: 1002, duration: 2.500s, episode steps: 404, steps per second: 162, episode reward: 242.978, mean reward: 0.601 [-9.873, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: -0.066 [-0.729, 1.406], loss: 5.934983, mae: 42.624603, mean_q: 57.211460
  476241/1100000: episode: 1003, duration: 1.543s, episode steps: 266, steps per second: 172, episode reward: 276.109, mean reward: 1.038 [-19.982, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.105 [-0.742, 1.390], loss: 6.809553, mae: 42.766148, mean_q: 57.283813
  476669/1100000: episode: 1004, duration: 2.569s, episode steps: 428, steps per second: 167, episode reward: 204.345, mean reward: 0.477 [-18.960, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.177 [-0.705, 1.456], loss: 6.305135, mae: 42.837238, mean_q: 57.511414
  476944/1100000: episode: 1005, duration: 1.615s, episode steps: 275, steps per second: 170, episode reward: 259.733, mean reward: 0.944 [-17.434, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.101 [-0.603, 1.450], loss: 3.509836, mae: 42.943443, mean_q: 57.604687
  477592/1100000: episode: 1006, duration: 3.999s, episode steps: 648, steps per second: 162, episode reward: 194.169, mean reward: 0.300 [-19.395, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.191 [-0.853, 1.398], loss: 6.019062, mae: 43.139233, mean_q: 57.787979
  477772/1100000: episode: 1007, duration: 1.036s, episode steps: 180, steps per second: 174, episode reward: -7.347, mean reward: -0.041 [-100.000, 14.207], mean action: 1.556 [0.000, 3.000], mean observation: 0.087 [-1.666, 1.392], loss: 4.369518, mae: 43.172634, mean_q: 57.732822
  478059/1100000: episode: 1008, duration: 1.672s, episode steps: 287, steps per second: 172, episode reward: -3.030, mean reward: -0.011 [-100.000, 14.461], mean action: 1.690 [0.000, 3.000], mean observation: 0.080 [-0.688, 1.447], loss: 6.241334, mae: 43.717648, mean_q: 58.532745
  478579/1100000: episode: 1009, duration: 3.355s, episode steps: 520, steps per second: 155, episode reward: 232.986, mean reward: 0.448 [-17.721, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.169 [-0.718, 1.419], loss: 4.569531, mae: 43.462776, mean_q: 58.134094
  478809/1100000: episode: 1010, duration: 1.337s, episode steps: 230, steps per second: 172, episode reward: 268.838, mean reward: 1.169 [-3.026, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.089 [-0.678, 1.391], loss: 4.996574, mae: 43.711887, mean_q: 58.477787
  478932/1100000: episode: 1011, duration: 0.699s, episode steps: 123, steps per second: 176, episode reward: -148.645, mean reward: -1.208 [-100.000, 2.261], mean action: 1.593 [0.000, 3.000], mean observation: 0.048 [-1.056, 1.506], loss: 4.764604, mae: 43.214451, mean_q: 57.565651
  479344/1100000: episode: 1012, duration: 2.452s, episode steps: 412, steps per second: 168, episode reward: 216.797, mean reward: 0.526 [-17.706, 100.000], mean action: 0.927 [0.000, 3.000], mean observation: 0.018 [-0.780, 1.409], loss: 5.553058, mae: 43.711758, mean_q: 58.334682
  479668/1100000: episode: 1013, duration: 1.903s, episode steps: 324, steps per second: 170, episode reward: 176.864, mean reward: 0.546 [-18.617, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.020 [-1.621, 1.551], loss: 6.392473, mae: 43.497765, mean_q: 58.134090
  480120/1100000: episode: 1014, duration: 2.682s, episode steps: 452, steps per second: 169, episode reward: 245.136, mean reward: 0.542 [-17.380, 100.000], mean action: 0.863 [0.000, 3.000], mean observation: 0.173 [-0.560, 1.402], loss: 6.025513, mae: 44.083431, mean_q: 59.022877
  480356/1100000: episode: 1015, duration: 1.371s, episode steps: 236, steps per second: 172, episode reward: 243.297, mean reward: 1.031 [-17.800, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.078 [-0.704, 1.412], loss: 4.414493, mae: 44.544228, mean_q: 59.478909
  481029/1100000: episode: 1016, duration: 4.239s, episode steps: 673, steps per second: 159, episode reward: 249.484, mean reward: 0.371 [-20.556, 100.000], mean action: 0.609 [0.000, 3.000], mean observation: 0.179 [-0.617, 1.417], loss: 6.961428, mae: 44.429768, mean_q: 59.363735
  481318/1100000: episode: 1017, duration: 1.717s, episode steps: 289, steps per second: 168, episode reward: 287.171, mean reward: 0.994 [-19.285, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.066 [-0.702, 1.410], loss: 7.851958, mae: 44.991909, mean_q: 60.110012
  481555/1100000: episode: 1018, duration: 1.370s, episode steps: 237, steps per second: 173, episode reward: 253.009, mean reward: 1.068 [-18.665, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.089 [-0.872, 1.399], loss: 7.081230, mae: 44.440655, mean_q: 59.283310
  481828/1100000: episode: 1019, duration: 1.610s, episode steps: 273, steps per second: 170, episode reward: -267.311, mean reward: -0.979 [-100.000, 5.496], mean action: 1.355 [0.000, 3.000], mean observation: 0.067 [-2.079, 1.581], loss: 4.222530, mae: 44.663589, mean_q: 59.897350
  482128/1100000: episode: 1020, duration: 1.754s, episode steps: 300, steps per second: 171, episode reward: 242.717, mean reward: 0.809 [-13.550, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: -0.014 [-0.766, 1.387], loss: 4.664531, mae: 44.644917, mean_q: 59.705963
  483128/1100000: episode: 1021, duration: 6.182s, episode steps: 1000, steps per second: 162, episode reward: 68.771, mean reward: 0.069 [-20.086, 23.541], mean action: 1.531 [0.000, 3.000], mean observation: 0.081 [-0.600, 1.413], loss: 7.966600, mae: 44.870625, mean_q: 60.007851
  483573/1100000: episode: 1022, duration: 2.659s, episode steps: 445, steps per second: 167, episode reward: 245.713, mean reward: 0.552 [-17.832, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.133 [-0.661, 1.450], loss: 7.182715, mae: 44.757603, mean_q: 59.816288
  483900/1100000: episode: 1023, duration: 1.887s, episode steps: 327, steps per second: 173, episode reward: -147.728, mean reward: -0.452 [-100.000, 22.395], mean action: 1.425 [0.000, 3.000], mean observation: 0.169 [-0.925, 1.500], loss: 6.938252, mae: 44.669628, mean_q: 59.815899
  484900/1100000: episode: 1024, duration: 6.995s, episode steps: 1000, steps per second: 143, episode reward: -2.211, mean reward: -0.002 [-13.737, 11.675], mean action: 1.850 [0.000, 3.000], mean observation: 0.132 [-0.868, 1.392], loss: 5.282774, mae: 44.617172, mean_q: 59.726017
  485309/1100000: episode: 1025, duration: 2.489s, episode steps: 409, steps per second: 164, episode reward: 236.318, mean reward: 0.578 [-19.074, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.126 [-0.704, 1.416], loss: 5.328240, mae: 44.413536, mean_q: 59.579739
  485588/1100000: episode: 1026, duration: 1.626s, episode steps: 279, steps per second: 172, episode reward: 240.054, mean reward: 0.860 [-17.054, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: -0.016 [-0.809, 1.391], loss: 5.358399, mae: 44.478649, mean_q: 59.698231
  486236/1100000: episode: 1027, duration: 4.129s, episode steps: 648, steps per second: 157, episode reward: 195.153, mean reward: 0.301 [-18.905, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: -0.007 [-0.600, 1.512], loss: 6.516032, mae: 44.347122, mean_q: 59.502106
  486395/1100000: episode: 1028, duration: 0.901s, episode steps: 159, steps per second: 176, episode reward: -205.554, mean reward: -1.293 [-100.000, 13.745], mean action: 1.415 [0.000, 3.000], mean observation: 0.175 [-1.075, 1.866], loss: 7.923860, mae: 44.728825, mean_q: 60.035336
  487395/1100000: episode: 1029, duration: 6.327s, episode steps: 1000, steps per second: 158, episode reward: 102.214, mean reward: 0.102 [-22.868, 19.424], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [-0.604, 1.410], loss: 6.945042, mae: 44.909142, mean_q: 60.193085
  487732/1100000: episode: 1030, duration: 1.974s, episode steps: 337, steps per second: 171, episode reward: 217.832, mean reward: 0.646 [-15.598, 100.000], mean action: 1.680 [0.000, 3.000], mean observation: -0.002 [-0.837, 1.394], loss: 8.140873, mae: 44.552155, mean_q: 59.802277
  488082/1100000: episode: 1031, duration: 2.097s, episode steps: 350, steps per second: 167, episode reward: 244.852, mean reward: 0.700 [-17.514, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.109 [-0.818, 1.430], loss: 8.309978, mae: 44.747978, mean_q: 60.009407
  488172/1100000: episode: 1032, duration: 0.511s, episode steps: 90, steps per second: 176, episode reward: -8.091, mean reward: -0.090 [-100.000, 17.501], mean action: 1.389 [0.000, 3.000], mean observation: 0.048 [-1.023, 1.410], loss: 4.310935, mae: 44.440735, mean_q: 59.668358
  488705/1100000: episode: 1033, duration: 3.456s, episode steps: 533, steps per second: 154, episode reward: 180.782, mean reward: 0.339 [-18.375, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.181 [-1.074, 1.400], loss: 5.050985, mae: 44.601601, mean_q: 59.801708
  488777/1100000: episode: 1034, duration: 0.412s, episode steps: 72, steps per second: 175, episode reward: -134.916, mean reward: -1.874 [-100.000, 6.938], mean action: 2.083 [0.000, 3.000], mean observation: -0.113 [-3.447, 1.390], loss: 7.173241, mae: 44.700459, mean_q: 60.023331
  489085/1100000: episode: 1035, duration: 1.799s, episode steps: 308, steps per second: 171, episode reward: 279.382, mean reward: 0.907 [-18.786, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.112 [-0.688, 1.401], loss: 8.228139, mae: 44.131813, mean_q: 59.261353
  489432/1100000: episode: 1036, duration: 2.039s, episode steps: 347, steps per second: 170, episode reward: 227.711, mean reward: 0.656 [-19.010, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: -0.013 [-0.600, 1.409], loss: 16.266907, mae: 44.682762, mean_q: 59.754890
  490424/1100000: episode: 1037, duration: 6.154s, episode steps: 992, steps per second: 161, episode reward: 193.591, mean reward: 0.195 [-19.739, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.027 [-0.600, 1.528], loss: 5.475737, mae: 44.831264, mean_q: 60.140018
  490714/1100000: episode: 1038, duration: 1.695s, episode steps: 290, steps per second: 171, episode reward: 237.733, mean reward: 0.820 [-8.838, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.060 [-0.691, 1.405], loss: 6.245505, mae: 44.758930, mean_q: 59.934059
  491714/1100000: episode: 1039, duration: 6.613s, episode steps: 1000, steps per second: 151, episode reward: 17.594, mean reward: 0.018 [-18.980, 19.032], mean action: 1.892 [0.000, 3.000], mean observation: 0.131 [-0.970, 1.400], loss: 6.258918, mae: 44.638126, mean_q: 59.736919
  492193/1100000: episode: 1040, duration: 2.812s, episode steps: 479, steps per second: 170, episode reward: 249.316, mean reward: 0.520 [-18.897, 100.000], mean action: 0.877 [0.000, 3.000], mean observation: 0.215 [-0.670, 1.431], loss: 7.036402, mae: 44.176434, mean_q: 59.212894
  492810/1100000: episode: 1041, duration: 3.806s, episode steps: 617, steps per second: 162, episode reward: 190.346, mean reward: 0.309 [-19.017, 100.000], mean action: 1.890 [0.000, 3.000], mean observation: 0.211 [-0.746, 1.386], loss: 6.465478, mae: 44.275417, mean_q: 59.275398
  493124/1100000: episode: 1042, duration: 1.862s, episode steps: 314, steps per second: 169, episode reward: 262.706, mean reward: 0.837 [-18.970, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.099 [-0.903, 1.468], loss: 5.847548, mae: 44.237228, mean_q: 59.211433
  493408/1100000: episode: 1043, duration: 1.645s, episode steps: 284, steps per second: 173, episode reward: -3.601, mean reward: -0.013 [-100.000, 13.023], mean action: 1.704 [0.000, 3.000], mean observation: -0.020 [-0.909, 1.499], loss: 6.214827, mae: 44.338379, mean_q: 59.552967
  493806/1100000: episode: 1044, duration: 2.378s, episode steps: 398, steps per second: 167, episode reward: -243.155, mean reward: -0.611 [-100.000, 13.692], mean action: 1.641 [0.000, 3.000], mean observation: 0.177 [-0.707, 2.086], loss: 6.774015, mae: 44.161922, mean_q: 59.120869
  494053/1100000: episode: 1045, duration: 1.429s, episode steps: 247, steps per second: 173, episode reward: 228.574, mean reward: 0.925 [-12.457, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.174 [-0.659, 1.445], loss: 5.620019, mae: 43.882950, mean_q: 58.571964
  494416/1100000: episode: 1046, duration: 2.192s, episode steps: 363, steps per second: 166, episode reward: 219.390, mean reward: 0.604 [-14.997, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: -0.019 [-0.769, 1.491], loss: 6.040223, mae: 44.058079, mean_q: 58.799129
  494845/1100000: episode: 1047, duration: 2.673s, episode steps: 429, steps per second: 161, episode reward: 272.596, mean reward: 0.635 [-23.989, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.125 [-0.608, 1.387], loss: 6.037527, mae: 44.112755, mean_q: 59.079365
  495333/1100000: episode: 1048, duration: 2.890s, episode steps: 488, steps per second: 169, episode reward: 274.958, mean reward: 0.563 [-12.289, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.045 [-0.797, 1.538], loss: 6.255279, mae: 43.884663, mean_q: 58.709415
  495738/1100000: episode: 1049, duration: 2.450s, episode steps: 405, steps per second: 165, episode reward: 239.539, mean reward: 0.591 [-10.180, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: -0.043 [-0.652, 1.411], loss: 5.874439, mae: 44.173824, mean_q: 59.000038
  496133/1100000: episode: 1050, duration: 2.372s, episode steps: 395, steps per second: 167, episode reward: 226.406, mean reward: 0.573 [-10.329, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: 0.164 [-0.729, 1.394], loss: 6.491539, mae: 43.889568, mean_q: 58.728931
  496488/1100000: episode: 1051, duration: 2.163s, episode steps: 355, steps per second: 164, episode reward: 255.734, mean reward: 0.720 [-10.198, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: -0.037 [-0.867, 1.398], loss: 7.061620, mae: 44.495369, mean_q: 59.383709
  496946/1100000: episode: 1052, duration: 2.712s, episode steps: 458, steps per second: 169, episode reward: 256.770, mean reward: 0.561 [-17.407, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.072 [-0.734, 1.430], loss: 6.675711, mae: 43.629658, mean_q: 58.254383
  497339/1100000: episode: 1053, duration: 2.418s, episode steps: 393, steps per second: 163, episode reward: 238.329, mean reward: 0.606 [-12.906, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.033 [-0.670, 1.440], loss: 7.419328, mae: 43.776428, mean_q: 58.592682
  498008/1100000: episode: 1054, duration: 4.339s, episode steps: 669, steps per second: 154, episode reward: 201.142, mean reward: 0.301 [-21.106, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: -0.012 [-0.701, 1.496], loss: 6.092651, mae: 43.836006, mean_q: 58.686916
  498642/1100000: episode: 1055, duration: 4.048s, episode steps: 634, steps per second: 157, episode reward: 241.630, mean reward: 0.381 [-16.968, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.187 [-0.757, 1.396], loss: 5.939165, mae: 43.806999, mean_q: 58.631428
  498941/1100000: episode: 1056, duration: 1.757s, episode steps: 299, steps per second: 170, episode reward: 269.326, mean reward: 0.901 [-17.941, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.099 [-0.736, 1.392], loss: 4.613753, mae: 44.039703, mean_q: 58.994736
  499350/1100000: episode: 1057, duration: 2.444s, episode steps: 409, steps per second: 167, episode reward: 241.974, mean reward: 0.592 [-17.448, 100.000], mean action: 0.895 [0.000, 3.000], mean observation: 0.224 [-0.646, 1.411], loss: 4.397836, mae: 43.817261, mean_q: 58.757168
  499700/1100000: episode: 1058, duration: 2.089s, episode steps: 350, steps per second: 168, episode reward: 181.233, mean reward: 0.518 [-19.995, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.195 [-0.769, 1.413], loss: 6.892331, mae: 43.702816, mean_q: 58.404251
  500193/1100000: episode: 1059, duration: 2.943s, episode steps: 493, steps per second: 168, episode reward: 246.049, mean reward: 0.499 [-9.531, 100.000], mean action: 1.941 [0.000, 3.000], mean observation: -0.025 [-0.695, 1.513], loss: 5.151817, mae: 43.891426, mean_q: 58.963356
  500792/1100000: episode: 1060, duration: 3.794s, episode steps: 599, steps per second: 158, episode reward: 204.009, mean reward: 0.341 [-19.315, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.261 [-0.642, 1.440], loss: 4.691711, mae: 44.247974, mean_q: 59.273777
  501527/1100000: episode: 1061, duration: 4.481s, episode steps: 735, steps per second: 164, episode reward: 236.461, mean reward: 0.322 [-18.519, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.030 [-0.600, 1.440], loss: 6.011286, mae: 44.015980, mean_q: 58.865231
  502050/1100000: episode: 1062, duration: 3.183s, episode steps: 523, steps per second: 164, episode reward: 196.881, mean reward: 0.376 [-19.046, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.117 [-0.824, 1.530], loss: 6.904999, mae: 44.068794, mean_q: 59.062828
  502722/1100000: episode: 1063, duration: 4.067s, episode steps: 672, steps per second: 165, episode reward: 199.266, mean reward: 0.297 [-15.550, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.133 [-0.700, 1.613], loss: 5.779304, mae: 44.096363, mean_q: 59.046432
  502895/1100000: episode: 1064, duration: 0.985s, episode steps: 173, steps per second: 176, episode reward: 26.395, mean reward: 0.153 [-100.000, 12.539], mean action: 1.324 [0.000, 3.000], mean observation: 0.120 [-0.803, 1.502], loss: 3.978559, mae: 44.053951, mean_q: 59.066170
  503219/1100000: episode: 1065, duration: 1.885s, episode steps: 324, steps per second: 172, episode reward: 243.380, mean reward: 0.751 [-21.359, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.109 [-0.752, 1.414], loss: 5.446999, mae: 44.018059, mean_q: 58.954239
  503487/1100000: episode: 1066, duration: 1.555s, episode steps: 268, steps per second: 172, episode reward: 183.863, mean reward: 0.686 [-17.400, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.175 [-0.818, 1.410], loss: 4.921547, mae: 44.193295, mean_q: 59.285740
  503940/1100000: episode: 1067, duration: 2.723s, episode steps: 453, steps per second: 166, episode reward: 227.845, mean reward: 0.503 [-10.207, 100.000], mean action: 1.464 [0.000, 3.000], mean observation: 0.132 [-0.739, 1.402], loss: 5.555006, mae: 44.178059, mean_q: 59.139885
  504323/1100000: episode: 1068, duration: 2.363s, episode steps: 383, steps per second: 162, episode reward: 270.470, mean reward: 0.706 [-17.799, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.142 [-0.712, 1.387], loss: 8.445389, mae: 44.287155, mean_q: 59.215977
  504881/1100000: episode: 1069, duration: 3.504s, episode steps: 558, steps per second: 159, episode reward: 195.419, mean reward: 0.350 [-20.405, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.008 [-0.733, 1.427], loss: 5.172661, mae: 44.548973, mean_q: 59.662018
  505263/1100000: episode: 1070, duration: 2.268s, episode steps: 382, steps per second: 168, episode reward: 262.500, mean reward: 0.687 [-17.839, 100.000], mean action: 0.856 [0.000, 3.000], mean observation: 0.127 [-0.758, 1.496], loss: 5.935233, mae: 44.919811, mean_q: 60.233425
  505648/1100000: episode: 1071, duration: 2.296s, episode steps: 385, steps per second: 168, episode reward: 237.485, mean reward: 0.617 [-19.851, 100.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.167 [-0.769, 1.396], loss: 7.048955, mae: 44.749565, mean_q: 59.796104
  506648/1100000: episode: 1072, duration: 6.429s, episode steps: 1000, steps per second: 156, episode reward: 99.012, mean reward: 0.099 [-19.910, 21.435], mean action: 2.158 [0.000, 3.000], mean observation: 0.242 [-0.733, 1.452], loss: 6.117327, mae: 44.313187, mean_q: 59.383625
  507180/1100000: episode: 1073, duration: 3.298s, episode steps: 532, steps per second: 161, episode reward: 234.711, mean reward: 0.441 [-13.918, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: -0.003 [-0.600, 1.495], loss: 5.220083, mae: 44.429424, mean_q: 59.448868
  507768/1100000: episode: 1074, duration: 3.508s, episode steps: 588, steps per second: 168, episode reward: 203.598, mean reward: 0.346 [-18.793, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.006 [-0.648, 1.408], loss: 4.554192, mae: 43.864422, mean_q: 58.833149
  508153/1100000: episode: 1075, duration: 2.274s, episode steps: 385, steps per second: 169, episode reward: 247.134, mean reward: 0.642 [-16.012, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.009 [-0.622, 1.393], loss: 7.585278, mae: 44.136070, mean_q: 58.947548
  508509/1100000: episode: 1076, duration: 2.086s, episode steps: 356, steps per second: 171, episode reward: 7.199, mean reward: 0.020 [-100.000, 12.192], mean action: 1.483 [0.000, 3.000], mean observation: -0.009 [-0.600, 1.468], loss: 4.461046, mae: 43.924503, mean_q: 58.787746
  508870/1100000: episode: 1077, duration: 2.145s, episode steps: 361, steps per second: 168, episode reward: 254.180, mean reward: 0.704 [-10.800, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.146 [-0.551, 1.491], loss: 5.629792, mae: 44.047123, mean_q: 58.931820
  509231/1100000: episode: 1078, duration: 2.098s, episode steps: 361, steps per second: 172, episode reward: -286.895, mean reward: -0.795 [-100.000, 25.388], mean action: 1.260 [0.000, 3.000], mean observation: -0.029 [-1.593, 1.412], loss: 8.204079, mae: 44.537430, mean_q: 59.533978
  509757/1100000: episode: 1079, duration: 3.142s, episode steps: 526, steps per second: 167, episode reward: 261.451, mean reward: 0.497 [-17.817, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.121 [-0.650, 1.411], loss: 5.393923, mae: 44.151119, mean_q: 59.162979
  510098/1100000: episode: 1080, duration: 2.052s, episode steps: 341, steps per second: 166, episode reward: 2.915, mean reward: 0.009 [-100.000, 5.221], mean action: 1.789 [0.000, 3.000], mean observation: -0.037 [-0.639, 1.404], loss: 7.868248, mae: 44.496067, mean_q: 59.666973
  510266/1100000: episode: 1081, duration: 0.961s, episode steps: 168, steps per second: 175, episode reward: 8.535, mean reward: 0.051 [-100.000, 12.477], mean action: 1.280 [0.000, 3.000], mean observation: 0.056 [-0.864, 1.416], loss: 6.786134, mae: 44.281586, mean_q: 59.531326
  510563/1100000: episode: 1082, duration: 1.759s, episode steps: 297, steps per second: 169, episode reward: 270.196, mean reward: 0.910 [-18.249, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: -0.030 [-0.667, 1.386], loss: 5.878408, mae: 44.316570, mean_q: 59.285656
  511028/1100000: episode: 1083, duration: 2.907s, episode steps: 465, steps per second: 160, episode reward: 206.777, mean reward: 0.445 [-10.847, 100.000], mean action: 1.510 [0.000, 3.000], mean observation: 0.090 [-0.672, 1.404], loss: 5.627297, mae: 44.870087, mean_q: 60.093403
  511481/1100000: episode: 1084, duration: 2.699s, episode steps: 453, steps per second: 168, episode reward: 244.227, mean reward: 0.539 [-14.370, 100.000], mean action: 2.011 [0.000, 3.000], mean observation: -0.009 [-0.647, 1.389], loss: 8.176263, mae: 44.490238, mean_q: 59.562458
  511951/1100000: episode: 1085, duration: 2.812s, episode steps: 470, steps per second: 167, episode reward: 241.727, mean reward: 0.514 [-7.433, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.173 [-0.666, 1.448], loss: 7.859599, mae: 44.657120, mean_q: 59.705719
  512430/1100000: episode: 1086, duration: 2.937s, episode steps: 479, steps per second: 163, episode reward: 214.029, mean reward: 0.447 [-19.932, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.012 [-0.649, 1.413], loss: 6.767826, mae: 44.592812, mean_q: 59.528435
  512820/1100000: episode: 1087, duration: 2.355s, episode steps: 390, steps per second: 166, episode reward: 246.899, mean reward: 0.633 [-17.509, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.206 [-0.784, 1.488], loss: 6.266365, mae: 44.763866, mean_q: 59.799480
  513567/1100000: episode: 1088, duration: 4.772s, episode steps: 747, steps per second: 157, episode reward: 190.063, mean reward: 0.254 [-18.756, 100.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.141 [-0.773, 1.432], loss: 8.374566, mae: 44.666386, mean_q: 59.720909
  513812/1100000: episode: 1089, duration: 1.428s, episode steps: 245, steps per second: 172, episode reward: 32.487, mean reward: 0.133 [-100.000, 5.283], mean action: 1.637 [0.000, 3.000], mean observation: 0.103 [-0.625, 1.401], loss: 7.302294, mae: 44.659557, mean_q: 59.580662
  514255/1100000: episode: 1090, duration: 2.834s, episode steps: 443, steps per second: 156, episode reward: 230.905, mean reward: 0.521 [-10.166, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.149 [-0.584, 1.400], loss: 8.661360, mae: 44.880867, mean_q: 59.974701
  514802/1100000: episode: 1091, duration: 3.357s, episode steps: 547, steps per second: 163, episode reward: 271.456, mean reward: 0.496 [-18.327, 100.000], mean action: 0.740 [0.000, 3.000], mean observation: 0.155 [-0.787, 1.387], loss: 8.604476, mae: 44.881062, mean_q: 59.919502
  515260/1100000: episode: 1092, duration: 2.734s, episode steps: 458, steps per second: 168, episode reward: 245.563, mean reward: 0.536 [-16.735, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.178 [-0.978, 1.507], loss: 6.551230, mae: 44.629417, mean_q: 59.761055
  515649/1100000: episode: 1093, duration: 2.330s, episode steps: 389, steps per second: 167, episode reward: 219.802, mean reward: 0.565 [-17.535, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.011 [-0.820, 1.409], loss: 6.197983, mae: 44.499813, mean_q: 59.614025
  515947/1100000: episode: 1094, duration: 1.721s, episode steps: 298, steps per second: 173, episode reward: 232.648, mean reward: 0.781 [-12.412, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: -0.037 [-0.605, 1.499], loss: 7.146597, mae: 44.434067, mean_q: 59.389065
  516096/1100000: episode: 1095, duration: 0.853s, episode steps: 149, steps per second: 175, episode reward: -12.459, mean reward: -0.084 [-100.000, 8.010], mean action: 1.577 [0.000, 3.000], mean observation: -0.032 [-0.702, 1.485], loss: 9.280018, mae: 44.294338, mean_q: 59.094955
  516368/1100000: episode: 1096, duration: 1.609s, episode steps: 272, steps per second: 169, episode reward: 261.762, mean reward: 0.962 [-10.440, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: -0.053 [-0.731, 1.385], loss: 8.127055, mae: 44.233555, mean_q: 59.224518
  516631/1100000: episode: 1097, duration: 1.563s, episode steps: 263, steps per second: 168, episode reward: 263.262, mean reward: 1.001 [-10.258, 100.000], mean action: 1.741 [0.000, 3.000], mean observation: 0.028 [-0.611, 1.413], loss: 6.889657, mae: 44.841373, mean_q: 59.638302
  517016/1100000: episode: 1098, duration: 2.249s, episode steps: 385, steps per second: 171, episode reward: 241.248, mean reward: 0.627 [-17.465, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.171 [-0.623, 1.405], loss: 6.478966, mae: 44.606808, mean_q: 59.355251
  517734/1100000: episode: 1099, duration: 4.766s, episode steps: 718, steps per second: 151, episode reward: 209.084, mean reward: 0.291 [-18.312, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.174 [-0.662, 1.397], loss: 7.345356, mae: 44.304909, mean_q: 59.256062
  518139/1100000: episode: 1100, duration: 2.504s, episode steps: 405, steps per second: 162, episode reward: 205.810, mean reward: 0.508 [-22.150, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.118 [-0.647, 1.428], loss: 7.292930, mae: 44.487896, mean_q: 59.435669
  518386/1100000: episode: 1101, duration: 1.452s, episode steps: 247, steps per second: 170, episode reward: 273.766, mean reward: 1.108 [-10.883, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.057 [-0.552, 1.394], loss: 6.230305, mae: 44.206299, mean_q: 59.031227
  518835/1100000: episode: 1102, duration: 2.709s, episode steps: 449, steps per second: 166, episode reward: 259.561, mean reward: 0.578 [-18.316, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.066 [-0.785, 1.386], loss: 5.176860, mae: 45.023258, mean_q: 60.230419
  519754/1100000: episode: 1103, duration: 5.783s, episode steps: 919, steps per second: 159, episode reward: 252.032, mean reward: 0.274 [-19.712, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.214 [-0.839, 1.394], loss: 7.870962, mae: 45.018185, mean_q: 60.077316
  520089/1100000: episode: 1104, duration: 1.981s, episode steps: 335, steps per second: 169, episode reward: 275.284, mean reward: 0.822 [-17.457, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.111 [-0.914, 1.513], loss: 6.474085, mae: 44.908405, mean_q: 60.029243
  520600/1100000: episode: 1105, duration: 3.086s, episode steps: 511, steps per second: 166, episode reward: 241.278, mean reward: 0.472 [-19.912, 100.000], mean action: 0.763 [0.000, 3.000], mean observation: 0.194 [-0.630, 1.510], loss: 6.928568, mae: 45.102661, mean_q: 60.063732
  521496/1100000: episode: 1106, duration: 5.620s, episode steps: 896, steps per second: 159, episode reward: 197.971, mean reward: 0.221 [-20.999, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.065 [-0.737, 1.385], loss: 6.185389, mae: 45.096931, mean_q: 60.093174
  521949/1100000: episode: 1107, duration: 2.775s, episode steps: 453, steps per second: 163, episode reward: 212.419, mean reward: 0.469 [-17.650, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: -0.054 [-0.721, 1.503], loss: 5.900299, mae: 44.476021, mean_q: 59.470085
  522325/1100000: episode: 1108, duration: 2.173s, episode steps: 376, steps per second: 173, episode reward: 227.197, mean reward: 0.604 [-19.708, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.189 [-0.753, 1.449], loss: 6.209117, mae: 45.114628, mean_q: 60.215172
  522604/1100000: episode: 1109, duration: 1.621s, episode steps: 279, steps per second: 172, episode reward: 209.636, mean reward: 0.751 [-10.861, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: -0.071 [-0.686, 1.396], loss: 7.074202, mae: 44.776814, mean_q: 59.746975
  523373/1100000: episode: 1110, duration: 5.061s, episode steps: 769, steps per second: 152, episode reward: 230.539, mean reward: 0.300 [-18.830, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.039 [-0.651, 1.388], loss: 6.153232, mae: 44.879887, mean_q: 60.073586
  524008/1100000: episode: 1111, duration: 4.112s, episode steps: 635, steps per second: 154, episode reward: 197.191, mean reward: 0.311 [-20.154, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.179 [-0.967, 1.423], loss: 6.719322, mae: 45.136841, mean_q: 60.512451
  524396/1100000: episode: 1112, duration: 2.299s, episode steps: 388, steps per second: 169, episode reward: 263.202, mean reward: 0.678 [-18.489, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.132 [-0.621, 1.421], loss: 7.357876, mae: 44.880749, mean_q: 60.000916
  524827/1100000: episode: 1113, duration: 2.618s, episode steps: 431, steps per second: 165, episode reward: 225.272, mean reward: 0.523 [-9.527, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: -0.017 [-0.600, 1.480], loss: 5.732988, mae: 45.480267, mean_q: 60.846226
  525183/1100000: episode: 1114, duration: 2.118s, episode steps: 356, steps per second: 168, episode reward: 274.600, mean reward: 0.771 [-9.595, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.085 [-0.715, 1.437], loss: 4.788434, mae: 45.352768, mean_q: 60.733612
  525727/1100000: episode: 1115, duration: 3.343s, episode steps: 544, steps per second: 163, episode reward: 262.140, mean reward: 0.482 [-18.154, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.013 [-0.671, 1.391], loss: 7.445557, mae: 45.152210, mean_q: 60.502117
  526060/1100000: episode: 1116, duration: 1.989s, episode steps: 333, steps per second: 167, episode reward: 221.205, mean reward: 0.664 [-20.380, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: -0.036 [-0.773, 1.402], loss: 5.766873, mae: 45.201900, mean_q: 60.566612
  526300/1100000: episode: 1117, duration: 1.392s, episode steps: 240, steps per second: 172, episode reward: 274.631, mean reward: 1.144 [-19.271, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.101 [-0.728, 1.400], loss: 8.060809, mae: 44.919697, mean_q: 60.192757
  526930/1100000: episode: 1118, duration: 3.846s, episode steps: 630, steps per second: 164, episode reward: 241.074, mean reward: 0.383 [-17.902, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.035 [-0.699, 1.404], loss: 6.490180, mae: 45.142899, mean_q: 60.425404
  527371/1100000: episode: 1119, duration: 2.733s, episode steps: 441, steps per second: 161, episode reward: 200.550, mean reward: 0.455 [-17.746, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: -0.016 [-0.728, 1.404], loss: 5.266740, mae: 45.389473, mean_q: 60.980625
  527614/1100000: episode: 1120, duration: 1.420s, episode steps: 243, steps per second: 171, episode reward: 283.476, mean reward: 1.167 [-7.764, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.195 [-0.766, 1.391], loss: 3.545583, mae: 45.080425, mean_q: 60.662800
  528028/1100000: episode: 1121, duration: 2.540s, episode steps: 414, steps per second: 163, episode reward: 259.653, mean reward: 0.627 [-10.713, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: -0.010 [-0.801, 1.385], loss: 6.066908, mae: 45.414139, mean_q: 61.004971
  528332/1100000: episode: 1122, duration: 1.820s, episode steps: 304, steps per second: 167, episode reward: 219.858, mean reward: 0.723 [-10.990, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: -0.060 [-0.799, 1.392], loss: 7.100978, mae: 45.131237, mean_q: 60.482307
  529089/1100000: episode: 1123, duration: 4.554s, episode steps: 757, steps per second: 166, episode reward: 206.464, mean reward: 0.273 [-17.871, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.048 [-0.741, 1.407], loss: 6.047963, mae: 45.864288, mean_q: 61.539501
  529405/1100000: episode: 1124, duration: 1.859s, episode steps: 316, steps per second: 170, episode reward: 250.244, mean reward: 0.792 [-12.997, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.150 [-0.722, 1.434], loss: 5.043524, mae: 45.818123, mean_q: 61.395210
  529736/1100000: episode: 1125, duration: 1.973s, episode steps: 331, steps per second: 168, episode reward: 283.756, mean reward: 0.857 [-19.169, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.125 [-0.799, 1.519], loss: 7.371396, mae: 45.998611, mean_q: 61.731625
  530005/1100000: episode: 1126, duration: 1.597s, episode steps: 269, steps per second: 168, episode reward: 237.246, mean reward: 0.882 [-10.403, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: -0.054 [-0.913, 1.388], loss: 6.017297, mae: 45.714611, mean_q: 61.414143
  530397/1100000: episode: 1127, duration: 2.362s, episode steps: 392, steps per second: 166, episode reward: 233.631, mean reward: 0.596 [-10.262, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: -0.035 [-0.819, 1.393], loss: 5.467690, mae: 45.508030, mean_q: 61.015228
  531108/1100000: episode: 1128, duration: 4.403s, episode steps: 711, steps per second: 161, episode reward: 192.453, mean reward: 0.271 [-23.887, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.029 [-0.792, 1.422], loss: 5.239243, mae: 45.773853, mean_q: 61.438076
  531391/1100000: episode: 1129, duration: 1.641s, episode steps: 283, steps per second: 172, episode reward: 242.820, mean reward: 0.858 [-16.123, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.213 [-0.796, 1.402], loss: 3.542844, mae: 45.133133, mean_q: 60.597420
  531651/1100000: episode: 1130, duration: 1.519s, episode steps: 260, steps per second: 171, episode reward: 252.230, mean reward: 0.970 [-9.603, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.207 [-0.828, 1.429], loss: 4.469150, mae: 45.397495, mean_q: 60.819443
  532366/1100000: episode: 1131, duration: 4.176s, episode steps: 715, steps per second: 171, episode reward: 226.171, mean reward: 0.316 [-17.679, 100.000], mean action: 0.517 [0.000, 3.000], mean observation: 0.264 [-0.906, 1.400], loss: 6.452164, mae: 45.096439, mean_q: 60.663090
  533058/1100000: episode: 1132, duration: 4.212s, episode steps: 692, steps per second: 164, episode reward: 248.855, mean reward: 0.360 [-19.915, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.244 [-0.927, 1.523], loss: 6.508742, mae: 44.872082, mean_q: 60.121319
  533314/1100000: episode: 1133, duration: 1.471s, episode steps: 256, steps per second: 174, episode reward: 252.944, mean reward: 0.988 [-19.109, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: -0.057 [-0.753, 1.439], loss: 2.927667, mae: 45.009315, mean_q: 60.599289
  533403/1100000: episode: 1134, duration: 0.512s, episode steps: 89, steps per second: 174, episode reward: -17.263, mean reward: -0.194 [-100.000, 18.863], mean action: 1.730 [0.000, 3.000], mean observation: -0.009 [-1.518, 1.389], loss: 12.246530, mae: 44.789787, mean_q: 60.168442
  533596/1100000: episode: 1135, duration: 1.126s, episode steps: 193, steps per second: 171, episode reward: -148.703, mean reward: -0.770 [-100.000, 5.644], mean action: 1.674 [0.000, 3.000], mean observation: 0.113 [-2.344, 1.481], loss: 5.334581, mae: 44.974102, mean_q: 60.150734
  534177/1100000: episode: 1136, duration: 3.802s, episode steps: 581, steps per second: 153, episode reward: 246.572, mean reward: 0.424 [-10.810, 100.000], mean action: 1.711 [0.000, 3.000], mean observation: 0.179 [-0.835, 1.505], loss: 4.553877, mae: 45.342129, mean_q: 60.747681
  534563/1100000: episode: 1137, duration: 2.246s, episode steps: 386, steps per second: 172, episode reward: 302.211, mean reward: 0.783 [-17.424, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.091 [-0.837, 1.551], loss: 5.897854, mae: 45.108860, mean_q: 60.391323
  534720/1100000: episode: 1138, duration: 0.896s, episode steps: 157, steps per second: 175, episode reward: -176.657, mean reward: -1.125 [-100.000, 5.376], mean action: 1.631 [0.000, 3.000], mean observation: -0.139 [-1.552, 1.399], loss: 6.429714, mae: 45.002987, mean_q: 60.161369
  535165/1100000: episode: 1139, duration: 2.761s, episode steps: 445, steps per second: 161, episode reward: 300.944, mean reward: 0.676 [-8.437, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.099 [-0.783, 1.466], loss: 5.640169, mae: 45.335323, mean_q: 60.781162
  535688/1100000: episode: 1140, duration: 3.056s, episode steps: 523, steps per second: 171, episode reward: 223.625, mean reward: 0.428 [-18.178, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.022 [-0.758, 1.444], loss: 6.448545, mae: 45.166515, mean_q: 60.461014
  536162/1100000: episode: 1141, duration: 2.828s, episode steps: 474, steps per second: 168, episode reward: 185.032, mean reward: 0.390 [-18.788, 100.000], mean action: 1.844 [0.000, 3.000], mean observation: 0.201 [-0.698, 1.413], loss: 6.830367, mae: 44.988636, mean_q: 60.391399
  536345/1100000: episode: 1142, duration: 1.104s, episode steps: 183, steps per second: 166, episode reward: 279.893, mean reward: 1.529 [-8.146, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.093 [-0.959, 1.393], loss: 4.017839, mae: 44.918087, mean_q: 60.299042
  536596/1100000: episode: 1143, duration: 1.461s, episode steps: 251, steps per second: 172, episode reward: 7.237, mean reward: 0.029 [-100.000, 17.519], mean action: 1.542 [0.000, 3.000], mean observation: 0.157 [-0.814, 1.531], loss: 5.279615, mae: 45.071457, mean_q: 60.403416
  536872/1100000: episode: 1144, duration: 1.615s, episode steps: 276, steps per second: 171, episode reward: 229.350, mean reward: 0.831 [-10.710, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.129 [-0.790, 1.406], loss: 7.182940, mae: 44.590931, mean_q: 59.702007
  537060/1100000: episode: 1145, duration: 1.084s, episode steps: 188, steps per second: 173, episode reward: 265.370, mean reward: 1.412 [-10.968, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: -0.078 [-0.931, 1.400], loss: 7.321807, mae: 44.682045, mean_q: 60.094822
  537263/1100000: episode: 1146, duration: 1.177s, episode steps: 203, steps per second: 173, episode reward: -198.839, mean reward: -0.980 [-100.000, 10.707], mean action: 2.074 [0.000, 3.000], mean observation: -0.090 [-1.397, 1.392], loss: 9.076995, mae: 44.656116, mean_q: 60.143211
  537571/1100000: episode: 1147, duration: 1.800s, episode steps: 308, steps per second: 171, episode reward: 265.466, mean reward: 0.862 [-17.681, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.206 [-0.715, 1.410], loss: 5.579076, mae: 44.756920, mean_q: 60.134373
  537970/1100000: episode: 1148, duration: 2.374s, episode steps: 399, steps per second: 168, episode reward: 241.409, mean reward: 0.605 [-19.898, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: -0.044 [-0.610, 1.442], loss: 9.009331, mae: 44.827637, mean_q: 60.090626
  538369/1100000: episode: 1149, duration: 2.333s, episode steps: 399, steps per second: 171, episode reward: 221.790, mean reward: 0.556 [-18.125, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.020 [-0.696, 1.479], loss: 5.768715, mae: 44.722500, mean_q: 60.136292
  538840/1100000: episode: 1150, duration: 2.934s, episode steps: 471, steps per second: 161, episode reward: 239.205, mean reward: 0.508 [-18.590, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.005 [-0.777, 1.399], loss: 5.604090, mae: 44.755989, mean_q: 59.963890
  539371/1100000: episode: 1151, duration: 3.195s, episode steps: 531, steps per second: 166, episode reward: 257.400, mean reward: 0.485 [-20.350, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.022 [-0.814, 1.405], loss: 6.741213, mae: 44.904800, mean_q: 60.231026
  539644/1100000: episode: 1152, duration: 1.632s, episode steps: 273, steps per second: 167, episode reward: 260.228, mean reward: 0.953 [-7.150, 100.000], mean action: 1.546 [0.000, 3.000], mean observation: 0.099 [-1.111, 1.406], loss: 5.124179, mae: 44.884995, mean_q: 60.389660
  539838/1100000: episode: 1153, duration: 1.113s, episode steps: 194, steps per second: 174, episode reward: 7.741, mean reward: 0.040 [-100.000, 16.522], mean action: 1.655 [0.000, 3.000], mean observation: 0.109 [-0.775, 1.449], loss: 5.901132, mae: 45.347027, mean_q: 60.724094
  540287/1100000: episode: 1154, duration: 2.616s, episode steps: 449, steps per second: 172, episode reward: 222.081, mean reward: 0.495 [-21.447, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.029 [-0.835, 1.406], loss: 8.244428, mae: 45.230358, mean_q: 60.809513
  540536/1100000: episode: 1155, duration: 1.467s, episode steps: 249, steps per second: 170, episode reward: 273.357, mean reward: 1.098 [-10.755, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.056 [-0.641, 1.388], loss: 5.817264, mae: 45.353481, mean_q: 60.920502
  540851/1100000: episode: 1156, duration: 1.875s, episode steps: 315, steps per second: 168, episode reward: 242.053, mean reward: 0.768 [-18.299, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.161 [-0.804, 1.446], loss: 7.492990, mae: 45.017391, mean_q: 60.482697
  541207/1100000: episode: 1157, duration: 2.120s, episode steps: 356, steps per second: 168, episode reward: 274.182, mean reward: 0.770 [-18.013, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.135 [-0.805, 1.425], loss: 6.771101, mae: 44.966740, mean_q: 60.200745
  541552/1100000: episode: 1158, duration: 2.014s, episode steps: 345, steps per second: 171, episode reward: 225.609, mean reward: 0.654 [-17.360, 100.000], mean action: 0.751 [0.000, 3.000], mean observation: 0.025 [-0.742, 1.414], loss: 6.223178, mae: 45.202084, mean_q: 60.743088
  541941/1100000: episode: 1159, duration: 2.299s, episode steps: 389, steps per second: 169, episode reward: 270.998, mean reward: 0.697 [-17.431, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.075 [-0.681, 1.414], loss: 4.880142, mae: 44.763958, mean_q: 59.913490
  542496/1100000: episode: 1160, duration: 3.386s, episode steps: 555, steps per second: 164, episode reward: 231.306, mean reward: 0.417 [-20.241, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.102 [-0.694, 1.463], loss: 5.736991, mae: 44.566605, mean_q: 59.777958
  542767/1100000: episode: 1161, duration: 1.595s, episode steps: 271, steps per second: 170, episode reward: -9.724, mean reward: -0.036 [-100.000, 15.531], mean action: 1.542 [0.000, 3.000], mean observation: 0.116 [-1.386, 1.493], loss: 5.616871, mae: 45.019276, mean_q: 60.408443
  543180/1100000: episode: 1162, duration: 2.539s, episode steps: 413, steps per second: 163, episode reward: 265.509, mean reward: 0.643 [-17.586, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.101 [-0.619, 1.411], loss: 6.000582, mae: 44.531712, mean_q: 59.705509
  543569/1100000: episode: 1163, duration: 2.374s, episode steps: 389, steps per second: 164, episode reward: -201.728, mean reward: -0.519 [-100.000, 21.314], mean action: 1.658 [0.000, 3.000], mean observation: 0.005 [-0.704, 1.448], loss: 5.472638, mae: 44.886127, mean_q: 60.142296
  544367/1100000: episode: 1164, duration: 5.399s, episode steps: 798, steps per second: 148, episode reward: 206.726, mean reward: 0.259 [-19.078, 100.000], mean action: 1.504 [0.000, 3.000], mean observation: 0.108 [-0.794, 1.431], loss: 6.664068, mae: 44.607182, mean_q: 59.779907
  544805/1100000: episode: 1165, duration: 2.688s, episode steps: 438, steps per second: 163, episode reward: 201.224, mean reward: 0.459 [-13.950, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: -0.011 [-0.650, 1.406], loss: 6.032629, mae: 44.429050, mean_q: 59.715576
  545211/1100000: episode: 1166, duration: 2.518s, episode steps: 406, steps per second: 161, episode reward: 267.568, mean reward: 0.659 [-9.947, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.071 [-0.615, 1.400], loss: 6.345401, mae: 44.159435, mean_q: 59.375210
  545712/1100000: episode: 1167, duration: 3.071s, episode steps: 501, steps per second: 163, episode reward: 246.020, mean reward: 0.491 [-17.852, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: -0.002 [-0.600, 1.497], loss: 5.975500, mae: 44.448357, mean_q: 59.698689
  545982/1100000: episode: 1168, duration: 1.580s, episode steps: 270, steps per second: 171, episode reward: 237.122, mean reward: 0.878 [-4.152, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.048 [-0.640, 1.411], loss: 6.348835, mae: 44.031864, mean_q: 59.226788
  546524/1100000: episode: 1169, duration: 3.356s, episode steps: 542, steps per second: 162, episode reward: 256.780, mean reward: 0.474 [-22.533, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.109 [-0.665, 1.391], loss: 7.736601, mae: 44.680973, mean_q: 60.115925
  546928/1100000: episode: 1170, duration: 2.437s, episode steps: 404, steps per second: 166, episode reward: 255.337, mean reward: 0.632 [-18.896, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.092 [-0.480, 1.422], loss: 6.150336, mae: 44.410156, mean_q: 59.629955
  547928/1100000: episode: 1171, duration: 7.027s, episode steps: 1000, steps per second: 142, episode reward: -7.512, mean reward: -0.008 [-9.125, 17.348], mean action: 2.099 [0.000, 3.000], mean observation: 0.032 [-0.700, 1.385], loss: 5.456030, mae: 44.416580, mean_q: 59.611004
  548603/1100000: episode: 1172, duration: 4.257s, episode steps: 675, steps per second: 159, episode reward: 168.464, mean reward: 0.250 [-20.503, 100.000], mean action: 1.764 [0.000, 3.000], mean observation: 0.198 [-0.820, 1.395], loss: 6.874918, mae: 44.064545, mean_q: 59.113194
  549058/1100000: episode: 1173, duration: 2.773s, episode steps: 455, steps per second: 164, episode reward: 274.048, mean reward: 0.602 [-11.559, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.071 [-0.812, 1.528], loss: 5.986962, mae: 44.407391, mean_q: 59.607452
  549309/1100000: episode: 1174, duration: 1.468s, episode steps: 251, steps per second: 171, episode reward: 247.667, mean reward: 0.987 [-3.855, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: -0.081 [-0.822, 1.390], loss: 5.769115, mae: 44.716663, mean_q: 60.047028
  549633/1100000: episode: 1175, duration: 1.910s, episode steps: 324, steps per second: 170, episode reward: 220.106, mean reward: 0.679 [-9.120, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: -0.050 [-0.750, 1.408], loss: 5.796809, mae: 44.439476, mean_q: 59.750343
  549850/1100000: episode: 1176, duration: 1.260s, episode steps: 217, steps per second: 172, episode reward: 266.310, mean reward: 1.227 [-10.415, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.052 [-0.644, 1.392], loss: 4.596289, mae: 44.757935, mean_q: 60.129761
  550222/1100000: episode: 1177, duration: 2.211s, episode steps: 372, steps per second: 168, episode reward: 235.531, mean reward: 0.633 [-18.748, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.163 [-0.737, 1.401], loss: 5.937011, mae: 44.641140, mean_q: 59.863941
  550759/1100000: episode: 1178, duration: 3.257s, episode steps: 537, steps per second: 165, episode reward: 283.542, mean reward: 0.528 [-19.884, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.105 [-0.656, 1.458], loss: 5.751427, mae: 44.514896, mean_q: 59.786385
  551063/1100000: episode: 1179, duration: 1.794s, episode steps: 304, steps per second: 169, episode reward: 228.919, mean reward: 0.753 [-11.688, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.155 [-0.666, 1.452], loss: 7.462473, mae: 44.805408, mean_q: 60.148323
  551310/1100000: episode: 1180, duration: 1.447s, episode steps: 247, steps per second: 171, episode reward: 261.346, mean reward: 1.058 [-17.341, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.105 [-0.698, 1.395], loss: 4.367509, mae: 44.734447, mean_q: 59.932949
  552009/1100000: episode: 1181, duration: 4.405s, episode steps: 699, steps per second: 159, episode reward: 171.292, mean reward: 0.245 [-22.316, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.144 [-0.764, 1.391], loss: 6.128508, mae: 44.927746, mean_q: 60.378929
  552439/1100000: episode: 1182, duration: 2.497s, episode steps: 430, steps per second: 172, episode reward: 239.817, mean reward: 0.558 [-19.050, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.166 [-0.754, 1.454], loss: 6.677300, mae: 44.659065, mean_q: 60.103840
  552876/1100000: episode: 1183, duration: 2.651s, episode steps: 437, steps per second: 165, episode reward: 272.988, mean reward: 0.625 [-20.109, 100.000], mean action: 1.034 [0.000, 3.000], mean observation: -0.021 [-0.812, 1.387], loss: 5.575791, mae: 44.890514, mean_q: 60.325764
  553543/1100000: episode: 1184, duration: 4.166s, episode steps: 667, steps per second: 160, episode reward: 258.706, mean reward: 0.388 [-17.712, 100.000], mean action: 0.847 [0.000, 3.000], mean observation: 0.194 [-0.790, 1.491], loss: 6.689560, mae: 45.094097, mean_q: 60.528912
  553838/1100000: episode: 1185, duration: 1.751s, episode steps: 295, steps per second: 168, episode reward: 245.334, mean reward: 0.832 [-9.837, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: -0.052 [-0.790, 1.398], loss: 5.874567, mae: 45.274857, mean_q: 60.851974
  554145/1100000: episode: 1186, duration: 1.789s, episode steps: 307, steps per second: 172, episode reward: 254.802, mean reward: 0.830 [-19.453, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.165 [-0.853, 1.398], loss: 6.496204, mae: 45.616947, mean_q: 61.257736
  554852/1100000: episode: 1187, duration: 4.246s, episode steps: 707, steps per second: 166, episode reward: 184.955, mean reward: 0.262 [-23.813, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.254 [-0.703, 1.408], loss: 5.703539, mae: 45.800106, mean_q: 61.581474
  555206/1100000: episode: 1188, duration: 2.084s, episode steps: 354, steps per second: 170, episode reward: -218.907, mean reward: -0.618 [-100.000, 8.633], mean action: 1.669 [0.000, 3.000], mean observation: 0.060 [-1.768, 1.447], loss: 10.059095, mae: 46.026989, mean_q: 61.737492
  555637/1100000: episode: 1189, duration: 2.582s, episode steps: 431, steps per second: 167, episode reward: 154.785, mean reward: 0.359 [-14.357, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: -0.028 [-0.620, 1.422], loss: 4.321294, mae: 45.789429, mean_q: 61.487041
  556637/1100000: episode: 1190, duration: 6.267s, episode steps: 1000, steps per second: 160, episode reward: 153.067, mean reward: 0.153 [-19.413, 23.194], mean action: 0.700 [0.000, 3.000], mean observation: 0.176 [-0.649, 1.418], loss: 7.137081, mae: 46.011795, mean_q: 61.685997
  556912/1100000: episode: 1191, duration: 1.604s, episode steps: 275, steps per second: 171, episode reward: 254.499, mean reward: 0.925 [-11.234, 100.000], mean action: 1.640 [0.000, 3.000], mean observation: -0.060 [-0.707, 1.486], loss: 6.449758, mae: 46.058159, mean_q: 61.655766
  557159/1100000: episode: 1192, duration: 1.417s, episode steps: 247, steps per second: 174, episode reward: 245.255, mean reward: 0.993 [-7.386, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.101 [-0.775, 1.478], loss: 5.019310, mae: 46.122570, mean_q: 61.798553
  557533/1100000: episode: 1193, duration: 2.210s, episode steps: 374, steps per second: 169, episode reward: 221.261, mean reward: 0.592 [-8.511, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: -0.025 [-0.613, 1.537], loss: 6.582390, mae: 46.186539, mean_q: 61.893768
  558533/1100000: episode: 1194, duration: 6.477s, episode steps: 1000, steps per second: 154, episode reward: 104.089, mean reward: 0.104 [-18.951, 21.640], mean action: 1.162 [0.000, 3.000], mean observation: 0.048 [-0.600, 1.423], loss: 5.842948, mae: 46.320766, mean_q: 62.055843
  559476/1100000: episode: 1195, duration: 6.053s, episode steps: 943, steps per second: 156, episode reward: 156.542, mean reward: 0.166 [-18.980, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: -0.003 [-0.600, 1.448], loss: 6.753252, mae: 46.420261, mean_q: 62.256378
  559802/1100000: episode: 1196, duration: 1.906s, episode steps: 326, steps per second: 171, episode reward: 258.185, mean reward: 0.792 [-18.551, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: -0.019 [-0.771, 1.459], loss: 5.395467, mae: 46.526421, mean_q: 62.379471
  560178/1100000: episode: 1197, duration: 2.223s, episode steps: 376, steps per second: 169, episode reward: 233.574, mean reward: 0.621 [-19.343, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.134 [-0.866, 1.405], loss: 5.136638, mae: 46.426842, mean_q: 62.278881
  560539/1100000: episode: 1198, duration: 2.204s, episode steps: 361, steps per second: 164, episode reward: 227.463, mean reward: 0.630 [-18.643, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.176 [-0.779, 1.402], loss: 6.238519, mae: 46.482189, mean_q: 62.312782
  561387/1100000: episode: 1199, duration: 5.501s, episode steps: 848, steps per second: 154, episode reward: 238.583, mean reward: 0.281 [-17.167, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.035 [-0.600, 1.444], loss: 5.475258, mae: 46.375462, mean_q: 62.254082
  561778/1100000: episode: 1200, duration: 2.370s, episode steps: 391, steps per second: 165, episode reward: 208.547, mean reward: 0.533 [-12.594, 100.000], mean action: 1.706 [0.000, 3.000], mean observation: -0.032 [-0.662, 1.402], loss: 7.681479, mae: 46.296967, mean_q: 62.197300
  561992/1100000: episode: 1201, duration: 1.243s, episode steps: 214, steps per second: 172, episode reward: 272.996, mean reward: 1.276 [-2.697, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.031 [-0.888, 1.387], loss: 7.305697, mae: 45.974010, mean_q: 61.850883
  562547/1100000: episode: 1202, duration: 3.262s, episode steps: 555, steps per second: 170, episode reward: 255.867, mean reward: 0.461 [-20.254, 100.000], mean action: 0.685 [0.000, 3.000], mean observation: 0.246 [-0.977, 1.389], loss: 5.479500, mae: 46.776852, mean_q: 62.880787
  563122/1100000: episode: 1203, duration: 3.497s, episode steps: 575, steps per second: 164, episode reward: 234.871, mean reward: 0.408 [-19.842, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.031 [-0.600, 1.509], loss: 5.260359, mae: 46.716473, mean_q: 62.762974
  563450/1100000: episode: 1204, duration: 2.048s, episode steps: 328, steps per second: 160, episode reward: 235.607, mean reward: 0.718 [-17.484, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.109 [-0.865, 1.408], loss: 4.722528, mae: 46.538502, mean_q: 62.566536
  563813/1100000: episode: 1205, duration: 2.137s, episode steps: 363, steps per second: 170, episode reward: 252.101, mean reward: 0.694 [-18.269, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.212 [-0.840, 1.400], loss: 6.015612, mae: 46.782066, mean_q: 62.747574
  564203/1100000: episode: 1206, duration: 2.302s, episode steps: 390, steps per second: 169, episode reward: 262.989, mean reward: 0.674 [-10.084, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.010 [-0.699, 1.394], loss: 3.928386, mae: 46.635105, mean_q: 62.478367
  564414/1100000: episode: 1207, duration: 1.212s, episode steps: 211, steps per second: 174, episode reward: 259.509, mean reward: 1.230 [-9.640, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.134 [-0.823, 1.395], loss: 9.647338, mae: 46.916260, mean_q: 62.765617
  564633/1100000: episode: 1208, duration: 1.254s, episode steps: 219, steps per second: 175, episode reward: 237.900, mean reward: 1.086 [-8.231, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.136 [-0.898, 1.418], loss: 7.304035, mae: 47.263260, mean_q: 63.352650
  565303/1100000: episode: 1209, duration: 4.078s, episode steps: 670, steps per second: 164, episode reward: 214.516, mean reward: 0.320 [-19.835, 100.000], mean action: 1.894 [0.000, 3.000], mean observation: 0.209 [-0.772, 1.391], loss: 5.017581, mae: 46.867550, mean_q: 62.812885
  565390/1100000: episode: 1210, duration: 0.497s, episode steps: 87, steps per second: 175, episode reward: -378.566, mean reward: -4.351 [-100.000, 4.224], mean action: 1.448 [0.000, 3.000], mean observation: 0.246 [-1.788, 2.937], loss: 4.521767, mae: 46.804119, mean_q: 62.872574
  565742/1100000: episode: 1211, duration: 2.122s, episode steps: 352, steps per second: 166, episode reward: 266.475, mean reward: 0.757 [-9.539, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.216 [-0.629, 1.404], loss: 8.041130, mae: 47.010742, mean_q: 63.179344
  565917/1100000: episode: 1212, duration: 1.013s, episode steps: 175, steps per second: 173, episode reward: -133.982, mean reward: -0.766 [-100.000, 16.676], mean action: 1.891 [0.000, 3.000], mean observation: -0.029 [-0.863, 1.592], loss: 6.757493, mae: 46.408783, mean_q: 61.764866
  566213/1100000: episode: 1213, duration: 1.723s, episode steps: 296, steps per second: 172, episode reward: 256.249, mean reward: 0.866 [-3.201, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.218 [-0.744, 1.480], loss: 4.666209, mae: 46.915913, mean_q: 63.185047
  566759/1100000: episode: 1214, duration: 3.375s, episode steps: 546, steps per second: 162, episode reward: 204.346, mean reward: 0.374 [-22.756, 100.000], mean action: 1.742 [0.000, 3.000], mean observation: 0.190 [-0.737, 1.400], loss: 5.281409, mae: 47.100185, mean_q: 63.072952
  567203/1100000: episode: 1215, duration: 2.839s, episode steps: 444, steps per second: 156, episode reward: 216.861, mean reward: 0.488 [-21.390, 100.000], mean action: 2.025 [0.000, 3.000], mean observation: 0.115 [-0.947, 1.387], loss: 8.500812, mae: 47.343758, mean_q: 63.426758
  567337/1100000: episode: 1216, duration: 0.765s, episode steps: 134, steps per second: 175, episode reward: 9.089, mean reward: 0.068 [-100.000, 14.340], mean action: 1.418 [0.000, 3.000], mean observation: 0.208 [-1.851, 1.508], loss: 6.489257, mae: 47.148533, mean_q: 63.093525
  567564/1100000: episode: 1217, duration: 1.308s, episode steps: 227, steps per second: 174, episode reward: 235.962, mean reward: 1.039 [-14.001, 100.000], mean action: 0.859 [0.000, 3.000], mean observation: 0.126 [-1.089, 1.389], loss: 6.449765, mae: 47.701008, mean_q: 64.191551
  567811/1100000: episode: 1218, duration: 1.426s, episode steps: 247, steps per second: 173, episode reward: 273.354, mean reward: 1.107 [-9.668, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.069 [-0.939, 1.480], loss: 5.886580, mae: 47.578789, mean_q: 64.009773
  568176/1100000: episode: 1219, duration: 2.159s, episode steps: 365, steps per second: 169, episode reward: 266.145, mean reward: 0.729 [-18.681, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: -0.022 [-0.809, 1.459], loss: 4.077849, mae: 47.551323, mean_q: 63.911163
  568563/1100000: episode: 1220, duration: 2.302s, episode steps: 387, steps per second: 168, episode reward: 198.114, mean reward: 0.512 [-11.321, 100.000], mean action: 1.654 [0.000, 3.000], mean observation: -0.004 [-0.734, 1.412], loss: 6.278678, mae: 47.684441, mean_q: 64.076340
  568845/1100000: episode: 1221, duration: 1.711s, episode steps: 282, steps per second: 165, episode reward: 249.182, mean reward: 0.884 [-14.824, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.014 [-0.781, 1.386], loss: 5.406009, mae: 47.348202, mean_q: 63.533176
  569251/1100000: episode: 1222, duration: 2.421s, episode steps: 406, steps per second: 168, episode reward: 255.235, mean reward: 0.629 [-17.930, 100.000], mean action: 1.897 [0.000, 3.000], mean observation: 0.077 [-0.761, 1.406], loss: 4.314325, mae: 47.371094, mean_q: 63.795921
  569788/1100000: episode: 1223, duration: 3.369s, episode steps: 537, steps per second: 159, episode reward: 240.185, mean reward: 0.447 [-18.280, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: -0.008 [-0.670, 1.401], loss: 6.248843, mae: 47.348274, mean_q: 63.620068
  569925/1100000: episode: 1224, duration: 0.796s, episode steps: 137, steps per second: 172, episode reward: -13.409, mean reward: -0.098 [-100.000, 11.366], mean action: 1.912 [0.000, 3.000], mean observation: -0.108 [-1.002, 1.752], loss: 3.283350, mae: 46.975487, mean_q: 63.174995
  570344/1100000: episode: 1225, duration: 2.431s, episode steps: 419, steps per second: 172, episode reward: 276.663, mean reward: 0.660 [-20.125, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.147 [-0.873, 1.467], loss: 8.066774, mae: 47.172070, mean_q: 63.392323
  570887/1100000: episode: 1226, duration: 3.433s, episode steps: 543, steps per second: 158, episode reward: 227.265, mean reward: 0.419 [-18.038, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: -0.006 [-0.684, 1.437], loss: 4.560778, mae: 47.520905, mean_q: 63.684269
  571162/1100000: episode: 1227, duration: 1.604s, episode steps: 275, steps per second: 171, episode reward: 268.616, mean reward: 0.977 [-17.894, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.097 [-0.865, 1.393], loss: 4.095636, mae: 47.745773, mean_q: 63.849857
  571467/1100000: episode: 1228, duration: 1.774s, episode steps: 305, steps per second: 172, episode reward: 269.113, mean reward: 0.882 [-18.422, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: 0.139 [-1.020, 1.475], loss: 8.653255, mae: 47.571323, mean_q: 63.624973
  571772/1100000: episode: 1229, duration: 1.784s, episode steps: 305, steps per second: 171, episode reward: 256.187, mean reward: 0.840 [-11.480, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.144 [-0.810, 1.480], loss: 6.821331, mae: 47.511456, mean_q: 63.461449
  572047/1100000: episode: 1230, duration: 1.572s, episode steps: 275, steps per second: 175, episode reward: 226.327, mean reward: 0.823 [-9.293, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: -0.000 [-0.711, 1.428], loss: 5.576298, mae: 47.627338, mean_q: 63.869785
  572558/1100000: episode: 1231, duration: 3.182s, episode steps: 511, steps per second: 161, episode reward: 251.483, mean reward: 0.492 [-18.491, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.120 [-0.612, 1.462], loss: 6.290612, mae: 47.565098, mean_q: 63.889610
  572874/1100000: episode: 1232, duration: 1.888s, episode steps: 316, steps per second: 167, episode reward: 248.610, mean reward: 0.787 [-18.781, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.057 [-0.743, 1.400], loss: 5.837669, mae: 47.853363, mean_q: 64.103813
  573447/1100000: episode: 1233, duration: 3.658s, episode steps: 573, steps per second: 157, episode reward: 198.421, mean reward: 0.346 [-17.897, 100.000], mean action: 2.272 [0.000, 3.000], mean observation: 0.237 [-0.743, 1.408], loss: 4.415591, mae: 47.719631, mean_q: 63.998638
  574022/1100000: episode: 1234, duration: 3.690s, episode steps: 575, steps per second: 156, episode reward: 220.093, mean reward: 0.383 [-20.427, 100.000], mean action: 0.932 [0.000, 3.000], mean observation: 0.035 [-0.684, 1.401], loss: 5.498113, mae: 47.686398, mean_q: 64.047523
  574580/1100000: episode: 1235, duration: 3.275s, episode steps: 558, steps per second: 170, episode reward: 247.221, mean reward: 0.443 [-17.757, 100.000], mean action: 0.541 [0.000, 3.000], mean observation: 0.256 [-1.102, 1.432], loss: 6.829147, mae: 47.890903, mean_q: 64.248512
  575107/1100000: episode: 1236, duration: 3.284s, episode steps: 527, steps per second: 160, episode reward: 221.389, mean reward: 0.420 [-22.549, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.040 [-0.989, 1.989], loss: 4.430484, mae: 47.769680, mean_q: 64.036125
  575270/1100000: episode: 1237, duration: 0.939s, episode steps: 163, steps per second: 174, episode reward: -114.323, mean reward: -0.701 [-100.000, 69.692], mean action: 1.460 [0.000, 3.000], mean observation: 0.131 [-0.834, 1.595], loss: 4.468857, mae: 47.532154, mean_q: 63.566406
  575503/1100000: episode: 1238, duration: 1.354s, episode steps: 233, steps per second: 172, episode reward: -92.763, mean reward: -0.398 [-100.000, 14.986], mean action: 1.742 [0.000, 3.000], mean observation: 0.221 [-1.603, 1.607], loss: 4.195747, mae: 47.791401, mean_q: 64.178719
  576157/1100000: episode: 1239, duration: 4.188s, episode steps: 654, steps per second: 156, episode reward: 207.236, mean reward: 0.317 [-19.681, 100.000], mean action: 2.245 [0.000, 3.000], mean observation: 0.240 [-0.930, 1.424], loss: 10.107008, mae: 48.065548, mean_q: 64.246666
  576561/1100000: episode: 1240, duration: 2.407s, episode steps: 404, steps per second: 168, episode reward: 279.031, mean reward: 0.691 [-9.207, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: -0.006 [-0.765, 1.388], loss: 6.057274, mae: 47.989399, mean_q: 64.388939
  577010/1100000: episode: 1241, duration: 2.684s, episode steps: 449, steps per second: 167, episode reward: 232.804, mean reward: 0.518 [-16.905, 100.000], mean action: 1.991 [0.000, 3.000], mean observation: 0.056 [-0.600, 1.527], loss: 6.942432, mae: 48.230301, mean_q: 64.722282
  577628/1100000: episode: 1242, duration: 3.776s, episode steps: 618, steps per second: 164, episode reward: 245.980, mean reward: 0.398 [-20.242, 100.000], mean action: 0.895 [0.000, 3.000], mean observation: 0.245 [-0.858, 1.392], loss: 4.362250, mae: 48.031769, mean_q: 64.514526
  577893/1100000: episode: 1243, duration: 1.549s, episode steps: 265, steps per second: 171, episode reward: 263.506, mean reward: 0.994 [-9.437, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.077 [-0.850, 1.457], loss: 7.282150, mae: 48.031933, mean_q: 64.553673
  578178/1100000: episode: 1244, duration: 1.706s, episode steps: 285, steps per second: 167, episode reward: 231.556, mean reward: 0.812 [-11.586, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.089 [-0.852, 1.404], loss: 7.887199, mae: 48.874622, mean_q: 65.437141
  578480/1100000: episode: 1245, duration: 1.789s, episode steps: 302, steps per second: 169, episode reward: 229.884, mean reward: 0.761 [-17.467, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.001 [-0.670, 1.411], loss: 7.465983, mae: 48.645393, mean_q: 65.170547
  578818/1100000: episode: 1246, duration: 2.012s, episode steps: 338, steps per second: 168, episode reward: 260.359, mean reward: 0.770 [-13.223, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.136 [-0.912, 1.415], loss: 4.675909, mae: 48.472015, mean_q: 65.103325
  579083/1100000: episode: 1247, duration: 1.545s, episode steps: 265, steps per second: 172, episode reward: 211.946, mean reward: 0.800 [-9.161, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.165 [-0.850, 1.417], loss: 5.440947, mae: 48.659122, mean_q: 65.111481
  579455/1100000: episode: 1248, duration: 2.262s, episode steps: 372, steps per second: 164, episode reward: 247.831, mean reward: 0.666 [-17.356, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.071 [-0.772, 1.407], loss: 5.016427, mae: 48.607826, mean_q: 65.374336
  579775/1100000: episode: 1249, duration: 1.862s, episode steps: 320, steps per second: 172, episode reward: 239.235, mean reward: 0.748 [-17.986, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.191 [-0.708, 1.402], loss: 3.403996, mae: 48.554420, mean_q: 65.158936
  580089/1100000: episode: 1250, duration: 1.858s, episode steps: 314, steps per second: 169, episode reward: 233.149, mean reward: 0.743 [-7.772, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: -0.051 [-0.759, 1.407], loss: 4.788542, mae: 48.554714, mean_q: 65.147041
  580520/1100000: episode: 1251, duration: 2.625s, episode steps: 431, steps per second: 164, episode reward: 246.175, mean reward: 0.571 [-18.046, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.166 [-0.885, 1.400], loss: 4.729086, mae: 48.718693, mean_q: 65.511230
  580849/1100000: episode: 1252, duration: 1.981s, episode steps: 329, steps per second: 166, episode reward: 263.365, mean reward: 0.801 [-17.350, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.111 [-1.119, 1.411], loss: 6.582362, mae: 48.282864, mean_q: 64.781136
  581078/1100000: episode: 1253, duration: 1.309s, episode steps: 229, steps per second: 175, episode reward: 217.472, mean reward: 0.950 [-19.422, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.112 [-0.956, 1.446], loss: 6.299806, mae: 48.471256, mean_q: 64.882172
  581559/1100000: episode: 1254, duration: 2.944s, episode steps: 481, steps per second: 163, episode reward: 179.840, mean reward: 0.374 [-18.199, 100.000], mean action: 2.268 [0.000, 3.000], mean observation: 0.202 [-0.715, 1.407], loss: 6.424983, mae: 48.580170, mean_q: 65.263657
  581959/1100000: episode: 1255, duration: 2.345s, episode steps: 400, steps per second: 171, episode reward: 216.587, mean reward: 0.541 [-11.566, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.142 [-1.501, 1.417], loss: 4.702130, mae: 48.480206, mean_q: 64.930511
  582329/1100000: episode: 1256, duration: 2.163s, episode steps: 370, steps per second: 171, episode reward: 223.516, mean reward: 0.604 [-10.392, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: -0.006 [-0.774, 1.450], loss: 3.367283, mae: 48.079945, mean_q: 64.540405
  582570/1100000: episode: 1257, duration: 1.401s, episode steps: 241, steps per second: 172, episode reward: 286.336, mean reward: 1.188 [-6.273, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.049 [-0.934, 1.546], loss: 6.822649, mae: 48.659958, mean_q: 64.962845
  582811/1100000: episode: 1258, duration: 1.386s, episode steps: 241, steps per second: 174, episode reward: 259.058, mean reward: 1.075 [-3.053, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.097 [-0.860, 1.482], loss: 3.848026, mae: 48.484215, mean_q: 65.308060
  583046/1100000: episode: 1259, duration: 1.383s, episode steps: 235, steps per second: 170, episode reward: 248.118, mean reward: 1.056 [-9.150, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.072 [-0.931, 1.417], loss: 5.097188, mae: 48.731464, mean_q: 65.502007
  583430/1100000: episode: 1260, duration: 2.335s, episode steps: 384, steps per second: 164, episode reward: 237.020, mean reward: 0.617 [-13.996, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.214 [-0.926, 1.439], loss: 5.411433, mae: 48.638569, mean_q: 65.362282
  583790/1100000: episode: 1261, duration: 2.121s, episode steps: 360, steps per second: 170, episode reward: 266.141, mean reward: 0.739 [-8.540, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: -0.001 [-0.747, 1.474], loss: 6.370024, mae: 48.531704, mean_q: 65.208328
  584163/1100000: episode: 1262, duration: 2.219s, episode steps: 373, steps per second: 168, episode reward: 209.530, mean reward: 0.562 [-18.141, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: -0.017 [-0.798, 1.452], loss: 6.970303, mae: 48.811329, mean_q: 65.416672
  584578/1100000: episode: 1263, duration: 2.433s, episode steps: 415, steps per second: 171, episode reward: 223.202, mean reward: 0.538 [-7.658, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.436], loss: 6.233638, mae: 48.834469, mean_q: 65.501541
  585085/1100000: episode: 1264, duration: 3.044s, episode steps: 507, steps per second: 167, episode reward: 212.842, mean reward: 0.420 [-21.187, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.137 [-0.711, 1.494], loss: 5.496936, mae: 49.102356, mean_q: 65.747704
  585627/1100000: episode: 1265, duration: 3.214s, episode steps: 542, steps per second: 169, episode reward: -217.484, mean reward: -0.401 [-100.000, 10.377], mean action: 1.133 [0.000, 3.000], mean observation: 0.131 [-0.665, 1.536], loss: 5.902056, mae: 49.341984, mean_q: 66.222267
  586328/1100000: episode: 1266, duration: 4.331s, episode steps: 701, steps per second: 162, episode reward: 211.589, mean reward: 0.302 [-20.559, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.160 [-0.914, 1.491], loss: 7.708523, mae: 49.324001, mean_q: 66.045586
  586724/1100000: episode: 1267, duration: 2.386s, episode steps: 396, steps per second: 166, episode reward: 259.011, mean reward: 0.654 [-11.165, 100.000], mean action: 2.056 [0.000, 3.000], mean observation: 0.087 [-0.674, 1.477], loss: 5.707056, mae: 49.492775, mean_q: 66.271851
  587003/1100000: episode: 1268, duration: 1.640s, episode steps: 279, steps per second: 170, episode reward: 260.859, mean reward: 0.935 [-11.164, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.203 [-0.690, 1.413], loss: 5.905064, mae: 49.475952, mean_q: 66.130692
  587168/1100000: episode: 1269, duration: 0.951s, episode steps: 165, steps per second: 174, episode reward: -34.603, mean reward: -0.210 [-100.000, 6.070], mean action: 1.497 [0.000, 3.000], mean observation: -0.044 [-0.678, 1.387], loss: 4.136034, mae: 49.388542, mean_q: 66.243729
  587429/1100000: episode: 1270, duration: 1.505s, episode steps: 261, steps per second: 173, episode reward: 296.628, mean reward: 1.137 [-7.379, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.206 [-0.603, 1.495], loss: 5.136020, mae: 49.889297, mean_q: 66.529167
  587628/1100000: episode: 1271, duration: 1.158s, episode steps: 199, steps per second: 172, episode reward: 74.779, mean reward: 0.376 [-100.000, 15.819], mean action: 1.558 [0.000, 3.000], mean observation: 0.068 [-0.721, 1.399], loss: 6.338675, mae: 49.677254, mean_q: 66.379807
  587951/1100000: episode: 1272, duration: 1.932s, episode steps: 323, steps per second: 167, episode reward: 292.959, mean reward: 0.907 [-19.004, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.104 [-1.034, 1.408], loss: 9.369881, mae: 50.484646, mean_q: 67.357224
  588112/1100000: episode: 1273, duration: 0.929s, episode steps: 161, steps per second: 173, episode reward: 294.688, mean reward: 1.830 [-2.901, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.060 [-1.049, 1.390], loss: 19.025909, mae: 50.669888, mean_q: 67.443336
  588482/1100000: episode: 1274, duration: 2.202s, episode steps: 370, steps per second: 168, episode reward: 227.522, mean reward: 0.615 [-8.445, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: -0.011 [-0.622, 1.413], loss: 6.180939, mae: 50.433777, mean_q: 67.281410
  588833/1100000: episode: 1275, duration: 2.102s, episode steps: 351, steps per second: 167, episode reward: 261.836, mean reward: 0.746 [-17.509, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: -0.031 [-0.716, 1.403], loss: 6.944218, mae: 50.259430, mean_q: 67.121788
  589080/1100000: episode: 1276, duration: 1.428s, episode steps: 247, steps per second: 173, episode reward: 225.008, mean reward: 0.911 [-17.988, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.184 [-0.964, 1.404], loss: 5.781665, mae: 51.235199, mean_q: 68.419403
  589291/1100000: episode: 1277, duration: 1.223s, episode steps: 211, steps per second: 173, episode reward: 309.934, mean reward: 1.469 [-8.070, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.034 [-1.135, 1.388], loss: 7.132051, mae: 50.797825, mean_q: 67.914574
  589761/1100000: episode: 1278, duration: 2.875s, episode steps: 470, steps per second: 163, episode reward: 231.586, mean reward: 0.493 [-13.835, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.009 [-0.917, 1.409], loss: 5.220870, mae: 50.847664, mean_q: 68.014656
  590135/1100000: episode: 1279, duration: 2.241s, episode steps: 374, steps per second: 167, episode reward: 235.967, mean reward: 0.631 [-17.810, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.211 [-1.260, 1.390], loss: 6.981270, mae: 51.039299, mean_q: 68.398300
  590507/1100000: episode: 1280, duration: 2.227s, episode steps: 372, steps per second: 167, episode reward: 219.592, mean reward: 0.590 [-17.299, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: -0.030 [-0.675, 1.398], loss: 6.732522, mae: 51.115501, mean_q: 68.499054
  590586/1100000: episode: 1281, duration: 0.479s, episode steps: 79, steps per second: 165, episode reward: 12.184, mean reward: 0.154 [-100.000, 21.423], mean action: 1.797 [0.000, 3.000], mean observation: 0.121 [-2.328, 1.390], loss: 2.254710, mae: 50.695927, mean_q: 68.315056
  590884/1100000: episode: 1282, duration: 1.766s, episode steps: 298, steps per second: 169, episode reward: -0.132, mean reward: -0.000 [-100.000, 15.919], mean action: 1.550 [0.000, 3.000], mean observation: -0.030 [-0.904, 1.392], loss: 13.014755, mae: 51.738678, mean_q: 68.850632
  590990/1100000: episode: 1283, duration: 0.598s, episode steps: 106, steps per second: 177, episode reward: -71.486, mean reward: -0.674 [-100.000, 8.516], mean action: 1.000 [0.000, 3.000], mean observation: 0.127 [-3.027, 1.512], loss: 6.993625, mae: 51.534344, mean_q: 68.952377
  591090/1100000: episode: 1284, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -75.836, mean reward: -0.758 [-100.000, 8.949], mean action: 1.520 [0.000, 3.000], mean observation: 0.105 [-3.065, 1.442], loss: 3.598819, mae: 51.648716, mean_q: 69.040344
  591157/1100000: episode: 1285, duration: 0.385s, episode steps: 67, steps per second: 174, episode reward: 5.190, mean reward: 0.077 [-100.000, 13.161], mean action: 1.896 [0.000, 3.000], mean observation: 0.019 [-1.239, 1.386], loss: 7.297074, mae: 53.045982, mean_q: 71.091690
  591328/1100000: episode: 1286, duration: 0.972s, episode steps: 171, steps per second: 176, episode reward: -64.502, mean reward: -0.377 [-100.000, 8.159], mean action: 1.444 [0.000, 3.000], mean observation: -0.060 [-0.679, 1.529], loss: 6.632287, mae: 52.110390, mean_q: 69.923904
  591404/1100000: episode: 1287, duration: 0.434s, episode steps: 76, steps per second: 175, episode reward: -18.121, mean reward: -0.238 [-100.000, 13.861], mean action: 1.395 [0.000, 3.000], mean observation: 0.095 [-2.204, 1.397], loss: 7.880283, mae: 52.306988, mean_q: 69.871422
  591487/1100000: episode: 1288, duration: 0.483s, episode steps: 83, steps per second: 172, episode reward: 33.181, mean reward: 0.400 [-100.000, 21.411], mean action: 1.976 [0.000, 3.000], mean observation: -0.053 [-1.099, 1.773], loss: 4.979942, mae: 53.895782, mean_q: 72.547943
  591874/1100000: episode: 1289, duration: 2.282s, episode steps: 387, steps per second: 170, episode reward: 236.833, mean reward: 0.612 [-10.572, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.007 [-0.994, 1.421], loss: 6.632679, mae: 52.976685, mean_q: 70.670250
  592045/1100000: episode: 1290, duration: 0.981s, episode steps: 171, steps per second: 174, episode reward: -35.639, mean reward: -0.208 [-100.000, 12.895], mean action: 1.585 [0.000, 3.000], mean observation: -0.047 [-0.813, 2.725], loss: 7.536978, mae: 53.096493, mean_q: 70.937202
  593045/1100000: episode: 1291, duration: 6.594s, episode steps: 1000, steps per second: 152, episode reward: -33.243, mean reward: -0.033 [-21.712, 18.508], mean action: 1.545 [0.000, 3.000], mean observation: -0.018 [-0.806, 1.404], loss: 15.722572, mae: 53.308273, mean_q: 71.172188
  593677/1100000: episode: 1292, duration: 4.159s, episode steps: 632, steps per second: 152, episode reward: 188.788, mean reward: 0.299 [-17.284, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.001 [-0.635, 1.403], loss: 6.991284, mae: 53.398735, mean_q: 71.342514
  594121/1100000: episode: 1293, duration: 2.723s, episode steps: 444, steps per second: 163, episode reward: 232.903, mean reward: 0.525 [-13.712, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: -0.037 [-0.738, 1.392], loss: 5.345675, mae: 53.367649, mean_q: 71.144569
  594226/1100000: episode: 1294, duration: 0.607s, episode steps: 105, steps per second: 173, episode reward: 15.892, mean reward: 0.151 [-100.000, 17.456], mean action: 1.771 [0.000, 3.000], mean observation: 0.087 [-0.882, 1.396], loss: 20.210758, mae: 53.891220, mean_q: 71.228142
  594640/1100000: episode: 1295, duration: 2.467s, episode steps: 414, steps per second: 168, episode reward: 240.368, mean reward: 0.581 [-19.728, 100.000], mean action: 1.853 [0.000, 3.000], mean observation: 0.234 [-1.004, 1.472], loss: 5.242994, mae: 53.718250, mean_q: 71.756744
  595117/1100000: episode: 1296, duration: 2.920s, episode steps: 477, steps per second: 163, episode reward: 205.661, mean reward: 0.431 [-17.972, 100.000], mean action: 1.922 [0.000, 3.000], mean observation: 0.030 [-0.667, 1.423], loss: 7.574484, mae: 53.792805, mean_q: 71.645538
  595216/1100000: episode: 1297, duration: 0.569s, episode steps: 99, steps per second: 174, episode reward: -34.804, mean reward: -0.352 [-100.000, 16.173], mean action: 1.667 [0.000, 3.000], mean observation: 0.018 [-1.981, 1.408], loss: 8.747040, mae: 54.485802, mean_q: 72.682121
  595532/1100000: episode: 1298, duration: 1.881s, episode steps: 316, steps per second: 168, episode reward: 173.318, mean reward: 0.548 [-8.751, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: -0.095 [-0.699, 1.411], loss: 9.218667, mae: 53.896294, mean_q: 72.009621
  595962/1100000: episode: 1299, duration: 2.617s, episode steps: 430, steps per second: 164, episode reward: 174.835, mean reward: 0.407 [-9.474, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: -0.041 [-0.619, 1.408], loss: 7.923942, mae: 54.178932, mean_q: 72.405151
  596309/1100000: episode: 1300, duration: 2.035s, episode steps: 347, steps per second: 170, episode reward: 275.805, mean reward: 0.795 [-13.788, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.147 [-0.895, 1.400], loss: 6.909618, mae: 54.079868, mean_q: 72.411972
  596597/1100000: episode: 1301, duration: 1.691s, episode steps: 288, steps per second: 170, episode reward: 263.118, mean reward: 0.914 [-8.692, 100.000], mean action: 0.729 [0.000, 3.000], mean observation: 0.127 [-1.064, 1.401], loss: 6.514157, mae: 54.180786, mean_q: 72.799988
  596998/1100000: episode: 1302, duration: 2.321s, episode steps: 401, steps per second: 173, episode reward: 229.821, mean reward: 0.573 [-23.116, 100.000], mean action: 0.736 [0.000, 3.000], mean observation: 0.225 [-0.916, 1.396], loss: 8.531578, mae: 54.610905, mean_q: 73.298447
  597186/1100000: episode: 1303, duration: 1.088s, episode steps: 188, steps per second: 173, episode reward: 275.978, mean reward: 1.468 [-17.654, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: 0.055 [-1.201, 1.399], loss: 4.376436, mae: 54.012737, mean_q: 72.441284
  597516/1100000: episode: 1304, duration: 1.951s, episode steps: 330, steps per second: 169, episode reward: 262.405, mean reward: 0.795 [-18.066, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.196 [-0.877, 1.400], loss: 6.881004, mae: 54.005226, mean_q: 72.506866
  597745/1100000: episode: 1305, duration: 1.330s, episode steps: 229, steps per second: 172, episode reward: 301.214, mean reward: 1.315 [-8.967, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.087 [-0.809, 1.387], loss: 4.756586, mae: 54.015881, mean_q: 72.443367
  598010/1100000: episode: 1306, duration: 1.570s, episode steps: 265, steps per second: 169, episode reward: 251.641, mean reward: 0.950 [-7.805, 100.000], mean action: 1.917 [0.000, 3.000], mean observation: -0.047 [-0.717, 1.386], loss: 4.985559, mae: 54.702564, mean_q: 73.326942
  598311/1100000: episode: 1307, duration: 1.747s, episode steps: 301, steps per second: 172, episode reward: 311.370, mean reward: 1.034 [-17.480, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.089 [-0.875, 1.392], loss: 7.326437, mae: 54.949051, mean_q: 73.539627
  598771/1100000: episode: 1308, duration: 2.789s, episode steps: 460, steps per second: 165, episode reward: 206.581, mean reward: 0.449 [-17.550, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.189 [-0.770, 1.461], loss: 6.421261, mae: 54.205872, mean_q: 72.562157
  599771/1100000: episode: 1309, duration: 6.505s, episode steps: 1000, steps per second: 154, episode reward: 27.027, mean reward: 0.027 [-19.441, 12.892], mean action: 1.570 [0.000, 3.000], mean observation: -0.102 [-0.833, 1.386], loss: 7.356332, mae: 53.875343, mean_q: 72.223595
  600771/1100000: episode: 1310, duration: 6.176s, episode steps: 1000, steps per second: 162, episode reward: 96.800, mean reward: 0.097 [-20.219, 14.415], mean action: 1.842 [0.000, 3.000], mean observation: 0.271 [-0.836, 1.452], loss: 8.000960, mae: 53.499622, mean_q: 71.773788
  601484/1100000: episode: 1311, duration: 4.667s, episode steps: 713, steps per second: 153, episode reward: 132.941, mean reward: 0.186 [-8.709, 100.000], mean action: 1.675 [0.000, 3.000], mean observation: -0.051 [-0.663, 1.472], loss: 6.746697, mae: 53.371159, mean_q: 71.609505
  601755/1100000: episode: 1312, duration: 1.606s, episode steps: 271, steps per second: 169, episode reward: 232.968, mean reward: 0.860 [-8.937, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.046 [-0.568, 1.398], loss: 6.421506, mae: 52.896908, mean_q: 71.012650
  602268/1100000: episode: 1313, duration: 3.277s, episode steps: 513, steps per second: 157, episode reward: 175.952, mean reward: 0.343 [-14.147, 100.000], mean action: 1.663 [0.000, 3.000], mean observation: -0.034 [-0.600, 1.406], loss: 5.496480, mae: 52.866615, mean_q: 70.966179
  602811/1100000: episode: 1314, duration: 3.286s, episode steps: 543, steps per second: 165, episode reward: 206.721, mean reward: 0.381 [-18.199, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: -0.031 [-0.600, 1.392], loss: 5.687279, mae: 52.493057, mean_q: 70.445671
  603237/1100000: episode: 1315, duration: 2.638s, episode steps: 426, steps per second: 162, episode reward: 50.117, mean reward: 0.118 [-100.000, 12.075], mean action: 1.714 [0.000, 3.000], mean observation: 0.010 [-0.600, 1.389], loss: 10.180744, mae: 52.565323, mean_q: 70.579483
  603668/1100000: episode: 1316, duration: 2.625s, episode steps: 431, steps per second: 164, episode reward: 197.244, mean reward: 0.458 [-14.061, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: -0.028 [-0.694, 1.406], loss: 5.014030, mae: 52.381657, mean_q: 70.465309
  604017/1100000: episode: 1317, duration: 2.096s, episode steps: 349, steps per second: 166, episode reward: 235.439, mean reward: 0.675 [-17.467, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.061 [-0.596, 1.438], loss: 4.550948, mae: 52.254005, mean_q: 70.390594
  604363/1100000: episode: 1318, duration: 2.037s, episode steps: 346, steps per second: 170, episode reward: 284.913, mean reward: 0.823 [-11.566, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.213 [-1.147, 1.433], loss: 5.750330, mae: 52.145260, mean_q: 69.917198
  604843/1100000: episode: 1319, duration: 2.817s, episode steps: 480, steps per second: 170, episode reward: 241.458, mean reward: 0.503 [-19.281, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.224 [-0.845, 1.412], loss: 5.907218, mae: 52.230331, mean_q: 70.098976
  604933/1100000: episode: 1320, duration: 0.512s, episode steps: 90, steps per second: 176, episode reward: -537.676, mean reward: -5.974 [-100.000, 1.990], mean action: 0.956 [0.000, 2.000], mean observation: 0.231 [-4.710, 3.136], loss: 5.040788, mae: 52.614758, mean_q: 71.033218
  605553/1100000: episode: 1321, duration: 4.029s, episode steps: 620, steps per second: 154, episode reward: 144.398, mean reward: 0.233 [-12.931, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: -0.041 [-0.632, 1.408], loss: 6.744627, mae: 52.371628, mean_q: 70.528481
  605659/1100000: episode: 1322, duration: 0.619s, episode steps: 106, steps per second: 171, episode reward: 36.496, mean reward: 0.344 [-100.000, 19.502], mean action: 1.877 [0.000, 3.000], mean observation: -0.029 [-0.866, 1.500], loss: 5.139546, mae: 52.479523, mean_q: 70.784790
  606278/1100000: episode: 1323, duration: 3.862s, episode steps: 619, steps per second: 160, episode reward: 225.746, mean reward: 0.365 [-17.559, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.234 [-0.798, 1.428], loss: 5.992953, mae: 52.313175, mean_q: 70.477882
  606545/1100000: episode: 1324, duration: 1.583s, episode steps: 267, steps per second: 169, episode reward: 262.695, mean reward: 0.984 [-17.480, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: -0.050 [-0.897, 1.398], loss: 8.416121, mae: 52.416325, mean_q: 70.444405
  606785/1100000: episode: 1325, duration: 1.415s, episode steps: 240, steps per second: 170, episode reward: -40.319, mean reward: -0.168 [-100.000, 12.137], mean action: 1.904 [0.000, 3.000], mean observation: 0.116 [-1.012, 1.410], loss: 5.624090, mae: 52.307995, mean_q: 70.156425
  607106/1100000: episode: 1326, duration: 1.898s, episode steps: 321, steps per second: 169, episode reward: 254.495, mean reward: 0.793 [-18.377, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.199 [-0.754, 1.409], loss: 5.876546, mae: 52.204998, mean_q: 70.015907
  608106/1100000: episode: 1327, duration: 6.411s, episode steps: 1000, steps per second: 156, episode reward: 112.561, mean reward: 0.113 [-20.525, 22.343], mean action: 1.259 [0.000, 3.000], mean observation: 0.191 [-0.874, 1.404], loss: 8.689781, mae: 52.064857, mean_q: 69.744324
  608490/1100000: episode: 1328, duration: 2.315s, episode steps: 384, steps per second: 166, episode reward: 245.376, mean reward: 0.639 [-20.225, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: -0.016 [-0.792, 1.426], loss: 8.280658, mae: 52.079239, mean_q: 69.606483
  608947/1100000: episode: 1329, duration: 2.829s, episode steps: 457, steps per second: 162, episode reward: 198.488, mean reward: 0.434 [-18.286, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: -0.015 [-0.600, 1.519], loss: 6.224472, mae: 51.427544, mean_q: 69.189804
  609814/1100000: episode: 1330, duration: 5.414s, episode steps: 867, steps per second: 160, episode reward: 179.050, mean reward: 0.207 [-18.191, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.022 [-0.600, 1.411], loss: 5.412563, mae: 51.402199, mean_q: 69.051064
  609928/1100000: episode: 1331, duration: 0.660s, episode steps: 114, steps per second: 173, episode reward: 20.902, mean reward: 0.183 [-100.000, 19.586], mean action: 1.939 [0.000, 3.000], mean observation: 0.053 [-0.853, 1.385], loss: 4.761303, mae: 51.978104, mean_q: 69.737373
  610730/1100000: episode: 1332, duration: 5.367s, episode steps: 802, steps per second: 149, episode reward: 159.843, mean reward: 0.199 [-17.648, 100.000], mean action: 1.642 [0.000, 3.000], mean observation: -0.038 [-0.933, 1.516], loss: 5.661977, mae: 50.887882, mean_q: 68.406281
  611730/1100000: episode: 1333, duration: 7.227s, episode steps: 1000, steps per second: 138, episode reward: 80.167, mean reward: 0.080 [-17.626, 22.222], mean action: 1.415 [0.000, 3.000], mean observation: 0.095 [-1.191, 1.627], loss: 5.734752, mae: 50.783894, mean_q: 68.282249
  612070/1100000: episode: 1334, duration: 2.036s, episode steps: 340, steps per second: 167, episode reward: 255.120, mean reward: 0.750 [-20.514, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.111 [-0.721, 1.415], loss: 4.558116, mae: 51.081688, mean_q: 68.706039
  612769/1100000: episode: 1335, duration: 4.454s, episode steps: 699, steps per second: 157, episode reward: 248.430, mean reward: 0.355 [-17.997, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.195 [-1.133, 1.429], loss: 5.780266, mae: 51.020184, mean_q: 68.594299
  613211/1100000: episode: 1336, duration: 2.601s, episode steps: 442, steps per second: 170, episode reward: 233.820, mean reward: 0.529 [-19.754, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: 0.254 [-0.987, 1.395], loss: 8.529242, mae: 50.897179, mean_q: 68.388908
  613467/1100000: episode: 1337, duration: 1.474s, episode steps: 256, steps per second: 174, episode reward: 216.182, mean reward: 0.844 [-19.900, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: -0.030 [-0.803, 1.409], loss: 6.378258, mae: 50.834263, mean_q: 68.089645
  614290/1100000: episode: 1338, duration: 5.021s, episode steps: 823, steps per second: 164, episode reward: 185.802, mean reward: 0.226 [-11.041, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.086 [-0.677, 2.632], loss: 8.602839, mae: 50.665836, mean_q: 67.996887
  614690/1100000: episode: 1339, duration: 2.347s, episode steps: 400, steps per second: 170, episode reward: 267.432, mean reward: 0.669 [-10.984, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: -0.009 [-0.908, 1.500], loss: 6.676188, mae: 50.642891, mean_q: 68.093803
  615081/1100000: episode: 1340, duration: 2.320s, episode steps: 391, steps per second: 169, episode reward: 230.933, mean reward: 0.591 [-18.725, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.176 [-0.875, 1.468], loss: 6.874538, mae: 50.855392, mean_q: 68.234665
  615490/1100000: episode: 1341, duration: 2.460s, episode steps: 409, steps per second: 166, episode reward: 265.252, mean reward: 0.649 [-9.657, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.140 [-0.901, 1.454], loss: 5.027756, mae: 50.995434, mean_q: 68.560043
  615809/1100000: episode: 1342, duration: 1.854s, episode steps: 319, steps per second: 172, episode reward: 286.487, mean reward: 0.898 [-9.928, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.080 [-0.792, 1.464], loss: 5.381799, mae: 50.823498, mean_q: 68.109619
  616809/1100000: episode: 1343, duration: 6.413s, episode steps: 1000, steps per second: 156, episode reward: 93.527, mean reward: 0.094 [-19.409, 22.619], mean action: 1.249 [0.000, 3.000], mean observation: 0.043 [-0.852, 1.406], loss: 6.343405, mae: 50.861202, mean_q: 68.274094
  617809/1100000: episode: 1344, duration: 6.285s, episode steps: 1000, steps per second: 159, episode reward: 116.947, mean reward: 0.117 [-18.781, 13.277], mean action: 0.856 [0.000, 3.000], mean observation: 0.055 [-0.908, 1.391], loss: 4.851101, mae: 50.991539, mean_q: 68.513077
  617917/1100000: episode: 1345, duration: 0.622s, episode steps: 108, steps per second: 174, episode reward: -45.706, mean reward: -0.423 [-100.000, 19.704], mean action: 1.861 [0.000, 3.000], mean observation: 0.062 [-1.279, 1.457], loss: 7.271063, mae: 50.443890, mean_q: 67.844620
  618917/1100000: episode: 1346, duration: 6.576s, episode steps: 1000, steps per second: 152, episode reward: 35.963, mean reward: 0.036 [-21.931, 23.474], mean action: 1.798 [0.000, 3.000], mean observation: 0.178 [-0.574, 1.408], loss: 6.386959, mae: 50.487251, mean_q: 67.763878
  619219/1100000: episode: 1347, duration: 1.762s, episode steps: 302, steps per second: 171, episode reward: 247.540, mean reward: 0.820 [-12.893, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.135 [-0.943, 1.406], loss: 4.805077, mae: 50.644310, mean_q: 68.059425
  619883/1100000: episode: 1348, duration: 4.045s, episode steps: 664, steps per second: 164, episode reward: 206.858, mean reward: 0.312 [-23.181, 100.000], mean action: 1.767 [0.000, 3.000], mean observation: 0.060 [-0.737, 1.405], loss: 5.759319, mae: 50.010300, mean_q: 67.209633
  620370/1100000: episode: 1349, duration: 2.899s, episode steps: 487, steps per second: 168, episode reward: 245.779, mean reward: 0.505 [-18.848, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.151 [-0.699, 1.508], loss: 4.700033, mae: 49.926285, mean_q: 67.215584
  620633/1100000: episode: 1350, duration: 1.526s, episode steps: 263, steps per second: 172, episode reward: 249.310, mean reward: 0.948 [-6.336, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.177 [-0.874, 1.487], loss: 6.083222, mae: 50.341381, mean_q: 67.786690
  621517/1100000: episode: 1351, duration: 5.544s, episode steps: 884, steps per second: 159, episode reward: 235.257, mean reward: 0.266 [-23.897, 100.000], mean action: 2.060 [0.000, 3.000], mean observation: 0.094 [-0.728, 1.415], loss: 5.163237, mae: 50.159401, mean_q: 67.685402
  621957/1100000: episode: 1352, duration: 2.686s, episode steps: 440, steps per second: 164, episode reward: 197.983, mean reward: 0.450 [-18.002, 100.000], mean action: 1.970 [0.000, 3.000], mean observation: 0.192 [-0.676, 1.427], loss: 5.609124, mae: 49.938839, mean_q: 67.311951
  622337/1100000: episode: 1353, duration: 2.332s, episode steps: 380, steps per second: 163, episode reward: 282.410, mean reward: 0.743 [-19.463, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.015 [-0.660, 1.391], loss: 8.014697, mae: 49.829540, mean_q: 67.161659
  622602/1100000: episode: 1354, duration: 1.570s, episode steps: 265, steps per second: 169, episode reward: 258.215, mean reward: 0.974 [-6.942, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.098 [-0.866, 1.396], loss: 6.788458, mae: 50.151806, mean_q: 67.612305
  622984/1100000: episode: 1355, duration: 2.370s, episode steps: 382, steps per second: 161, episode reward: 251.869, mean reward: 0.659 [-19.129, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.078 [-0.794, 1.395], loss: 11.021945, mae: 49.865131, mean_q: 67.221764
  623168/1100000: episode: 1356, duration: 1.052s, episode steps: 184, steps per second: 175, episode reward: -190.989, mean reward: -1.038 [-100.000, 29.740], mean action: 1.527 [0.000, 3.000], mean observation: 0.127 [-0.869, 1.894], loss: 5.652060, mae: 50.099663, mean_q: 67.550453
  624168/1100000: episode: 1357, duration: 6.445s, episode steps: 1000, steps per second: 155, episode reward: 22.655, mean reward: 0.023 [-22.960, 14.842], mean action: 1.500 [0.000, 3.000], mean observation: 0.069 [-0.721, 1.437], loss: 5.630799, mae: 50.054470, mean_q: 67.282295
  624719/1100000: episode: 1358, duration: 3.600s, episode steps: 551, steps per second: 153, episode reward: 192.837, mean reward: 0.350 [-18.809, 100.000], mean action: 2.076 [0.000, 3.000], mean observation: 0.169 [-0.909, 1.464], loss: 8.124919, mae: 50.109802, mean_q: 67.276840
  624859/1100000: episode: 1359, duration: 0.804s, episode steps: 140, steps per second: 174, episode reward: -227.359, mean reward: -1.624 [-100.000, 5.638], mean action: 1.714 [0.000, 3.000], mean observation: 0.127 [-1.050, 2.140], loss: 9.980684, mae: 49.836567, mean_q: 67.032745
  625188/1100000: episode: 1360, duration: 1.917s, episode steps: 329, steps per second: 172, episode reward: -17.812, mean reward: -0.054 [-100.000, 18.211], mean action: 1.480 [0.000, 3.000], mean observation: -0.006 [-1.108, 1.937], loss: 10.464219, mae: 50.172077, mean_q: 67.392540
  625866/1100000: episode: 1361, duration: 4.435s, episode steps: 678, steps per second: 153, episode reward: 195.302, mean reward: 0.288 [-18.433, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: -0.018 [-0.859, 1.387], loss: 7.333150, mae: 50.452423, mean_q: 67.635750
  626603/1100000: episode: 1362, duration: 4.727s, episode steps: 737, steps per second: 156, episode reward: 274.973, mean reward: 0.373 [-19.199, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.170 [-0.888, 1.401], loss: 5.559762, mae: 50.682518, mean_q: 68.262772
  627064/1100000: episode: 1363, duration: 2.806s, episode steps: 461, steps per second: 164, episode reward: 253.312, mean reward: 0.549 [-19.865, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.127 [-0.871, 1.386], loss: 4.565156, mae: 50.713032, mean_q: 68.342979
  627876/1100000: episode: 1364, duration: 5.142s, episode steps: 812, steps per second: 158, episode reward: 225.186, mean reward: 0.277 [-19.200, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.166 [-0.772, 1.390], loss: 5.501693, mae: 50.714256, mean_q: 68.196854
  628413/1100000: episode: 1365, duration: 3.262s, episode steps: 537, steps per second: 165, episode reward: 184.527, mean reward: 0.344 [-19.500, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.190 [-1.159, 1.419], loss: 6.105883, mae: 50.760967, mean_q: 68.205788
  629259/1100000: episode: 1366, duration: 5.208s, episode steps: 846, steps per second: 162, episode reward: 239.258, mean reward: 0.283 [-19.655, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.272 [-0.931, 1.417], loss: 6.959543, mae: 50.715279, mean_q: 68.110268
  629545/1100000: episode: 1367, duration: 1.648s, episode steps: 286, steps per second: 174, episode reward: 269.422, mean reward: 0.942 [-9.544, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.006 [-0.768, 1.520], loss: 5.486095, mae: 50.612167, mean_q: 67.938255
  629836/1100000: episode: 1368, duration: 1.693s, episode steps: 291, steps per second: 172, episode reward: 227.211, mean reward: 0.781 [-12.183, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.201 [-0.858, 1.461], loss: 4.562732, mae: 50.707493, mean_q: 68.151031
  630582/1100000: episode: 1369, duration: 4.866s, episode steps: 746, steps per second: 153, episode reward: 228.238, mean reward: 0.306 [-17.515, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.001 [-0.768, 1.434], loss: 7.345043, mae: 51.034660, mean_q: 68.387047
  630809/1100000: episode: 1370, duration: 1.317s, episode steps: 227, steps per second: 172, episode reward: 274.838, mean reward: 1.211 [-4.219, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.076 [-0.877, 1.418], loss: 5.791297, mae: 50.691498, mean_q: 68.078789
  631208/1100000: episode: 1371, duration: 2.420s, episode steps: 399, steps per second: 165, episode reward: 228.725, mean reward: 0.573 [-17.479, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.086 [-0.666, 1.403], loss: 5.376912, mae: 50.243694, mean_q: 67.568459
  631779/1100000: episode: 1372, duration: 3.524s, episode steps: 571, steps per second: 162, episode reward: 245.431, mean reward: 0.430 [-21.297, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: 0.014 [-0.805, 1.390], loss: 5.848896, mae: 50.557636, mean_q: 67.902321
  632058/1100000: episode: 1373, duration: 1.657s, episode steps: 279, steps per second: 168, episode reward: 255.204, mean reward: 0.915 [-3.835, 100.000], mean action: 2.029 [0.000, 3.000], mean observation: 0.113 [-0.810, 1.411], loss: 6.040651, mae: 50.394432, mean_q: 67.578979
  632431/1100000: episode: 1374, duration: 2.246s, episode steps: 373, steps per second: 166, episode reward: 228.364, mean reward: 0.612 [-17.765, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.119 [-0.586, 1.445], loss: 6.266020, mae: 50.351616, mean_q: 67.816116
  632881/1100000: episode: 1375, duration: 2.634s, episode steps: 450, steps per second: 171, episode reward: 47.294, mean reward: 0.105 [-100.000, 14.828], mean action: 1.280 [0.000, 3.000], mean observation: 0.012 [-0.703, 1.527], loss: 7.523606, mae: 49.719135, mean_q: 66.757858
  633522/1100000: episode: 1376, duration: 3.809s, episode steps: 641, steps per second: 168, episode reward: 50.319, mean reward: 0.079 [-100.000, 12.636], mean action: 1.250 [0.000, 3.000], mean observation: 0.077 [-0.714, 1.472], loss: 5.694369, mae: 49.712490, mean_q: 66.731667
  633645/1100000: episode: 1377, duration: 0.701s, episode steps: 123, steps per second: 176, episode reward: 23.268, mean reward: 0.189 [-100.000, 21.415], mean action: 1.772 [0.000, 3.000], mean observation: 0.160 [-1.517, 1.455], loss: 5.440068, mae: 50.080360, mean_q: 67.178299
  634529/1100000: episode: 1378, duration: 5.520s, episode steps: 884, steps per second: 160, episode reward: 167.809, mean reward: 0.190 [-17.483, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: 0.035 [-0.747, 1.494], loss: 7.055870, mae: 49.645157, mean_q: 66.600395
  635529/1100000: episode: 1379, duration: 6.360s, episode steps: 1000, steps per second: 157, episode reward: 74.873, mean reward: 0.075 [-18.773, 22.418], mean action: 1.966 [0.000, 3.000], mean observation: 0.138 [-0.858, 1.409], loss: 5.875576, mae: 49.608585, mean_q: 66.404503
  635901/1100000: episode: 1380, duration: 2.229s, episode steps: 372, steps per second: 167, episode reward: 237.117, mean reward: 0.637 [-19.589, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.171 [-1.051, 1.489], loss: 7.262972, mae: 49.598637, mean_q: 66.266716
  635997/1100000: episode: 1381, duration: 0.546s, episode steps: 96, steps per second: 176, episode reward: -51.553, mean reward: -0.537 [-100.000, 13.477], mean action: 1.312 [0.000, 3.000], mean observation: 0.123 [-0.825, 1.406], loss: 6.296996, mae: 49.696167, mean_q: 66.643417
  636239/1100000: episode: 1382, duration: 1.425s, episode steps: 242, steps per second: 170, episode reward: 281.110, mean reward: 1.162 [-4.467, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.088 [-0.820, 1.481], loss: 7.554089, mae: 49.302662, mean_q: 65.734947
  636389/1100000: episode: 1383, duration: 0.854s, episode steps: 150, steps per second: 176, episode reward: -5.974, mean reward: -0.040 [-100.000, 11.270], mean action: 1.193 [0.000, 3.000], mean observation: 0.111 [-1.019, 1.844], loss: 8.045075, mae: 49.983177, mean_q: 66.554527
  636548/1100000: episode: 1384, duration: 0.918s, episode steps: 159, steps per second: 173, episode reward: -180.948, mean reward: -1.138 [-100.000, 66.256], mean action: 1.509 [0.000, 3.000], mean observation: 0.116 [-1.019, 2.162], loss: 9.506383, mae: 49.479107, mean_q: 65.512367
  636713/1100000: episode: 1385, duration: 0.949s, episode steps: 165, steps per second: 174, episode reward: -151.442, mean reward: -0.918 [-100.000, 29.176], mean action: 1.855 [0.000, 3.000], mean observation: -0.036 [-1.674, 1.396], loss: 8.016156, mae: 50.393822, mean_q: 67.501488
  636978/1100000: episode: 1386, duration: 1.538s, episode steps: 265, steps per second: 172, episode reward: 288.952, mean reward: 1.090 [-18.380, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: -0.050 [-0.771, 1.386], loss: 6.734660, mae: 49.818550, mean_q: 66.362190
  637179/1100000: episode: 1387, duration: 1.150s, episode steps: 201, steps per second: 175, episode reward: 247.220, mean reward: 1.230 [-8.712, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.154 [-1.002, 1.407], loss: 8.365459, mae: 49.475437, mean_q: 66.219772
  637441/1100000: episode: 1388, duration: 1.537s, episode steps: 262, steps per second: 170, episode reward: 254.159, mean reward: 0.970 [-11.018, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.075 [-0.787, 1.399], loss: 5.524190, mae: 49.815094, mean_q: 66.775909
  637697/1100000: episode: 1389, duration: 1.503s, episode steps: 256, steps per second: 170, episode reward: 278.375, mean reward: 1.087 [-8.520, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.080 [-0.668, 1.393], loss: 28.681454, mae: 50.051819, mean_q: 67.414474
  637885/1100000: episode: 1390, duration: 1.080s, episode steps: 188, steps per second: 174, episode reward: 270.772, mean reward: 1.440 [-9.420, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: -0.078 [-0.790, 1.391], loss: 7.681289, mae: 50.348728, mean_q: 67.674957
  638171/1100000: episode: 1391, duration: 1.684s, episode steps: 286, steps per second: 170, episode reward: 189.240, mean reward: 0.662 [-20.946, 100.000], mean action: 2.206 [0.000, 3.000], mean observation: 0.083 [-1.126, 1.385], loss: 5.639968, mae: 50.018551, mean_q: 67.356812
  638361/1100000: episode: 1392, duration: 1.075s, episode steps: 190, steps per second: 177, episode reward: 251.390, mean reward: 1.323 [-10.585, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: -0.057 [-0.991, 1.471], loss: 5.057179, mae: 49.890244, mean_q: 67.151344
  638813/1100000: episode: 1393, duration: 2.845s, episode steps: 452, steps per second: 159, episode reward: 211.465, mean reward: 0.468 [-19.239, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: 0.012 [-0.787, 1.414], loss: 11.347013, mae: 50.517761, mean_q: 67.217499
  639013/1100000: episode: 1394, duration: 1.160s, episode steps: 200, steps per second: 172, episode reward: -55.559, mean reward: -0.278 [-100.000, 7.379], mean action: 1.685 [0.000, 3.000], mean observation: 0.080 [-0.890, 1.401], loss: 6.852756, mae: 50.120262, mean_q: 67.255768
  639259/1100000: episode: 1395, duration: 1.462s, episode steps: 246, steps per second: 168, episode reward: 287.069, mean reward: 1.167 [-9.735, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.102 [-1.047, 1.386], loss: 11.390877, mae: 50.683640, mean_q: 67.716476
  639361/1100000: episode: 1396, duration: 0.590s, episode steps: 102, steps per second: 173, episode reward: -9.690, mean reward: -0.095 [-100.000, 16.987], mean action: 1.833 [0.000, 3.000], mean observation: 0.039 [-0.963, 1.636], loss: 4.224930, mae: 50.536915, mean_q: 67.989868
  639765/1100000: episode: 1397, duration: 2.386s, episode steps: 404, steps per second: 169, episode reward: 229.687, mean reward: 0.569 [-18.014, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.217 [-0.922, 1.407], loss: 8.013010, mae: 49.958519, mean_q: 66.890442
  640299/1100000: episode: 1398, duration: 3.181s, episode steps: 534, steps per second: 168, episode reward: 225.202, mean reward: 0.422 [-21.936, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.200 [-0.800, 1.439], loss: 6.220965, mae: 50.007538, mean_q: 66.951920
  640586/1100000: episode: 1399, duration: 1.680s, episode steps: 287, steps per second: 171, episode reward: 266.736, mean reward: 0.929 [-10.023, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.110 [-0.977, 1.391], loss: 7.484591, mae: 50.183640, mean_q: 67.246513
  640814/1100000: episode: 1400, duration: 1.321s, episode steps: 228, steps per second: 173, episode reward: 246.966, mean reward: 1.083 [-3.501, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: 0.176 [-0.848, 1.459], loss: 5.161934, mae: 49.926167, mean_q: 66.698273
  641151/1100000: episode: 1401, duration: 2.078s, episode steps: 337, steps per second: 162, episode reward: 224.147, mean reward: 0.665 [-11.591, 100.000], mean action: 1.864 [0.000, 3.000], mean observation: 0.143 [-0.765, 1.429], loss: 6.131474, mae: 50.190277, mean_q: 67.027405
  641397/1100000: episode: 1402, duration: 1.429s, episode steps: 246, steps per second: 172, episode reward: 245.605, mean reward: 0.998 [-10.396, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: -0.070 [-0.961, 1.524], loss: 5.114448, mae: 49.986916, mean_q: 67.066994
  641815/1100000: episode: 1403, duration: 2.568s, episode steps: 418, steps per second: 163, episode reward: 264.704, mean reward: 0.633 [-18.189, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.215 [-0.732, 1.414], loss: 8.296078, mae: 50.029850, mean_q: 67.087944
  642040/1100000: episode: 1404, duration: 1.383s, episode steps: 225, steps per second: 163, episode reward: 204.172, mean reward: 0.907 [-16.424, 100.000], mean action: 1.684 [0.000, 3.000], mean observation: -0.022 [-1.034, 1.408], loss: 4.356627, mae: 50.021538, mean_q: 67.048378
  642325/1100000: episode: 1405, duration: 1.701s, episode steps: 285, steps per second: 168, episode reward: 232.891, mean reward: 0.817 [-18.518, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.127 [-0.810, 1.386], loss: 8.973465, mae: 50.320835, mean_q: 67.573311
  642700/1100000: episode: 1406, duration: 2.216s, episode steps: 375, steps per second: 169, episode reward: 225.703, mean reward: 0.602 [-18.003, 100.000], mean action: 2.187 [0.000, 3.000], mean observation: 0.069 [-0.918, 1.471], loss: 10.145824, mae: 50.256168, mean_q: 67.308807
  643385/1100000: episode: 1407, duration: 4.119s, episode steps: 685, steps per second: 166, episode reward: 183.380, mean reward: 0.268 [-14.807, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.181 [-0.574, 1.420], loss: 6.793690, mae: 49.893940, mean_q: 66.995247
  643726/1100000: episode: 1408, duration: 2.060s, episode steps: 341, steps per second: 166, episode reward: 277.978, mean reward: 0.815 [-22.831, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.002 [-0.966, 1.386], loss: 10.072037, mae: 49.609230, mean_q: 66.992920
  643957/1100000: episode: 1409, duration: 1.351s, episode steps: 231, steps per second: 171, episode reward: 216.725, mean reward: 0.938 [-13.855, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: -0.021 [-0.744, 1.400], loss: 6.541221, mae: 49.776169, mean_q: 67.079781
  644957/1100000: episode: 1410, duration: 6.312s, episode steps: 1000, steps per second: 158, episode reward: 133.472, mean reward: 0.133 [-19.618, 12.289], mean action: 0.962 [0.000, 3.000], mean observation: 0.017 [-0.969, 1.388], loss: 6.970369, mae: 50.076073, mean_q: 67.388672
  645652/1100000: episode: 1411, duration: 4.420s, episode steps: 695, steps per second: 157, episode reward: 223.365, mean reward: 0.321 [-20.207, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: -0.018 [-0.693, 1.491], loss: 10.788448, mae: 49.547810, mean_q: 66.742027
  646260/1100000: episode: 1412, duration: 3.913s, episode steps: 608, steps per second: 155, episode reward: 230.255, mean reward: 0.379 [-18.540, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.102 [-0.706, 1.414], loss: 5.764723, mae: 49.360046, mean_q: 66.555443
  646557/1100000: episode: 1413, duration: 1.775s, episode steps: 297, steps per second: 167, episode reward: 201.694, mean reward: 0.679 [-10.517, 100.000], mean action: 2.559 [0.000, 3.000], mean observation: -0.070 [-0.869, 1.387], loss: 17.814617, mae: 49.536358, mean_q: 66.500565
  646980/1100000: episode: 1414, duration: 2.610s, episode steps: 423, steps per second: 162, episode reward: 216.366, mean reward: 0.512 [-11.242, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.071 [-0.677, 1.419], loss: 6.539580, mae: 49.393410, mean_q: 66.668686
  647410/1100000: episode: 1415, duration: 2.642s, episode steps: 430, steps per second: 163, episode reward: 236.608, mean reward: 0.550 [-19.428, 100.000], mean action: 2.491 [0.000, 3.000], mean observation: 0.203 [-0.719, 1.412], loss: 6.457257, mae: 49.087597, mean_q: 66.228935
  647752/1100000: episode: 1416, duration: 2.025s, episode steps: 342, steps per second: 169, episode reward: 253.730, mean reward: 0.742 [-9.275, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: -0.045 [-0.743, 1.402], loss: 10.055296, mae: 49.222370, mean_q: 66.502785
  648074/1100000: episode: 1417, duration: 1.919s, episode steps: 322, steps per second: 168, episode reward: 223.982, mean reward: 0.696 [-12.321, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.041 [-0.796, 1.450], loss: 8.644099, mae: 48.782780, mean_q: 65.980858
  648409/1100000: episode: 1418, duration: 1.964s, episode steps: 335, steps per second: 171, episode reward: -26.204, mean reward: -0.078 [-100.000, 13.299], mean action: 1.612 [0.000, 3.000], mean observation: -0.034 [-0.620, 1.400], loss: 6.076198, mae: 49.125263, mean_q: 66.360229
  648515/1100000: episode: 1419, duration: 0.608s, episode steps: 106, steps per second: 174, episode reward: -31.214, mean reward: -0.294 [-100.000, 10.579], mean action: 1.811 [0.000, 3.000], mean observation: 0.068 [-0.855, 1.441], loss: 6.558276, mae: 49.007393, mean_q: 66.182167
  648971/1100000: episode: 1420, duration: 2.760s, episode steps: 456, steps per second: 165, episode reward: 223.478, mean reward: 0.490 [-18.921, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.012 [-1.126, 1.407], loss: 7.326163, mae: 49.126637, mean_q: 66.404648
  649285/1100000: episode: 1421, duration: 1.914s, episode steps: 314, steps per second: 164, episode reward: 280.519, mean reward: 0.893 [-17.643, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.113 [-0.778, 1.389], loss: 10.240007, mae: 48.838829, mean_q: 65.967438
  649599/1100000: episode: 1422, duration: 1.858s, episode steps: 314, steps per second: 169, episode reward: 234.148, mean reward: 0.746 [-17.306, 100.000], mean action: 1.768 [0.000, 3.000], mean observation: 0.061 [-0.819, 1.401], loss: 4.690997, mae: 48.770283, mean_q: 65.926506
  649986/1100000: episode: 1423, duration: 2.273s, episode steps: 387, steps per second: 170, episode reward: 213.008, mean reward: 0.550 [-14.106, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.158 [-0.809, 1.407], loss: 6.317355, mae: 48.932625, mean_q: 66.059692
  650798/1100000: episode: 1424, duration: 5.150s, episode steps: 812, steps per second: 158, episode reward: 236.345, mean reward: 0.291 [-21.087, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.051 [-0.759, 1.521], loss: 13.431212, mae: 48.684444, mean_q: 65.729012
  650903/1100000: episode: 1425, duration: 0.604s, episode steps: 105, steps per second: 174, episode reward: -27.367, mean reward: -0.261 [-100.000, 19.238], mean action: 1.914 [0.000, 3.000], mean observation: -0.096 [-1.633, 1.412], loss: 8.130970, mae: 48.040585, mean_q: 64.967720
  651294/1100000: episode: 1426, duration: 2.372s, episode steps: 391, steps per second: 165, episode reward: 230.889, mean reward: 0.591 [-20.711, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.007 [-0.743, 1.410], loss: 6.524559, mae: 48.950298, mean_q: 66.045578
  651517/1100000: episode: 1427, duration: 1.314s, episode steps: 223, steps per second: 170, episode reward: 233.831, mean reward: 1.049 [-11.418, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.076 [-0.736, 1.412], loss: 6.825406, mae: 48.189606, mean_q: 65.053261
  652517/1100000: episode: 1428, duration: 7.086s, episode steps: 1000, steps per second: 141, episode reward: -55.718, mean reward: -0.056 [-6.734, 5.710], mean action: 1.638 [0.000, 3.000], mean observation: -0.052 [-0.813, 1.421], loss: 5.691453, mae: 48.287727, mean_q: 65.188583
  652594/1100000: episode: 1429, duration: 0.441s, episode steps: 77, steps per second: 174, episode reward: -143.410, mean reward: -1.862 [-100.000, 11.252], mean action: 1.987 [0.000, 3.000], mean observation: -0.056 [-1.386, 1.402], loss: 3.286048, mae: 48.161034, mean_q: 65.075592
  653127/1100000: episode: 1430, duration: 3.148s, episode steps: 533, steps per second: 169, episode reward: 231.838, mean reward: 0.435 [-18.587, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.203 [-0.945, 1.402], loss: 6.396932, mae: 47.785885, mean_q: 64.339073
  653663/1100000: episode: 1431, duration: 3.345s, episode steps: 536, steps per second: 160, episode reward: -112.088, mean reward: -0.209 [-100.000, 14.835], mean action: 1.823 [0.000, 3.000], mean observation: -0.015 [-0.761, 1.391], loss: 16.697275, mae: 47.799259, mean_q: 64.337906
  653974/1100000: episode: 1432, duration: 1.801s, episode steps: 311, steps per second: 173, episode reward: 292.912, mean reward: 0.942 [-11.086, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: 0.070 [-0.703, 1.425], loss: 5.287118, mae: 46.922668, mean_q: 63.270134
  654753/1100000: episode: 1433, duration: 5.046s, episode steps: 779, steps per second: 154, episode reward: 100.072, mean reward: 0.128 [-18.747, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: -0.053 [-0.734, 1.410], loss: 6.209853, mae: 47.381470, mean_q: 63.834473
  655029/1100000: episode: 1434, duration: 1.642s, episode steps: 276, steps per second: 168, episode reward: 195.228, mean reward: 0.707 [-19.446, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.067 [-0.868, 1.407], loss: 5.211473, mae: 47.287083, mean_q: 63.747807
  656029/1100000: episode: 1435, duration: 6.536s, episode steps: 1000, steps per second: 153, episode reward: 142.469, mean reward: 0.142 [-21.501, 23.177], mean action: 0.854 [0.000, 3.000], mean observation: 0.269 [-0.829, 1.388], loss: 8.675241, mae: 47.256962, mean_q: 63.618984
  656414/1100000: episode: 1436, duration: 2.287s, episode steps: 385, steps per second: 168, episode reward: 277.749, mean reward: 0.721 [-8.077, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.092 [-0.802, 1.440], loss: 4.393210, mae: 46.589455, mean_q: 62.774258
  657414/1100000: episode: 1437, duration: 6.686s, episode steps: 1000, steps per second: 150, episode reward: 109.963, mean reward: 0.110 [-18.597, 15.042], mean action: 1.250 [0.000, 3.000], mean observation: 0.235 [-0.632, 1.389], loss: 7.404888, mae: 46.421047, mean_q: 62.578758
  657695/1100000: episode: 1438, duration: 1.657s, episode steps: 281, steps per second: 170, episode reward: 226.641, mean reward: 0.807 [-7.461, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.159 [-0.648, 1.403], loss: 6.198409, mae: 46.693333, mean_q: 62.969070
  657948/1100000: episode: 1439, duration: 1.477s, episode steps: 253, steps per second: 171, episode reward: -246.542, mean reward: -0.974 [-100.000, 9.423], mean action: 1.621 [0.000, 3.000], mean observation: -0.170 [-1.760, 1.407], loss: 6.037640, mae: 46.821590, mean_q: 63.291103
  658697/1100000: episode: 1440, duration: 4.541s, episode steps: 749, steps per second: 165, episode reward: 237.195, mean reward: 0.317 [-19.487, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.221 [-0.781, 1.481], loss: 5.990289, mae: 46.721085, mean_q: 63.207027
  658776/1100000: episode: 1441, duration: 0.453s, episode steps: 79, steps per second: 174, episode reward: -301.187, mean reward: -3.812 [-100.000, 1.854], mean action: 1.506 [0.000, 3.000], mean observation: 0.076 [-1.763, 1.642], loss: 8.746271, mae: 46.755753, mean_q: 63.137199
  659019/1100000: episode: 1442, duration: 1.409s, episode steps: 243, steps per second: 172, episode reward: 250.443, mean reward: 1.031 [-17.385, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.079 [-0.827, 1.399], loss: 8.721483, mae: 46.687252, mean_q: 62.998306
  659292/1100000: episode: 1443, duration: 1.598s, episode steps: 273, steps per second: 171, episode reward: 204.733, mean reward: 0.750 [-17.330, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.190 [-0.732, 1.410], loss: 8.758291, mae: 47.312836, mean_q: 63.598515
  660068/1100000: episode: 1444, duration: 5.057s, episode steps: 776, steps per second: 153, episode reward: 142.947, mean reward: 0.184 [-12.390, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: -0.045 [-0.705, 1.400], loss: 7.103427, mae: 46.609150, mean_q: 62.853436
  660356/1100000: episode: 1445, duration: 1.687s, episode steps: 288, steps per second: 171, episode reward: 274.209, mean reward: 0.952 [-17.678, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.092 [-0.667, 1.492], loss: 4.254799, mae: 46.554348, mean_q: 62.709309
  660569/1100000: episode: 1446, duration: 1.224s, episode steps: 213, steps per second: 174, episode reward: -109.624, mean reward: -0.515 [-100.000, 11.531], mean action: 1.592 [0.000, 3.000], mean observation: -0.073 [-0.772, 2.288], loss: 5.257421, mae: 47.031197, mean_q: 63.553669
  661351/1100000: episode: 1447, duration: 5.397s, episode steps: 782, steps per second: 145, episode reward: 178.058, mean reward: 0.228 [-19.350, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.009 [-0.758, 1.450], loss: 5.587852, mae: 46.560600, mean_q: 62.819355
  661955/1100000: episode: 1448, duration: 3.751s, episode steps: 604, steps per second: 161, episode reward: 217.228, mean reward: 0.360 [-23.384, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.222 [-0.691, 1.409], loss: 6.307661, mae: 46.840958, mean_q: 63.264862
  662955/1100000: episode: 1449, duration: 6.370s, episode steps: 1000, steps per second: 157, episode reward: 71.826, mean reward: 0.072 [-17.167, 16.511], mean action: 1.708 [0.000, 3.000], mean observation: -0.001 [-0.600, 1.419], loss: 7.078091, mae: 46.469978, mean_q: 62.718086
  663210/1100000: episode: 1450, duration: 1.486s, episode steps: 255, steps per second: 172, episode reward: 198.258, mean reward: 0.777 [-10.333, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: -0.087 [-0.872, 1.420], loss: 6.019505, mae: 46.681400, mean_q: 63.074833
  663534/1100000: episode: 1451, duration: 1.906s, episode steps: 324, steps per second: 170, episode reward: 213.490, mean reward: 0.659 [-7.353, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.200 [-0.531, 1.397], loss: 7.813505, mae: 46.135387, mean_q: 62.321747
  663922/1100000: episode: 1452, duration: 2.320s, episode steps: 388, steps per second: 167, episode reward: 260.153, mean reward: 0.670 [-17.589, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.124 [-0.889, 1.470], loss: 5.811581, mae: 46.262360, mean_q: 62.466557
  664160/1100000: episode: 1453, duration: 1.370s, episode steps: 238, steps per second: 174, episode reward: 269.894, mean reward: 1.134 [-9.454, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.050 [-0.852, 1.568], loss: 8.020391, mae: 46.347099, mean_q: 62.497433
  665160/1100000: episode: 1454, duration: 6.294s, episode steps: 1000, steps per second: 159, episode reward: 115.848, mean reward: 0.116 [-20.283, 21.123], mean action: 1.321 [0.000, 3.000], mean observation: 0.142 [-0.786, 1.388], loss: 7.024424, mae: 46.759216, mean_q: 63.076149
  665396/1100000: episode: 1455, duration: 1.360s, episode steps: 236, steps per second: 174, episode reward: 225.165, mean reward: 0.954 [-7.363, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.103 [-0.773, 1.410], loss: 8.184908, mae: 46.162209, mean_q: 62.194073
  665630/1100000: episode: 1456, duration: 1.353s, episode steps: 234, steps per second: 173, episode reward: 222.705, mean reward: 0.952 [-17.839, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.183 [-0.686, 1.405], loss: 5.265723, mae: 46.533836, mean_q: 62.868023
  666146/1100000: episode: 1457, duration: 3.190s, episode steps: 516, steps per second: 162, episode reward: 254.531, mean reward: 0.493 [-11.533, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.112 [-0.739, 1.462], loss: 5.055429, mae: 46.878166, mean_q: 63.301464
  666667/1100000: episode: 1458, duration: 3.182s, episode steps: 521, steps per second: 164, episode reward: 198.983, mean reward: 0.382 [-19.384, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.017 [-0.850, 1.412], loss: 7.398774, mae: 46.752857, mean_q: 62.982208
  667003/1100000: episode: 1459, duration: 2.010s, episode steps: 336, steps per second: 167, episode reward: 233.905, mean reward: 0.696 [-18.069, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: -0.035 [-0.890, 1.389], loss: 6.928002, mae: 46.016804, mean_q: 61.947823
  667480/1100000: episode: 1460, duration: 2.908s, episode steps: 477, steps per second: 164, episode reward: -141.544, mean reward: -0.297 [-100.000, 14.427], mean action: 1.658 [0.000, 3.000], mean observation: -0.144 [-1.001, 1.408], loss: 5.804537, mae: 46.124676, mean_q: 62.194195
  667912/1100000: episode: 1461, duration: 2.652s, episode steps: 432, steps per second: 163, episode reward: 270.492, mean reward: 0.626 [-19.088, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.126 [-0.905, 1.531], loss: 8.213435, mae: 46.495613, mean_q: 62.663593
  668161/1100000: episode: 1462, duration: 1.439s, episode steps: 249, steps per second: 173, episode reward: 197.090, mean reward: 0.792 [-3.250, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: -0.029 [-0.688, 1.412], loss: 6.105893, mae: 46.499908, mean_q: 62.734547
  668832/1100000: episode: 1463, duration: 4.172s, episode steps: 671, steps per second: 161, episode reward: 191.554, mean reward: 0.285 [-18.169, 100.000], mean action: 1.900 [0.000, 3.000], mean observation: 0.220 [-0.560, 1.409], loss: 6.080713, mae: 46.104595, mean_q: 62.298317
  669611/1100000: episode: 1464, duration: 4.779s, episode steps: 779, steps per second: 163, episode reward: 280.074, mean reward: 0.360 [-19.064, 100.000], mean action: 0.701 [0.000, 3.000], mean observation: 0.141 [-0.892, 1.469], loss: 6.621349, mae: 46.353989, mean_q: 62.544041
  669790/1100000: episode: 1465, duration: 1.030s, episode steps: 179, steps per second: 174, episode reward: 229.253, mean reward: 1.281 [-2.702, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.209 [-0.803, 1.400], loss: 9.196261, mae: 46.492844, mean_q: 62.653461
  670059/1100000: episode: 1466, duration: 1.559s, episode steps: 269, steps per second: 173, episode reward: 233.179, mean reward: 0.867 [-8.136, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: 0.182 [-0.707, 1.494], loss: 7.390911, mae: 46.032722, mean_q: 62.227020
  670784/1100000: episode: 1467, duration: 4.579s, episode steps: 725, steps per second: 158, episode reward: 186.329, mean reward: 0.257 [-18.545, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.010 [-0.734, 1.396], loss: 5.949426, mae: 46.443523, mean_q: 62.679558
  670899/1100000: episode: 1468, duration: 0.704s, episode steps: 115, steps per second: 163, episode reward: -88.710, mean reward: -0.771 [-100.000, 5.248], mean action: 1.661 [0.000, 3.000], mean observation: 0.119 [-0.930, 3.252], loss: 4.927107, mae: 45.561512, mean_q: 61.543190
  671128/1100000: episode: 1469, duration: 1.342s, episode steps: 229, steps per second: 171, episode reward: 241.187, mean reward: 1.053 [-8.999, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.070 [-0.818, 1.406], loss: 4.793916, mae: 46.543148, mean_q: 62.800697
  671414/1100000: episode: 1470, duration: 1.693s, episode steps: 286, steps per second: 169, episode reward: 280.333, mean reward: 0.980 [-2.248, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.047 [-0.748, 1.407], loss: 5.441614, mae: 46.290142, mean_q: 62.430424
  671736/1100000: episode: 1471, duration: 1.933s, episode steps: 322, steps per second: 167, episode reward: 271.343, mean reward: 0.843 [-18.055, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.084 [-0.595, 1.533], loss: 4.903865, mae: 46.311501, mean_q: 62.594975
  672007/1100000: episode: 1472, duration: 1.572s, episode steps: 271, steps per second: 172, episode reward: 240.616, mean reward: 0.888 [-18.912, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: -0.014 [-0.817, 1.393], loss: 4.538099, mae: 46.078064, mean_q: 62.257011
  672303/1100000: episode: 1473, duration: 1.751s, episode steps: 296, steps per second: 169, episode reward: 240.388, mean reward: 0.812 [-8.703, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: -0.024 [-0.652, 1.391], loss: 6.755796, mae: 45.899673, mean_q: 61.968960
  672687/1100000: episode: 1474, duration: 2.356s, episode steps: 384, steps per second: 163, episode reward: 261.502, mean reward: 0.681 [-17.493, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: -0.005 [-0.731, 1.443], loss: 7.281637, mae: 46.005325, mean_q: 62.183155
  673248/1100000: episode: 1475, duration: 3.559s, episode steps: 561, steps per second: 158, episode reward: 257.757, mean reward: 0.459 [-22.313, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.185 [-0.768, 1.420], loss: 6.831618, mae: 46.139038, mean_q: 62.260311
  673335/1100000: episode: 1476, duration: 0.501s, episode steps: 87, steps per second: 174, episode reward: -1.722, mean reward: -0.020 [-100.000, 11.035], mean action: 1.575 [0.000, 3.000], mean observation: 0.064 [-0.903, 1.793], loss: 3.100397, mae: 46.738285, mean_q: 63.202885
  673564/1100000: episode: 1477, duration: 1.326s, episode steps: 229, steps per second: 173, episode reward: -62.311, mean reward: -0.272 [-100.000, 10.421], mean action: 1.629 [0.000, 3.000], mean observation: 0.138 [-0.726, 2.635], loss: 4.347713, mae: 45.923031, mean_q: 61.961914
  674564/1100000: episode: 1478, duration: 6.401s, episode steps: 1000, steps per second: 156, episode reward: 152.845, mean reward: 0.153 [-18.029, 15.898], mean action: 0.940 [0.000, 3.000], mean observation: 0.043 [-0.796, 1.400], loss: 5.578778, mae: 46.248505, mean_q: 62.359261
  674866/1100000: episode: 1479, duration: 1.777s, episode steps: 302, steps per second: 170, episode reward: 236.435, mean reward: 0.783 [-19.702, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.085 [-0.779, 1.419], loss: 5.487880, mae: 45.875893, mean_q: 61.812435
  675764/1100000: episode: 1480, duration: 6.080s, episode steps: 898, steps per second: 148, episode reward: -246.809, mean reward: -0.275 [-100.000, 20.681], mean action: 1.284 [0.000, 3.000], mean observation: 0.130 [-1.621, 1.402], loss: 6.151622, mae: 45.870647, mean_q: 61.819496
  676243/1100000: episode: 1481, duration: 2.970s, episode steps: 479, steps per second: 161, episode reward: 242.210, mean reward: 0.506 [-17.748, 100.000], mean action: 1.925 [0.000, 3.000], mean observation: 0.246 [-0.763, 1.386], loss: 5.117261, mae: 45.783070, mean_q: 61.777328
  676548/1100000: episode: 1482, duration: 1.785s, episode steps: 305, steps per second: 171, episode reward: -171.136, mean reward: -0.561 [-100.000, 44.979], mean action: 1.702 [0.000, 3.000], mean observation: 0.052 [-1.231, 1.411], loss: 4.828550, mae: 45.160515, mean_q: 60.846977
  676763/1100000: episode: 1483, duration: 1.243s, episode steps: 215, steps per second: 173, episode reward: -240.978, mean reward: -1.121 [-100.000, 7.421], mean action: 1.712 [0.000, 3.000], mean observation: 0.178 [-0.970, 1.968], loss: 8.906812, mae: 45.772655, mean_q: 61.621838
  676907/1100000: episode: 1484, duration: 0.820s, episode steps: 144, steps per second: 176, episode reward: -120.899, mean reward: -0.840 [-100.000, 13.150], mean action: 1.493 [0.000, 3.000], mean observation: 0.033 [-1.208, 1.421], loss: 4.627444, mae: 45.497383, mean_q: 61.340034
  677226/1100000: episode: 1485, duration: 1.889s, episode steps: 319, steps per second: 169, episode reward: 287.385, mean reward: 0.901 [-17.397, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.088 [-0.716, 1.406], loss: 5.405856, mae: 45.663261, mean_q: 61.494133
  677718/1100000: episode: 1486, duration: 2.996s, episode steps: 492, steps per second: 164, episode reward: 221.578, mean reward: 0.450 [-17.552, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.017 [-0.674, 1.394], loss: 7.082030, mae: 45.366634, mean_q: 61.123997
  678029/1100000: episode: 1487, duration: 1.838s, episode steps: 311, steps per second: 169, episode reward: 289.540, mean reward: 0.931 [-8.924, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.086 [-0.863, 1.398], loss: 11.039719, mae: 45.507259, mean_q: 61.345684
  678266/1100000: episode: 1488, duration: 1.412s, episode steps: 237, steps per second: 168, episode reward: 265.225, mean reward: 1.119 [-9.842, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.162 [-0.786, 1.392], loss: 7.118181, mae: 45.203751, mean_q: 60.912231
  678641/1100000: episode: 1489, duration: 2.190s, episode steps: 375, steps per second: 171, episode reward: 207.853, mean reward: 0.554 [-18.805, 100.000], mean action: 1.933 [0.000, 3.000], mean observation: -0.032 [-0.723, 1.396], loss: 10.495255, mae: 44.982178, mean_q: 60.603397
  679188/1100000: episode: 1490, duration: 3.348s, episode steps: 547, steps per second: 163, episode reward: 290.239, mean reward: 0.531 [-18.967, 100.000], mean action: 0.814 [0.000, 3.000], mean observation: 0.114 [-1.000, 1.394], loss: 8.530369, mae: 45.378548, mean_q: 61.146996
  679395/1100000: episode: 1491, duration: 1.195s, episode steps: 207, steps per second: 173, episode reward: 246.528, mean reward: 1.191 [-11.213, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.069 [-0.854, 1.409], loss: 4.988605, mae: 44.706573, mean_q: 60.252617
  679625/1100000: episode: 1492, duration: 1.336s, episode steps: 230, steps per second: 172, episode reward: 241.873, mean reward: 1.052 [-9.851, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.063 [-0.831, 1.404], loss: 8.038568, mae: 45.266998, mean_q: 60.959446
  679880/1100000: episode: 1493, duration: 1.487s, episode steps: 255, steps per second: 171, episode reward: 258.670, mean reward: 1.014 [-10.619, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.063 [-0.706, 1.412], loss: 6.856420, mae: 44.854198, mean_q: 60.413845
  680014/1100000: episode: 1494, duration: 0.769s, episode steps: 134, steps per second: 174, episode reward: -461.692, mean reward: -3.445 [-100.000, 5.919], mean action: 1.836 [0.000, 3.000], mean observation: 0.057 [-2.459, 4.455], loss: 9.734652, mae: 44.932739, mean_q: 60.616741
  680230/1100000: episode: 1495, duration: 1.253s, episode steps: 216, steps per second: 172, episode reward: 207.755, mean reward: 0.962 [-9.791, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.146 [-1.027, 1.432], loss: 4.573988, mae: 45.097931, mean_q: 60.911518
  681065/1100000: episode: 1496, duration: 5.504s, episode steps: 835, steps per second: 152, episode reward: 197.984, mean reward: 0.237 [-18.924, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.220 [-0.681, 1.409], loss: 6.736345, mae: 45.179111, mean_q: 60.720997
  681448/1100000: episode: 1497, duration: 2.237s, episode steps: 383, steps per second: 171, episode reward: 289.868, mean reward: 0.757 [-18.381, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.122 [-0.781, 1.485], loss: 5.075932, mae: 44.907402, mean_q: 60.393581
  681854/1100000: episode: 1498, duration: 2.433s, episode steps: 406, steps per second: 167, episode reward: 253.613, mean reward: 0.625 [-21.811, 100.000], mean action: 0.830 [0.000, 3.000], mean observation: 0.105 [-0.699, 1.397], loss: 6.496391, mae: 45.016369, mean_q: 60.532139
  682072/1100000: episode: 1499, duration: 1.277s, episode steps: 218, steps per second: 171, episode reward: 275.647, mean reward: 1.264 [-9.540, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.203 [-0.918, 1.387], loss: 5.887455, mae: 45.330303, mean_q: 61.102673
  683072/1100000: episode: 1500, duration: 6.437s, episode steps: 1000, steps per second: 155, episode reward: 133.577, mean reward: 0.134 [-19.760, 23.755], mean action: 0.788 [0.000, 3.000], mean observation: 0.255 [-0.726, 1.522], loss: 6.363818, mae: 45.226616, mean_q: 60.784752
  683522/1100000: episode: 1501, duration: 2.690s, episode steps: 450, steps per second: 167, episode reward: 220.810, mean reward: 0.491 [-19.621, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.213 [-0.823, 1.468], loss: 6.977035, mae: 45.552391, mean_q: 61.343349
  683857/1100000: episode: 1502, duration: 1.994s, episode steps: 335, steps per second: 168, episode reward: 240.686, mean reward: 0.718 [-9.996, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.191 [-0.702, 1.442], loss: 6.399599, mae: 45.142345, mean_q: 60.718422
  684352/1100000: episode: 1503, duration: 3.046s, episode steps: 495, steps per second: 163, episode reward: 154.486, mean reward: 0.312 [-10.396, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.011 [-0.615, 1.405], loss: 4.718866, mae: 44.947586, mean_q: 60.521332
  684635/1100000: episode: 1504, duration: 1.636s, episode steps: 283, steps per second: 173, episode reward: 226.568, mean reward: 0.801 [-2.953, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: -0.070 [-0.683, 1.418], loss: 8.302602, mae: 44.965874, mean_q: 60.509048
  684939/1100000: episode: 1505, duration: 1.786s, episode steps: 304, steps per second: 170, episode reward: 250.525, mean reward: 0.824 [-19.611, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.220 [-0.850, 1.400], loss: 4.841167, mae: 44.624386, mean_q: 59.973003
  685633/1100000: episode: 1506, duration: 4.398s, episode steps: 694, steps per second: 158, episode reward: 216.425, mean reward: 0.312 [-19.632, 100.000], mean action: 1.996 [0.000, 3.000], mean observation: 0.178 [-0.916, 1.409], loss: 6.398117, mae: 44.806847, mean_q: 60.245880
  685917/1100000: episode: 1507, duration: 1.656s, episode steps: 284, steps per second: 172, episode reward: 261.158, mean reward: 0.920 [-8.017, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: -0.036 [-0.831, 1.409], loss: 5.538780, mae: 44.882137, mean_q: 60.116333
  686295/1100000: episode: 1508, duration: 2.265s, episode steps: 378, steps per second: 167, episode reward: 241.645, mean reward: 0.639 [-14.931, 100.000], mean action: 1.923 [0.000, 3.000], mean observation: 0.021 [-0.600, 1.389], loss: 5.249299, mae: 44.989029, mean_q: 60.563553
  686822/1100000: episode: 1509, duration: 3.215s, episode steps: 527, steps per second: 164, episode reward: 191.766, mean reward: 0.364 [-19.045, 100.000], mean action: 0.852 [0.000, 3.000], mean observation: -0.005 [-0.719, 1.408], loss: 5.051325, mae: 44.856644, mean_q: 60.257710
  687098/1100000: episode: 1510, duration: 1.624s, episode steps: 276, steps per second: 170, episode reward: 283.676, mean reward: 1.028 [-17.409, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: -0.000 [-0.874, 1.387], loss: 6.076380, mae: 44.804050, mean_q: 60.240086
  687466/1100000: episode: 1511, duration: 2.177s, episode steps: 368, steps per second: 169, episode reward: 221.307, mean reward: 0.601 [-12.135, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.202 [-0.873, 1.411], loss: 6.495352, mae: 44.817860, mean_q: 60.291691
  687738/1100000: episode: 1512, duration: 1.578s, episode steps: 272, steps per second: 172, episode reward: 242.968, mean reward: 0.893 [-6.067, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: -0.040 [-0.774, 1.399], loss: 7.667383, mae: 44.708599, mean_q: 60.104961
  687988/1100000: episode: 1513, duration: 1.485s, episode steps: 250, steps per second: 168, episode reward: -216.417, mean reward: -0.866 [-100.000, 10.102], mean action: 1.880 [0.000, 3.000], mean observation: -0.068 [-1.793, 1.394], loss: 5.640697, mae: 44.795277, mean_q: 60.256512
  688316/1100000: episode: 1514, duration: 1.949s, episode steps: 328, steps per second: 168, episode reward: 291.515, mean reward: 0.889 [-17.509, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.117 [-0.681, 1.411], loss: 6.805346, mae: 44.451725, mean_q: 59.881393
  688675/1100000: episode: 1515, duration: 2.216s, episode steps: 359, steps per second: 162, episode reward: 261.331, mean reward: 0.728 [-17.677, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.078 [-0.785, 1.395], loss: 8.417656, mae: 44.827984, mean_q: 60.075394
  689628/1100000: episode: 1516, duration: 6.323s, episode steps: 953, steps per second: 151, episode reward: 144.937, mean reward: 0.152 [-19.137, 100.000], mean action: 2.013 [0.000, 3.000], mean observation: 0.178 [-0.703, 1.392], loss: 7.398610, mae: 44.887814, mean_q: 60.150127
  690233/1100000: episode: 1517, duration: 3.753s, episode steps: 605, steps per second: 161, episode reward: 291.722, mean reward: 0.482 [-19.596, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.163 [-0.735, 1.436], loss: 5.564060, mae: 44.918247, mean_q: 60.027420
  690826/1100000: episode: 1518, duration: 3.764s, episode steps: 593, steps per second: 158, episode reward: 200.981, mean reward: 0.339 [-13.511, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.152 [-0.663, 1.402], loss: 5.938599, mae: 45.094414, mean_q: 60.223072
  691235/1100000: episode: 1519, duration: 2.413s, episode steps: 409, steps per second: 170, episode reward: 278.474, mean reward: 0.681 [-19.026, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.122 [-0.843, 1.416], loss: 7.774185, mae: 44.866707, mean_q: 60.005520
  691920/1100000: episode: 1520, duration: 3.947s, episode steps: 685, steps per second: 174, episode reward: 242.789, mean reward: 0.354 [-18.340, 100.000], mean action: 0.578 [0.000, 3.000], mean observation: 0.071 [-0.869, 1.394], loss: 6.386712, mae: 45.071869, mean_q: 60.272686
  692225/1100000: episode: 1521, duration: 1.793s, episode steps: 305, steps per second: 170, episode reward: 236.037, mean reward: 0.774 [-17.491, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.120 [-1.374, 1.414], loss: 5.194284, mae: 44.672344, mean_q: 60.005291
  692596/1100000: episode: 1522, duration: 2.162s, episode steps: 371, steps per second: 172, episode reward: 292.519, mean reward: 0.788 [-18.515, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.094 [-0.880, 1.463], loss: 5.130453, mae: 44.882977, mean_q: 59.950428
  693065/1100000: episode: 1523, duration: 2.726s, episode steps: 469, steps per second: 172, episode reward: 249.073, mean reward: 0.531 [-19.980, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.010 [-0.801, 1.491], loss: 5.505952, mae: 44.915073, mean_q: 60.132370
  693445/1100000: episode: 1524, duration: 2.203s, episode steps: 380, steps per second: 172, episode reward: 270.316, mean reward: 0.711 [-17.595, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.045 [-1.303, 1.386], loss: 5.770730, mae: 44.441898, mean_q: 59.656609
  693796/1100000: episode: 1525, duration: 2.078s, episode steps: 351, steps per second: 169, episode reward: 284.879, mean reward: 0.812 [-21.442, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.016 [-0.744, 1.395], loss: 5.687163, mae: 44.619774, mean_q: 59.856026
  694796/1100000: episode: 1526, duration: 6.916s, episode steps: 1000, steps per second: 145, episode reward: 80.687, mean reward: 0.081 [-19.103, 13.052], mean action: 1.817 [0.000, 3.000], mean observation: 0.192 [-0.651, 1.393], loss: 6.277359, mae: 44.990593, mean_q: 60.220875
  695139/1100000: episode: 1527, duration: 2.050s, episode steps: 343, steps per second: 167, episode reward: 276.726, mean reward: 0.807 [-19.658, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: -0.012 [-0.713, 1.393], loss: 6.579033, mae: 44.970066, mean_q: 60.152840
  695872/1100000: episode: 1528, duration: 4.798s, episode steps: 733, steps per second: 153, episode reward: 226.024, mean reward: 0.308 [-19.042, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.023 [-0.693, 1.435], loss: 5.268348, mae: 44.848579, mean_q: 60.226318
  696123/1100000: episode: 1529, duration: 1.458s, episode steps: 251, steps per second: 172, episode reward: 221.652, mean reward: 0.883 [-10.767, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.158 [-0.881, 1.420], loss: 4.672420, mae: 44.452827, mean_q: 59.749306
  696530/1100000: episode: 1530, duration: 2.542s, episode steps: 407, steps per second: 160, episode reward: 288.851, mean reward: 0.710 [-17.081, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.044 [-0.829, 1.386], loss: 5.980206, mae: 44.649944, mean_q: 59.762577
  696840/1100000: episode: 1531, duration: 1.817s, episode steps: 310, steps per second: 171, episode reward: 231.382, mean reward: 0.746 [-13.711, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.153 [-0.619, 1.404], loss: 6.412918, mae: 44.712563, mean_q: 59.764473
  697080/1100000: episode: 1532, duration: 1.399s, episode steps: 240, steps per second: 172, episode reward: 271.843, mean reward: 1.133 [-17.426, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.066 [-0.729, 1.499], loss: 7.237534, mae: 44.775684, mean_q: 59.910801
  697523/1100000: episode: 1533, duration: 2.669s, episode steps: 443, steps per second: 166, episode reward: 227.014, mean reward: 0.512 [-18.437, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.143 [-0.812, 1.403], loss: 8.441451, mae: 44.791912, mean_q: 59.964966
  698308/1100000: episode: 1534, duration: 4.979s, episode steps: 785, steps per second: 158, episode reward: 228.276, mean reward: 0.291 [-19.070, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.213 [-0.760, 1.430], loss: 7.078120, mae: 45.098648, mean_q: 60.353168
  698594/1100000: episode: 1535, duration: 1.665s, episode steps: 286, steps per second: 172, episode reward: 303.886, mean reward: 1.063 [-10.544, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.121 [-0.655, 1.506], loss: 4.638022, mae: 44.762100, mean_q: 59.894444
  698869/1100000: episode: 1536, duration: 1.619s, episode steps: 275, steps per second: 170, episode reward: 252.878, mean reward: 0.920 [-19.386, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.113 [-0.750, 1.402], loss: 4.106287, mae: 45.200428, mean_q: 60.593075
  699107/1100000: episode: 1537, duration: 1.366s, episode steps: 238, steps per second: 174, episode reward: 280.875, mean reward: 1.180 [-7.207, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.132 [-0.776, 1.451], loss: 7.683587, mae: 45.487980, mean_q: 60.893269
  699357/1100000: episode: 1538, duration: 1.439s, episode steps: 250, steps per second: 174, episode reward: 245.369, mean reward: 0.981 [-17.586, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.098 [-0.760, 1.426], loss: 7.124683, mae: 46.085625, mean_q: 61.512524
  699436/1100000: episode: 1539, duration: 0.456s, episode steps: 79, steps per second: 173, episode reward: -41.195, mean reward: -0.521 [-100.000, 14.007], mean action: 1.987 [0.000, 3.000], mean observation: 0.011 [-0.875, 1.391], loss: 16.062929, mae: 45.806370, mean_q: 61.112133
  699537/1100000: episode: 1540, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 7.556, mean reward: 0.075 [-100.000, 10.240], mean action: 1.842 [0.000, 3.000], mean observation: -0.072 [-1.197, 1.388], loss: 5.045317, mae: 45.077766, mean_q: 60.520676
  699736/1100000: episode: 1541, duration: 1.153s, episode steps: 199, steps per second: 173, episode reward: 237.476, mean reward: 1.193 [-9.386, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: -0.088 [-0.961, 1.397], loss: 5.266536, mae: 46.128986, mean_q: 61.896790
  700019/1100000: episode: 1542, duration: 1.642s, episode steps: 283, steps per second: 172, episode reward: 271.643, mean reward: 0.960 [-10.211, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.144 [-0.687, 1.432], loss: 4.256533, mae: 46.098278, mean_q: 61.990719
  701019/1100000: episode: 1543, duration: 6.626s, episode steps: 1000, steps per second: 151, episode reward: 86.172, mean reward: 0.086 [-17.879, 23.737], mean action: 1.845 [0.000, 3.000], mean observation: 0.144 [-0.708, 1.395], loss: 7.085461, mae: 46.387932, mean_q: 62.212688
  701308/1100000: episode: 1544, duration: 1.685s, episode steps: 289, steps per second: 171, episode reward: 223.868, mean reward: 0.775 [-17.448, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.160 [-0.769, 1.411], loss: 4.988714, mae: 46.652637, mean_q: 62.719913
  701636/1100000: episode: 1545, duration: 1.924s, episode steps: 328, steps per second: 170, episode reward: 237.242, mean reward: 0.723 [-17.566, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.240 [-0.884, 1.508], loss: 4.263525, mae: 46.188816, mean_q: 62.023270
  701955/1100000: episode: 1546, duration: 1.854s, episode steps: 319, steps per second: 172, episode reward: 255.167, mean reward: 0.800 [-17.548, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.244 [-0.785, 1.399], loss: 8.512730, mae: 46.098911, mean_q: 61.964512
  702233/1100000: episode: 1547, duration: 1.606s, episode steps: 278, steps per second: 173, episode reward: 203.148, mean reward: 0.731 [-10.027, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.189 [-0.682, 1.401], loss: 6.093468, mae: 46.162617, mean_q: 62.017929
  702537/1100000: episode: 1548, duration: 1.782s, episode steps: 304, steps per second: 171, episode reward: 246.713, mean reward: 0.812 [-18.638, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.105 [-0.699, 1.401], loss: 4.926607, mae: 46.595886, mean_q: 62.542828
  702805/1100000: episode: 1549, duration: 1.553s, episode steps: 268, steps per second: 173, episode reward: 285.485, mean reward: 1.065 [-17.354, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.066 [-1.051, 1.401], loss: 5.921143, mae: 46.595383, mean_q: 62.613689
  703223/1100000: episode: 1550, duration: 2.482s, episode steps: 418, steps per second: 168, episode reward: 217.748, mean reward: 0.521 [-17.721, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.202 [-0.802, 1.497], loss: 6.725654, mae: 46.516644, mean_q: 62.453358
  703622/1100000: episode: 1551, duration: 2.394s, episode steps: 399, steps per second: 167, episode reward: 197.608, mean reward: 0.495 [-21.571, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.222 [-0.783, 1.414], loss: 6.690885, mae: 46.550110, mean_q: 62.464603
  704067/1100000: episode: 1552, duration: 2.599s, episode steps: 445, steps per second: 171, episode reward: 280.530, mean reward: 0.630 [-18.044, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.157 [-0.822, 1.404], loss: 6.474854, mae: 46.689957, mean_q: 62.608910
  704447/1100000: episode: 1553, duration: 2.260s, episode steps: 380, steps per second: 168, episode reward: 213.718, mean reward: 0.562 [-18.413, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.184 [-0.642, 1.423], loss: 7.318535, mae: 45.855808, mean_q: 61.510185
  705250/1100000: episode: 1554, duration: 5.242s, episode steps: 803, steps per second: 153, episode reward: 114.406, mean reward: 0.142 [-19.195, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: -0.013 [-0.937, 1.413], loss: 4.974569, mae: 46.649323, mean_q: 62.475433
  705511/1100000: episode: 1555, duration: 1.496s, episode steps: 261, steps per second: 174, episode reward: 288.629, mean reward: 1.106 [-9.341, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.065 [-0.794, 1.494], loss: 4.918526, mae: 46.822891, mean_q: 62.830761
  705764/1100000: episode: 1556, duration: 1.468s, episode steps: 253, steps per second: 172, episode reward: 272.729, mean reward: 1.078 [-5.090, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.085 [-0.767, 1.400], loss: 6.969566, mae: 46.497463, mean_q: 62.502495
  706587/1100000: episode: 1557, duration: 5.231s, episode steps: 823, steps per second: 157, episode reward: 201.237, mean reward: 0.245 [-23.444, 100.000], mean action: 2.411 [0.000, 3.000], mean observation: 0.270 [-0.837, 1.392], loss: 5.373916, mae: 46.410873, mean_q: 62.225323
  706973/1100000: episode: 1558, duration: 2.332s, episode steps: 386, steps per second: 166, episode reward: 232.843, mean reward: 0.603 [-13.676, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.018 [-0.911, 1.503], loss: 5.072656, mae: 46.437107, mean_q: 62.101433
  707208/1100000: episode: 1559, duration: 1.365s, episode steps: 235, steps per second: 172, episode reward: 249.356, mean reward: 1.061 [-9.283, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.080 [-0.797, 1.399], loss: 9.837002, mae: 46.542614, mean_q: 62.099415
  707730/1100000: episode: 1560, duration: 3.198s, episode steps: 522, steps per second: 163, episode reward: 241.118, mean reward: 0.462 [-19.120, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.212 [-0.716, 1.391], loss: 5.348251, mae: 46.070999, mean_q: 61.677700
  707965/1100000: episode: 1561, duration: 1.365s, episode steps: 235, steps per second: 172, episode reward: 246.046, mean reward: 1.047 [-9.128, 100.000], mean action: 1.647 [0.000, 3.000], mean observation: -0.041 [-0.701, 1.406], loss: 8.508281, mae: 46.131458, mean_q: 61.608860
  708455/1100000: episode: 1562, duration: 2.966s, episode steps: 490, steps per second: 165, episode reward: 205.713, mean reward: 0.420 [-18.339, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.192 [-0.704, 1.401], loss: 5.219998, mae: 45.946911, mean_q: 61.615582
  708735/1100000: episode: 1563, duration: 1.642s, episode steps: 280, steps per second: 171, episode reward: 290.883, mean reward: 1.039 [-9.809, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.064 [-0.786, 1.433], loss: 6.308808, mae: 46.221134, mean_q: 61.761272
  709024/1100000: episode: 1564, duration: 1.686s, episode steps: 289, steps per second: 171, episode reward: 262.538, mean reward: 0.908 [-17.339, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: -0.023 [-0.609, 1.425], loss: 4.949924, mae: 45.992737, mean_q: 61.646187
  709348/1100000: episode: 1565, duration: 1.929s, episode steps: 324, steps per second: 168, episode reward: -223.942, mean reward: -0.691 [-100.000, 14.295], mean action: 1.549 [0.000, 3.000], mean observation: -0.011 [-1.601, 1.399], loss: 5.266142, mae: 46.508156, mean_q: 62.179180
  709708/1100000: episode: 1566, duration: 2.092s, episode steps: 360, steps per second: 172, episode reward: 290.302, mean reward: 0.806 [-8.994, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.042 [-1.537, 1.424], loss: 6.786826, mae: 46.069466, mean_q: 61.734505
  710017/1100000: episode: 1567, duration: 1.807s, episode steps: 309, steps per second: 171, episode reward: 274.724, mean reward: 0.889 [-19.782, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: -0.018 [-0.999, 1.413], loss: 5.756163, mae: 46.228085, mean_q: 62.074230
  710276/1100000: episode: 1568, duration: 1.632s, episode steps: 259, steps per second: 159, episode reward: 277.174, mean reward: 1.070 [-11.478, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.045 [-0.669, 1.400], loss: 4.565735, mae: 46.371990, mean_q: 62.358116
  710514/1100000: episode: 1569, duration: 1.398s, episode steps: 238, steps per second: 170, episode reward: 280.256, mean reward: 1.178 [-9.279, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.054 [-0.720, 1.408], loss: 4.928801, mae: 46.664505, mean_q: 62.672195
  710711/1100000: episode: 1570, duration: 1.133s, episode steps: 197, steps per second: 174, episode reward: 1.242, mean reward: 0.006 [-100.000, 20.349], mean action: 1.223 [0.000, 3.000], mean observation: -0.025 [-0.703, 1.429], loss: 7.319561, mae: 46.703503, mean_q: 62.661942
  711100/1100000: episode: 1571, duration: 2.358s, episode steps: 389, steps per second: 165, episode reward: 270.263, mean reward: 0.695 [-17.718, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.098 [-1.224, 1.396], loss: 4.149406, mae: 46.540489, mean_q: 62.626762
  711341/1100000: episode: 1572, duration: 1.402s, episode steps: 241, steps per second: 172, episode reward: 275.924, mean reward: 1.145 [-17.328, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.084 [-0.970, 1.482], loss: 7.354297, mae: 46.646160, mean_q: 62.659298
  711706/1100000: episode: 1573, duration: 2.179s, episode steps: 365, steps per second: 168, episode reward: 263.559, mean reward: 0.722 [-17.369, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: -0.039 [-1.152, 1.398], loss: 5.943957, mae: 46.427002, mean_q: 62.388741
  712033/1100000: episode: 1574, duration: 1.928s, episode steps: 327, steps per second: 170, episode reward: 233.973, mean reward: 0.716 [-17.334, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.109 [-0.841, 1.409], loss: 5.930830, mae: 46.986172, mean_q: 63.105804
  712518/1100000: episode: 1575, duration: 2.854s, episode steps: 485, steps per second: 170, episode reward: 270.027, mean reward: 0.557 [-20.513, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.123 [-0.921, 1.403], loss: 6.112298, mae: 46.902691, mean_q: 63.106216
  713105/1100000: episode: 1576, duration: 3.628s, episode steps: 587, steps per second: 162, episode reward: 249.743, mean reward: 0.425 [-17.890, 100.000], mean action: 0.796 [0.000, 3.000], mean observation: 0.151 [-0.615, 1.455], loss: 3.657873, mae: 46.659790, mean_q: 62.737183
  713423/1100000: episode: 1577, duration: 1.859s, episode steps: 318, steps per second: 171, episode reward: 249.653, mean reward: 0.785 [-12.709, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: 0.146 [-0.688, 1.416], loss: 8.072326, mae: 46.563244, mean_q: 62.483429
  713861/1100000: episode: 1578, duration: 2.640s, episode steps: 438, steps per second: 166, episode reward: 200.350, mean reward: 0.457 [-9.858, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.206 [-0.795, 1.460], loss: 5.634775, mae: 46.833984, mean_q: 62.875359
  714299/1100000: episode: 1579, duration: 2.698s, episode steps: 438, steps per second: 162, episode reward: 276.732, mean reward: 0.632 [-18.451, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.125 [-0.874, 1.387], loss: 5.770700, mae: 47.312191, mean_q: 63.604343
  715299/1100000: episode: 1580, duration: 6.263s, episode steps: 1000, steps per second: 160, episode reward: 127.930, mean reward: 0.128 [-17.827, 22.403], mean action: 0.832 [0.000, 3.000], mean observation: 0.178 [-0.897, 1.405], loss: 5.554805, mae: 47.197788, mean_q: 63.346172
  715640/1100000: episode: 1581, duration: 2.023s, episode steps: 341, steps per second: 169, episode reward: 210.620, mean reward: 0.618 [-17.867, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: -0.019 [-0.873, 1.404], loss: 7.446086, mae: 47.320339, mean_q: 63.626976
  715833/1100000: episode: 1582, duration: 1.116s, episode steps: 193, steps per second: 173, episode reward: -248.498, mean reward: -1.288 [-100.000, 10.251], mean action: 1.570 [0.000, 3.000], mean observation: -0.007 [-1.847, 1.394], loss: 5.706763, mae: 47.198288, mean_q: 63.537079
  715929/1100000: episode: 1583, duration: 0.557s, episode steps: 96, steps per second: 172, episode reward: -171.290, mean reward: -1.784 [-100.000, 11.275], mean action: 1.729 [0.000, 3.000], mean observation: 0.116 [-1.133, 2.992], loss: 6.373032, mae: 47.277981, mean_q: 63.491608
  716110/1100000: episode: 1584, duration: 1.051s, episode steps: 181, steps per second: 172, episode reward: 250.264, mean reward: 1.383 [-7.197, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.158 [-0.968, 1.395], loss: 5.109200, mae: 47.645023, mean_q: 64.205765
  716514/1100000: episode: 1585, duration: 2.428s, episode steps: 404, steps per second: 166, episode reward: 213.790, mean reward: 0.529 [-19.025, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: -0.005 [-0.644, 1.405], loss: 6.407716, mae: 47.592812, mean_q: 63.891331
  717105/1100000: episode: 1586, duration: 3.761s, episode steps: 591, steps per second: 157, episode reward: 230.144, mean reward: 0.389 [-20.047, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.247 [-0.920, 1.404], loss: 5.787225, mae: 47.649479, mean_q: 64.019653
  717262/1100000: episode: 1587, duration: 0.898s, episode steps: 157, steps per second: 175, episode reward: 2.013, mean reward: 0.013 [-100.000, 14.456], mean action: 1.478 [0.000, 3.000], mean observation: 0.038 [-0.725, 1.489], loss: 3.494697, mae: 47.793728, mean_q: 64.116127
  717601/1100000: episode: 1588, duration: 2.047s, episode steps: 339, steps per second: 166, episode reward: 214.315, mean reward: 0.632 [-18.544, 100.000], mean action: 1.676 [0.000, 3.000], mean observation: 0.194 [-0.763, 1.402], loss: 4.688931, mae: 47.874760, mean_q: 64.251167
  718328/1100000: episode: 1589, duration: 4.555s, episode steps: 727, steps per second: 160, episode reward: 154.055, mean reward: 0.212 [-11.589, 100.000], mean action: 1.670 [0.000, 3.000], mean observation: 0.094 [-0.835, 1.427], loss: 6.256647, mae: 47.540653, mean_q: 64.004822
  718914/1100000: episode: 1590, duration: 3.592s, episode steps: 586, steps per second: 163, episode reward: 231.976, mean reward: 0.396 [-19.583, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.238 [-0.678, 1.389], loss: 4.054587, mae: 47.363029, mean_q: 63.686260
  719371/1100000: episode: 1591, duration: 2.762s, episode steps: 457, steps per second: 165, episode reward: 264.718, mean reward: 0.579 [-18.774, 100.000], mean action: 0.849 [0.000, 3.000], mean observation: 0.015 [-0.629, 1.395], loss: 4.802576, mae: 47.359650, mean_q: 63.638054
  719761/1100000: episode: 1592, duration: 2.357s, episode steps: 390, steps per second: 165, episode reward: 207.198, mean reward: 0.531 [-17.269, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.015 [-0.710, 1.415], loss: 5.821587, mae: 48.294701, mean_q: 64.956947
  720343/1100000: episode: 1593, duration: 3.878s, episode steps: 582, steps per second: 150, episode reward: 198.028, mean reward: 0.340 [-20.641, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.044 [-0.782, 1.416], loss: 5.778775, mae: 47.867733, mean_q: 64.501846
  720478/1100000: episode: 1594, duration: 0.772s, episode steps: 135, steps per second: 175, episode reward: 8.891, mean reward: 0.066 [-100.000, 12.640], mean action: 1.741 [0.000, 3.000], mean observation: 0.084 [-1.064, 1.412], loss: 5.497462, mae: 48.053825, mean_q: 64.730682
  720781/1100000: episode: 1595, duration: 1.807s, episode steps: 303, steps per second: 168, episode reward: 186.781, mean reward: 0.616 [-12.018, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.114 [-0.871, 1.431], loss: 4.428773, mae: 48.615917, mean_q: 65.327934
  721044/1100000: episode: 1596, duration: 1.531s, episode steps: 263, steps per second: 172, episode reward: 288.754, mean reward: 1.098 [-8.732, 100.000], mean action: 1.510 [0.000, 3.000], mean observation: 0.069 [-1.026, 1.498], loss: 4.998159, mae: 48.933132, mean_q: 65.911804
  721355/1100000: episode: 1597, duration: 1.837s, episode steps: 311, steps per second: 169, episode reward: 194.429, mean reward: 0.625 [-13.559, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: -0.038 [-0.748, 1.394], loss: 5.746343, mae: 48.713455, mean_q: 65.522499
  721595/1100000: episode: 1598, duration: 1.534s, episode steps: 240, steps per second: 156, episode reward: 247.479, mean reward: 1.031 [-2.877, 100.000], mean action: 1.533 [0.000, 3.000], mean observation: -0.057 [-0.721, 1.391], loss: 6.657251, mae: 48.873386, mean_q: 65.710159
  722048/1100000: episode: 1599, duration: 2.886s, episode steps: 453, steps per second: 157, episode reward: 288.754, mean reward: 0.637 [-18.622, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.073 [-0.781, 1.390], loss: 5.046070, mae: 49.064045, mean_q: 66.081680
  722341/1100000: episode: 1600, duration: 1.707s, episode steps: 293, steps per second: 172, episode reward: 222.311, mean reward: 0.759 [-8.655, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.167 [-0.593, 1.398], loss: 6.499953, mae: 49.336945, mean_q: 66.406212
  722643/1100000: episode: 1601, duration: 1.774s, episode steps: 302, steps per second: 170, episode reward: 246.000, mean reward: 0.815 [-17.806, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: -0.028 [-0.833, 1.461], loss: 6.507504, mae: 49.584621, mean_q: 66.622826
  723643/1100000: episode: 1602, duration: 6.146s, episode steps: 1000, steps per second: 163, episode reward: 108.199, mean reward: 0.108 [-21.399, 22.426], mean action: 0.759 [0.000, 3.000], mean observation: 0.172 [-0.947, 1.402], loss: 4.733040, mae: 49.549046, mean_q: 66.634094
  724148/1100000: episode: 1603, duration: 3.161s, episode steps: 505, steps per second: 160, episode reward: 228.268, mean reward: 0.452 [-19.723, 100.000], mean action: 2.109 [0.000, 3.000], mean observation: 0.157 [-0.567, 1.401], loss: 6.323821, mae: 49.460320, mean_q: 66.637604
  724492/1100000: episode: 1604, duration: 2.054s, episode steps: 344, steps per second: 168, episode reward: 295.369, mean reward: 0.859 [-17.858, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: -0.013 [-0.755, 1.386], loss: 4.383853, mae: 49.350166, mean_q: 66.470642
  724911/1100000: episode: 1605, duration: 2.498s, episode steps: 419, steps per second: 168, episode reward: 268.993, mean reward: 0.642 [-18.421, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.098 [-0.648, 1.394], loss: 4.812426, mae: 49.480824, mean_q: 66.559601
  725251/1100000: episode: 1606, duration: 2.012s, episode steps: 340, steps per second: 169, episode reward: 227.409, mean reward: 0.669 [-11.086, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: -0.019 [-1.119, 1.498], loss: 5.243110, mae: 49.179131, mean_q: 66.155357
  725600/1100000: episode: 1607, duration: 2.052s, episode steps: 349, steps per second: 170, episode reward: -454.524, mean reward: -1.302 [-100.000, 5.590], mean action: 1.504 [0.000, 3.000], mean observation: 0.014 [-2.966, 3.839], loss: 5.484104, mae: 49.405384, mean_q: 66.577843
  725847/1100000: episode: 1608, duration: 1.432s, episode steps: 247, steps per second: 172, episode reward: 240.567, mean reward: 0.974 [-10.981, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: -0.022 [-0.744, 1.420], loss: 3.970748, mae: 48.948349, mean_q: 65.751328
  726172/1100000: episode: 1609, duration: 1.891s, episode steps: 325, steps per second: 172, episode reward: 216.695, mean reward: 0.667 [-9.449, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: -0.016 [-0.868, 1.409], loss: 6.615833, mae: 49.015583, mean_q: 65.856415
  727172/1100000: episode: 1610, duration: 6.123s, episode steps: 1000, steps per second: 163, episode reward: 88.928, mean reward: 0.089 [-19.968, 21.915], mean action: 1.669 [0.000, 3.000], mean observation: 0.109 [-0.753, 1.401], loss: 6.757815, mae: 49.570126, mean_q: 66.639175
  727735/1100000: episode: 1611, duration: 3.487s, episode steps: 563, steps per second: 161, episode reward: 248.219, mean reward: 0.441 [-19.640, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.126 [-0.669, 1.403], loss: 6.579477, mae: 49.514034, mean_q: 66.467102
  728735/1100000: episode: 1612, duration: 6.779s, episode steps: 1000, steps per second: 148, episode reward: 98.911, mean reward: 0.099 [-20.000, 23.181], mean action: 1.590 [0.000, 3.000], mean observation: 0.181 [-0.631, 1.393], loss: 5.487788, mae: 49.630623, mean_q: 66.729675
  729675/1100000: episode: 1613, duration: 6.599s, episode steps: 940, steps per second: 142, episode reward: 112.096, mean reward: 0.119 [-9.818, 100.000], mean action: 1.693 [0.000, 3.000], mean observation: -0.047 [-0.825, 1.446], loss: 6.751356, mae: 49.681126, mean_q: 66.750999
  729795/1100000: episode: 1614, duration: 0.693s, episode steps: 120, steps per second: 173, episode reward: -30.599, mean reward: -0.255 [-100.000, 14.330], mean action: 1.508 [0.000, 3.000], mean observation: 0.003 [-1.091, 1.399], loss: 5.751055, mae: 49.881432, mean_q: 66.412605
  729991/1100000: episode: 1615, duration: 1.129s, episode steps: 196, steps per second: 174, episode reward: 200.313, mean reward: 1.022 [-9.475, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: -0.060 [-0.856, 1.412], loss: 7.290800, mae: 49.969429, mean_q: 66.665306
  730352/1100000: episode: 1616, duration: 2.149s, episode steps: 361, steps per second: 168, episode reward: 242.412, mean reward: 0.672 [-17.762, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.128 [-0.723, 1.391], loss: 6.103383, mae: 50.027203, mean_q: 67.065300
  730638/1100000: episode: 1617, duration: 1.664s, episode steps: 286, steps per second: 172, episode reward: 242.074, mean reward: 0.846 [-3.143, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: -0.049 [-0.725, 1.436], loss: 10.723924, mae: 50.409161, mean_q: 67.586479
  731638/1100000: episode: 1618, duration: 6.463s, episode steps: 1000, steps per second: 155, episode reward: 86.834, mean reward: 0.087 [-19.964, 21.394], mean action: 0.954 [0.000, 3.000], mean observation: 0.174 [-0.823, 1.416], loss: 7.279215, mae: 49.604595, mean_q: 66.550911
  732079/1100000: episode: 1619, duration: 2.658s, episode steps: 441, steps per second: 166, episode reward: 167.199, mean reward: 0.379 [-12.621, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.095 [-0.690, 1.529], loss: 4.958757, mae: 49.919598, mean_q: 67.154510
  732416/1100000: episode: 1620, duration: 1.991s, episode steps: 337, steps per second: 169, episode reward: 229.426, mean reward: 0.681 [-9.256, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.130 [-0.748, 1.411], loss: 5.215084, mae: 50.053104, mean_q: 67.027451
  732756/1100000: episode: 1621, duration: 2.029s, episode steps: 340, steps per second: 168, episode reward: 233.474, mean reward: 0.687 [-21.909, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.174 [-0.775, 1.391], loss: 5.076656, mae: 50.145451, mean_q: 67.420937
  733546/1100000: episode: 1622, duration: 4.929s, episode steps: 790, steps per second: 160, episode reward: 237.285, mean reward: 0.300 [-18.085, 100.000], mean action: 1.801 [0.000, 3.000], mean observation: 0.235 [-0.692, 1.386], loss: 5.869421, mae: 49.981487, mean_q: 67.187363
  734155/1100000: episode: 1623, duration: 3.738s, episode steps: 609, steps per second: 163, episode reward: 201.305, mean reward: 0.331 [-19.325, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: -0.032 [-0.923, 1.522], loss: 5.397782, mae: 50.104469, mean_q: 67.160095
  734425/1100000: episode: 1624, duration: 1.616s, episode steps: 270, steps per second: 167, episode reward: 236.119, mean reward: 0.875 [-10.346, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.096 [-0.687, 1.387], loss: 6.370227, mae: 49.512451, mean_q: 66.211365
  734765/1100000: episode: 1625, duration: 2.021s, episode steps: 340, steps per second: 168, episode reward: 226.383, mean reward: 0.666 [-17.637, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.176 [-0.813, 1.457], loss: 6.681823, mae: 49.915199, mean_q: 66.838753
  735581/1100000: episode: 1626, duration: 5.517s, episode steps: 816, steps per second: 148, episode reward: 152.313, mean reward: 0.187 [-21.148, 100.000], mean action: 1.958 [0.000, 3.000], mean observation: -0.030 [-0.837, 1.390], loss: 7.661008, mae: 50.039124, mean_q: 67.102089
  735987/1100000: episode: 1627, duration: 2.455s, episode steps: 406, steps per second: 165, episode reward: 161.337, mean reward: 0.397 [-20.300, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: -0.021 [-0.910, 1.414], loss: 5.549125, mae: 50.029495, mean_q: 66.946770
  736727/1100000: episode: 1628, duration: 4.698s, episode steps: 740, steps per second: 158, episode reward: 231.033, mean reward: 0.312 [-22.596, 100.000], mean action: 0.686 [0.000, 3.000], mean observation: 0.251 [-0.685, 1.483], loss: 7.045448, mae: 49.436497, mean_q: 66.219299
  736932/1100000: episode: 1629, duration: 1.209s, episode steps: 205, steps per second: 170, episode reward: 47.936, mean reward: 0.234 [-100.000, 11.720], mean action: 1.566 [0.000, 3.000], mean observation: 0.099 [-0.828, 1.426], loss: 5.663426, mae: 49.202759, mean_q: 65.997108
  737306/1100000: episode: 1630, duration: 2.238s, episode steps: 374, steps per second: 167, episode reward: 195.126, mean reward: 0.522 [-11.588, 100.000], mean action: 1.821 [0.000, 3.000], mean observation: 0.167 [-0.756, 1.399], loss: 5.396661, mae: 49.358120, mean_q: 66.032158
  737726/1100000: episode: 1631, duration: 2.551s, episode steps: 420, steps per second: 165, episode reward: 285.052, mean reward: 0.679 [-17.355, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.097 [-0.731, 1.406], loss: 6.701637, mae: 49.490532, mean_q: 66.190742
  738010/1100000: episode: 1632, duration: 1.630s, episode steps: 284, steps per second: 174, episode reward: 248.025, mean reward: 0.873 [-5.164, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.202 [-0.767, 1.537], loss: 4.865958, mae: 49.113556, mean_q: 65.936661
  738914/1100000: episode: 1633, duration: 5.732s, episode steps: 904, steps per second: 158, episode reward: -241.510, mean reward: -0.267 [-100.000, 26.801], mean action: 1.302 [0.000, 3.000], mean observation: 0.039 [-0.832, 1.505], loss: 4.958638, mae: 49.116756, mean_q: 65.809311
  739227/1100000: episode: 1634, duration: 1.856s, episode steps: 313, steps per second: 169, episode reward: 268.105, mean reward: 0.857 [-10.484, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.104 [-0.799, 1.434], loss: 7.880554, mae: 49.112190, mean_q: 65.838730
  739422/1100000: episode: 1635, duration: 1.193s, episode steps: 195, steps per second: 164, episode reward: 219.380, mean reward: 1.125 [-11.076, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.143 [-0.961, 1.398], loss: 5.199080, mae: 49.634209, mean_q: 66.409492
  739715/1100000: episode: 1636, duration: 1.711s, episode steps: 293, steps per second: 171, episode reward: 210.972, mean reward: 0.720 [-24.625, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.161 [-0.682, 1.399], loss: 6.251925, mae: 49.239014, mean_q: 65.783020
  739983/1100000: episode: 1637, duration: 1.535s, episode steps: 268, steps per second: 175, episode reward: 210.580, mean reward: 0.786 [-8.550, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.178 [-0.703, 1.416], loss: 4.036846, mae: 49.347752, mean_q: 66.061646
  740199/1100000: episode: 1638, duration: 1.250s, episode steps: 216, steps per second: 173, episode reward: 233.699, mean reward: 1.082 [-19.290, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.191 [-0.741, 1.412], loss: 5.585255, mae: 49.777081, mean_q: 66.972626
  740663/1100000: episode: 1639, duration: 2.718s, episode steps: 464, steps per second: 171, episode reward: 242.685, mean reward: 0.523 [-19.342, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: 0.004 [-0.919, 1.395], loss: 5.740111, mae: 49.558292, mean_q: 66.301926
  741251/1100000: episode: 1640, duration: 3.943s, episode steps: 588, steps per second: 149, episode reward: 243.491, mean reward: 0.414 [-10.882, 100.000], mean action: 2.116 [0.000, 3.000], mean observation: 0.211 [-0.602, 1.396], loss: 5.156806, mae: 49.306606, mean_q: 65.899544
  741548/1100000: episode: 1641, duration: 1.755s, episode steps: 297, steps per second: 169, episode reward: 274.731, mean reward: 0.925 [-17.346, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.122 [-0.839, 1.389], loss: 6.112373, mae: 49.236172, mean_q: 65.953888
  742011/1100000: episode: 1642, duration: 2.867s, episode steps: 463, steps per second: 161, episode reward: 270.216, mean reward: 0.584 [-17.826, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.181 [-0.881, 1.484], loss: 5.830276, mae: 48.919575, mean_q: 65.325645
  742237/1100000: episode: 1643, duration: 1.284s, episode steps: 226, steps per second: 176, episode reward: 249.304, mean reward: 1.103 [-3.301, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: -0.041 [-1.063, 1.434], loss: 6.629003, mae: 49.036232, mean_q: 65.591988
  742762/1100000: episode: 1644, duration: 3.100s, episode steps: 525, steps per second: 169, episode reward: 218.078, mean reward: 0.415 [-19.520, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.035 [-0.818, 1.426], loss: 4.554718, mae: 49.086514, mean_q: 65.687447
  743177/1100000: episode: 1645, duration: 2.592s, episode steps: 415, steps per second: 160, episode reward: 222.924, mean reward: 0.537 [-21.877, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.216 [-0.770, 1.400], loss: 5.348959, mae: 48.655228, mean_q: 64.799118
  743495/1100000: episode: 1646, duration: 1.897s, episode steps: 318, steps per second: 168, episode reward: -266.579, mean reward: -0.838 [-100.000, 16.349], mean action: 1.352 [0.000, 3.000], mean observation: -0.008 [-2.133, 1.396], loss: 11.193903, mae: 49.402676, mean_q: 65.876556
  743845/1100000: episode: 1647, duration: 2.118s, episode steps: 350, steps per second: 165, episode reward: 251.763, mean reward: 0.719 [-17.484, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.129 [-0.722, 1.492], loss: 5.037552, mae: 48.920300, mean_q: 65.388504
  744294/1100000: episode: 1648, duration: 2.707s, episode steps: 449, steps per second: 166, episode reward: 225.888, mean reward: 0.503 [-18.936, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.190 [-0.645, 1.407], loss: 5.599010, mae: 48.529568, mean_q: 64.911324
  744694/1100000: episode: 1649, duration: 2.472s, episode steps: 400, steps per second: 162, episode reward: 220.732, mean reward: 0.552 [-10.905, 100.000], mean action: 1.625 [0.000, 3.000], mean observation: 0.080 [-0.893, 1.403], loss: 5.376530, mae: 47.956318, mean_q: 64.160118
  745270/1100000: episode: 1650, duration: 3.561s, episode steps: 576, steps per second: 162, episode reward: 210.983, mean reward: 0.366 [-17.773, 100.000], mean action: 2.252 [0.000, 3.000], mean observation: 0.173 [-0.771, 1.386], loss: 6.900287, mae: 48.373821, mean_q: 64.664864
  745794/1100000: episode: 1651, duration: 3.267s, episode steps: 524, steps per second: 160, episode reward: 194.095, mean reward: 0.370 [-14.624, 100.000], mean action: 2.086 [0.000, 3.000], mean observation: 0.131 [-0.661, 1.491], loss: 6.035431, mae: 48.509590, mean_q: 65.063705
  746055/1100000: episode: 1652, duration: 1.528s, episode steps: 261, steps per second: 171, episode reward: 258.718, mean reward: 0.991 [-9.515, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.085 [-0.849, 1.442], loss: 4.646463, mae: 47.801006, mean_q: 63.878010
  746377/1100000: episode: 1653, duration: 1.926s, episode steps: 322, steps per second: 167, episode reward: 203.182, mean reward: 0.631 [-8.462, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: -0.040 [-0.883, 1.403], loss: 4.834089, mae: 47.919388, mean_q: 63.762024
  746605/1100000: episode: 1654, duration: 1.354s, episode steps: 228, steps per second: 168, episode reward: 261.074, mean reward: 1.145 [-8.187, 100.000], mean action: 2.132 [0.000, 3.000], mean observation: 0.099 [-0.690, 1.395], loss: 4.472848, mae: 48.222485, mean_q: 64.274315
  746811/1100000: episode: 1655, duration: 1.222s, episode steps: 206, steps per second: 169, episode reward: -167.310, mean reward: -0.812 [-100.000, 27.442], mean action: 1.922 [0.000, 3.000], mean observation: -0.024 [-0.999, 1.635], loss: 6.362854, mae: 48.310776, mean_q: 64.544968
  747447/1100000: episode: 1656, duration: 3.802s, episode steps: 636, steps per second: 167, episode reward: 201.809, mean reward: 0.317 [-18.571, 100.000], mean action: 0.904 [0.000, 3.000], mean observation: 0.251 [-0.832, 1.410], loss: 5.942287, mae: 48.094242, mean_q: 64.167984
  747676/1100000: episode: 1657, duration: 1.327s, episode steps: 229, steps per second: 173, episode reward: -50.414, mean reward: -0.220 [-100.000, 9.160], mean action: 1.782 [0.000, 3.000], mean observation: 0.144 [-1.092, 1.568], loss: 6.958476, mae: 48.265778, mean_q: 64.754982
  747895/1100000: episode: 1658, duration: 1.257s, episode steps: 219, steps per second: 174, episode reward: 262.775, mean reward: 1.200 [-10.425, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.086 [-0.757, 1.397], loss: 7.812470, mae: 48.260899, mean_q: 64.463470
  748201/1100000: episode: 1659, duration: 1.771s, episode steps: 306, steps per second: 173, episode reward: 281.959, mean reward: 0.921 [-9.482, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: -0.013 [-0.814, 1.458], loss: 9.683840, mae: 48.377666, mean_q: 64.586777
  748383/1100000: episode: 1660, duration: 1.055s, episode steps: 182, steps per second: 172, episode reward: -19.514, mean reward: -0.107 [-100.000, 17.147], mean action: 1.758 [0.000, 3.000], mean observation: 0.120 [-1.038, 1.522], loss: 5.478335, mae: 48.431919, mean_q: 64.667412
  748707/1100000: episode: 1661, duration: 1.867s, episode steps: 324, steps per second: 174, episode reward: 255.234, mean reward: 0.788 [-17.453, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: -0.011 [-1.144, 1.503], loss: 6.110509, mae: 48.192390, mean_q: 64.492516
  749073/1100000: episode: 1662, duration: 2.186s, episode steps: 366, steps per second: 167, episode reward: 287.306, mean reward: 0.785 [-17.705, 100.000], mean action: 0.836 [0.000, 3.000], mean observation: 0.115 [-0.855, 1.403], loss: 7.073745, mae: 48.528572, mean_q: 64.780762
  749381/1100000: episode: 1663, duration: 1.884s, episode steps: 308, steps per second: 163, episode reward: 309.563, mean reward: 1.005 [-6.830, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.096 [-0.906, 1.479], loss: 5.610069, mae: 48.408382, mean_q: 64.687202
  749563/1100000: episode: 1664, duration: 1.053s, episode steps: 182, steps per second: 173, episode reward: 46.545, mean reward: 0.256 [-100.000, 15.790], mean action: 1.962 [0.000, 3.000], mean observation: -0.008 [-0.613, 1.398], loss: 9.625948, mae: 49.072289, mean_q: 65.677109
  749762/1100000: episode: 1665, duration: 1.141s, episode steps: 199, steps per second: 174, episode reward: 15.912, mean reward: 0.080 [-100.000, 16.642], mean action: 1.608 [0.000, 3.000], mean observation: 0.034 [-0.691, 1.677], loss: 7.500375, mae: 48.708054, mean_q: 65.193756
  750723/1100000: episode: 1666, duration: 6.778s, episode steps: 961, steps per second: 142, episode reward: 164.335, mean reward: 0.171 [-20.919, 100.000], mean action: 2.211 [0.000, 3.000], mean observation: 0.161 [-0.762, 1.548], loss: 8.348978, mae: 48.570625, mean_q: 64.771828
  751044/1100000: episode: 1667, duration: 1.876s, episode steps: 321, steps per second: 171, episode reward: 292.508, mean reward: 0.911 [-9.259, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: 0.072 [-0.680, 1.517], loss: 4.531613, mae: 47.823429, mean_q: 64.069778
  751453/1100000: episode: 1668, duration: 2.385s, episode steps: 409, steps per second: 171, episode reward: 220.396, mean reward: 0.539 [-17.951, 100.000], mean action: 0.775 [0.000, 3.000], mean observation: 0.224 [-0.989, 1.471], loss: 9.289558, mae: 48.349110, mean_q: 64.490242
  751706/1100000: episode: 1669, duration: 1.489s, episode steps: 253, steps per second: 170, episode reward: -264.767, mean reward: -1.047 [-100.000, 10.847], mean action: 1.810 [0.000, 3.000], mean observation: 0.119 [-1.285, 1.958], loss: 7.191062, mae: 48.243763, mean_q: 64.604744
  752063/1100000: episode: 1670, duration: 2.109s, episode steps: 357, steps per second: 169, episode reward: -184.500, mean reward: -0.517 [-100.000, 6.791], mean action: 1.930 [0.000, 3.000], mean observation: 0.020 [-0.906, 1.395], loss: 7.051175, mae: 48.346161, mean_q: 64.682808
  752236/1100000: episode: 1671, duration: 0.995s, episode steps: 173, steps per second: 174, episode reward: -2.137, mean reward: -0.012 [-100.000, 18.616], mean action: 1.723 [0.000, 3.000], mean observation: 0.079 [-0.727, 2.955], loss: 7.748205, mae: 48.435017, mean_q: 64.299248
  752395/1100000: episode: 1672, duration: 0.929s, episode steps: 159, steps per second: 171, episode reward: 14.112, mean reward: 0.089 [-100.000, 15.208], mean action: 1.849 [0.000, 3.000], mean observation: 0.058 [-0.816, 1.388], loss: 7.767454, mae: 48.570110, mean_q: 64.906487
  752514/1100000: episode: 1673, duration: 0.686s, episode steps: 119, steps per second: 174, episode reward: -148.521, mean reward: -1.248 [-100.000, 6.075], mean action: 1.790 [0.000, 3.000], mean observation: 0.052 [-0.973, 1.395], loss: 5.405164, mae: 48.006252, mean_q: 64.361336
  752673/1100000: episode: 1674, duration: 0.915s, episode steps: 159, steps per second: 174, episode reward: 20.758, mean reward: 0.131 [-100.000, 25.073], mean action: 1.742 [0.000, 3.000], mean observation: 0.068 [-0.849, 2.090], loss: 4.547542, mae: 48.482948, mean_q: 64.613823
  752864/1100000: episode: 1675, duration: 1.104s, episode steps: 191, steps per second: 173, episode reward: -0.470, mean reward: -0.002 [-100.000, 24.065], mean action: 1.864 [0.000, 3.000], mean observation: -0.086 [-0.987, 1.393], loss: 7.081249, mae: 48.330067, mean_q: 64.581253
  753157/1100000: episode: 1676, duration: 1.709s, episode steps: 293, steps per second: 171, episode reward: 243.763, mean reward: 0.832 [-3.267, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.074 [-0.591, 1.406], loss: 7.516331, mae: 48.363747, mean_q: 64.287086
  753250/1100000: episode: 1677, duration: 0.536s, episode steps: 93, steps per second: 174, episode reward: -0.785, mean reward: -0.008 [-100.000, 21.507], mean action: 1.763 [0.000, 3.000], mean observation: -0.127 [-1.014, 1.776], loss: 4.166314, mae: 48.429482, mean_q: 64.632156
  753364/1100000: episode: 1678, duration: 0.656s, episode steps: 114, steps per second: 174, episode reward: -121.002, mean reward: -1.061 [-100.000, 5.789], mean action: 1.868 [0.000, 3.000], mean observation: 0.060 [-1.090, 1.393], loss: 7.172805, mae: 48.607388, mean_q: 64.914635
  753748/1100000: episode: 1679, duration: 2.283s, episode steps: 384, steps per second: 168, episode reward: 233.124, mean reward: 0.607 [-19.300, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.124 [-0.827, 1.522], loss: 7.240925, mae: 48.395130, mean_q: 64.431190
  754183/1100000: episode: 1680, duration: 2.634s, episode steps: 435, steps per second: 165, episode reward: 228.217, mean reward: 0.525 [-17.377, 100.000], mean action: 0.910 [0.000, 3.000], mean observation: 0.112 [-0.680, 1.414], loss: 6.018847, mae: 48.317429, mean_q: 64.344452
  754624/1100000: episode: 1681, duration: 2.645s, episode steps: 441, steps per second: 167, episode reward: 275.913, mean reward: 0.626 [-17.432, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.183 [-0.884, 1.458], loss: 8.329420, mae: 48.638969, mean_q: 64.761917
  754848/1100000: episode: 1682, duration: 1.293s, episode steps: 224, steps per second: 173, episode reward: -7.006, mean reward: -0.031 [-100.000, 14.673], mean action: 1.576 [0.000, 3.000], mean observation: 0.002 [-0.824, 1.861], loss: 6.436286, mae: 48.229572, mean_q: 64.070724
  755116/1100000: episode: 1683, duration: 1.581s, episode steps: 268, steps per second: 169, episode reward: 297.575, mean reward: 1.110 [-19.158, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.060 [-0.738, 1.400], loss: 8.357344, mae: 48.386208, mean_q: 64.483109
  755925/1100000: episode: 1684, duration: 5.049s, episode steps: 809, steps per second: 160, episode reward: 238.005, mean reward: 0.294 [-20.282, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.015 [-0.816, 1.388], loss: 7.314197, mae: 48.198250, mean_q: 64.190445
  756704/1100000: episode: 1685, duration: 4.855s, episode steps: 779, steps per second: 160, episode reward: 212.557, mean reward: 0.273 [-17.821, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.029 [-0.905, 1.400], loss: 8.178584, mae: 47.920994, mean_q: 63.642902
  757018/1100000: episode: 1686, duration: 1.860s, episode steps: 314, steps per second: 169, episode reward: -140.068, mean reward: -0.446 [-100.000, 28.510], mean action: 1.538 [0.000, 3.000], mean observation: 0.059 [-1.815, 1.489], loss: 10.259907, mae: 47.852776, mean_q: 63.889114
  757323/1100000: episode: 1687, duration: 1.780s, episode steps: 305, steps per second: 171, episode reward: 273.479, mean reward: 0.897 [-17.675, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.095 [-0.843, 1.411], loss: 7.465936, mae: 48.428043, mean_q: 64.492493
  757873/1100000: episode: 1688, duration: 3.450s, episode steps: 550, steps per second: 159, episode reward: 263.033, mean reward: 0.478 [-21.378, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.116 [-0.782, 1.386], loss: 6.657986, mae: 48.279900, mean_q: 64.062653
  758498/1100000: episode: 1689, duration: 3.995s, episode steps: 625, steps per second: 156, episode reward: 225.152, mean reward: 0.360 [-18.488, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: -0.001 [-0.768, 1.399], loss: 7.918534, mae: 48.034725, mean_q: 63.797474
  758678/1100000: episode: 1690, duration: 1.037s, episode steps: 180, steps per second: 174, episode reward: 1.220, mean reward: 0.007 [-100.000, 14.207], mean action: 1.761 [0.000, 3.000], mean observation: 0.214 [-1.452, 1.403], loss: 7.623203, mae: 48.216888, mean_q: 64.179909
  759009/1100000: episode: 1691, duration: 2.009s, episode steps: 331, steps per second: 165, episode reward: 176.481, mean reward: 0.533 [-15.012, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: -0.048 [-0.731, 1.400], loss: 8.095727, mae: 47.954109, mean_q: 63.543182
  759328/1100000: episode: 1692, duration: 2.027s, episode steps: 319, steps per second: 157, episode reward: 206.208, mean reward: 0.646 [-15.577, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.098 [-0.653, 1.409], loss: 7.760149, mae: 48.275677, mean_q: 64.199921
  759729/1100000: episode: 1693, duration: 2.381s, episode steps: 401, steps per second: 168, episode reward: 227.086, mean reward: 0.566 [-19.522, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.180 [-0.849, 1.521], loss: 5.982220, mae: 48.445133, mean_q: 64.600464
  760048/1100000: episode: 1694, duration: 1.881s, episode steps: 319, steps per second: 170, episode reward: 290.802, mean reward: 0.912 [-9.505, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: 0.054 [-0.833, 1.453], loss: 6.333468, mae: 48.636414, mean_q: 65.078461
  760266/1100000: episode: 1695, duration: 1.269s, episode steps: 218, steps per second: 172, episode reward: 199.024, mean reward: 0.913 [-16.129, 100.000], mean action: 2.326 [0.000, 3.000], mean observation: 0.063 [-1.116, 1.447], loss: 7.557507, mae: 48.445259, mean_q: 64.581841
  760491/1100000: episode: 1696, duration: 1.296s, episode steps: 225, steps per second: 174, episode reward: 242.056, mean reward: 1.076 [-9.439, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.062 [-0.796, 1.421], loss: 9.439448, mae: 48.942577, mean_q: 65.233704
  761111/1100000: episode: 1697, duration: 3.841s, episode steps: 620, steps per second: 161, episode reward: 215.064, mean reward: 0.347 [-19.177, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.229 [-1.251, 1.391], loss: 9.076556, mae: 48.788925, mean_q: 65.048401
  761738/1100000: episode: 1698, duration: 3.836s, episode steps: 627, steps per second: 163, episode reward: 211.311, mean reward: 0.337 [-20.477, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.166 [-0.792, 1.420], loss: 6.590453, mae: 48.582439, mean_q: 64.836800
  761935/1100000: episode: 1699, duration: 1.142s, episode steps: 197, steps per second: 172, episode reward: -8.622, mean reward: -0.044 [-100.000, 11.926], mean action: 1.741 [0.000, 3.000], mean observation: -0.038 [-0.877, 1.385], loss: 4.785222, mae: 49.389153, mean_q: 65.873276
  762533/1100000: episode: 1700, duration: 3.567s, episode steps: 598, steps per second: 168, episode reward: -313.375, mean reward: -0.524 [-100.000, 15.913], mean action: 1.522 [0.000, 3.000], mean observation: 0.161 [-2.173, 1.407], loss: 7.128232, mae: 48.727234, mean_q: 65.039482
  762857/1100000: episode: 1701, duration: 1.910s, episode steps: 324, steps per second: 170, episode reward: 235.597, mean reward: 0.727 [-17.615, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: -0.058 [-0.600, 1.429], loss: 6.085561, mae: 49.000343, mean_q: 65.299080
  763416/1100000: episode: 1702, duration: 3.511s, episode steps: 559, steps per second: 159, episode reward: 242.781, mean reward: 0.434 [-12.983, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: -0.031 [-0.950, 1.389], loss: 8.571564, mae: 49.108494, mean_q: 65.195045
  763632/1100000: episode: 1703, duration: 1.244s, episode steps: 216, steps per second: 174, episode reward: 252.914, mean reward: 1.171 [-9.404, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.050 [-0.778, 1.430], loss: 7.340368, mae: 49.182480, mean_q: 65.265778
  763934/1100000: episode: 1704, duration: 1.771s, episode steps: 302, steps per second: 171, episode reward: 259.032, mean reward: 0.858 [-9.856, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: -0.041 [-0.717, 1.394], loss: 8.625337, mae: 49.057682, mean_q: 65.043823
  764259/1100000: episode: 1705, duration: 1.947s, episode steps: 325, steps per second: 167, episode reward: 260.949, mean reward: 0.803 [-8.367, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: -0.035 [-0.825, 1.396], loss: 7.001223, mae: 48.842705, mean_q: 64.987358
  764541/1100000: episode: 1706, duration: 1.652s, episode steps: 282, steps per second: 171, episode reward: 279.726, mean reward: 0.992 [-19.028, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.101 [-0.705, 1.462], loss: 7.272546, mae: 49.167164, mean_q: 65.221115
  764843/1100000: episode: 1707, duration: 1.785s, episode steps: 302, steps per second: 169, episode reward: 272.057, mean reward: 0.901 [-13.224, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.122 [-0.883, 1.390], loss: 7.187426, mae: 48.863216, mean_q: 65.130806
  765218/1100000: episode: 1708, duration: 2.308s, episode steps: 375, steps per second: 162, episode reward: 234.397, mean reward: 0.625 [-8.355, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.160 [-0.595, 1.407], loss: 6.633077, mae: 49.098217, mean_q: 65.468880
  765471/1100000: episode: 1709, duration: 1.500s, episode steps: 253, steps per second: 169, episode reward: 260.642, mean reward: 1.030 [-18.356, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: -0.075 [-0.836, 1.393], loss: 6.827287, mae: 48.917206, mean_q: 65.357635
  766276/1100000: episode: 1710, duration: 5.157s, episode steps: 805, steps per second: 156, episode reward: 127.472, mean reward: 0.158 [-18.655, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: -0.024 [-0.600, 1.409], loss: 5.527568, mae: 49.068802, mean_q: 65.390907
  766723/1100000: episode: 1711, duration: 2.854s, episode steps: 447, steps per second: 157, episode reward: 197.319, mean reward: 0.441 [-7.641, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: -0.051 [-0.678, 1.386], loss: 7.715048, mae: 48.783447, mean_q: 64.892769
  767723/1100000: episode: 1712, duration: 6.075s, episode steps: 1000, steps per second: 165, episode reward: 125.124, mean reward: 0.125 [-19.599, 21.352], mean action: 1.346 [0.000, 3.000], mean observation: 0.265 [-0.731, 1.572], loss: 7.820030, mae: 48.766273, mean_q: 64.744553
  768278/1100000: episode: 1713, duration: 3.367s, episode steps: 555, steps per second: 165, episode reward: 255.962, mean reward: 0.461 [-18.001, 100.000], mean action: 0.838 [0.000, 3.000], mean observation: 0.019 [-0.879, 1.529], loss: 7.690273, mae: 48.101856, mean_q: 63.969975
  768355/1100000: episode: 1714, duration: 0.442s, episode steps: 77, steps per second: 174, episode reward: -50.646, mean reward: -0.658 [-100.000, 8.000], mean action: 1.961 [0.000, 3.000], mean observation: -0.085 [-1.219, 2.590], loss: 9.571463, mae: 47.470657, mean_q: 63.330769
  768864/1100000: episode: 1715, duration: 3.180s, episode steps: 509, steps per second: 160, episode reward: 262.661, mean reward: 0.516 [-17.284, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.152 [-0.781, 1.389], loss: 8.105103, mae: 48.631939, mean_q: 64.432388
  769251/1100000: episode: 1716, duration: 2.373s, episode steps: 387, steps per second: 163, episode reward: 201.077, mean reward: 0.520 [-20.548, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.158 [-0.686, 1.453], loss: 6.906922, mae: 48.763283, mean_q: 64.726723
  769594/1100000: episode: 1717, duration: 2.048s, episode steps: 343, steps per second: 167, episode reward: 286.044, mean reward: 0.834 [-10.674, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.133 [-0.714, 1.411], loss: 7.276452, mae: 48.823307, mean_q: 65.090599
  770261/1100000: episode: 1718, duration: 3.964s, episode steps: 667, steps per second: 168, episode reward: 209.302, mean reward: 0.314 [-18.560, 100.000], mean action: 0.684 [0.000, 3.000], mean observation: 0.018 [-0.791, 1.398], loss: 7.038726, mae: 49.118507, mean_q: 64.716537
  770354/1100000: episode: 1719, duration: 0.532s, episode steps: 93, steps per second: 175, episode reward: -17.670, mean reward: -0.190 [-100.000, 16.133], mean action: 1.645 [0.000, 3.000], mean observation: 0.029 [-1.135, 1.389], loss: 6.824161, mae: 48.642624, mean_q: 63.983677
  770476/1100000: episode: 1720, duration: 0.700s, episode steps: 122, steps per second: 174, episode reward: -48.701, mean reward: -0.399 [-100.000, 13.053], mean action: 1.639 [0.000, 3.000], mean observation: 0.236 [-4.490, 1.512], loss: 7.019629, mae: 48.508675, mean_q: 64.011818
  770770/1100000: episode: 1721, duration: 1.753s, episode steps: 294, steps per second: 168, episode reward: 230.645, mean reward: 0.785 [-9.937, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.058 [-0.798, 1.433], loss: 7.691268, mae: 48.682556, mean_q: 64.569542
  771053/1100000: episode: 1722, duration: 1.636s, episode steps: 283, steps per second: 173, episode reward: 290.185, mean reward: 1.025 [-19.384, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.124 [-0.729, 1.492], loss: 7.177683, mae: 49.639683, mean_q: 65.276848
  771586/1100000: episode: 1723, duration: 3.218s, episode steps: 533, steps per second: 166, episode reward: 259.517, mean reward: 0.487 [-20.297, 100.000], mean action: 0.899 [0.000, 3.000], mean observation: 0.216 [-0.780, 1.392], loss: 9.223382, mae: 48.998119, mean_q: 64.671967
  771899/1100000: episode: 1724, duration: 1.854s, episode steps: 313, steps per second: 169, episode reward: -24.164, mean reward: -0.077 [-100.000, 20.727], mean action: 1.700 [0.000, 3.000], mean observation: 0.301 [-1.591, 2.771], loss: 6.650390, mae: 49.245129, mean_q: 65.168465
  772619/1100000: episode: 1725, duration: 4.494s, episode steps: 720, steps per second: 160, episode reward: 208.565, mean reward: 0.290 [-18.847, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.016 [-0.600, 1.400], loss: 8.642300, mae: 49.485416, mean_q: 65.636406
  773333/1100000: episode: 1726, duration: 4.427s, episode steps: 714, steps per second: 161, episode reward: 205.371, mean reward: 0.288 [-18.625, 100.000], mean action: 1.889 [0.000, 3.000], mean observation: 0.259 [-0.803, 1.413], loss: 6.783745, mae: 49.461395, mean_q: 65.785866
  773561/1100000: episode: 1727, duration: 1.328s, episode steps: 228, steps per second: 172, episode reward: 265.436, mean reward: 1.164 [-9.262, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.012 [-0.730, 1.391], loss: 8.232490, mae: 49.123550, mean_q: 65.326714
  773793/1100000: episode: 1728, duration: 1.353s, episode steps: 232, steps per second: 171, episode reward: -7.761, mean reward: -0.033 [-100.000, 22.856], mean action: 1.845 [0.000, 3.000], mean observation: 0.074 [-1.059, 1.767], loss: 6.828897, mae: 49.252075, mean_q: 65.245056
  774082/1100000: episode: 1729, duration: 1.696s, episode steps: 289, steps per second: 170, episode reward: 282.857, mean reward: 0.979 [-11.617, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.083 [-0.731, 1.387], loss: 6.946801, mae: 49.741581, mean_q: 66.095337
  774352/1100000: episode: 1730, duration: 1.598s, episode steps: 270, steps per second: 169, episode reward: 266.183, mean reward: 0.986 [-8.381, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: -0.079 [-0.660, 1.388], loss: 9.755743, mae: 49.528168, mean_q: 65.995163
  774669/1100000: episode: 1731, duration: 1.892s, episode steps: 317, steps per second: 168, episode reward: 262.419, mean reward: 0.828 [-10.332, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: -0.068 [-0.775, 1.408], loss: 10.716567, mae: 48.958138, mean_q: 65.320969
  774980/1100000: episode: 1732, duration: 1.827s, episode steps: 311, steps per second: 170, episode reward: 230.476, mean reward: 0.741 [-9.382, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: -0.044 [-0.644, 1.464], loss: 6.414538, mae: 49.861824, mean_q: 66.470100
  775590/1100000: episode: 1733, duration: 3.798s, episode steps: 610, steps per second: 161, episode reward: 208.045, mean reward: 0.341 [-19.782, 100.000], mean action: 1.613 [0.000, 3.000], mean observation: 0.035 [-0.814, 1.423], loss: 6.782520, mae: 49.516731, mean_q: 65.792992
  775884/1100000: episode: 1734, duration: 1.757s, episode steps: 294, steps per second: 167, episode reward: 250.470, mean reward: 0.852 [-3.338, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: -0.095 [-0.764, 1.397], loss: 9.142858, mae: 49.521736, mean_q: 65.776588
  776676/1100000: episode: 1735, duration: 4.897s, episode steps: 792, steps per second: 162, episode reward: 177.332, mean reward: 0.224 [-18.868, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.282 [-0.700, 1.393], loss: 7.082207, mae: 49.828899, mean_q: 66.556786
  777181/1100000: episode: 1736, duration: 3.017s, episode steps: 505, steps per second: 167, episode reward: 247.918, mean reward: 0.491 [-17.553, 100.000], mean action: 0.903 [0.000, 3.000], mean observation: 0.003 [-0.759, 1.411], loss: 7.810845, mae: 49.670002, mean_q: 66.221527
  777710/1100000: episode: 1737, duration: 3.186s, episode steps: 529, steps per second: 166, episode reward: 162.783, mean reward: 0.308 [-20.579, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.116 [-0.689, 1.417], loss: 6.582429, mae: 49.609074, mean_q: 66.140198
  777800/1100000: episode: 1738, duration: 0.515s, episode steps: 90, steps per second: 175, episode reward: 5.753, mean reward: 0.064 [-100.000, 17.420], mean action: 1.700 [0.000, 3.000], mean observation: 0.055 [-1.485, 1.410], loss: 7.116372, mae: 50.691872, mean_q: 67.424782
  778239/1100000: episode: 1739, duration: 2.659s, episode steps: 439, steps per second: 165, episode reward: 234.696, mean reward: 0.535 [-23.687, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.008 [-0.704, 1.386], loss: 8.247646, mae: 50.143162, mean_q: 66.932076
  778419/1100000: episode: 1740, duration: 1.033s, episode steps: 180, steps per second: 174, episode reward: -93.815, mean reward: -0.521 [-100.000, 3.725], mean action: 1.456 [0.000, 3.000], mean observation: 0.042 [-1.003, 1.408], loss: 7.619994, mae: 49.318806, mean_q: 65.808456
  778911/1100000: episode: 1741, duration: 3.197s, episode steps: 492, steps per second: 154, episode reward: 204.568, mean reward: 0.416 [-10.784, 100.000], mean action: 2.236 [0.000, 3.000], mean observation: 0.146 [-0.434, 1.443], loss: 5.891773, mae: 49.717834, mean_q: 66.462524
  779066/1100000: episode: 1742, duration: 0.913s, episode steps: 155, steps per second: 170, episode reward: -345.068, mean reward: -2.226 [-100.000, 11.850], mean action: 1.871 [0.000, 3.000], mean observation: 0.026 [-2.197, 1.554], loss: 8.081398, mae: 49.594261, mean_q: 65.875290
  779618/1100000: episode: 1743, duration: 3.492s, episode steps: 552, steps per second: 158, episode reward: 203.219, mean reward: 0.368 [-20.124, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: -0.027 [-0.664, 1.397], loss: 5.820670, mae: 49.640293, mean_q: 66.267647
  779864/1100000: episode: 1744, duration: 1.417s, episode steps: 246, steps per second: 174, episode reward: 272.681, mean reward: 1.108 [-17.882, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.058 [-0.612, 1.466], loss: 8.486337, mae: 50.016918, mean_q: 66.484100
  780010/1100000: episode: 1745, duration: 0.830s, episode steps: 146, steps per second: 176, episode reward: -106.049, mean reward: -0.726 [-100.000, 3.099], mean action: 1.370 [0.000, 3.000], mean observation: 0.034 [-1.002, 1.417], loss: 9.655171, mae: 50.102448, mean_q: 66.890289
  780738/1100000: episode: 1746, duration: 4.688s, episode steps: 728, steps per second: 155, episode reward: 176.056, mean reward: 0.242 [-18.153, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: -0.032 [-0.765, 1.396], loss: 8.293762, mae: 50.177937, mean_q: 67.168602
  781147/1100000: episode: 1747, duration: 2.446s, episode steps: 409, steps per second: 167, episode reward: 224.558, mean reward: 0.549 [-17.402, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: -0.025 [-0.621, 1.458], loss: 6.968337, mae: 50.694668, mean_q: 67.889000
  781369/1100000: episode: 1748, duration: 1.279s, episode steps: 222, steps per second: 174, episode reward: 241.759, mean reward: 1.089 [-9.196, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.173 [-0.915, 1.481], loss: 5.346689, mae: 50.580986, mean_q: 68.083855
  781598/1100000: episode: 1749, duration: 1.334s, episode steps: 229, steps per second: 172, episode reward: -219.301, mean reward: -0.958 [-100.000, 3.706], mean action: 1.651 [0.000, 3.000], mean observation: 0.110 [-0.984, 1.691], loss: 6.825512, mae: 50.629044, mean_q: 68.097191
  781992/1100000: episode: 1750, duration: 2.390s, episode steps: 394, steps per second: 165, episode reward: 217.226, mean reward: 0.551 [-13.992, 100.000], mean action: 2.122 [0.000, 3.000], mean observation: 0.123 [-0.773, 1.400], loss: 5.852146, mae: 50.856541, mean_q: 68.285873
  782559/1100000: episode: 1751, duration: 3.414s, episode steps: 567, steps per second: 166, episode reward: 273.119, mean reward: 0.482 [-18.334, 100.000], mean action: 0.661 [0.000, 3.000], mean observation: 0.140 [-0.980, 1.507], loss: 6.265236, mae: 50.393414, mean_q: 67.740089
  782879/1100000: episode: 1752, duration: 1.914s, episode steps: 320, steps per second: 167, episode reward: 233.678, mean reward: 0.730 [-18.645, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.186 [-0.718, 1.443], loss: 7.938977, mae: 51.008461, mean_q: 68.391418
  783482/1100000: episode: 1753, duration: 3.657s, episode steps: 603, steps per second: 165, episode reward: 199.017, mean reward: 0.330 [-18.400, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.006 [-0.603, 1.413], loss: 8.535318, mae: 50.714443, mean_q: 68.021706
  784272/1100000: episode: 1754, duration: 5.027s, episode steps: 790, steps per second: 157, episode reward: 219.253, mean reward: 0.278 [-19.108, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.005 [-0.761, 1.477], loss: 7.688993, mae: 51.019444, mean_q: 68.412437
  784839/1100000: episode: 1755, duration: 3.466s, episode steps: 567, steps per second: 164, episode reward: 222.453, mean reward: 0.392 [-20.131, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.117 [-0.650, 1.393], loss: 11.357121, mae: 50.728760, mean_q: 68.111664
  785265/1100000: episode: 1756, duration: 2.535s, episode steps: 426, steps per second: 168, episode reward: 243.248, mean reward: 0.571 [-21.390, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.211 [-0.751, 1.395], loss: 6.731249, mae: 51.046303, mean_q: 68.495697
  785712/1100000: episode: 1757, duration: 2.708s, episode steps: 447, steps per second: 165, episode reward: 231.721, mean reward: 0.518 [-10.305, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.215 [-0.877, 1.535], loss: 5.913309, mae: 50.940643, mean_q: 68.405487
  785956/1100000: episode: 1758, duration: 1.404s, episode steps: 244, steps per second: 174, episode reward: 257.561, mean reward: 1.056 [-2.926, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.045 [-0.698, 1.511], loss: 5.100084, mae: 51.459763, mean_q: 69.182236
  786179/1100000: episode: 1759, duration: 1.314s, episode steps: 223, steps per second: 170, episode reward: 256.154, mean reward: 1.149 [-6.716, 100.000], mean action: 2.058 [0.000, 3.000], mean observation: -0.031 [-0.785, 1.387], loss: 6.624517, mae: 51.318455, mean_q: 68.657898
  786730/1100000: episode: 1760, duration: 3.465s, episode steps: 551, steps per second: 159, episode reward: 249.679, mean reward: 0.453 [-19.111, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.142 [-0.878, 1.403], loss: 9.722515, mae: 50.946575, mean_q: 68.311066
  786963/1100000: episode: 1761, duration: 1.346s, episode steps: 233, steps per second: 173, episode reward: 259.751, mean reward: 1.115 [-9.821, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.052 [-0.707, 1.474], loss: 8.264924, mae: 50.943718, mean_q: 68.253235
  787303/1100000: episode: 1762, duration: 1.993s, episode steps: 340, steps per second: 171, episode reward: 226.025, mean reward: 0.665 [-20.329, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.148 [-0.755, 1.398], loss: 7.008226, mae: 50.675472, mean_q: 68.071220
  787576/1100000: episode: 1763, duration: 1.600s, episode steps: 273, steps per second: 171, episode reward: 265.317, mean reward: 0.972 [-9.648, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.055 [-0.697, 1.504], loss: 5.589680, mae: 50.840424, mean_q: 68.162048
  787925/1100000: episode: 1764, duration: 2.088s, episode steps: 349, steps per second: 167, episode reward: 222.287, mean reward: 0.637 [-13.536, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: -0.007 [-0.727, 1.400], loss: 5.284418, mae: 50.477459, mean_q: 67.585655
  788328/1100000: episode: 1765, duration: 2.370s, episode steps: 403, steps per second: 170, episode reward: 191.750, mean reward: 0.476 [-19.617, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.189 [-0.810, 1.408], loss: 6.518206, mae: 50.929752, mean_q: 68.096321
  788698/1100000: episode: 1766, duration: 2.270s, episode steps: 370, steps per second: 163, episode reward: 200.505, mean reward: 0.542 [-14.512, 100.000], mean action: 2.019 [0.000, 3.000], mean observation: 0.004 [-0.632, 1.406], loss: 8.060990, mae: 51.012058, mean_q: 68.178413
  789064/1100000: episode: 1767, duration: 2.248s, episode steps: 366, steps per second: 163, episode reward: 242.684, mean reward: 0.663 [-8.629, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: -0.053 [-0.701, 1.417], loss: 5.964862, mae: 51.084049, mean_q: 68.310486
  789357/1100000: episode: 1768, duration: 1.718s, episode steps: 293, steps per second: 171, episode reward: 266.165, mean reward: 0.908 [-9.377, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.092 [-0.723, 1.410], loss: 6.324203, mae: 51.267178, mean_q: 68.641159
  789606/1100000: episode: 1769, duration: 1.472s, episode steps: 249, steps per second: 169, episode reward: 314.580, mean reward: 1.263 [-17.535, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.052 [-1.078, 1.390], loss: 9.079096, mae: 51.528576, mean_q: 68.900032
  789991/1100000: episode: 1770, duration: 2.326s, episode steps: 385, steps per second: 166, episode reward: 249.440, mean reward: 0.648 [-9.409, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.093 [-0.788, 1.434], loss: 8.255445, mae: 51.497375, mean_q: 68.625702
  790441/1100000: episode: 1771, duration: 2.701s, episode steps: 450, steps per second: 167, episode reward: 233.905, mean reward: 0.520 [-9.628, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: -0.021 [-0.711, 1.390], loss: 7.664972, mae: 51.718887, mean_q: 68.898071
  790938/1100000: episode: 1772, duration: 3.049s, episode steps: 497, steps per second: 163, episode reward: 237.775, mean reward: 0.478 [-17.967, 100.000], mean action: 1.811 [0.000, 3.000], mean observation: -0.040 [-0.826, 1.396], loss: 5.574121, mae: 51.653027, mean_q: 68.964493
  791087/1100000: episode: 1773, duration: 0.858s, episode steps: 149, steps per second: 174, episode reward: -174.999, mean reward: -1.174 [-100.000, 2.970], mean action: 1.846 [0.000, 3.000], mean observation: 0.297 [-0.375, 1.398], loss: 7.049581, mae: 51.714272, mean_q: 69.006020
  791955/1100000: episode: 1774, duration: 5.587s, episode steps: 868, steps per second: 155, episode reward: 225.257, mean reward: 0.260 [-21.197, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.214 [-0.765, 1.455], loss: 6.764776, mae: 51.408863, mean_q: 68.561188
  792466/1100000: episode: 1775, duration: 3.219s, episode steps: 511, steps per second: 159, episode reward: 210.131, mean reward: 0.411 [-21.909, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.143 [-1.330, 1.399], loss: 5.810095, mae: 51.110916, mean_q: 68.388916
  793108/1100000: episode: 1776, duration: 3.785s, episode steps: 642, steps per second: 170, episode reward: 238.017, mean reward: 0.371 [-17.590, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.040 [-0.764, 1.621], loss: 7.664029, mae: 51.165123, mean_q: 68.311104
  793240/1100000: episode: 1777, duration: 0.757s, episode steps: 132, steps per second: 174, episode reward: -41.772, mean reward: -0.316 [-100.000, 13.611], mean action: 1.598 [0.000, 3.000], mean observation: 0.108 [-0.770, 2.756], loss: 6.642539, mae: 50.916294, mean_q: 68.071198
  793552/1100000: episode: 1778, duration: 1.829s, episode steps: 312, steps per second: 171, episode reward: 237.397, mean reward: 0.761 [-8.417, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.154 [-0.865, 1.512], loss: 5.950306, mae: 51.579346, mean_q: 69.219673
  794059/1100000: episode: 1779, duration: 3.246s, episode steps: 507, steps per second: 156, episode reward: 243.632, mean reward: 0.481 [-20.625, 100.000], mean action: 1.821 [0.000, 3.000], mean observation: -0.015 [-0.793, 1.408], loss: 6.363933, mae: 51.967670, mean_q: 69.643005
  794467/1100000: episode: 1780, duration: 2.366s, episode steps: 408, steps per second: 172, episode reward: 247.007, mean reward: 0.605 [-17.710, 100.000], mean action: 0.853 [0.000, 3.000], mean observation: 0.222 [-1.080, 1.386], loss: 7.258114, mae: 51.986923, mean_q: 69.631790
  794577/1100000: episode: 1781, duration: 0.629s, episode steps: 110, steps per second: 175, episode reward: -164.835, mean reward: -1.499 [-100.000, 8.391], mean action: 1.791 [0.000, 3.000], mean observation: -0.181 [-1.060, 1.750], loss: 7.753102, mae: 52.474422, mean_q: 70.245911
  795075/1100000: episode: 1782, duration: 3.093s, episode steps: 498, steps per second: 161, episode reward: 241.204, mean reward: 0.484 [-16.944, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.014 [-0.681, 1.460], loss: 7.914946, mae: 52.230114, mean_q: 69.854561
  796075/1100000: episode: 1783, duration: 7.424s, episode steps: 1000, steps per second: 135, episode reward: 36.085, mean reward: 0.036 [-12.140, 21.323], mean action: 1.669 [0.000, 3.000], mean observation: -0.035 [-0.600, 1.404], loss: 7.850866, mae: 52.298775, mean_q: 69.919868
  796189/1100000: episode: 1784, duration: 0.650s, episode steps: 114, steps per second: 175, episode reward: -59.288, mean reward: -0.520 [-100.000, 8.665], mean action: 1.281 [0.000, 3.000], mean observation: 0.167 [-3.373, 1.406], loss: 10.288022, mae: 51.817532, mean_q: 68.857674
  796466/1100000: episode: 1785, duration: 1.643s, episode steps: 277, steps per second: 169, episode reward: 254.821, mean reward: 0.920 [-6.077, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: -0.086 [-0.993, 1.397], loss: 5.792896, mae: 52.602703, mean_q: 70.314911
  796658/1100000: episode: 1786, duration: 1.106s, episode steps: 192, steps per second: 174, episode reward: 247.981, mean reward: 1.292 [-9.063, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.201 [-1.227, 1.544], loss: 10.058567, mae: 52.747204, mean_q: 70.335541
  797013/1100000: episode: 1787, duration: 2.162s, episode steps: 355, steps per second: 164, episode reward: 193.928, mean reward: 0.546 [-10.287, 100.000], mean action: 1.876 [0.000, 3.000], mean observation: 0.046 [-0.831, 1.421], loss: 8.017232, mae: 52.855370, mean_q: 70.469757
  798013/1100000: episode: 1788, duration: 6.150s, episode steps: 1000, steps per second: 163, episode reward: 116.598, mean reward: 0.117 [-20.374, 23.256], mean action: 0.746 [0.000, 3.000], mean observation: 0.175 [-0.723, 1.397], loss: 6.150959, mae: 52.952545, mean_q: 70.621033
  798449/1100000: episode: 1789, duration: 2.681s, episode steps: 436, steps per second: 163, episode reward: 200.222, mean reward: 0.459 [-18.700, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: -0.017 [-0.834, 1.429], loss: 6.905640, mae: 52.714905, mean_q: 70.370872
  798704/1100000: episode: 1790, duration: 1.472s, episode steps: 255, steps per second: 173, episode reward: 283.833, mean reward: 1.113 [-3.418, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.074 [-0.735, 1.478], loss: 7.721041, mae: 53.143879, mean_q: 70.906494
  798857/1100000: episode: 1791, duration: 0.876s, episode steps: 153, steps per second: 175, episode reward: -20.575, mean reward: -0.134 [-100.000, 10.059], mean action: 1.824 [0.000, 3.000], mean observation: 0.061 [-0.869, 1.441], loss: 10.715868, mae: 53.603016, mean_q: 71.011185
  799113/1100000: episode: 1792, duration: 1.489s, episode steps: 256, steps per second: 172, episode reward: 229.510, mean reward: 0.897 [-12.593, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.094 [-0.595, 1.412], loss: 7.549371, mae: 53.315826, mean_q: 71.240990
  799235/1100000: episode: 1793, duration: 0.693s, episode steps: 122, steps per second: 176, episode reward: -5.274, mean reward: -0.043 [-100.000, 18.736], mean action: 1.131 [0.000, 3.000], mean observation: 0.143 [-1.797, 1.444], loss: 4.734437, mae: 53.785423, mean_q: 71.954414
  799713/1100000: episode: 1794, duration: 2.918s, episode steps: 478, steps per second: 164, episode reward: 224.653, mean reward: 0.470 [-19.468, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: -0.010 [-0.671, 1.435], loss: 7.000321, mae: 53.329922, mean_q: 71.269737
  800172/1100000: episode: 1795, duration: 2.741s, episode steps: 459, steps per second: 167, episode reward: -331.314, mean reward: -0.722 [-100.000, 15.460], mean action: 1.107 [0.000, 3.000], mean observation: 0.008 [-1.823, 1.417], loss: 7.445139, mae: 53.898983, mean_q: 72.142067
  800822/1100000: episode: 1796, duration: 4.180s, episode steps: 650, steps per second: 156, episode reward: 169.910, mean reward: 0.261 [-16.980, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.129 [-0.968, 1.403], loss: 8.692518, mae: 54.105972, mean_q: 72.363640
  800910/1100000: episode: 1797, duration: 0.501s, episode steps: 88, steps per second: 175, episode reward: -105.562, mean reward: -1.200 [-100.000, 6.730], mean action: 1.614 [0.000, 3.000], mean observation: 0.095 [-2.778, 1.413], loss: 7.992357, mae: 54.544289, mean_q: 72.904236
  801413/1100000: episode: 1798, duration: 3.046s, episode steps: 503, steps per second: 165, episode reward: 220.480, mean reward: 0.438 [-17.726, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.002 [-0.782, 1.400], loss: 9.866862, mae: 54.429478, mean_q: 72.588310
  801704/1100000: episode: 1799, duration: 1.699s, episode steps: 291, steps per second: 171, episode reward: 252.113, mean reward: 0.866 [-10.570, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.081 [-0.794, 1.452], loss: 6.778329, mae: 54.876549, mean_q: 73.342667
  801948/1100000: episode: 1800, duration: 1.440s, episode steps: 244, steps per second: 169, episode reward: 256.606, mean reward: 1.052 [-18.332, 100.000], mean action: 1.664 [0.000, 3.000], mean observation: -0.016 [-0.842, 1.400], loss: 6.104525, mae: 54.437260, mean_q: 72.830513
  802188/1100000: episode: 1801, duration: 1.395s, episode steps: 240, steps per second: 172, episode reward: 228.915, mean reward: 0.954 [-10.272, 100.000], mean action: 1.896 [0.000, 3.000], mean observation: -0.053 [-0.648, 1.412], loss: 7.830733, mae: 54.289574, mean_q: 72.264664
  802370/1100000: episode: 1802, duration: 1.050s, episode steps: 182, steps per second: 173, episode reward: -543.729, mean reward: -2.988 [-100.000, 108.050], mean action: 1.473 [0.000, 3.000], mean observation: 0.492 [-6.056, 4.990], loss: 8.516492, mae: 53.678658, mean_q: 71.986626
  802958/1100000: episode: 1803, duration: 3.595s, episode steps: 588, steps per second: 164, episode reward: 210.945, mean reward: 0.359 [-24.660, 100.000], mean action: 2.129 [0.000, 3.000], mean observation: 0.175 [-1.205, 1.421], loss: 7.914243, mae: 54.114922, mean_q: 72.261856
  803061/1100000: episode: 1804, duration: 0.587s, episode steps: 103, steps per second: 175, episode reward: -54.681, mean reward: -0.531 [-100.000, 5.287], mean action: 1.330 [0.000, 3.000], mean observation: 0.175 [-3.510, 1.395], loss: 8.255578, mae: 54.386223, mean_q: 72.904518
  804061/1100000: episode: 1805, duration: 6.765s, episode steps: 1000, steps per second: 148, episode reward: 47.173, mean reward: 0.047 [-19.403, 13.932], mean action: 2.143 [0.000, 3.000], mean observation: 0.221 [-1.324, 1.411], loss: 7.628865, mae: 53.720123, mean_q: 71.675774
  805061/1100000: episode: 1806, duration: 6.608s, episode steps: 1000, steps per second: 151, episode reward: 85.097, mean reward: 0.085 [-19.533, 23.469], mean action: 0.921 [0.000, 3.000], mean observation: 0.046 [-0.610, 1.408], loss: 7.172910, mae: 52.916454, mean_q: 70.713615
  805772/1100000: episode: 1807, duration: 4.616s, episode steps: 711, steps per second: 154, episode reward: 202.455, mean reward: 0.285 [-18.261, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.134 [-0.868, 1.610], loss: 24.900673, mae: 52.075413, mean_q: 69.576813
  806115/1100000: episode: 1808, duration: 2.064s, episode steps: 343, steps per second: 166, episode reward: 249.398, mean reward: 0.727 [-8.925, 100.000], mean action: 1.464 [0.000, 3.000], mean observation: 0.074 [-0.716, 1.391], loss: 17.148224, mae: 51.772961, mean_q: 69.087746
  806541/1100000: episode: 1809, duration: 2.548s, episode steps: 426, steps per second: 167, episode reward: 176.476, mean reward: 0.414 [-14.965, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.123 [-0.901, 1.519], loss: 11.956679, mae: 51.280029, mean_q: 68.532333
  807265/1100000: episode: 1810, duration: 4.397s, episode steps: 724, steps per second: 165, episode reward: 206.958, mean reward: 0.286 [-18.981, 100.000], mean action: 1.936 [0.000, 3.000], mean observation: 0.096 [-0.616, 1.411], loss: 7.778337, mae: 51.142094, mean_q: 68.380730
  807970/1100000: episode: 1811, duration: 4.624s, episode steps: 705, steps per second: 152, episode reward: 208.493, mean reward: 0.296 [-19.402, 100.000], mean action: 1.562 [0.000, 3.000], mean observation: 0.159 [-0.935, 1.385], loss: 6.773997, mae: 51.294655, mean_q: 68.703430
  808123/1100000: episode: 1812, duration: 0.899s, episode steps: 153, steps per second: 170, episode reward: -80.328, mean reward: -0.525 [-100.000, 3.447], mean action: 1.778 [0.000, 3.000], mean observation: -0.009 [-1.001, 1.395], loss: 5.232875, mae: 51.611099, mean_q: 69.227364
  808782/1100000: episode: 1813, duration: 4.236s, episode steps: 659, steps per second: 156, episode reward: 259.380, mean reward: 0.394 [-18.065, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.116 [-0.812, 1.392], loss: 5.999437, mae: 51.318333, mean_q: 68.786819
  808877/1100000: episode: 1814, duration: 0.544s, episode steps: 95, steps per second: 174, episode reward: -192.863, mean reward: -2.030 [-100.000, 4.140], mean action: 1.600 [0.000, 3.000], mean observation: 0.231 [-1.484, 5.852], loss: 26.579494, mae: 51.595482, mean_q: 69.051872
  809877/1100000: episode: 1815, duration: 6.520s, episode steps: 1000, steps per second: 153, episode reward: 22.277, mean reward: 0.022 [-24.483, 20.716], mean action: 1.593 [0.000, 3.000], mean observation: 0.178 [-1.002, 1.411], loss: 8.689863, mae: 50.763412, mean_q: 67.732697
  810244/1100000: episode: 1816, duration: 2.173s, episode steps: 367, steps per second: 169, episode reward: 204.040, mean reward: 0.556 [-17.545, 100.000], mean action: 2.234 [0.000, 3.000], mean observation: 0.019 [-0.600, 1.396], loss: 7.364273, mae: 51.038483, mean_q: 67.712898
  810686/1100000: episode: 1817, duration: 2.677s, episode steps: 442, steps per second: 165, episode reward: 237.024, mean reward: 0.536 [-10.948, 100.000], mean action: 1.726 [0.000, 3.000], mean observation: -0.030 [-0.600, 1.397], loss: 5.303589, mae: 50.744255, mean_q: 67.766884
  811038/1100000: episode: 1818, duration: 2.152s, episode steps: 352, steps per second: 164, episode reward: 195.435, mean reward: 0.555 [-9.632, 100.000], mean action: 1.838 [0.000, 3.000], mean observation: 0.141 [-0.483, 1.405], loss: 8.374882, mae: 50.375011, mean_q: 67.012337
  811424/1100000: episode: 1819, duration: 2.295s, episode steps: 386, steps per second: 168, episode reward: 237.461, mean reward: 0.615 [-10.637, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.085 [-0.555, 1.405], loss: 6.514180, mae: 50.532402, mean_q: 67.132874
  812062/1100000: episode: 1820, duration: 3.925s, episode steps: 638, steps per second: 163, episode reward: 248.424, mean reward: 0.389 [-18.548, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.084 [-0.757, 1.387], loss: 9.813478, mae: 50.650154, mean_q: 67.488106
  812834/1100000: episode: 1821, duration: 4.942s, episode steps: 772, steps per second: 156, episode reward: 201.734, mean reward: 0.261 [-19.686, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.037 [-0.672, 1.404], loss: 5.785987, mae: 50.430683, mean_q: 67.410019
  813572/1100000: episode: 1822, duration: 5.169s, episode steps: 738, steps per second: 143, episode reward: 250.700, mean reward: 0.340 [-19.552, 100.000], mean action: 2.222 [0.000, 3.000], mean observation: 0.057 [-0.691, 1.389], loss: 5.825209, mae: 50.523296, mean_q: 67.670723
  814536/1100000: episode: 1823, duration: 6.112s, episode steps: 964, steps per second: 158, episode reward: 275.936, mean reward: 0.286 [-19.913, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.182 [-0.706, 1.523], loss: 8.969769, mae: 49.821423, mean_q: 66.679642
  815093/1100000: episode: 1824, duration: 3.293s, episode steps: 557, steps per second: 169, episode reward: 200.719, mean reward: 0.360 [-19.441, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.187 [-0.841, 1.434], loss: 7.785307, mae: 49.485744, mean_q: 66.398056
  815504/1100000: episode: 1825, duration: 2.546s, episode steps: 411, steps per second: 161, episode reward: 234.812, mean reward: 0.571 [-19.454, 100.000], mean action: 1.516 [0.000, 3.000], mean observation: 0.052 [-0.871, 1.538], loss: 6.613943, mae: 49.844643, mean_q: 66.930855
  815823/1100000: episode: 1826, duration: 1.898s, episode steps: 319, steps per second: 168, episode reward: 216.786, mean reward: 0.680 [-18.958, 100.000], mean action: 1.922 [0.000, 3.000], mean observation: 0.178 [-0.749, 1.391], loss: 6.797174, mae: 49.693096, mean_q: 66.897408
  816192/1100000: episode: 1827, duration: 2.230s, episode steps: 369, steps per second: 165, episode reward: 181.713, mean reward: 0.492 [-19.863, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.187 [-0.618, 1.413], loss: 5.595706, mae: 49.160622, mean_q: 66.100914
  816645/1100000: episode: 1828, duration: 2.679s, episode steps: 453, steps per second: 169, episode reward: 239.846, mean reward: 0.529 [-18.154, 100.000], mean action: 0.766 [0.000, 3.000], mean observation: 0.120 [-0.816, 1.404], loss: 8.020386, mae: 49.322693, mean_q: 66.376595
  817007/1100000: episode: 1829, duration: 2.145s, episode steps: 362, steps per second: 169, episode reward: 256.066, mean reward: 0.707 [-10.906, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: -0.044 [-0.699, 1.483], loss: 5.942988, mae: 49.387581, mean_q: 66.324326
  817352/1100000: episode: 1830, duration: 2.032s, episode steps: 345, steps per second: 170, episode reward: 290.323, mean reward: 0.842 [-7.876, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.103 [-0.481, 1.500], loss: 6.601540, mae: 49.317589, mean_q: 66.346786
  817657/1100000: episode: 1831, duration: 1.891s, episode steps: 305, steps per second: 161, episode reward: 224.908, mean reward: 0.737 [-19.500, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.179 [-0.770, 1.481], loss: 4.917789, mae: 49.116394, mean_q: 65.920738
  817965/1100000: episode: 1832, duration: 1.842s, episode steps: 308, steps per second: 167, episode reward: 255.064, mean reward: 0.828 [-10.902, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: 0.048 [-1.273, 1.405], loss: 4.789190, mae: 49.107410, mean_q: 65.734291
  818205/1100000: episode: 1833, duration: 1.390s, episode steps: 240, steps per second: 173, episode reward: 215.040, mean reward: 0.896 [-9.527, 100.000], mean action: 1.688 [0.000, 3.000], mean observation: 0.199 [-0.851, 1.398], loss: 4.335574, mae: 49.170612, mean_q: 66.095619
  819205/1100000: episode: 1834, duration: 6.368s, episode steps: 1000, steps per second: 157, episode reward: 102.743, mean reward: 0.103 [-22.946, 23.516], mean action: 1.097 [0.000, 3.000], mean observation: 0.255 [-0.764, 1.399], loss: 9.932546, mae: 49.107491, mean_q: 65.900894
  820205/1100000: episode: 1835, duration: 6.516s, episode steps: 1000, steps per second: 153, episode reward: 58.420, mean reward: 0.058 [-24.450, 24.822], mean action: 2.278 [0.000, 3.000], mean observation: 0.229 [-0.451, 1.405], loss: 5.774918, mae: 48.865475, mean_q: 65.762512
  821205/1100000: episode: 1836, duration: 6.741s, episode steps: 1000, steps per second: 148, episode reward: 84.646, mean reward: 0.085 [-17.287, 15.725], mean action: 1.384 [0.000, 3.000], mean observation: 0.017 [-0.667, 1.395], loss: 5.437078, mae: 49.006020, mean_q: 65.894936
  821433/1100000: episode: 1837, duration: 1.324s, episode steps: 228, steps per second: 172, episode reward: 248.862, mean reward: 1.091 [-3.352, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.073 [-0.898, 1.396], loss: 5.107442, mae: 48.963551, mean_q: 65.848534
  821935/1100000: episode: 1838, duration: 3.084s, episode steps: 502, steps per second: 163, episode reward: 228.609, mean reward: 0.455 [-24.256, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.001 [-0.610, 1.394], loss: 5.420799, mae: 48.915298, mean_q: 65.657455
  822531/1100000: episode: 1839, duration: 3.522s, episode steps: 596, steps per second: 169, episode reward: 216.960, mean reward: 0.364 [-18.280, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.206 [-0.808, 1.518], loss: 6.490993, mae: 49.062302, mean_q: 66.063560
  822896/1100000: episode: 1840, duration: 2.140s, episode steps: 365, steps per second: 171, episode reward: 208.525, mean reward: 0.571 [-12.142, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: -0.031 [-0.651, 1.409], loss: 5.308374, mae: 48.800049, mean_q: 65.630074
  823154/1100000: episode: 1841, duration: 1.504s, episode steps: 258, steps per second: 172, episode reward: 298.049, mean reward: 1.155 [-9.831, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.052 [-1.122, 1.391], loss: 6.886875, mae: 48.957184, mean_q: 65.903107
  823789/1100000: episode: 1842, duration: 4.061s, episode steps: 635, steps per second: 156, episode reward: 167.798, mean reward: 0.264 [-17.364, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: -0.004 [-0.661, 1.407], loss: 7.217595, mae: 49.175297, mean_q: 66.180031
  824146/1100000: episode: 1843, duration: 2.108s, episode steps: 357, steps per second: 169, episode reward: 258.088, mean reward: 0.723 [-9.499, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: -0.020 [-0.704, 1.487], loss: 6.663696, mae: 49.582596, mean_q: 66.794022
  824442/1100000: episode: 1844, duration: 1.714s, episode steps: 296, steps per second: 173, episode reward: -78.294, mean reward: -0.265 [-100.000, 8.837], mean action: 1.537 [0.000, 3.000], mean observation: 0.050 [-2.336, 1.412], loss: 11.017831, mae: 49.575188, mean_q: 66.744194
  824819/1100000: episode: 1845, duration: 2.330s, episode steps: 377, steps per second: 162, episode reward: 269.429, mean reward: 0.715 [-9.950, 100.000], mean action: 1.562 [0.000, 3.000], mean observation: 0.120 [-0.795, 1.386], loss: 7.038522, mae: 49.513325, mean_q: 66.746017
  825819/1100000: episode: 1846, duration: 6.625s, episode steps: 1000, steps per second: 151, episode reward: 73.406, mean reward: 0.073 [-19.125, 13.699], mean action: 1.331 [0.000, 3.000], mean observation: 0.274 [-0.795, 1.442], loss: 6.894646, mae: 49.802830, mean_q: 67.101753
  826180/1100000: episode: 1847, duration: 2.177s, episode steps: 361, steps per second: 166, episode reward: 186.978, mean reward: 0.518 [-13.540, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.135 [-0.872, 1.413], loss: 5.268985, mae: 49.957897, mean_q: 67.279053
  827180/1100000: episode: 1848, duration: 6.326s, episode steps: 1000, steps per second: 158, episode reward: 21.393, mean reward: 0.021 [-12.596, 36.504], mean action: 1.666 [0.000, 3.000], mean observation: 0.164 [-0.553, 1.406], loss: 6.961773, mae: 49.815506, mean_q: 67.123955
  827552/1100000: episode: 1849, duration: 2.254s, episode steps: 372, steps per second: 165, episode reward: 246.714, mean reward: 0.663 [-21.477, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.003 [-0.739, 1.389], loss: 5.517643, mae: 50.000610, mean_q: 67.273216
  827762/1100000: episode: 1850, duration: 1.223s, episode steps: 210, steps per second: 172, episode reward: -237.638, mean reward: -1.132 [-100.000, 38.601], mean action: 1.610 [0.000, 3.000], mean observation: -0.089 [-0.728, 2.357], loss: 6.087715, mae: 49.909695, mean_q: 67.181908
  828290/1100000: episode: 1851, duration: 3.271s, episode steps: 528, steps per second: 161, episode reward: 269.747, mean reward: 0.511 [-20.659, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.114 [-0.870, 1.519], loss: 3.957040, mae: 49.785210, mean_q: 67.112320
  828668/1100000: episode: 1852, duration: 2.230s, episode steps: 378, steps per second: 169, episode reward: 212.396, mean reward: 0.562 [-17.648, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.201 [-0.893, 1.426], loss: 6.852612, mae: 49.947865, mean_q: 67.258759
  829099/1100000: episode: 1853, duration: 2.690s, episode steps: 431, steps per second: 160, episode reward: 261.467, mean reward: 0.607 [-20.311, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.100 [-0.609, 1.503], loss: 7.644712, mae: 49.772068, mean_q: 66.972130
  829455/1100000: episode: 1854, duration: 2.219s, episode steps: 356, steps per second: 160, episode reward: 255.781, mean reward: 0.718 [-18.083, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.129 [-0.714, 1.403], loss: 4.402385, mae: 49.736099, mean_q: 66.986420
  830252/1100000: episode: 1855, duration: 4.930s, episode steps: 797, steps per second: 162, episode reward: 195.776, mean reward: 0.246 [-19.504, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.037 [-0.668, 1.401], loss: 5.143813, mae: 49.424316, mean_q: 66.517342
  830466/1100000: episode: 1856, duration: 1.233s, episode steps: 214, steps per second: 174, episode reward: 22.146, mean reward: 0.103 [-100.000, 13.072], mean action: 1.636 [0.000, 3.000], mean observation: -0.018 [-1.393, 1.397], loss: 5.514109, mae: 48.909649, mean_q: 65.806183
  830769/1100000: episode: 1857, duration: 1.792s, episode steps: 303, steps per second: 169, episode reward: -75.943, mean reward: -0.251 [-100.000, 15.192], mean action: 1.620 [0.000, 3.000], mean observation: -0.003 [-1.243, 1.401], loss: 4.971648, mae: 49.446640, mean_q: 66.441116
  831492/1100000: episode: 1858, duration: 4.558s, episode steps: 723, steps per second: 159, episode reward: 222.380, mean reward: 0.308 [-17.825, 100.000], mean action: 1.935 [0.000, 3.000], mean observation: 0.171 [-0.748, 1.502], loss: 10.674486, mae: 49.310425, mean_q: 66.113647
  831852/1100000: episode: 1859, duration: 2.167s, episode steps: 360, steps per second: 166, episode reward: 245.676, mean reward: 0.682 [-18.800, 100.000], mean action: 2.192 [0.000, 3.000], mean observation: 0.012 [-0.751, 1.404], loss: 10.100298, mae: 49.296768, mean_q: 66.276230
  832777/1100000: episode: 1860, duration: 6.000s, episode steps: 925, steps per second: 154, episode reward: 193.068, mean reward: 0.209 [-19.063, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.246 [-0.765, 1.407], loss: 7.042420, mae: 49.330364, mean_q: 66.442207
  833037/1100000: episode: 1861, duration: 1.518s, episode steps: 260, steps per second: 171, episode reward: 226.961, mean reward: 0.873 [-15.046, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: -0.038 [-0.705, 1.418], loss: 9.014145, mae: 48.992790, mean_q: 65.997597
  833306/1100000: episode: 1862, duration: 1.566s, episode steps: 269, steps per second: 172, episode reward: 280.755, mean reward: 1.044 [-8.692, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.076 [-0.869, 1.520], loss: 5.609429, mae: 49.792057, mean_q: 66.966133
  833587/1100000: episode: 1863, duration: 1.663s, episode steps: 281, steps per second: 169, episode reward: 244.911, mean reward: 0.872 [-11.263, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: 0.086 [-0.841, 1.439], loss: 6.184607, mae: 49.308517, mean_q: 66.237526
  833921/1100000: episode: 1864, duration: 2.022s, episode steps: 334, steps per second: 165, episode reward: 231.505, mean reward: 0.693 [-8.645, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: -0.054 [-0.640, 1.420], loss: 5.040451, mae: 49.813572, mean_q: 66.989899
  834133/1100000: episode: 1865, duration: 1.222s, episode steps: 212, steps per second: 174, episode reward: 62.592, mean reward: 0.295 [-100.000, 11.611], mean action: 1.561 [0.000, 3.000], mean observation: -0.027 [-0.846, 1.388], loss: 10.264556, mae: 49.329582, mean_q: 66.299286
  834384/1100000: episode: 1866, duration: 1.435s, episode steps: 251, steps per second: 175, episode reward: 233.737, mean reward: 0.931 [-10.247, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: -0.036 [-0.764, 1.412], loss: 8.216625, mae: 49.470222, mean_q: 66.400253
  835384/1100000: episode: 1867, duration: 6.472s, episode steps: 1000, steps per second: 155, episode reward: 115.934, mean reward: 0.116 [-19.912, 19.230], mean action: 1.635 [0.000, 3.000], mean observation: 0.229 [-0.849, 1.394], loss: 8.088093, mae: 49.419125, mean_q: 66.392616
  835459/1100000: episode: 1868, duration: 0.428s, episode steps: 75, steps per second: 175, episode reward: -63.820, mean reward: -0.851 [-100.000, 23.942], mean action: 1.320 [0.000, 3.000], mean observation: 0.005 [-1.173, 3.566], loss: 11.333341, mae: 49.530811, mean_q: 66.477127
  835908/1100000: episode: 1869, duration: 2.739s, episode steps: 449, steps per second: 164, episode reward: 273.474, mean reward: 0.609 [-18.355, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.130 [-0.596, 1.435], loss: 6.869558, mae: 48.939354, mean_q: 65.763138
  836207/1100000: episode: 1870, duration: 1.799s, episode steps: 299, steps per second: 166, episode reward: -280.691, mean reward: -0.939 [-100.000, 63.767], mean action: 1.953 [0.000, 3.000], mean observation: 0.250 [-4.914, 1.817], loss: 4.625600, mae: 49.169899, mean_q: 66.022560
  836299/1100000: episode: 1871, duration: 0.522s, episode steps: 92, steps per second: 176, episode reward: -219.180, mean reward: -2.382 [-100.000, 20.507], mean action: 1.576 [0.000, 3.000], mean observation: -0.109 [-5.960, 1.403], loss: 11.025581, mae: 48.931343, mean_q: 65.414719
  836675/1100000: episode: 1872, duration: 2.214s, episode steps: 376, steps per second: 170, episode reward: 251.651, mean reward: 0.669 [-17.788, 100.000], mean action: 0.918 [0.000, 3.000], mean observation: -0.018 [-0.682, 1.409], loss: 7.614293, mae: 49.622238, mean_q: 66.688072
  837097/1100000: episode: 1873, duration: 2.631s, episode steps: 422, steps per second: 160, episode reward: 278.984, mean reward: 0.661 [-17.453, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.130 [-0.519, 1.414], loss: 7.178760, mae: 49.099384, mean_q: 65.913338
  837741/1100000: episode: 1874, duration: 3.993s, episode steps: 644, steps per second: 161, episode reward: 211.372, mean reward: 0.328 [-21.191, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.146 [-0.762, 1.403], loss: 6.423591, mae: 49.108540, mean_q: 65.802628
  837885/1100000: episode: 1875, duration: 0.832s, episode steps: 144, steps per second: 173, episode reward: -292.476, mean reward: -2.031 [-100.000, 31.950], mean action: 1.625 [0.000, 3.000], mean observation: -0.080 [-2.675, 1.410], loss: 7.778487, mae: 49.345108, mean_q: 66.182793
  838265/1100000: episode: 1876, duration: 2.256s, episode steps: 380, steps per second: 168, episode reward: 213.613, mean reward: 0.562 [-19.772, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.166 [-0.673, 1.402], loss: 5.791509, mae: 49.362583, mean_q: 66.337616
  838597/1100000: episode: 1877, duration: 2.021s, episode steps: 332, steps per second: 164, episode reward: 282.334, mean reward: 0.850 [-9.232, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.115 [-0.670, 1.424], loss: 5.599652, mae: 49.475292, mean_q: 66.341499
  838883/1100000: episode: 1878, duration: 1.674s, episode steps: 286, steps per second: 171, episode reward: 251.958, mean reward: 0.881 [-10.256, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: -0.013 [-0.889, 1.538], loss: 12.402074, mae: 48.966816, mean_q: 65.433487
  839381/1100000: episode: 1879, duration: 3.038s, episode steps: 498, steps per second: 164, episode reward: 209.349, mean reward: 0.420 [-17.470, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.209 [-0.675, 1.414], loss: 5.852829, mae: 48.930744, mean_q: 65.593452
  839880/1100000: episode: 1880, duration: 3.103s, episode steps: 499, steps per second: 161, episode reward: 186.944, mean reward: 0.375 [-10.468, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.189 [-0.624, 1.541], loss: 4.327581, mae: 48.383503, mean_q: 64.877235
  840230/1100000: episode: 1881, duration: 2.128s, episode steps: 350, steps per second: 164, episode reward: -103.949, mean reward: -0.297 [-100.000, 7.489], mean action: 1.854 [0.000, 3.000], mean observation: 0.097 [-1.006, 1.401], loss: 6.126071, mae: 48.743973, mean_q: 65.508087
  840435/1100000: episode: 1882, duration: 1.198s, episode steps: 205, steps per second: 171, episode reward: -52.601, mean reward: -0.257 [-100.000, 11.999], mean action: 1.732 [0.000, 3.000], mean observation: -0.016 [-0.870, 1.405], loss: 4.712365, mae: 48.238007, mean_q: 64.779510
  840747/1100000: episode: 1883, duration: 1.864s, episode steps: 312, steps per second: 167, episode reward: -166.777, mean reward: -0.535 [-100.000, 11.465], mean action: 1.814 [0.000, 3.000], mean observation: 0.022 [-1.748, 1.507], loss: 8.471767, mae: 48.182720, mean_q: 64.658905
  840980/1100000: episode: 1884, duration: 1.369s, episode steps: 233, steps per second: 170, episode reward: -295.108, mean reward: -1.267 [-100.000, 7.748], mean action: 1.923 [0.000, 3.000], mean observation: 0.057 [-3.410, 1.461], loss: 8.977439, mae: 48.742733, mean_q: 65.734077
  841453/1100000: episode: 1885, duration: 2.992s, episode steps: 473, steps per second: 158, episode reward: 183.460, mean reward: 0.388 [-20.181, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.048 [-0.679, 1.404], loss: 8.118399, mae: 49.251835, mean_q: 66.108940
  841956/1100000: episode: 1886, duration: 3.167s, episode steps: 503, steps per second: 159, episode reward: 192.627, mean reward: 0.383 [-17.999, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.084 [-0.794, 1.398], loss: 4.721703, mae: 49.141232, mean_q: 66.002792
  842363/1100000: episode: 1887, duration: 2.457s, episode steps: 407, steps per second: 166, episode reward: 188.710, mean reward: 0.464 [-14.211, 100.000], mean action: 1.654 [0.000, 3.000], mean observation: 0.138 [-0.773, 1.402], loss: 9.908535, mae: 49.024513, mean_q: 65.789574
  842665/1100000: episode: 1888, duration: 1.836s, episode steps: 302, steps per second: 164, episode reward: 255.990, mean reward: 0.848 [-11.746, 100.000], mean action: 1.930 [0.000, 3.000], mean observation: 0.063 [-0.971, 1.394], loss: 7.587972, mae: 48.654491, mean_q: 65.135208
  843297/1100000: episode: 1889, duration: 3.826s, episode steps: 632, steps per second: 165, episode reward: 264.578, mean reward: 0.419 [-20.653, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.245 [-0.913, 1.409], loss: 7.683064, mae: 48.744900, mean_q: 65.425407
  843623/1100000: episode: 1890, duration: 1.966s, episode steps: 326, steps per second: 166, episode reward: 237.139, mean reward: 0.727 [-11.350, 100.000], mean action: 2.135 [0.000, 3.000], mean observation: -0.015 [-0.647, 1.406], loss: 7.846496, mae: 48.538734, mean_q: 65.107063
  844164/1100000: episode: 1891, duration: 3.263s, episode steps: 541, steps per second: 166, episode reward: 206.774, mean reward: 0.382 [-22.651, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.006 [-0.730, 1.432], loss: 5.827963, mae: 48.869137, mean_q: 65.505341
  844612/1100000: episode: 1892, duration: 2.835s, episode steps: 448, steps per second: 158, episode reward: 217.673, mean reward: 0.486 [-20.751, 100.000], mean action: 1.871 [0.000, 3.000], mean observation: 0.212 [-0.718, 1.392], loss: 5.785060, mae: 48.565437, mean_q: 65.074959
  844893/1100000: episode: 1893, duration: 1.668s, episode steps: 281, steps per second: 168, episode reward: 221.564, mean reward: 0.788 [-17.408, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: -0.023 [-0.733, 1.406], loss: 5.068768, mae: 48.615742, mean_q: 65.578773
  845193/1100000: episode: 1894, duration: 1.776s, episode steps: 300, steps per second: 169, episode reward: 208.302, mean reward: 0.694 [-13.837, 100.000], mean action: 1.693 [0.000, 3.000], mean observation: 0.026 [-0.678, 1.445], loss: 9.508225, mae: 48.429993, mean_q: 65.004074
  846193/1100000: episode: 1895, duration: 6.827s, episode steps: 1000, steps per second: 146, episode reward: 133.184, mean reward: 0.133 [-18.148, 20.871], mean action: 1.764 [0.000, 3.000], mean observation: 0.229 [-0.933, 1.688], loss: 7.770332, mae: 48.883781, mean_q: 65.577888
  846288/1100000: episode: 1896, duration: 0.543s, episode steps: 95, steps per second: 175, episode reward: -118.246, mean reward: -1.245 [-100.000, 14.597], mean action: 1.884 [0.000, 3.000], mean observation: 0.104 [-4.134, 1.454], loss: 5.116621, mae: 49.142860, mean_q: 65.887909
  847288/1100000: episode: 1897, duration: 7.016s, episode steps: 1000, steps per second: 143, episode reward: -181.811, mean reward: -0.182 [-4.808, 5.572], mean action: 1.690 [0.000, 3.000], mean observation: -0.008 [-0.865, 1.393], loss: 5.548238, mae: 48.612194, mean_q: 65.287407
  847395/1100000: episode: 1898, duration: 0.616s, episode steps: 107, steps per second: 174, episode reward: -278.250, mean reward: -2.600 [-100.000, 4.178], mean action: 1.841 [0.000, 3.000], mean observation: 0.163 [-2.224, 3.951], loss: 5.510954, mae: 47.844765, mean_q: 64.314377
  847723/1100000: episode: 1899, duration: 1.937s, episode steps: 328, steps per second: 169, episode reward: 260.305, mean reward: 0.794 [-18.231, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.097 [-0.814, 1.453], loss: 6.042805, mae: 48.482849, mean_q: 64.716705
  848181/1100000: episode: 1900, duration: 2.825s, episode steps: 458, steps per second: 162, episode reward: 269.315, mean reward: 0.588 [-10.986, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: -0.005 [-0.685, 1.524], loss: 7.014095, mae: 48.354107, mean_q: 64.375275
  848418/1100000: episode: 1901, duration: 1.392s, episode steps: 237, steps per second: 170, episode reward: 230.084, mean reward: 0.971 [-11.031, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: -0.061 [-0.685, 1.393], loss: 5.851060, mae: 48.484447, mean_q: 64.578766
  849063/1100000: episode: 1902, duration: 3.923s, episode steps: 645, steps per second: 164, episode reward: 229.040, mean reward: 0.355 [-18.765, 100.000], mean action: 0.895 [0.000, 3.000], mean observation: 0.013 [-0.738, 1.387], loss: 6.672554, mae: 48.507393, mean_q: 64.805923
  849677/1100000: episode: 1903, duration: 3.883s, episode steps: 614, steps per second: 158, episode reward: 195.626, mean reward: 0.319 [-17.354, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.223 [-0.735, 1.402], loss: 7.273045, mae: 48.151436, mean_q: 64.259209
  850217/1100000: episode: 1904, duration: 3.324s, episode steps: 540, steps per second: 162, episode reward: 264.123, mean reward: 0.489 [-18.681, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.092 [-0.785, 1.390], loss: 9.701467, mae: 48.397182, mean_q: 64.580162
  850540/1100000: episode: 1905, duration: 1.965s, episode steps: 323, steps per second: 164, episode reward: 198.655, mean reward: 0.615 [-13.791, 100.000], mean action: 1.712 [0.000, 3.000], mean observation: 0.031 [-0.676, 1.469], loss: 7.536433, mae: 48.275253, mean_q: 64.690788
  851161/1100000: episode: 1906, duration: 3.789s, episode steps: 621, steps per second: 164, episode reward: 211.986, mean reward: 0.341 [-17.556, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.240 [-0.650, 1.436], loss: 5.741164, mae: 48.405487, mean_q: 64.694054
  851598/1100000: episode: 1907, duration: 2.725s, episode steps: 437, steps per second: 160, episode reward: 262.740, mean reward: 0.601 [-13.163, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.085 [-0.671, 1.499], loss: 7.201602, mae: 48.579422, mean_q: 64.969391
  852598/1100000: episode: 1908, duration: 6.518s, episode steps: 1000, steps per second: 153, episode reward: -26.099, mean reward: -0.026 [-4.859, 5.237], mean action: 1.589 [0.000, 3.000], mean observation: -0.012 [-0.801, 1.392], loss: 7.048295, mae: 48.635616, mean_q: 65.038277
  852889/1100000: episode: 1909, duration: 1.737s, episode steps: 291, steps per second: 168, episode reward: 235.441, mean reward: 0.809 [-9.071, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.054 [-0.667, 1.421], loss: 10.528320, mae: 48.155258, mean_q: 64.502647
  853232/1100000: episode: 1910, duration: 2.030s, episode steps: 343, steps per second: 169, episode reward: 229.283, mean reward: 0.668 [-18.175, 100.000], mean action: 1.653 [0.000, 3.000], mean observation: 0.132 [-0.567, 1.466], loss: 5.141859, mae: 48.616573, mean_q: 65.165016
  854232/1100000: episode: 1911, duration: 6.692s, episode steps: 1000, steps per second: 149, episode reward: 46.367, mean reward: 0.046 [-18.301, 14.389], mean action: 1.640 [0.000, 3.000], mean observation: 0.254 [-0.856, 1.400], loss: 6.084667, mae: 48.841301, mean_q: 65.362137
  855232/1100000: episode: 1912, duration: 6.229s, episode steps: 1000, steps per second: 161, episode reward: 86.653, mean reward: 0.087 [-19.096, 22.601], mean action: 1.478 [0.000, 3.000], mean observation: 0.262 [-0.631, 1.447], loss: 5.755483, mae: 48.776764, mean_q: 65.328613
  855821/1100000: episode: 1913, duration: 3.764s, episode steps: 589, steps per second: 156, episode reward: 173.021, mean reward: 0.294 [-19.963, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.195 [-0.652, 1.404], loss: 10.131858, mae: 48.396374, mean_q: 64.621803
  856266/1100000: episode: 1914, duration: 2.804s, episode steps: 445, steps per second: 159, episode reward: 267.247, mean reward: 0.601 [-19.325, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.125 [-0.755, 1.388], loss: 12.170703, mae: 48.109200, mean_q: 64.435181
  856629/1100000: episode: 1915, duration: 2.220s, episode steps: 363, steps per second: 164, episode reward: 222.440, mean reward: 0.613 [-18.087, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.094 [-0.592, 1.411], loss: 6.602262, mae: 48.099571, mean_q: 64.318588
  857106/1100000: episode: 1916, duration: 2.905s, episode steps: 477, steps per second: 164, episode reward: 238.440, mean reward: 0.500 [-17.425, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.146 [-0.736, 1.507], loss: 5.921985, mae: 48.284824, mean_q: 64.558594
  858106/1100000: episode: 1917, duration: 6.437s, episode steps: 1000, steps per second: 155, episode reward: -61.295, mean reward: -0.061 [-5.227, 4.828], mean action: 1.785 [0.000, 3.000], mean observation: -0.025 [-0.667, 1.408], loss: 7.045511, mae: 48.044804, mean_q: 64.346680
  858609/1100000: episode: 1918, duration: 3.043s, episode steps: 503, steps per second: 165, episode reward: 247.588, mean reward: 0.492 [-12.009, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.128 [-0.915, 1.453], loss: 5.844407, mae: 48.157429, mean_q: 64.307564
  858780/1100000: episode: 1919, duration: 0.988s, episode steps: 171, steps per second: 173, episode reward: 52.000, mean reward: 0.304 [-100.000, 11.293], mean action: 1.696 [0.000, 3.000], mean observation: 0.067 [-0.737, 1.388], loss: 7.480130, mae: 48.397160, mean_q: 64.873970
  859160/1100000: episode: 1920, duration: 2.283s, episode steps: 380, steps per second: 166, episode reward: 220.318, mean reward: 0.580 [-11.574, 100.000], mean action: 1.668 [0.000, 3.000], mean observation: 0.031 [-0.480, 1.397], loss: 4.415302, mae: 47.831280, mean_q: 64.091591
  859499/1100000: episode: 1921, duration: 2.049s, episode steps: 339, steps per second: 165, episode reward: 261.773, mean reward: 0.772 [-2.886, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: -0.060 [-0.800, 1.409], loss: 3.556270, mae: 47.908848, mean_q: 64.023590
  859941/1100000: episode: 1922, duration: 2.681s, episode steps: 442, steps per second: 165, episode reward: 230.512, mean reward: 0.522 [-9.750, 100.000], mean action: 2.113 [0.000, 3.000], mean observation: 0.060 [-0.779, 1.996], loss: 4.320059, mae: 47.521740, mean_q: 63.798794
  860232/1100000: episode: 1923, duration: 1.743s, episode steps: 291, steps per second: 167, episode reward: 229.811, mean reward: 0.790 [-3.128, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.051 [-0.569, 1.409], loss: 7.379524, mae: 47.918571, mean_q: 64.154022
  860881/1100000: episode: 1924, duration: 4.099s, episode steps: 649, steps per second: 158, episode reward: 117.188, mean reward: 0.181 [-14.345, 100.000], mean action: 1.781 [0.000, 3.000], mean observation: 0.125 [-0.888, 1.412], loss: 7.633600, mae: 47.443737, mean_q: 63.492153
  861430/1100000: episode: 1925, duration: 3.347s, episode steps: 549, steps per second: 164, episode reward: 215.853, mean reward: 0.393 [-19.470, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.005 [-1.020, 1.423], loss: 5.623829, mae: 46.953667, mean_q: 62.856300
  862006/1100000: episode: 1926, duration: 3.586s, episode steps: 576, steps per second: 161, episode reward: 218.879, mean reward: 0.380 [-20.945, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.019 [-0.600, 1.423], loss: 4.718312, mae: 46.964111, mean_q: 62.929905
  862428/1100000: episode: 1927, duration: 2.519s, episode steps: 422, steps per second: 168, episode reward: 218.601, mean reward: 0.518 [-8.278, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: -0.052 [-0.672, 1.464], loss: 5.296977, mae: 46.555668, mean_q: 62.153862
  862776/1100000: episode: 1928, duration: 2.096s, episode steps: 348, steps per second: 166, episode reward: 191.408, mean reward: 0.550 [-11.710, 100.000], mean action: 2.032 [0.000, 3.000], mean observation: 0.001 [-0.695, 1.409], loss: 4.138710, mae: 46.843410, mean_q: 62.707199
  863028/1100000: episode: 1929, duration: 1.473s, episode steps: 252, steps per second: 171, episode reward: 276.855, mean reward: 1.099 [-8.880, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.069 [-0.624, 1.400], loss: 10.305682, mae: 46.533585, mean_q: 62.253613
  863571/1100000: episode: 1930, duration: 3.311s, episode steps: 543, steps per second: 164, episode reward: 216.235, mean reward: 0.398 [-19.769, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.143 [-0.780, 1.449], loss: 5.065772, mae: 46.514748, mean_q: 62.105663
  863932/1100000: episode: 1931, duration: 2.198s, episode steps: 361, steps per second: 164, episode reward: 216.796, mean reward: 0.601 [-9.434, 100.000], mean action: 2.230 [0.000, 3.000], mean observation: 0.072 [-0.836, 1.406], loss: 4.868950, mae: 46.028732, mean_q: 61.487350
  864503/1100000: episode: 1932, duration: 3.460s, episode steps: 571, steps per second: 165, episode reward: 209.260, mean reward: 0.366 [-19.600, 100.000], mean action: 2.515 [0.000, 3.000], mean observation: 0.117 [-0.731, 1.393], loss: 5.554038, mae: 46.148491, mean_q: 61.728554
  864858/1100000: episode: 1933, duration: 2.080s, episode steps: 355, steps per second: 171, episode reward: 248.621, mean reward: 0.700 [-18.298, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: -0.011 [-0.600, 1.460], loss: 6.328490, mae: 46.189602, mean_q: 61.769382
  865168/1100000: episode: 1934, duration: 1.815s, episode steps: 310, steps per second: 171, episode reward: -220.564, mean reward: -0.711 [-100.000, 12.630], mean action: 1.571 [0.000, 3.000], mean observation: 0.045 [-1.659, 1.444], loss: 10.723421, mae: 46.412735, mean_q: 62.346294
  865401/1100000: episode: 1935, duration: 1.356s, episode steps: 233, steps per second: 172, episode reward: 7.983, mean reward: 0.034 [-100.000, 13.393], mean action: 1.790 [0.000, 3.000], mean observation: -0.104 [-0.931, 1.399], loss: 3.787687, mae: 46.131237, mean_q: 62.052910
  865759/1100000: episode: 1936, duration: 2.153s, episode steps: 358, steps per second: 166, episode reward: 276.850, mean reward: 0.773 [-17.502, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.096 [-0.741, 1.426], loss: 6.117588, mae: 46.529236, mean_q: 62.459095
  866243/1100000: episode: 1937, duration: 3.119s, episode steps: 484, steps per second: 155, episode reward: 170.480, mean reward: 0.352 [-17.320, 100.000], mean action: 1.932 [0.000, 3.000], mean observation: 0.000 [-0.679, 1.412], loss: 6.341563, mae: 46.195576, mean_q: 61.894493
  867243/1100000: episode: 1938, duration: 6.534s, episode steps: 1000, steps per second: 153, episode reward: 110.243, mean reward: 0.110 [-20.194, 21.064], mean action: 1.235 [0.000, 3.000], mean observation: 0.149 [-0.917, 1.386], loss: 6.881080, mae: 46.523865, mean_q: 62.319046
  868243/1100000: episode: 1939, duration: 6.121s, episode steps: 1000, steps per second: 163, episode reward: 95.088, mean reward: 0.095 [-17.478, 13.020], mean action: 0.925 [0.000, 3.000], mean observation: 0.049 [-0.761, 1.407], loss: 5.775377, mae: 45.968143, mean_q: 61.544491
  869046/1100000: episode: 1940, duration: 5.550s, episode steps: 803, steps per second: 145, episode reward: 149.449, mean reward: 0.186 [-18.824, 100.000], mean action: 1.959 [0.000, 3.000], mean observation: 0.145 [-0.699, 1.403], loss: 6.652330, mae: 45.190582, mean_q: 60.459255
  869318/1100000: episode: 1941, duration: 1.606s, episode steps: 272, steps per second: 169, episode reward: 237.417, mean reward: 0.873 [-12.839, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.082 [-0.748, 1.385], loss: 3.624083, mae: 45.150894, mean_q: 60.566967
  869735/1100000: episode: 1942, duration: 2.481s, episode steps: 417, steps per second: 168, episode reward: 191.467, mean reward: 0.459 [-13.732, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.170 [-0.700, 1.433], loss: 5.259604, mae: 45.125740, mean_q: 60.207157
  870074/1100000: episode: 1943, duration: 2.041s, episode steps: 339, steps per second: 166, episode reward: 236.169, mean reward: 0.697 [-17.616, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.103 [-0.827, 1.407], loss: 6.065609, mae: 45.497856, mean_q: 60.631359
  870410/1100000: episode: 1944, duration: 1.992s, episode steps: 336, steps per second: 169, episode reward: 259.040, mean reward: 0.771 [-8.791, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.073 [-0.668, 1.415], loss: 6.299551, mae: 45.464622, mean_q: 60.737183
  871410/1100000: episode: 1945, duration: 6.533s, episode steps: 1000, steps per second: 153, episode reward: 115.071, mean reward: 0.115 [-19.212, 22.829], mean action: 1.464 [0.000, 3.000], mean observation: 0.063 [-0.652, 1.391], loss: 5.350667, mae: 45.347195, mean_q: 60.524609
  871924/1100000: episode: 1946, duration: 3.141s, episode steps: 514, steps per second: 164, episode reward: 229.587, mean reward: 0.447 [-17.672, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: -0.005 [-0.642, 1.517], loss: 5.316092, mae: 44.709061, mean_q: 59.956764
  872417/1100000: episode: 1947, duration: 2.956s, episode steps: 493, steps per second: 167, episode reward: 198.956, mean reward: 0.404 [-6.882, 100.000], mean action: 1.635 [0.000, 3.000], mean observation: -0.046 [-0.737, 1.514], loss: 4.897219, mae: 45.136631, mean_q: 60.452602
  872782/1100000: episode: 1948, duration: 2.147s, episode steps: 365, steps per second: 170, episode reward: 256.177, mean reward: 0.702 [-13.854, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: -0.027 [-0.600, 1.387], loss: 3.944607, mae: 45.031651, mean_q: 60.196880
  873114/1100000: episode: 1949, duration: 1.950s, episode steps: 332, steps per second: 170, episode reward: 262.548, mean reward: 0.791 [-17.332, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.114 [-0.670, 1.431], loss: 4.271359, mae: 45.110970, mean_q: 60.300480
  873430/1100000: episode: 1950, duration: 1.940s, episode steps: 316, steps per second: 163, episode reward: 242.534, mean reward: 0.768 [-8.576, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.082 [-0.819, 1.403], loss: 5.577975, mae: 44.941776, mean_q: 59.871353
  873884/1100000: episode: 1951, duration: 2.746s, episode steps: 454, steps per second: 165, episode reward: -41.071, mean reward: -0.090 [-100.000, 12.468], mean action: 1.780 [0.000, 3.000], mean observation: 0.081 [-0.860, 1.456], loss: 5.032377, mae: 44.976898, mean_q: 59.917793
  874424/1100000: episode: 1952, duration: 3.201s, episode steps: 540, steps per second: 169, episode reward: 191.531, mean reward: 0.355 [-14.831, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.110 [-0.759, 1.411], loss: 5.376081, mae: 45.087696, mean_q: 60.276318
  874928/1100000: episode: 1953, duration: 3.083s, episode steps: 504, steps per second: 163, episode reward: 245.175, mean reward: 0.486 [-18.004, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: -0.019 [-0.715, 1.389], loss: 4.788225, mae: 44.871132, mean_q: 60.023514
  875202/1100000: episode: 1954, duration: 1.614s, episode steps: 274, steps per second: 170, episode reward: 226.647, mean reward: 0.827 [-3.515, 100.000], mean action: 1.854 [0.000, 3.000], mean observation: 0.006 [-0.769, 1.395], loss: 4.401924, mae: 45.322674, mean_q: 60.633320
  875686/1100000: episode: 1955, duration: 3.001s, episode steps: 484, steps per second: 161, episode reward: 259.783, mean reward: 0.537 [-18.297, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.123 [-0.669, 1.478], loss: 11.492363, mae: 44.862411, mean_q: 59.810631
  876073/1100000: episode: 1956, duration: 2.335s, episode steps: 387, steps per second: 166, episode reward: 246.725, mean reward: 0.638 [-10.130, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.012 [-0.861, 1.405], loss: 5.599106, mae: 45.334644, mean_q: 60.597221
  876777/1100000: episode: 1957, duration: 4.519s, episode steps: 704, steps per second: 156, episode reward: 149.317, mean reward: 0.212 [-10.317, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: -0.034 [-0.600, 1.430], loss: 6.077176, mae: 45.329552, mean_q: 60.438610
  877777/1100000: episode: 1958, duration: 6.975s, episode steps: 1000, steps per second: 143, episode reward: 4.375, mean reward: 0.004 [-7.782, 5.886], mean action: 1.833 [0.000, 3.000], mean observation: -0.031 [-0.690, 1.390], loss: 5.229056, mae: 45.067406, mean_q: 60.300739
  878140/1100000: episode: 1959, duration: 2.228s, episode steps: 363, steps per second: 163, episode reward: 242.413, mean reward: 0.668 [-17.334, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: -0.056 [-0.739, 1.391], loss: 4.771246, mae: 45.364162, mean_q: 60.573486
  878500/1100000: episode: 1960, duration: 2.138s, episode steps: 360, steps per second: 168, episode reward: 237.185, mean reward: 0.659 [-12.206, 100.000], mean action: 2.036 [0.000, 3.000], mean observation: 0.033 [-0.680, 1.391], loss: 4.028192, mae: 45.150986, mean_q: 60.503159
  878876/1100000: episode: 1961, duration: 2.255s, episode steps: 376, steps per second: 167, episode reward: 197.801, mean reward: 0.526 [-19.764, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.191 [-0.795, 1.401], loss: 5.715077, mae: 45.561836, mean_q: 60.886024
  879208/1100000: episode: 1962, duration: 1.983s, episode steps: 332, steps per second: 167, episode reward: 254.860, mean reward: 0.768 [-19.618, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.126 [-0.820, 1.515], loss: 6.078987, mae: 45.326202, mean_q: 60.300709
  879692/1100000: episode: 1963, duration: 3.008s, episode steps: 484, steps per second: 161, episode reward: 195.340, mean reward: 0.404 [-19.473, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: -0.020 [-0.731, 1.390], loss: 7.920501, mae: 45.259895, mean_q: 60.478878
  880036/1100000: episode: 1964, duration: 2.086s, episode steps: 344, steps per second: 165, episode reward: 254.520, mean reward: 0.740 [-9.521, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: -0.038 [-0.702, 1.391], loss: 4.923854, mae: 45.383427, mean_q: 60.345009
  880353/1100000: episode: 1965, duration: 1.868s, episode steps: 317, steps per second: 170, episode reward: 247.233, mean reward: 0.780 [-17.594, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: -0.060 [-0.662, 1.411], loss: 6.209132, mae: 45.039604, mean_q: 60.224777
  880676/1100000: episode: 1966, duration: 1.917s, episode steps: 323, steps per second: 169, episode reward: 287.278, mean reward: 0.889 [-10.509, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.060 [-0.732, 1.451], loss: 5.510626, mae: 45.813282, mean_q: 61.396164
  881028/1100000: episode: 1967, duration: 2.104s, episode steps: 352, steps per second: 167, episode reward: 251.724, mean reward: 0.715 [-17.829, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: -0.012 [-0.650, 1.437], loss: 4.505729, mae: 45.151505, mean_q: 60.276402
  881368/1100000: episode: 1968, duration: 2.062s, episode steps: 340, steps per second: 165, episode reward: 242.562, mean reward: 0.713 [-8.824, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.184 [-0.750, 1.388], loss: 6.192497, mae: 45.382267, mean_q: 60.523506
  882058/1100000: episode: 1969, duration: 4.215s, episode steps: 690, steps per second: 164, episode reward: 233.550, mean reward: 0.338 [-13.359, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.166 [-0.726, 1.401], loss: 5.399225, mae: 45.040901, mean_q: 60.084068
  882261/1100000: episode: 1970, duration: 1.180s, episode steps: 203, steps per second: 172, episode reward: -138.394, mean reward: -0.682 [-100.000, 7.272], mean action: 1.512 [0.000, 3.000], mean observation: 0.102 [-2.271, 1.402], loss: 4.759966, mae: 44.796642, mean_q: 59.929466
  882365/1100000: episode: 1971, duration: 0.588s, episode steps: 104, steps per second: 177, episode reward: -44.153, mean reward: -0.425 [-100.000, 16.949], mean action: 1.163 [0.000, 3.000], mean observation: 0.183 [-1.151, 1.498], loss: 2.777649, mae: 44.959480, mean_q: 60.278419
  882954/1100000: episode: 1972, duration: 3.606s, episode steps: 589, steps per second: 163, episode reward: 183.430, mean reward: 0.311 [-12.340, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: 0.132 [-0.650, 1.472], loss: 7.262268, mae: 45.291485, mean_q: 60.397987
  883334/1100000: episode: 1973, duration: 2.356s, episode steps: 380, steps per second: 161, episode reward: 232.522, mean reward: 0.612 [-12.089, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.151 [-0.814, 1.388], loss: 4.170343, mae: 44.999073, mean_q: 60.340786
  883760/1100000: episode: 1974, duration: 2.578s, episode steps: 426, steps per second: 165, episode reward: -33.083, mean reward: -0.078 [-100.000, 16.887], mean action: 1.692 [0.000, 3.000], mean observation: 0.074 [-0.658, 1.430], loss: 5.079075, mae: 44.634453, mean_q: 59.557888
  884145/1100000: episode: 1975, duration: 2.300s, episode steps: 385, steps per second: 167, episode reward: 249.690, mean reward: 0.649 [-12.462, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: -0.056 [-0.691, 1.406], loss: 6.644956, mae: 44.396866, mean_q: 59.329514
  884408/1100000: episode: 1976, duration: 1.507s, episode steps: 263, steps per second: 174, episode reward: 248.180, mean reward: 0.944 [-11.827, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: -0.059 [-0.692, 1.468], loss: 5.570200, mae: 44.738766, mean_q: 59.784168
  885124/1100000: episode: 1977, duration: 4.495s, episode steps: 716, steps per second: 159, episode reward: 186.934, mean reward: 0.261 [-18.051, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.144 [-0.699, 1.386], loss: 6.250930, mae: 44.871857, mean_q: 59.689457
  885552/1100000: episode: 1978, duration: 2.646s, episode steps: 428, steps per second: 162, episode reward: 251.329, mean reward: 0.587 [-18.243, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.150 [-1.397, 1.518], loss: 6.576768, mae: 45.073036, mean_q: 59.911304
  885883/1100000: episode: 1979, duration: 2.003s, episode steps: 331, steps per second: 165, episode reward: 236.521, mean reward: 0.715 [-17.418, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: -0.032 [-0.770, 1.448], loss: 5.115793, mae: 44.983776, mean_q: 60.147598
  886217/1100000: episode: 1980, duration: 1.992s, episode steps: 334, steps per second: 168, episode reward: 266.366, mean reward: 0.798 [-4.066, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.227 [-0.634, 1.393], loss: 6.147279, mae: 45.219090, mean_q: 60.100651
  886705/1100000: episode: 1981, duration: 2.888s, episode steps: 488, steps per second: 169, episode reward: 243.221, mean reward: 0.498 [-20.525, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.152 [-0.709, 1.393], loss: 6.091237, mae: 45.109447, mean_q: 60.050892
  887109/1100000: episode: 1982, duration: 2.387s, episode steps: 404, steps per second: 169, episode reward: 46.505, mean reward: 0.115 [-100.000, 15.091], mean action: 1.725 [0.000, 3.000], mean observation: 0.144 [-0.705, 1.412], loss: 4.627782, mae: 44.909885, mean_q: 59.950100
  887724/1100000: episode: 1983, duration: 4.107s, episode steps: 615, steps per second: 150, episode reward: 216.801, mean reward: 0.353 [-17.400, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: -0.007 [-0.815, 1.492], loss: 6.532144, mae: 44.934029, mean_q: 59.943275
  888049/1100000: episode: 1984, duration: 1.901s, episode steps: 325, steps per second: 171, episode reward: 280.080, mean reward: 0.862 [-13.916, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.057 [-1.103, 1.400], loss: 6.798582, mae: 45.216793, mean_q: 59.882092
  888837/1100000: episode: 1985, duration: 5.232s, episode steps: 788, steps per second: 151, episode reward: 234.736, mean reward: 0.298 [-18.700, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.158 [-0.576, 1.411], loss: 4.940218, mae: 45.081318, mean_q: 59.966835
  889022/1100000: episode: 1986, duration: 1.065s, episode steps: 185, steps per second: 174, episode reward: -471.370, mean reward: -2.548 [-100.000, 3.823], mean action: 1.962 [0.000, 3.000], mean observation: 0.224 [-0.837, 2.436], loss: 5.731982, mae: 45.207062, mean_q: 59.918823
  889152/1100000: episode: 1987, duration: 0.750s, episode steps: 130, steps per second: 173, episode reward: -193.659, mean reward: -1.490 [-100.000, 2.440], mean action: 1.562 [0.000, 3.000], mean observation: 0.324 [-0.438, 1.426], loss: 7.081706, mae: 45.277500, mean_q: 59.717648
  890108/1100000: episode: 1988, duration: 6.201s, episode steps: 956, steps per second: 154, episode reward: 236.749, mean reward: 0.248 [-22.952, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.023 [-0.738, 1.516], loss: 9.572430, mae: 45.263596, mean_q: 60.408051
  890505/1100000: episode: 1989, duration: 2.331s, episode steps: 397, steps per second: 170, episode reward: 272.991, mean reward: 0.688 [-17.794, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.088 [-0.650, 1.390], loss: 5.663079, mae: 44.805992, mean_q: 59.729179
  890881/1100000: episode: 1990, duration: 2.281s, episode steps: 376, steps per second: 165, episode reward: 234.707, mean reward: 0.624 [-5.579, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.173 [-0.520, 1.393], loss: 8.320411, mae: 44.899452, mean_q: 59.961185
  891750/1100000: episode: 1991, duration: 5.565s, episode steps: 869, steps per second: 156, episode reward: 163.710, mean reward: 0.188 [-12.487, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: -0.041 [-0.705, 1.521], loss: 6.644555, mae: 44.821896, mean_q: 59.834976
  892269/1100000: episode: 1992, duration: 3.397s, episode steps: 519, steps per second: 153, episode reward: 189.705, mean reward: 0.366 [-9.414, 100.000], mean action: 1.717 [0.000, 3.000], mean observation: 0.089 [-1.353, 1.649], loss: 6.179157, mae: 44.912540, mean_q: 59.944294
  893120/1100000: episode: 1993, duration: 5.556s, episode steps: 851, steps per second: 153, episode reward: 175.964, mean reward: 0.207 [-17.565, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: -0.016 [-0.600, 1.398], loss: 5.642875, mae: 44.687622, mean_q: 59.519142
  894120/1100000: episode: 1994, duration: 6.398s, episode steps: 1000, steps per second: 156, episode reward: -34.828, mean reward: -0.035 [-5.029, 5.351], mean action: 1.793 [0.000, 3.000], mean observation: 0.077 [-0.558, 1.416], loss: 5.829121, mae: 44.337223, mean_q: 59.220421
  894611/1100000: episode: 1995, duration: 3.020s, episode steps: 491, steps per second: 163, episode reward: 271.464, mean reward: 0.553 [-18.604, 100.000], mean action: 0.986 [0.000, 3.000], mean observation: 0.081 [-0.734, 1.385], loss: 8.196960, mae: 44.604767, mean_q: 59.349884
  895038/1100000: episode: 1996, duration: 2.746s, episode steps: 427, steps per second: 155, episode reward: -297.412, mean reward: -0.697 [-100.000, 29.245], mean action: 1.681 [0.000, 3.000], mean observation: 0.131 [-0.726, 1.713], loss: 9.370789, mae: 44.634293, mean_q: 59.246620
  895262/1100000: episode: 1997, duration: 1.294s, episode steps: 224, steps per second: 173, episode reward: 266.370, mean reward: 1.189 [-10.117, 100.000], mean action: 1.464 [0.000, 3.000], mean observation: -0.080 [-0.628, 1.387], loss: 6.207780, mae: 43.962090, mean_q: 58.460815
  895558/1100000: episode: 1998, duration: 1.759s, episode steps: 296, steps per second: 168, episode reward: 211.027, mean reward: 0.713 [-9.533, 100.000], mean action: 1.530 [0.000, 3.000], mean observation: -0.057 [-0.731, 1.404], loss: 10.752653, mae: 44.174274, mean_q: 58.787518
  895722/1100000: episode: 1999, duration: 0.942s, episode steps: 164, steps per second: 174, episode reward: -57.859, mean reward: -0.353 [-100.000, 15.099], mean action: 1.537 [0.000, 3.000], mean observation: 0.022 [-0.697, 1.406], loss: 6.850987, mae: 44.298824, mean_q: 59.035156
  896678/1100000: episode: 2000, duration: 5.967s, episode steps: 956, steps per second: 160, episode reward: -233.134, mean reward: -0.244 [-100.000, 30.439], mean action: 1.683 [0.000, 3.000], mean observation: 0.078 [-1.387, 2.180], loss: 6.901214, mae: 44.515263, mean_q: 59.223881
  896984/1100000: episode: 2001, duration: 1.788s, episode steps: 306, steps per second: 171, episode reward: 273.295, mean reward: 0.893 [-9.481, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.106 [-0.615, 1.448], loss: 8.539792, mae: 44.305702, mean_q: 58.906097
  897085/1100000: episode: 2002, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: -198.468, mean reward: -1.965 [-100.000, 2.100], mean action: 1.782 [0.000, 3.000], mean observation: 0.359 [-0.267, 1.403], loss: 2.099139, mae: 43.634293, mean_q: 58.266300
  897337/1100000: episode: 2003, duration: 1.441s, episode steps: 252, steps per second: 175, episode reward: 258.328, mean reward: 1.025 [-11.281, 100.000], mean action: 0.869 [0.000, 3.000], mean observation: 0.024 [-0.737, 1.391], loss: 6.193020, mae: 43.935448, mean_q: 58.359608
  897611/1100000: episode: 2004, duration: 1.593s, episode steps: 274, steps per second: 172, episode reward: 285.215, mean reward: 1.041 [-5.147, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.059 [-0.665, 1.639], loss: 6.288773, mae: 44.241531, mean_q: 58.940998
  898127/1100000: episode: 2005, duration: 3.071s, episode steps: 516, steps per second: 168, episode reward: 236.470, mean reward: 0.458 [-19.275, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.009 [-0.600, 1.402], loss: 9.267548, mae: 44.549129, mean_q: 59.269184
  898657/1100000: episode: 2006, duration: 3.354s, episode steps: 530, steps per second: 158, episode reward: 209.513, mean reward: 0.395 [-19.329, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.248 [-0.494, 1.413], loss: 6.473378, mae: 44.391388, mean_q: 59.212532
  898975/1100000: episode: 2007, duration: 1.898s, episode steps: 318, steps per second: 168, episode reward: 248.015, mean reward: 0.780 [-18.395, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.085 [-0.672, 1.447], loss: 9.512775, mae: 44.501266, mean_q: 59.547913
  899300/1100000: episode: 2008, duration: 1.935s, episode steps: 325, steps per second: 168, episode reward: 244.832, mean reward: 0.753 [-20.133, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: -0.027 [-0.766, 1.452], loss: 4.834312, mae: 44.551327, mean_q: 59.601719
  899781/1100000: episode: 2009, duration: 2.958s, episode steps: 481, steps per second: 163, episode reward: 248.867, mean reward: 0.517 [-10.422, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.006 [-0.604, 1.825], loss: 8.802988, mae: 44.642159, mean_q: 59.498856
  900399/1100000: episode: 2010, duration: 3.807s, episode steps: 618, steps per second: 162, episode reward: 144.758, mean reward: 0.234 [-14.588, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.167 [-0.914, 1.416], loss: 6.903153, mae: 44.575607, mean_q: 59.570477
  901155/1100000: episode: 2011, duration: 4.698s, episode steps: 756, steps per second: 161, episode reward: 241.493, mean reward: 0.319 [-18.458, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.126 [-0.657, 1.426], loss: 7.081354, mae: 44.557034, mean_q: 59.505096
  901470/1100000: episode: 2012, duration: 1.863s, episode steps: 315, steps per second: 169, episode reward: 263.669, mean reward: 0.837 [-17.880, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.068 [-0.514, 1.486], loss: 4.861495, mae: 44.395218, mean_q: 59.511795
  902064/1100000: episode: 2013, duration: 3.819s, episode steps: 594, steps per second: 156, episode reward: 183.524, mean reward: 0.309 [-17.891, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.174 [-0.630, 1.400], loss: 5.248538, mae: 44.202103, mean_q: 59.125683
  903047/1100000: episode: 2014, duration: 7.192s, episode steps: 983, steps per second: 137, episode reward: 209.242, mean reward: 0.213 [-17.443, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.042 [-0.600, 1.713], loss: 5.461680, mae: 44.094257, mean_q: 59.034924
  903720/1100000: episode: 2015, duration: 4.232s, episode steps: 673, steps per second: 159, episode reward: 143.989, mean reward: 0.214 [-12.297, 100.000], mean action: 1.866 [0.000, 3.000], mean observation: 0.112 [-0.617, 1.423], loss: 5.429701, mae: 44.003925, mean_q: 58.987110
  904386/1100000: episode: 2016, duration: 4.276s, episode steps: 666, steps per second: 156, episode reward: 229.156, mean reward: 0.344 [-18.710, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.026 [-0.600, 1.645], loss: 6.755934, mae: 43.704697, mean_q: 58.527203
  905025/1100000: episode: 2017, duration: 3.969s, episode steps: 639, steps per second: 161, episode reward: 210.608, mean reward: 0.330 [-20.055, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: 0.143 [-0.661, 1.409], loss: 5.933606, mae: 43.609947, mean_q: 58.373020
  905900/1100000: episode: 2018, duration: 5.912s, episode steps: 875, steps per second: 148, episode reward: 194.085, mean reward: 0.222 [-19.446, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: -0.025 [-0.733, 1.647], loss: 4.821128, mae: 43.336845, mean_q: 58.090916
  906391/1100000: episode: 2019, duration: 2.925s, episode steps: 491, steps per second: 168, episode reward: 238.826, mean reward: 0.486 [-18.889, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.024 [-0.600, 1.666], loss: 5.148798, mae: 43.555111, mean_q: 58.571308
  906854/1100000: episode: 2020, duration: 2.849s, episode steps: 463, steps per second: 163, episode reward: 226.743, mean reward: 0.490 [-19.787, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: 0.115 [-0.667, 1.389], loss: 5.478202, mae: 43.240990, mean_q: 58.133980
  907589/1100000: episode: 2021, duration: 4.650s, episode steps: 735, steps per second: 158, episode reward: 145.082, mean reward: 0.197 [-14.130, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.005 [-0.688, 1.405], loss: 6.350604, mae: 43.218945, mean_q: 58.078918
  908189/1100000: episode: 2022, duration: 3.913s, episode steps: 600, steps per second: 153, episode reward: 191.574, mean reward: 0.319 [-18.132, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: -0.030 [-0.739, 1.493], loss: 6.916251, mae: 43.467350, mean_q: 58.364571
  908681/1100000: episode: 2023, duration: 2.985s, episode steps: 492, steps per second: 165, episode reward: 276.365, mean reward: 0.562 [-13.858, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.091 [-0.901, 1.513], loss: 5.042982, mae: 43.300114, mean_q: 58.099564
  909066/1100000: episode: 2024, duration: 2.283s, episode steps: 385, steps per second: 169, episode reward: 180.970, mean reward: 0.470 [-8.936, 100.000], mean action: 1.649 [0.000, 3.000], mean observation: 0.172 [-0.636, 1.411], loss: 5.709229, mae: 43.146896, mean_q: 57.921539
  909830/1100000: episode: 2025, duration: 5.165s, episode steps: 764, steps per second: 148, episode reward: 219.009, mean reward: 0.287 [-14.310, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.014 [-0.600, 1.667], loss: 4.805452, mae: 42.957817, mean_q: 57.647358
  910061/1100000: episode: 2026, duration: 1.345s, episode steps: 231, steps per second: 172, episode reward: 227.372, mean reward: 0.984 [-10.109, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.053 [-0.663, 1.412], loss: 6.667696, mae: 42.498581, mean_q: 56.934525
  910311/1100000: episode: 2027, duration: 1.481s, episode steps: 250, steps per second: 169, episode reward: 3.374, mean reward: 0.013 [-100.000, 13.647], mean action: 1.696 [0.000, 3.000], mean observation: -0.061 [-0.703, 1.398], loss: 7.309604, mae: 42.946098, mean_q: 57.682991
  910866/1100000: episode: 2028, duration: 3.415s, episode steps: 555, steps per second: 162, episode reward: -194.463, mean reward: -0.350 [-100.000, 13.829], mean action: 1.641 [0.000, 3.000], mean observation: -0.000 [-0.871, 1.414], loss: 6.307200, mae: 42.763226, mean_q: 57.540035
  911427/1100000: episode: 2029, duration: 3.380s, episode steps: 561, steps per second: 166, episode reward: 221.978, mean reward: 0.396 [-17.766, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: -0.017 [-0.936, 1.519], loss: 4.191229, mae: 43.065044, mean_q: 57.859753
  911797/1100000: episode: 2030, duration: 2.243s, episode steps: 370, steps per second: 165, episode reward: 244.517, mean reward: 0.661 [-8.375, 100.000], mean action: 1.927 [0.000, 3.000], mean observation: 0.049 [-0.600, 1.530], loss: 6.944426, mae: 43.658344, mean_q: 58.694942
  912402/1100000: episode: 2031, duration: 3.700s, episode steps: 605, steps per second: 164, episode reward: 205.634, mean reward: 0.340 [-23.475, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.038 [-0.662, 1.412], loss: 8.180900, mae: 43.674767, mean_q: 58.740837
  912608/1100000: episode: 2032, duration: 1.180s, episode steps: 206, steps per second: 175, episode reward: 2.605, mean reward: 0.013 [-100.000, 10.737], mean action: 1.330 [0.000, 3.000], mean observation: -0.047 [-0.675, 1.431], loss: 8.185941, mae: 43.874039, mean_q: 59.118439
  913459/1100000: episode: 2033, duration: 5.589s, episode steps: 851, steps per second: 152, episode reward: 144.349, mean reward: 0.170 [-18.225, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.256 [-1.019, 1.413], loss: 6.622551, mae: 43.519131, mean_q: 58.574966
  914029/1100000: episode: 2034, duration: 3.781s, episode steps: 570, steps per second: 151, episode reward: 236.338, mean reward: 0.415 [-18.638, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: -0.015 [-0.635, 1.405], loss: 5.484629, mae: 43.767387, mean_q: 58.872490
  914380/1100000: episode: 2035, duration: 2.063s, episode steps: 351, steps per second: 170, episode reward: 263.607, mean reward: 0.751 [-18.007, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.101 [-0.542, 1.424], loss: 4.923762, mae: 43.849716, mean_q: 59.046619
  914924/1100000: episode: 2036, duration: 3.377s, episode steps: 544, steps per second: 161, episode reward: 245.432, mean reward: 0.451 [-19.923, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: -0.034 [-0.779, 1.422], loss: 6.019124, mae: 44.073853, mean_q: 59.312183
  915197/1100000: episode: 2037, duration: 1.612s, episode steps: 273, steps per second: 169, episode reward: -14.360, mean reward: -0.053 [-100.000, 17.029], mean action: 1.857 [0.000, 3.000], mean observation: 0.126 [-0.505, 1.409], loss: 7.358185, mae: 44.378021, mean_q: 59.633171
  915624/1100000: episode: 2038, duration: 2.662s, episode steps: 427, steps per second: 160, episode reward: 253.102, mean reward: 0.593 [-17.315, 100.000], mean action: 2.096 [0.000, 3.000], mean observation: 0.042 [-0.794, 1.395], loss: 5.567747, mae: 44.132103, mean_q: 59.459621
  916192/1100000: episode: 2039, duration: 3.565s, episode steps: 568, steps per second: 159, episode reward: 252.494, mean reward: 0.445 [-17.538, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.004 [-0.693, 1.406], loss: 5.701263, mae: 44.366081, mean_q: 59.634670
  916501/1100000: episode: 2040, duration: 1.828s, episode steps: 309, steps per second: 169, episode reward: 257.322, mean reward: 0.833 [-8.096, 100.000], mean action: 1.951 [0.000, 3.000], mean observation: -0.054 [-0.752, 1.416], loss: 7.388685, mae: 44.801899, mean_q: 60.237080
  916727/1100000: episode: 2041, duration: 1.306s, episode steps: 226, steps per second: 173, episode reward: -276.969, mean reward: -1.226 [-100.000, 4.869], mean action: 1.562 [0.000, 3.000], mean observation: 0.081 [-2.249, 1.425], loss: 4.630876, mae: 44.655525, mean_q: 60.080578
  917133/1100000: episode: 2042, duration: 2.460s, episode steps: 406, steps per second: 165, episode reward: 262.280, mean reward: 0.646 [-18.727, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.191 [-0.742, 1.388], loss: 6.690490, mae: 44.583553, mean_q: 60.006535
  917405/1100000: episode: 2043, duration: 1.610s, episode steps: 272, steps per second: 169, episode reward: 225.607, mean reward: 0.829 [-18.480, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: -0.005 [-0.723, 1.393], loss: 4.841267, mae: 45.196266, mean_q: 60.746510
  918405/1100000: episode: 2044, duration: 6.412s, episode steps: 1000, steps per second: 156, episode reward: 119.959, mean reward: 0.120 [-19.492, 13.203], mean action: 2.425 [0.000, 3.000], mean observation: 0.111 [-0.782, 1.525], loss: 5.900939, mae: 44.974693, mean_q: 60.502117
  918856/1100000: episode: 2045, duration: 2.718s, episode steps: 451, steps per second: 166, episode reward: 257.699, mean reward: 0.571 [-17.126, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.002 [-0.600, 1.393], loss: 6.894736, mae: 45.559940, mean_q: 61.311920
  919199/1100000: episode: 2046, duration: 1.977s, episode steps: 343, steps per second: 173, episode reward: 279.591, mean reward: 0.815 [-7.885, 100.000], mean action: 0.764 [0.000, 3.000], mean observation: 0.269 [-1.178, 1.431], loss: 4.991665, mae: 45.620426, mean_q: 61.462311
  919699/1100000: episode: 2047, duration: 3.170s, episode steps: 500, steps per second: 158, episode reward: 228.839, mean reward: 0.458 [-18.081, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.138 [-0.633, 1.517], loss: 5.888428, mae: 45.588436, mean_q: 61.407562
  920037/1100000: episode: 2048, duration: 2.037s, episode steps: 338, steps per second: 166, episode reward: 17.301, mean reward: 0.051 [-100.000, 11.104], mean action: 1.852 [0.000, 3.000], mean observation: -0.087 [-0.726, 1.519], loss: 5.257307, mae: 45.633446, mean_q: 61.466507
  920681/1100000: episode: 2049, duration: 3.879s, episode steps: 644, steps per second: 166, episode reward: 239.933, mean reward: 0.373 [-23.818, 100.000], mean action: 1.815 [0.000, 3.000], mean observation: 0.056 [-0.600, 1.462], loss: 6.344620, mae: 45.706623, mean_q: 61.528023
  920832/1100000: episode: 2050, duration: 0.870s, episode steps: 151, steps per second: 174, episode reward: 7.500, mean reward: 0.050 [-100.000, 13.021], mean action: 1.497 [0.000, 3.000], mean observation: 0.097 [-0.793, 2.791], loss: 6.792264, mae: 46.324078, mean_q: 62.147156
  921345/1100000: episode: 2051, duration: 3.157s, episode steps: 513, steps per second: 162, episode reward: 239.685, mean reward: 0.467 [-18.297, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.018 [-0.600, 1.404], loss: 5.493820, mae: 46.188923, mean_q: 62.182617
  921608/1100000: episode: 2052, duration: 1.552s, episode steps: 263, steps per second: 169, episode reward: 246.868, mean reward: 0.939 [-17.380, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.085 [-0.674, 1.407], loss: 8.540874, mae: 46.657688, mean_q: 62.906116
  922029/1100000: episode: 2053, duration: 2.463s, episode steps: 421, steps per second: 171, episode reward: 240.826, mean reward: 0.572 [-19.822, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.002 [-0.696, 1.430], loss: 6.565306, mae: 47.169201, mean_q: 63.338463
  922803/1100000: episode: 2054, duration: 5.064s, episode steps: 774, steps per second: 153, episode reward: 203.999, mean reward: 0.264 [-19.983, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.051 [-0.674, 1.484], loss: 7.587264, mae: 47.054768, mean_q: 63.172752
  923225/1100000: episode: 2055, duration: 2.594s, episode steps: 422, steps per second: 163, episode reward: 247.068, mean reward: 0.585 [-3.170, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.127 [-0.606, 1.465], loss: 7.275777, mae: 47.528610, mean_q: 63.871075
  924225/1100000: episode: 2056, duration: 6.207s, episode steps: 1000, steps per second: 161, episode reward: 138.150, mean reward: 0.138 [-19.936, 21.533], mean action: 1.476 [0.000, 3.000], mean observation: 0.138 [-0.636, 1.495], loss: 7.302391, mae: 47.674931, mean_q: 64.048225
  924661/1100000: episode: 2057, duration: 2.657s, episode steps: 436, steps per second: 164, episode reward: 255.461, mean reward: 0.586 [-10.494, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: -0.048 [-0.664, 1.489], loss: 7.475119, mae: 47.718246, mean_q: 64.136459
  925018/1100000: episode: 2058, duration: 2.110s, episode steps: 357, steps per second: 169, episode reward: 248.307, mean reward: 0.696 [-17.794, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: -0.067 [-0.709, 1.409], loss: 5.900328, mae: 47.845654, mean_q: 64.208397
  925329/1100000: episode: 2059, duration: 1.849s, episode steps: 311, steps per second: 168, episode reward: 251.031, mean reward: 0.807 [-10.185, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.157 [-0.705, 1.397], loss: 5.783252, mae: 47.401989, mean_q: 63.661087
  925766/1100000: episode: 2060, duration: 2.565s, episode steps: 437, steps per second: 170, episode reward: 216.811, mean reward: 0.496 [-19.893, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.242 [-0.809, 1.404], loss: 6.449211, mae: 47.879364, mean_q: 64.240204
  926482/1100000: episode: 2061, duration: 4.581s, episode steps: 716, steps per second: 156, episode reward: 216.076, mean reward: 0.302 [-20.595, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.222 [-0.973, 1.420], loss: 8.498213, mae: 48.369701, mean_q: 64.882843
  926755/1100000: episode: 2062, duration: 1.604s, episode steps: 273, steps per second: 170, episode reward: 232.827, mean reward: 0.853 [-18.456, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.143 [-0.498, 1.399], loss: 5.163535, mae: 48.508049, mean_q: 65.145050
  927089/1100000: episode: 2063, duration: 1.962s, episode steps: 334, steps per second: 170, episode reward: 186.849, mean reward: 0.559 [-11.601, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.187 [-0.659, 1.407], loss: 8.231116, mae: 48.397774, mean_q: 64.734917
  927477/1100000: episode: 2064, duration: 2.318s, episode steps: 388, steps per second: 167, episode reward: 215.878, mean reward: 0.556 [-10.959, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.442], loss: 7.855853, mae: 48.653614, mean_q: 65.439445
  927831/1100000: episode: 2065, duration: 2.123s, episode steps: 354, steps per second: 167, episode reward: -67.029, mean reward: -0.189 [-100.000, 15.494], mean action: 1.850 [0.000, 3.000], mean observation: 0.159 [-0.794, 1.767], loss: 4.836042, mae: 49.323467, mean_q: 66.129753
  928251/1100000: episode: 2066, duration: 2.650s, episode steps: 420, steps per second: 158, episode reward: 266.792, mean reward: 0.635 [-18.073, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.208 [-0.705, 1.394], loss: 4.976428, mae: 49.472759, mean_q: 66.326271
  928609/1100000: episode: 2067, duration: 2.097s, episode steps: 358, steps per second: 171, episode reward: 239.909, mean reward: 0.670 [-17.369, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.165 [-0.852, 1.523], loss: 9.562607, mae: 49.197002, mean_q: 65.951683
  928946/1100000: episode: 2068, duration: 2.038s, episode steps: 337, steps per second: 165, episode reward: 259.353, mean reward: 0.770 [-3.907, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: -0.006 [-0.600, 1.413], loss: 9.008211, mae: 49.470867, mean_q: 66.403061
  929654/1100000: episode: 2069, duration: 4.432s, episode steps: 708, steps per second: 160, episode reward: 214.357, mean reward: 0.303 [-12.956, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.191 [-0.863, 1.459], loss: 7.605639, mae: 49.640579, mean_q: 66.722527
  930015/1100000: episode: 2070, duration: 2.136s, episode steps: 361, steps per second: 169, episode reward: 274.784, mean reward: 0.761 [-9.450, 100.000], mean action: 0.958 [0.000, 3.000], mean observation: 0.222 [-0.824, 1.400], loss: 7.415726, mae: 49.948429, mean_q: 67.021111
  930487/1100000: episode: 2071, duration: 2.876s, episode steps: 472, steps per second: 164, episode reward: 233.793, mean reward: 0.495 [-17.850, 100.000], mean action: 1.504 [0.000, 3.000], mean observation: 0.128 [-0.580, 1.423], loss: 10.345019, mae: 49.470741, mean_q: 66.539085
  931097/1100000: episode: 2072, duration: 3.819s, episode steps: 610, steps per second: 160, episode reward: 244.616, mean reward: 0.401 [-17.476, 100.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.129 [-0.763, 1.421], loss: 6.622530, mae: 49.023907, mean_q: 65.853256
  931570/1100000: episode: 2073, duration: 2.801s, episode steps: 473, steps per second: 169, episode reward: 273.086, mean reward: 0.577 [-19.556, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.105 [-0.650, 1.409], loss: 7.338789, mae: 49.258114, mean_q: 66.181892
  932570/1100000: episode: 2074, duration: 6.374s, episode steps: 1000, steps per second: 157, episode reward: 106.411, mean reward: 0.106 [-20.591, 12.237], mean action: 0.915 [0.000, 3.000], mean observation: 0.020 [-0.710, 1.485], loss: 5.847960, mae: 48.591427, mean_q: 65.229881
  933105/1100000: episode: 2075, duration: 3.367s, episode steps: 535, steps per second: 159, episode reward: 187.126, mean reward: 0.350 [-18.029, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: -0.015 [-0.801, 1.402], loss: 6.046457, mae: 48.518471, mean_q: 65.137627
  933517/1100000: episode: 2076, duration: 2.626s, episode steps: 412, steps per second: 157, episode reward: 217.664, mean reward: 0.528 [-10.317, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.090 [-0.665, 1.414], loss: 5.407368, mae: 48.360199, mean_q: 64.961136
  933791/1100000: episode: 2077, duration: 1.611s, episode steps: 274, steps per second: 170, episode reward: 230.149, mean reward: 0.840 [-15.039, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.270 [-1.227, 1.442], loss: 7.463439, mae: 48.185223, mean_q: 64.739853
  934160/1100000: episode: 2078, duration: 2.236s, episode steps: 369, steps per second: 165, episode reward: 237.451, mean reward: 0.643 [-10.301, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: -0.054 [-0.658, 1.418], loss: 6.546199, mae: 48.245586, mean_q: 65.004753
  934645/1100000: episode: 2079, duration: 3.018s, episode steps: 485, steps per second: 161, episode reward: 238.996, mean reward: 0.493 [-10.568, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: -0.037 [-0.763, 1.435], loss: 5.522243, mae: 48.193699, mean_q: 64.671242
  934898/1100000: episode: 2080, duration: 1.479s, episode steps: 253, steps per second: 171, episode reward: -209.507, mean reward: -0.828 [-100.000, 11.280], mean action: 1.585 [0.000, 3.000], mean observation: 0.040 [-1.275, 1.627], loss: 4.652582, mae: 48.402336, mean_q: 65.183136
  935190/1100000: episode: 2081, duration: 1.717s, episode steps: 292, steps per second: 170, episode reward: -75.629, mean reward: -0.259 [-100.000, 24.461], mean action: 1.668 [0.000, 3.000], mean observation: 0.012 [-0.630, 1.805], loss: 5.068291, mae: 47.997021, mean_q: 64.410248
  935773/1100000: episode: 2082, duration: 3.830s, episode steps: 583, steps per second: 152, episode reward: 216.119, mean reward: 0.371 [-17.547, 100.000], mean action: 1.693 [0.000, 3.000], mean observation: 0.137 [-0.691, 1.402], loss: 6.969628, mae: 48.583092, mean_q: 65.130737
  936039/1100000: episode: 2083, duration: 1.539s, episode steps: 266, steps per second: 173, episode reward: -197.280, mean reward: -0.742 [-100.000, 10.555], mean action: 1.410 [0.000, 3.000], mean observation: 0.141 [-0.742, 1.444], loss: 7.028956, mae: 49.236938, mean_q: 65.919746
  936548/1100000: episode: 2084, duration: 3.296s, episode steps: 509, steps per second: 154, episode reward: 217.975, mean reward: 0.428 [-18.737, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: -0.023 [-0.759, 1.499], loss: 6.463417, mae: 49.252350, mean_q: 66.210770
  936911/1100000: episode: 2085, duration: 2.229s, episode steps: 363, steps per second: 163, episode reward: 240.298, mean reward: 0.662 [-9.800, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.035 [-0.727, 1.388], loss: 7.003983, mae: 49.024212, mean_q: 65.946991
  937217/1100000: episode: 2086, duration: 1.794s, episode steps: 306, steps per second: 171, episode reward: -21.728, mean reward: -0.071 [-100.000, 14.745], mean action: 1.833 [0.000, 3.000], mean observation: 0.241 [-1.122, 1.904], loss: 8.347969, mae: 48.969486, mean_q: 65.715454
  937431/1100000: episode: 2087, duration: 1.252s, episode steps: 214, steps per second: 171, episode reward: -278.024, mean reward: -1.299 [-100.000, 2.607], mean action: 1.883 [0.000, 3.000], mean observation: 0.267 [-0.643, 2.067], loss: 7.580742, mae: 49.257721, mean_q: 66.075752
  937593/1100000: episode: 2088, duration: 0.926s, episode steps: 162, steps per second: 175, episode reward: -31.934, mean reward: -0.197 [-100.000, 16.249], mean action: 1.543 [0.000, 3.000], mean observation: 0.107 [-0.865, 1.700], loss: 6.446504, mae: 49.533047, mean_q: 66.519920
  938048/1100000: episode: 2089, duration: 2.735s, episode steps: 455, steps per second: 166, episode reward: 232.537, mean reward: 0.511 [-12.160, 100.000], mean action: 1.776 [0.000, 3.000], mean observation: 0.050 [-0.785, 1.430], loss: 9.866756, mae: 49.599689, mean_q: 66.450653
  939048/1100000: episode: 2090, duration: 6.389s, episode steps: 1000, steps per second: 157, episode reward: 150.938, mean reward: 0.151 [-20.734, 21.763], mean action: 1.504 [0.000, 3.000], mean observation: 0.045 [-0.725, 1.388], loss: 7.630435, mae: 49.390041, mean_q: 66.147812
  939199/1100000: episode: 2091, duration: 0.870s, episode steps: 151, steps per second: 174, episode reward: -135.027, mean reward: -0.894 [-100.000, 4.220], mean action: 1.536 [0.000, 3.000], mean observation: 0.015 [-1.006, 1.390], loss: 6.086911, mae: 48.639862, mean_q: 65.114349
  939493/1100000: episode: 2092, duration: 1.753s, episode steps: 294, steps per second: 168, episode reward: 227.528, mean reward: 0.774 [-22.458, 100.000], mean action: 2.279 [0.000, 3.000], mean observation: 0.016 [-0.782, 1.401], loss: 10.316541, mae: 49.077633, mean_q: 65.644630
  939852/1100000: episode: 2093, duration: 2.128s, episode steps: 359, steps per second: 169, episode reward: 213.997, mean reward: 0.596 [-24.414, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: -0.010 [-0.774, 1.392], loss: 7.262578, mae: 49.182648, mean_q: 65.703499
  940112/1100000: episode: 2094, duration: 1.528s, episode steps: 260, steps per second: 170, episode reward: -290.522, mean reward: -1.117 [-100.000, 19.420], mean action: 1.819 [0.000, 3.000], mean observation: -0.052 [-2.407, 1.404], loss: 16.934679, mae: 49.202499, mean_q: 65.913628
  940342/1100000: episode: 2095, duration: 1.336s, episode steps: 230, steps per second: 172, episode reward: 244.604, mean reward: 1.063 [-3.066, 100.000], mean action: 0.800 [0.000, 3.000], mean observation: 0.271 [-1.113, 1.385], loss: 7.796601, mae: 49.052563, mean_q: 65.604950
  940841/1100000: episode: 2096, duration: 3.018s, episode steps: 499, steps per second: 165, episode reward: 229.545, mean reward: 0.460 [-20.761, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.021 [-0.719, 1.401], loss: 6.359565, mae: 49.491764, mean_q: 66.207916
  941812/1100000: episode: 2097, duration: 6.316s, episode steps: 971, steps per second: 154, episode reward: -100.164, mean reward: -0.103 [-100.000, 20.854], mean action: 1.802 [0.000, 3.000], mean observation: 0.021 [-0.778, 1.474], loss: 7.186956, mae: 49.343655, mean_q: 65.969284
  941988/1100000: episode: 2098, duration: 1.005s, episode steps: 176, steps per second: 175, episode reward: -211.838, mean reward: -1.204 [-100.000, 4.560], mean action: 1.733 [0.000, 3.000], mean observation: -0.012 [-1.600, 1.481], loss: 9.713840, mae: 49.165798, mean_q: 65.233612
  942362/1100000: episode: 2099, duration: 2.222s, episode steps: 374, steps per second: 168, episode reward: -136.383, mean reward: -0.365 [-100.000, 4.086], mean action: 1.762 [0.000, 3.000], mean observation: -0.002 [-1.000, 1.421], loss: 10.680343, mae: 49.504356, mean_q: 66.232346
  942611/1100000: episode: 2100, duration: 1.456s, episode steps: 249, steps per second: 171, episode reward: 254.043, mean reward: 1.020 [-9.907, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: -0.062 [-0.708, 1.399], loss: 11.648265, mae: 49.865078, mean_q: 66.658020
  943089/1100000: episode: 2101, duration: 2.850s, episode steps: 478, steps per second: 168, episode reward: 219.105, mean reward: 0.458 [-19.667, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.008 [-0.664, 1.395], loss: 6.494147, mae: 49.772339, mean_q: 66.234940
  943569/1100000: episode: 2102, duration: 2.972s, episode steps: 480, steps per second: 161, episode reward: 276.864, mean reward: 0.577 [-21.393, 100.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.138 [-0.722, 1.395], loss: 11.818508, mae: 49.660213, mean_q: 65.976517
  943667/1100000: episode: 2103, duration: 0.560s, episode steps: 98, steps per second: 175, episode reward: -207.699, mean reward: -2.119 [-100.000, 2.200], mean action: 1.990 [0.000, 3.000], mean observation: 0.047 [-1.432, 1.390], loss: 8.148262, mae: 49.982964, mean_q: 66.494148
  943956/1100000: episode: 2104, duration: 1.703s, episode steps: 289, steps per second: 170, episode reward: 243.587, mean reward: 0.843 [-9.716, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.083 [-0.814, 1.406], loss: 10.836382, mae: 49.653030, mean_q: 66.042885
  944083/1100000: episode: 2105, duration: 0.727s, episode steps: 127, steps per second: 175, episode reward: -201.623, mean reward: -1.588 [-100.000, 3.453], mean action: 1.819 [0.000, 3.000], mean observation: 0.047 [-1.341, 1.393], loss: 5.170767, mae: 48.979942, mean_q: 65.732430
  944501/1100000: episode: 2106, duration: 2.513s, episode steps: 418, steps per second: 166, episode reward: 215.637, mean reward: 0.516 [-18.439, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.159 [-0.861, 1.514], loss: 14.383630, mae: 49.842335, mean_q: 66.422699
  944959/1100000: episode: 2107, duration: 2.813s, episode steps: 458, steps per second: 163, episode reward: -241.429, mean reward: -0.527 [-100.000, 4.950], mean action: 1.915 [0.000, 3.000], mean observation: 0.078 [-1.001, 1.388], loss: 11.362203, mae: 49.304703, mean_q: 65.558754
  945162/1100000: episode: 2108, duration: 1.178s, episode steps: 203, steps per second: 172, episode reward: 239.464, mean reward: 1.180 [-12.010, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.155 [-0.726, 1.409], loss: 8.064322, mae: 49.006042, mean_q: 65.478951
  945750/1100000: episode: 2109, duration: 3.563s, episode steps: 588, steps per second: 165, episode reward: 181.601, mean reward: 0.309 [-23.630, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: -0.005 [-0.600, 1.435], loss: 9.332369, mae: 48.922352, mean_q: 65.190422
  945877/1100000: episode: 2110, duration: 0.709s, episode steps: 127, steps per second: 179, episode reward: -172.061, mean reward: -1.355 [-100.000, 3.121], mean action: 1.315 [0.000, 3.000], mean observation: 0.082 [-1.121, 1.542], loss: 10.214331, mae: 49.255512, mean_q: 65.749413
  946417/1100000: episode: 2111, duration: 3.465s, episode steps: 540, steps per second: 156, episode reward: 228.762, mean reward: 0.424 [-11.541, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: -0.010 [-0.680, 1.488], loss: 6.340432, mae: 48.925159, mean_q: 65.365021
  947417/1100000: episode: 2112, duration: 6.978s, episode steps: 1000, steps per second: 143, episode reward: -21.492, mean reward: -0.021 [-5.381, 5.145], mean action: 1.649 [0.000, 3.000], mean observation: 0.082 [-0.667, 1.397], loss: 8.475919, mae: 48.545406, mean_q: 64.890182
  947565/1100000: episode: 2113, duration: 0.841s, episode steps: 148, steps per second: 176, episode reward: -151.308, mean reward: -1.022 [-100.000, 2.861], mean action: 1.554 [0.000, 3.000], mean observation: 0.047 [-1.002, 1.434], loss: 6.525835, mae: 48.207443, mean_q: 64.655327
  948159/1100000: episode: 2114, duration: 3.687s, episode steps: 594, steps per second: 161, episode reward: 212.638, mean reward: 0.358 [-8.760, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: -0.013 [-0.645, 1.404], loss: 6.387399, mae: 48.129417, mean_q: 64.534515
  948497/1100000: episode: 2115, duration: 1.965s, episode steps: 338, steps per second: 172, episode reward: 233.886, mean reward: 0.692 [-10.587, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: -0.024 [-0.600, 1.467], loss: 8.484167, mae: 48.181095, mean_q: 64.370712
  948982/1100000: episode: 2116, duration: 2.957s, episode steps: 485, steps per second: 164, episode reward: 274.881, mean reward: 0.567 [-17.437, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.127 [-0.613, 1.392], loss: 6.874191, mae: 48.180508, mean_q: 64.274742
  949731/1100000: episode: 2117, duration: 4.973s, episode steps: 749, steps per second: 151, episode reward: 194.997, mean reward: 0.260 [-18.245, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.119 [-0.532, 1.389], loss: 7.967661, mae: 47.592007, mean_q: 63.477680
  950109/1100000: episode: 2118, duration: 2.213s, episode steps: 378, steps per second: 171, episode reward: 274.570, mean reward: 0.726 [-10.062, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.048 [-1.347, 1.401], loss: 6.244164, mae: 47.102119, mean_q: 62.909103
  950414/1100000: episode: 2119, duration: 1.831s, episode steps: 305, steps per second: 167, episode reward: 234.919, mean reward: 0.770 [-10.515, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: 0.155 [-0.880, 1.399], loss: 6.580027, mae: 47.020794, mean_q: 62.912975
  950820/1100000: episode: 2120, duration: 2.466s, episode steps: 406, steps per second: 165, episode reward: 231.248, mean reward: 0.570 [-17.978, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.103 [-0.652, 1.477], loss: 5.315024, mae: 46.843597, mean_q: 62.590599
  951568/1100000: episode: 2121, duration: 5.102s, episode steps: 748, steps per second: 147, episode reward: 241.122, mean reward: 0.322 [-20.739, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.204 [-0.825, 1.395], loss: 6.283665, mae: 46.594242, mean_q: 62.167957
  952163/1100000: episode: 2122, duration: 3.935s, episode steps: 595, steps per second: 151, episode reward: -103.672, mean reward: -0.174 [-100.000, 14.786], mean action: 1.800 [0.000, 3.000], mean observation: 0.097 [-0.712, 1.423], loss: 7.764078, mae: 46.089706, mean_q: 61.380493
  952812/1100000: episode: 2123, duration: 3.918s, episode steps: 649, steps per second: 166, episode reward: 267.875, mean reward: 0.413 [-12.392, 100.000], mean action: 2.072 [0.000, 3.000], mean observation: 0.001 [-0.788, 1.404], loss: 8.373298, mae: 45.845814, mean_q: 61.009445
  953541/1100000: episode: 2124, duration: 4.777s, episode steps: 729, steps per second: 153, episode reward: 200.530, mean reward: 0.275 [-17.789, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.242 [-1.007, 1.398], loss: 7.444086, mae: 45.509247, mean_q: 60.643936
  954158/1100000: episode: 2125, duration: 3.953s, episode steps: 617, steps per second: 156, episode reward: 260.217, mean reward: 0.422 [-20.668, 100.000], mean action: 1.669 [0.000, 3.000], mean observation: 0.101 [-0.703, 1.527], loss: 6.157200, mae: 45.494076, mean_q: 60.829842
  954434/1100000: episode: 2126, duration: 1.611s, episode steps: 276, steps per second: 171, episode reward: 256.853, mean reward: 0.931 [-8.750, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: -0.025 [-0.600, 1.430], loss: 6.882861, mae: 45.266117, mean_q: 60.633812
  954904/1100000: episode: 2127, duration: 2.797s, episode steps: 470, steps per second: 168, episode reward: 240.749, mean reward: 0.512 [-17.591, 100.000], mean action: 0.766 [0.000, 3.000], mean observation: 0.239 [-1.014, 1.482], loss: 7.553894, mae: 45.507481, mean_q: 60.879467
  955288/1100000: episode: 2128, duration: 2.332s, episode steps: 384, steps per second: 165, episode reward: 236.748, mean reward: 0.617 [-18.215, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.218 [-0.754, 1.388], loss: 9.155696, mae: 45.400066, mean_q: 60.781460
  956288/1100000: episode: 2129, duration: 6.428s, episode steps: 1000, steps per second: 156, episode reward: -45.230, mean reward: -0.045 [-4.842, 5.527], mean action: 1.822 [0.000, 3.000], mean observation: 0.104 [-0.493, 1.416], loss: 7.171042, mae: 45.413937, mean_q: 60.804573
  956915/1100000: episode: 2130, duration: 3.844s, episode steps: 627, steps per second: 163, episode reward: 238.546, mean reward: 0.380 [-18.951, 100.000], mean action: 1.876 [0.000, 3.000], mean observation: 0.118 [-0.669, 1.404], loss: 6.384697, mae: 45.006927, mean_q: 60.337944
  957615/1100000: episode: 2131, duration: 4.497s, episode steps: 700, steps per second: 156, episode reward: 201.280, mean reward: 0.288 [-17.992, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.057 [-0.696, 1.402], loss: 6.436258, mae: 45.016491, mean_q: 60.183544
  958039/1100000: episode: 2132, duration: 2.563s, episode steps: 424, steps per second: 165, episode reward: 258.097, mean reward: 0.609 [-10.832, 100.000], mean action: 1.575 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.485], loss: 5.159295, mae: 45.161358, mean_q: 60.549068
  958265/1100000: episode: 2133, duration: 1.297s, episode steps: 226, steps per second: 174, episode reward: 215.606, mean reward: 0.954 [-14.985, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.010 [-0.600, 1.403], loss: 8.227237, mae: 45.033653, mean_q: 60.450836
  958580/1100000: episode: 2134, duration: 1.899s, episode steps: 315, steps per second: 166, episode reward: 264.353, mean reward: 0.839 [-17.806, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.058 [-0.590, 1.396], loss: 7.258197, mae: 44.884430, mean_q: 60.092869
  959473/1100000: episode: 2135, duration: 5.468s, episode steps: 893, steps per second: 163, episode reward: 174.213, mean reward: 0.195 [-20.296, 100.000], mean action: 0.795 [0.000, 3.000], mean observation: 0.276 [-0.562, 1.452], loss: 6.577071, mae: 44.572529, mean_q: 59.547478
  959821/1100000: episode: 2136, duration: 2.049s, episode steps: 348, steps per second: 170, episode reward: 215.002, mean reward: 0.618 [-16.291, 100.000], mean action: 2.066 [0.000, 3.000], mean observation: 0.019 [-0.630, 1.454], loss: 9.502697, mae: 44.538830, mean_q: 59.491558
  960394/1100000: episode: 2137, duration: 3.565s, episode steps: 573, steps per second: 161, episode reward: 213.688, mean reward: 0.373 [-17.242, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: -0.015 [-0.600, 1.416], loss: 8.744011, mae: 44.796478, mean_q: 59.987137
  960846/1100000: episode: 2138, duration: 2.677s, episode steps: 452, steps per second: 169, episode reward: 216.000, mean reward: 0.478 [-16.170, 100.000], mean action: 1.863 [0.000, 3.000], mean observation: 0.065 [-0.668, 1.450], loss: 6.363302, mae: 44.358173, mean_q: 59.305473
  961489/1100000: episode: 2139, duration: 4.005s, episode steps: 643, steps per second: 161, episode reward: 195.180, mean reward: 0.304 [-18.521, 100.000], mean action: 1.966 [0.000, 3.000], mean observation: 0.223 [-0.481, 1.462], loss: 6.497063, mae: 44.376648, mean_q: 59.636841
  962023/1100000: episode: 2140, duration: 3.290s, episode steps: 534, steps per second: 162, episode reward: 191.181, mean reward: 0.358 [-18.630, 100.000], mean action: 0.895 [0.000, 3.000], mean observation: 0.013 [-0.669, 1.412], loss: 8.134639, mae: 44.164165, mean_q: 59.247696
  962504/1100000: episode: 2141, duration: 2.971s, episode steps: 481, steps per second: 162, episode reward: 250.355, mean reward: 0.520 [-19.208, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.208 [-0.840, 1.386], loss: 4.679629, mae: 44.545658, mean_q: 59.589809
  963239/1100000: episode: 2142, duration: 4.925s, episode steps: 735, steps per second: 149, episode reward: 141.592, mean reward: 0.193 [-21.318, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: -0.011 [-0.897, 1.401], loss: 6.596006, mae: 43.859150, mean_q: 58.825626
  963828/1100000: episode: 2143, duration: 3.737s, episode steps: 589, steps per second: 158, episode reward: 189.757, mean reward: 0.322 [-11.064, 100.000], mean action: 1.671 [0.000, 3.000], mean observation: 0.019 [-0.744, 1.432], loss: 9.109960, mae: 43.817635, mean_q: 58.749855
  964104/1100000: episode: 2144, duration: 1.639s, episode steps: 276, steps per second: 168, episode reward: 261.316, mean reward: 0.947 [-11.830, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: -0.032 [-0.737, 1.388], loss: 4.937496, mae: 43.811020, mean_q: 58.840317
  964601/1100000: episode: 2145, duration: 3.012s, episode steps: 497, steps per second: 165, episode reward: 264.860, mean reward: 0.533 [-7.952, 100.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.053 [-0.753, 1.439], loss: 6.515435, mae: 43.671982, mean_q: 58.661400
  964831/1100000: episode: 2146, duration: 1.338s, episode steps: 230, steps per second: 172, episode reward: 13.665, mean reward: 0.059 [-100.000, 12.357], mean action: 1.870 [0.000, 3.000], mean observation: -0.011 [-0.659, 1.497], loss: 3.768070, mae: 43.588257, mean_q: 58.645805
  965193/1100000: episode: 2147, duration: 2.171s, episode steps: 362, steps per second: 167, episode reward: 250.258, mean reward: 0.691 [-22.632, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: -0.035 [-0.817, 1.387], loss: 7.975884, mae: 44.014839, mean_q: 58.850525
  965683/1100000: episode: 2148, duration: 3.074s, episode steps: 490, steps per second: 159, episode reward: 247.358, mean reward: 0.505 [-19.050, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: -0.025 [-0.738, 1.408], loss: 6.850458, mae: 43.625824, mean_q: 58.371994
  966416/1100000: episode: 2149, duration: 4.920s, episode steps: 733, steps per second: 149, episode reward: 215.940, mean reward: 0.295 [-18.004, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.041 [-0.793, 1.404], loss: 6.897741, mae: 43.473713, mean_q: 58.339115
  967416/1100000: episode: 2150, duration: 7.020s, episode steps: 1000, steps per second: 142, episode reward: -1.319, mean reward: -0.001 [-4.165, 5.600], mean action: 1.718 [0.000, 3.000], mean observation: 0.118 [-0.493, 1.389], loss: 7.655276, mae: 43.207809, mean_q: 57.970860
  967880/1100000: episode: 2151, duration: 2.936s, episode steps: 464, steps per second: 158, episode reward: 191.082, mean reward: 0.412 [-17.935, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.192 [-0.714, 1.418], loss: 6.436865, mae: 43.155327, mean_q: 57.935383
  968019/1100000: episode: 2152, duration: 0.800s, episode steps: 139, steps per second: 174, episode reward: -51.139, mean reward: -0.368 [-100.000, 10.014], mean action: 1.755 [0.000, 3.000], mean observation: -0.113 [-0.745, 1.411], loss: 8.077119, mae: 43.369495, mean_q: 58.232456
  968151/1100000: episode: 2153, duration: 0.766s, episode steps: 132, steps per second: 172, episode reward: -5.260, mean reward: -0.040 [-100.000, 9.605], mean action: 2.121 [0.000, 3.000], mean observation: -0.124 [-0.718, 1.388], loss: 6.728291, mae: 43.439075, mean_q: 58.330811
  968604/1100000: episode: 2154, duration: 2.811s, episode steps: 453, steps per second: 161, episode reward: 224.421, mean reward: 0.495 [-18.256, 100.000], mean action: 2.161 [0.000, 3.000], mean observation: 0.089 [-0.547, 1.433], loss: 5.446594, mae: 43.251404, mean_q: 58.136093
  968958/1100000: episode: 2155, duration: 2.157s, episode steps: 354, steps per second: 164, episode reward: 238.925, mean reward: 0.675 [-17.349, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.075 [-0.552, 1.400], loss: 9.907743, mae: 43.584080, mean_q: 58.333721
  969283/1100000: episode: 2156, duration: 1.937s, episode steps: 325, steps per second: 168, episode reward: 292.967, mean reward: 0.901 [-2.871, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.096 [-0.786, 1.392], loss: 5.062529, mae: 43.565975, mean_q: 58.409073
  969845/1100000: episode: 2157, duration: 3.523s, episode steps: 562, steps per second: 160, episode reward: 212.389, mean reward: 0.378 [-20.144, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.213 [-0.599, 1.399], loss: 8.431867, mae: 43.389511, mean_q: 58.146046
  970286/1100000: episode: 2158, duration: 2.627s, episode steps: 441, steps per second: 168, episode reward: 221.432, mean reward: 0.502 [-19.503, 100.000], mean action: 2.345 [0.000, 3.000], mean observation: 0.071 [-0.742, 1.411], loss: 7.489858, mae: 43.439426, mean_q: 58.290623
  970610/1100000: episode: 2159, duration: 1.924s, episode steps: 324, steps per second: 168, episode reward: 235.682, mean reward: 0.727 [-17.297, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.009 [-1.002, 1.406], loss: 6.879544, mae: 43.314552, mean_q: 58.186813
  970780/1100000: episode: 2160, duration: 0.981s, episode steps: 170, steps per second: 173, episode reward: 18.907, mean reward: 0.111 [-100.000, 8.829], mean action: 1.582 [0.000, 3.000], mean observation: -0.031 [-0.600, 1.504], loss: 4.257597, mae: 43.271030, mean_q: 57.805550
  971167/1100000: episode: 2161, duration: 2.329s, episode steps: 387, steps per second: 166, episode reward: 256.931, mean reward: 0.664 [-3.119, 100.000], mean action: 1.584 [0.000, 3.000], mean observation: 0.082 [-0.498, 1.443], loss: 7.677768, mae: 43.305000, mean_q: 58.001549
  971494/1100000: episode: 2162, duration: 1.921s, episode steps: 327, steps per second: 170, episode reward: 243.476, mean reward: 0.745 [-9.325, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.072 [-0.590, 1.449], loss: 4.341692, mae: 43.176159, mean_q: 57.913841
  971992/1100000: episode: 2163, duration: 3.198s, episode steps: 498, steps per second: 156, episode reward: 265.468, mean reward: 0.533 [-20.712, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: -0.011 [-0.780, 1.424], loss: 4.516084, mae: 43.603657, mean_q: 58.547215
  972308/1100000: episode: 2164, duration: 1.913s, episode steps: 316, steps per second: 165, episode reward: 230.256, mean reward: 0.729 [-10.886, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.200 [-0.716, 1.428], loss: 7.840080, mae: 43.569088, mean_q: 58.426418
  972717/1100000: episode: 2165, duration: 2.554s, episode steps: 409, steps per second: 160, episode reward: 257.546, mean reward: 0.630 [-9.356, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.060 [-0.555, 1.393], loss: 6.861128, mae: 43.428875, mean_q: 58.267521
  973717/1100000: episode: 2166, duration: 6.548s, episode steps: 1000, steps per second: 153, episode reward: 123.645, mean reward: 0.124 [-18.266, 21.996], mean action: 1.015 [0.000, 3.000], mean observation: 0.225 [-0.499, 1.501], loss: 5.864788, mae: 43.726368, mean_q: 58.693153
  974205/1100000: episode: 2167, duration: 3.064s, episode steps: 488, steps per second: 159, episode reward: 185.203, mean reward: 0.380 [-12.374, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.187 [-0.846, 1.399], loss: 12.676431, mae: 43.459145, mean_q: 58.398880
  974654/1100000: episode: 2168, duration: 2.721s, episode steps: 449, steps per second: 165, episode reward: 256.334, mean reward: 0.571 [-6.159, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.100 [-0.780, 1.646], loss: 6.774190, mae: 44.018147, mean_q: 59.040047
  975539/1100000: episode: 2169, duration: 5.938s, episode steps: 885, steps per second: 149, episode reward: 233.158, mean reward: 0.263 [-19.927, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.139 [-0.522, 1.399], loss: 10.266933, mae: 44.302364, mean_q: 59.391094
  975941/1100000: episode: 2170, duration: 2.368s, episode steps: 402, steps per second: 170, episode reward: 233.335, mean reward: 0.580 [-18.230, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.198 [-0.655, 1.493], loss: 9.209378, mae: 44.468319, mean_q: 59.803669
  976250/1100000: episode: 2171, duration: 1.839s, episode steps: 309, steps per second: 168, episode reward: 248.246, mean reward: 0.803 [-9.215, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.101 [-0.624, 1.398], loss: 11.533417, mae: 44.387074, mean_q: 59.474926
  976366/1100000: episode: 2172, duration: 0.666s, episode steps: 116, steps per second: 174, episode reward: -183.089, mean reward: -1.578 [-100.000, 12.146], mean action: 1.586 [0.000, 3.000], mean observation: 0.141 [-0.802, 2.305], loss: 8.537778, mae: 44.879318, mean_q: 60.082066
  976639/1100000: episode: 2173, duration: 1.622s, episode steps: 273, steps per second: 168, episode reward: -323.038, mean reward: -1.183 [-100.000, 4.757], mean action: 1.722 [0.000, 3.000], mean observation: 0.136 [-2.140, 1.398], loss: 7.439897, mae: 44.448997, mean_q: 59.723602
  976868/1100000: episode: 2174, duration: 1.335s, episode steps: 229, steps per second: 172, episode reward: 277.608, mean reward: 1.212 [-12.813, 100.000], mean action: 2.100 [0.000, 3.000], mean observation: 0.007 [-0.825, 1.390], loss: 6.559030, mae: 44.472744, mean_q: 59.847954
  977506/1100000: episode: 2175, duration: 4.105s, episode steps: 638, steps per second: 155, episode reward: 216.313, mean reward: 0.339 [-18.375, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: -0.018 [-0.779, 1.407], loss: 8.001407, mae: 44.632122, mean_q: 59.927078
  977879/1100000: episode: 2176, duration: 2.226s, episode steps: 373, steps per second: 168, episode reward: 223.440, mean reward: 0.599 [-10.188, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.208 [-0.967, 1.397], loss: 5.791668, mae: 44.727604, mean_q: 60.038174
  978380/1100000: episode: 2177, duration: 3.080s, episode steps: 501, steps per second: 163, episode reward: 259.346, mean reward: 0.518 [-17.752, 100.000], mean action: 1.832 [0.000, 3.000], mean observation: 0.085 [-0.716, 1.417], loss: 6.280972, mae: 44.478168, mean_q: 59.750256
  979380/1100000: episode: 2178, duration: 6.815s, episode steps: 1000, steps per second: 147, episode reward: 31.598, mean reward: 0.032 [-19.924, 21.001], mean action: 1.621 [0.000, 3.000], mean observation: 0.141 [-0.688, 1.487], loss: 6.339201, mae: 44.375790, mean_q: 59.546608
  979569/1100000: episode: 2179, duration: 1.085s, episode steps: 189, steps per second: 174, episode reward: 7.064, mean reward: 0.037 [-100.000, 13.641], mean action: 1.354 [0.000, 3.000], mean observation: -0.043 [-0.858, 1.485], loss: 7.884798, mae: 44.354874, mean_q: 59.463543
  979898/1100000: episode: 2180, duration: 1.927s, episode steps: 329, steps per second: 171, episode reward: 233.645, mean reward: 0.710 [-11.304, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.242 [-0.863, 1.497], loss: 5.323530, mae: 43.993847, mean_q: 58.921291
  980412/1100000: episode: 2181, duration: 3.154s, episode steps: 514, steps per second: 163, episode reward: 253.725, mean reward: 0.494 [-18.474, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.007 [-0.665, 1.415], loss: 7.967811, mae: 43.956318, mean_q: 58.793682
  980995/1100000: episode: 2182, duration: 3.492s, episode steps: 583, steps per second: 167, episode reward: 253.863, mean reward: 0.435 [-18.579, 100.000], mean action: 0.887 [0.000, 3.000], mean observation: 0.035 [-0.940, 1.392], loss: 7.015544, mae: 43.825031, mean_q: 58.851173
  981477/1100000: episode: 2183, duration: 2.990s, episode steps: 482, steps per second: 161, episode reward: 272.687, mean reward: 0.566 [-17.882, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.123 [-0.570, 1.423], loss: 6.052725, mae: 43.715534, mean_q: 58.645653
  982196/1100000: episode: 2184, duration: 4.649s, episode steps: 719, steps per second: 155, episode reward: 226.822, mean reward: 0.315 [-17.777, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: 0.246 [-0.496, 1.408], loss: 6.765271, mae: 43.852020, mean_q: 58.636398
  982489/1100000: episode: 2185, duration: 1.745s, episode steps: 293, steps per second: 168, episode reward: 243.864, mean reward: 0.832 [-10.467, 100.000], mean action: 1.546 [0.000, 3.000], mean observation: -0.044 [-1.015, 1.406], loss: 5.944097, mae: 43.871372, mean_q: 58.803226
  982710/1100000: episode: 2186, duration: 1.299s, episode steps: 221, steps per second: 170, episode reward: -253.548, mean reward: -1.147 [-100.000, 21.190], mean action: 1.828 [0.000, 3.000], mean observation: -0.102 [-2.136, 1.391], loss: 5.098954, mae: 43.705669, mean_q: 58.466492
  983118/1100000: episode: 2187, duration: 2.405s, episode steps: 408, steps per second: 170, episode reward: 221.596, mean reward: 0.543 [-14.694, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: -0.023 [-0.626, 1.400], loss: 6.523841, mae: 43.900677, mean_q: 58.736759
  983816/1100000: episode: 2188, duration: 4.361s, episode steps: 698, steps per second: 160, episode reward: 231.222, mean reward: 0.331 [-18.498, 100.000], mean action: 0.867 [0.000, 3.000], mean observation: 0.117 [-0.655, 1.438], loss: 6.104321, mae: 43.606789, mean_q: 58.224079
  984268/1100000: episode: 2189, duration: 2.716s, episode steps: 452, steps per second: 166, episode reward: 229.986, mean reward: 0.509 [-19.356, 100.000], mean action: 1.830 [0.000, 3.000], mean observation: 0.034 [-0.682, 1.397], loss: 8.304281, mae: 43.618847, mean_q: 58.323772
  984667/1100000: episode: 2190, duration: 2.377s, episode steps: 399, steps per second: 168, episode reward: 205.808, mean reward: 0.516 [-10.160, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: -0.015 [-0.839, 1.426], loss: 7.751261, mae: 43.736343, mean_q: 58.457722
  985099/1100000: episode: 2191, duration: 2.604s, episode steps: 432, steps per second: 166, episode reward: 223.713, mean reward: 0.518 [-14.445, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.110 [-1.256, 1.464], loss: 6.232355, mae: 43.614826, mean_q: 58.123875
  985431/1100000: episode: 2192, duration: 2.022s, episode steps: 332, steps per second: 164, episode reward: 253.299, mean reward: 0.763 [-18.628, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.069 [-0.643, 1.441], loss: 6.356169, mae: 43.591270, mean_q: 58.005219
  985857/1100000: episode: 2193, duration: 2.530s, episode steps: 426, steps per second: 168, episode reward: 211.989, mean reward: 0.498 [-12.786, 100.000], mean action: 2.054 [0.000, 3.000], mean observation: 0.144 [-0.516, 1.423], loss: 4.979958, mae: 44.058926, mean_q: 58.755798
  986193/1100000: episode: 2194, duration: 1.972s, episode steps: 336, steps per second: 170, episode reward: -140.446, mean reward: -0.418 [-100.000, 10.678], mean action: 1.699 [0.000, 3.000], mean observation: 0.102 [-1.681, 1.456], loss: 7.389250, mae: 44.136322, mean_q: 58.995861
  986466/1100000: episode: 2195, duration: 1.581s, episode steps: 273, steps per second: 173, episode reward: 221.568, mean reward: 0.812 [-9.776, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: -0.017 [-0.639, 1.452], loss: 7.797204, mae: 44.255131, mean_q: 58.722652
  986749/1100000: episode: 2196, duration: 1.644s, episode steps: 283, steps per second: 172, episode reward: 211.699, mean reward: 0.748 [-16.088, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: -0.017 [-1.326, 1.418], loss: 6.570627, mae: 44.108616, mean_q: 58.833755
  987290/1100000: episode: 2197, duration: 3.297s, episode steps: 541, steps per second: 164, episode reward: 288.196, mean reward: 0.533 [-9.924, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.117 [-0.763, 1.387], loss: 5.315680, mae: 43.979523, mean_q: 58.451382
  987875/1100000: episode: 2198, duration: 3.589s, episode steps: 585, steps per second: 163, episode reward: 221.641, mean reward: 0.379 [-19.530, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: 0.105 [-0.793, 1.417], loss: 5.244125, mae: 43.868534, mean_q: 58.415485
  988151/1100000: episode: 2199, duration: 1.603s, episode steps: 276, steps per second: 172, episode reward: 244.849, mean reward: 0.887 [-11.782, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: -0.002 [-0.842, 1.430], loss: 7.371675, mae: 44.200798, mean_q: 58.875000
  988542/1100000: episode: 2200, duration: 2.402s, episode steps: 391, steps per second: 163, episode reward: 197.637, mean reward: 0.505 [-11.845, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: -0.017 [-0.866, 1.412], loss: 4.969083, mae: 44.246803, mean_q: 58.848507
  988810/1100000: episode: 2201, duration: 1.568s, episode steps: 268, steps per second: 171, episode reward: 202.176, mean reward: 0.754 [-19.800, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.266 [-0.916, 1.425], loss: 7.272242, mae: 44.577820, mean_q: 59.495518
  989250/1100000: episode: 2202, duration: 2.709s, episode steps: 440, steps per second: 162, episode reward: 264.455, mean reward: 0.601 [-19.562, 100.000], mean action: 0.814 [0.000, 3.000], mean observation: 0.028 [-0.600, 1.395], loss: 6.337288, mae: 43.963913, mean_q: 58.520618
  989640/1100000: episode: 2203, duration: 2.398s, episode steps: 390, steps per second: 163, episode reward: 283.457, mean reward: 0.727 [-19.606, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.066 [-0.764, 1.404], loss: 6.262067, mae: 44.268112, mean_q: 58.980377
  989967/1100000: episode: 2204, duration: 1.947s, episode steps: 327, steps per second: 168, episode reward: 229.686, mean reward: 0.702 [-18.870, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.014 [-0.600, 1.419], loss: 5.644861, mae: 44.468304, mean_q: 59.318104
  990365/1100000: episode: 2205, duration: 2.360s, episode steps: 398, steps per second: 169, episode reward: 268.429, mean reward: 0.674 [-11.335, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: -0.027 [-0.643, 1.402], loss: 5.470766, mae: 44.477097, mean_q: 59.207893
  991365/1100000: episode: 2206, duration: 7.579s, episode steps: 1000, steps per second: 132, episode reward: 50.994, mean reward: 0.051 [-9.159, 14.655], mean action: 1.670 [0.000, 3.000], mean observation: 0.062 [-0.583, 1.631], loss: 5.288539, mae: 44.402802, mean_q: 59.043484
  991885/1100000: episode: 2207, duration: 3.110s, episode steps: 520, steps per second: 167, episode reward: 228.955, mean reward: 0.440 [-18.752, 100.000], mean action: 1.821 [0.000, 3.000], mean observation: 0.090 [-0.649, 1.387], loss: 6.986809, mae: 44.286179, mean_q: 58.858105
  991975/1100000: episode: 2208, duration: 0.510s, episode steps: 90, steps per second: 177, episode reward: -79.884, mean reward: -0.888 [-100.000, 31.190], mean action: 1.289 [0.000, 3.000], mean observation: -0.039 [-1.223, 2.048], loss: 4.823805, mae: 44.418907, mean_q: 59.363213
  992326/1100000: episode: 2209, duration: 2.068s, episode steps: 351, steps per second: 170, episode reward: 227.280, mean reward: 0.648 [-9.713, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.003 [-0.600, 1.467], loss: 6.974268, mae: 44.151966, mean_q: 58.858219
  992693/1100000: episode: 2210, duration: 2.236s, episode steps: 367, steps per second: 164, episode reward: 258.340, mean reward: 0.704 [-11.070, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: -0.041 [-0.710, 1.413], loss: 9.315979, mae: 44.283310, mean_q: 58.797840
  993070/1100000: episode: 2211, duration: 2.300s, episode steps: 377, steps per second: 164, episode reward: 267.153, mean reward: 0.709 [-17.916, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.115 [-0.741, 1.388], loss: 10.517450, mae: 43.963963, mean_q: 58.320488
  993275/1100000: episode: 2212, duration: 1.170s, episode steps: 205, steps per second: 175, episode reward: -246.567, mean reward: -1.203 [-100.000, 5.379], mean action: 1.268 [0.000, 3.000], mean observation: -0.045 [-1.912, 1.487], loss: 6.108933, mae: 44.113537, mean_q: 58.553505
  993649/1100000: episode: 2213, duration: 2.208s, episode steps: 374, steps per second: 169, episode reward: 242.927, mean reward: 0.650 [-11.962, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: 0.067 [-0.686, 1.393], loss: 6.512403, mae: 44.022831, mean_q: 58.509834
  994157/1100000: episode: 2214, duration: 3.045s, episode steps: 508, steps per second: 167, episode reward: 30.390, mean reward: 0.060 [-100.000, 13.656], mean action: 1.898 [0.000, 3.000], mean observation: 0.087 [-0.724, 1.388], loss: 5.739100, mae: 44.173923, mean_q: 58.643768
  994605/1100000: episode: 2215, duration: 2.793s, episode steps: 448, steps per second: 160, episode reward: 254.617, mean reward: 0.568 [-17.658, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.047 [-0.562, 1.512], loss: 5.858028, mae: 44.263557, mean_q: 58.939991
  995091/1100000: episode: 2216, duration: 2.932s, episode steps: 486, steps per second: 166, episode reward: 237.255, mean reward: 0.488 [-22.205, 100.000], mean action: 1.586 [0.000, 3.000], mean observation: 0.051 [-0.736, 2.186], loss: 6.430352, mae: 44.351002, mean_q: 58.741978
  995447/1100000: episode: 2217, duration: 2.105s, episode steps: 356, steps per second: 169, episode reward: 256.840, mean reward: 0.721 [-13.843, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.010 [-0.788, 1.409], loss: 7.582425, mae: 44.646381, mean_q: 59.381111
  996118/1100000: episode: 2218, duration: 4.206s, episode steps: 671, steps per second: 160, episode reward: 222.881, mean reward: 0.332 [-18.695, 100.000], mean action: 2.258 [0.000, 3.000], mean observation: 0.242 [-0.586, 1.392], loss: 5.729640, mae: 44.680859, mean_q: 59.229858
  996330/1100000: episode: 2219, duration: 1.241s, episode steps: 212, steps per second: 171, episode reward: 281.149, mean reward: 1.326 [-8.566, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.202 [-0.772, 1.390], loss: 11.388847, mae: 44.726986, mean_q: 59.261810
  996765/1100000: episode: 2220, duration: 2.680s, episode steps: 435, steps per second: 162, episode reward: 245.213, mean reward: 0.564 [-17.765, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.100 [-0.599, 1.401], loss: 7.050725, mae: 45.050842, mean_q: 59.752457
  997005/1100000: episode: 2221, duration: 1.412s, episode steps: 240, steps per second: 170, episode reward: 262.606, mean reward: 1.094 [-8.714, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.070 [-0.540, 1.393], loss: 8.379650, mae: 45.006306, mean_q: 59.668774
  997339/1100000: episode: 2222, duration: 1.947s, episode steps: 334, steps per second: 172, episode reward: 230.446, mean reward: 0.690 [-18.898, 100.000], mean action: 0.808 [0.000, 3.000], mean observation: 0.205 [-0.629, 1.400], loss: 8.730416, mae: 44.693787, mean_q: 59.415401
  997519/1100000: episode: 2223, duration: 1.041s, episode steps: 180, steps per second: 173, episode reward: -191.486, mean reward: -1.064 [-100.000, 35.934], mean action: 1.656 [0.000, 3.000], mean observation: 0.026 [-0.863, 1.810], loss: 7.900618, mae: 44.939697, mean_q: 59.911285
  997895/1100000: episode: 2224, duration: 2.206s, episode steps: 376, steps per second: 170, episode reward: 5.581, mean reward: 0.015 [-100.000, 4.422], mean action: 1.670 [0.000, 3.000], mean observation: 0.087 [-0.858, 1.423], loss: 6.939935, mae: 45.161983, mean_q: 59.875347
  998454/1100000: episode: 2225, duration: 3.527s, episode steps: 559, steps per second: 158, episode reward: 205.626, mean reward: 0.368 [-18.935, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.114 [-0.610, 1.410], loss: 7.247835, mae: 45.368160, mean_q: 60.401779
  998966/1100000: episode: 2226, duration: 3.097s, episode steps: 512, steps per second: 165, episode reward: 207.720, mean reward: 0.406 [-19.988, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.274 [-1.302, 1.395], loss: 5.576213, mae: 45.229897, mean_q: 60.123642
  999813/1100000: episode: 2227, duration: 5.106s, episode steps: 847, steps per second: 166, episode reward: 233.751, mean reward: 0.276 [-18.271, 100.000], mean action: 0.771 [0.000, 3.000], mean observation: 0.038 [-0.600, 1.388], loss: 6.631164, mae: 45.268326, mean_q: 60.177776
 1000279/1100000: episode: 2228, duration: 2.888s, episode steps: 466, steps per second: 161, episode reward: 224.234, mean reward: 0.481 [-18.807, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.210 [-0.956, 1.715], loss: 5.812245, mae: 45.175541, mean_q: 59.979008
 1000532/1100000: episode: 2229, duration: 1.486s, episode steps: 253, steps per second: 170, episode reward: 219.218, mean reward: 0.866 [-10.776, 100.000], mean action: 2.091 [0.000, 3.000], mean observation: -0.026 [-0.634, 1.387], loss: 7.691937, mae: 45.296715, mean_q: 60.130863
 1001153/1100000: episode: 2230, duration: 4.094s, episode steps: 621, steps per second: 152, episode reward: 288.031, mean reward: 0.464 [-21.788, 100.000], mean action: 0.829 [0.000, 3.000], mean observation: 0.141 [-0.778, 1.395], loss: 9.613713, mae: 45.157913, mean_q: 59.856697
 1001527/1100000: episode: 2231, duration: 2.202s, episode steps: 374, steps per second: 170, episode reward: 246.548, mean reward: 0.659 [-3.156, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.075 [-0.588, 1.423], loss: 4.790868, mae: 45.156982, mean_q: 60.097980
 1001833/1100000: episode: 2232, duration: 1.815s, episode steps: 306, steps per second: 169, episode reward: 234.824, mean reward: 0.767 [-17.375, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.188 [-0.791, 1.412], loss: 6.124376, mae: 45.021713, mean_q: 59.802784
 1002365/1100000: episode: 2233, duration: 3.240s, episode steps: 532, steps per second: 164, episode reward: 254.272, mean reward: 0.478 [-17.318, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.261 [-0.679, 1.405], loss: 7.049191, mae: 45.170120, mean_q: 59.982677
 1002815/1100000: episode: 2234, duration: 2.729s, episode steps: 450, steps per second: 165, episode reward: 226.766, mean reward: 0.504 [-14.414, 100.000], mean action: 2.096 [0.000, 3.000], mean observation: 0.192 [-0.837, 1.517], loss: 5.115067, mae: 44.813297, mean_q: 59.727646
 1003233/1100000: episode: 2235, duration: 2.525s, episode steps: 418, steps per second: 166, episode reward: 252.842, mean reward: 0.605 [-19.728, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: -0.025 [-0.681, 1.435], loss: 5.029235, mae: 44.745934, mean_q: 59.567654
 1004218/1100000: episode: 2236, duration: 6.716s, episode steps: 985, steps per second: 147, episode reward: 175.955, mean reward: 0.179 [-18.737, 100.000], mean action: 2.013 [0.000, 3.000], mean observation: 0.158 [-0.766, 1.438], loss: 5.909951, mae: 44.769997, mean_q: 59.077602
 1004509/1100000: episode: 2237, duration: 1.754s, episode steps: 291, steps per second: 166, episode reward: 233.924, mean reward: 0.804 [-10.178, 100.000], mean action: 1.615 [0.000, 3.000], mean observation: 0.020 [-0.535, 1.410], loss: 6.132395, mae: 44.529987, mean_q: 58.634933
 1004843/1100000: episode: 2238, duration: 1.992s, episode steps: 334, steps per second: 168, episode reward: 268.392, mean reward: 0.804 [-6.899, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.097 [-0.677, 1.463], loss: 7.195037, mae: 44.871750, mean_q: 59.303108
 1005599/1100000: episode: 2239, duration: 4.941s, episode steps: 756, steps per second: 153, episode reward: 144.931, mean reward: 0.192 [-22.524, 100.000], mean action: 1.985 [0.000, 3.000], mean observation: 0.048 [-0.793, 1.433], loss: 12.751119, mae: 44.829201, mean_q: 59.242352
 1006103/1100000: episode: 2240, duration: 3.124s, episode steps: 504, steps per second: 161, episode reward: 211.724, mean reward: 0.420 [-18.987, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: -0.000 [-0.600, 1.399], loss: 5.968835, mae: 44.649948, mean_q: 59.271748
 1006927/1100000: episode: 2241, duration: 5.414s, episode steps: 824, steps per second: 152, episode reward: -183.023, mean reward: -0.222 [-100.000, 23.014], mean action: 1.862 [0.000, 3.000], mean observation: 0.159 [-1.370, 1.896], loss: 6.540097, mae: 44.964008, mean_q: 59.952679
 1007371/1100000: episode: 2242, duration: 2.699s, episode steps: 444, steps per second: 165, episode reward: 265.556, mean reward: 0.598 [-23.315, 100.000], mean action: 0.926 [0.000, 3.000], mean observation: 0.122 [-0.726, 1.391], loss: 4.162055, mae: 44.892803, mean_q: 60.098408
 1007754/1100000: episode: 2243, duration: 2.253s, episode steps: 383, steps per second: 170, episode reward: 251.193, mean reward: 0.656 [-17.886, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.245 [-0.641, 1.410], loss: 7.205625, mae: 45.203819, mean_q: 60.455093
 1008015/1100000: episode: 2244, duration: 1.548s, episode steps: 261, steps per second: 169, episode reward: 212.816, mean reward: 0.815 [-13.019, 100.000], mean action: 1.697 [0.000, 3.000], mean observation: -0.051 [-0.800, 1.388], loss: 4.816695, mae: 44.754288, mean_q: 59.880306
 1008455/1100000: episode: 2245, duration: 2.605s, episode steps: 440, steps per second: 169, episode reward: 240.864, mean reward: 0.547 [-18.184, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.108 [-0.786, 1.415], loss: 5.006789, mae: 44.554066, mean_q: 59.658588
 1009072/1100000: episode: 2246, duration: 3.834s, episode steps: 617, steps per second: 161, episode reward: 246.928, mean reward: 0.400 [-10.932, 100.000], mean action: 1.781 [0.000, 3.000], mean observation: 0.030 [-0.732, 1.389], loss: 6.982663, mae: 44.724579, mean_q: 59.817875
 1009414/1100000: episode: 2247, duration: 1.979s, episode steps: 342, steps per second: 173, episode reward: -128.006, mean reward: -0.374 [-100.000, 5.173], mean action: 1.749 [0.000, 3.000], mean observation: 0.050 [-0.690, 1.402], loss: 7.493029, mae: 44.385548, mean_q: 59.360260
 1009985/1100000: episode: 2248, duration: 3.603s, episode steps: 571, steps per second: 158, episode reward: 212.103, mean reward: 0.371 [-9.744, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: -0.017 [-1.107, 1.506], loss: 5.746132, mae: 44.818768, mean_q: 59.936974
 1010400/1100000: episode: 2249, duration: 2.497s, episode steps: 415, steps per second: 166, episode reward: 233.458, mean reward: 0.563 [-10.034, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: -0.010 [-0.651, 1.412], loss: 8.167900, mae: 44.948387, mean_q: 60.385208
 1010706/1100000: episode: 2250, duration: 1.833s, episode steps: 306, steps per second: 167, episode reward: 265.836, mean reward: 0.869 [-2.542, 100.000], mean action: 1.722 [0.000, 3.000], mean observation: 0.040 [-0.546, 1.421], loss: 5.204034, mae: 45.318718, mean_q: 60.881931
 1011017/1100000: episode: 2251, duration: 1.927s, episode steps: 311, steps per second: 161, episode reward: 293.775, mean reward: 0.945 [-8.874, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: 0.031 [-0.722, 1.387], loss: 5.170683, mae: 45.100475, mean_q: 60.685616
 1011726/1100000: episode: 2252, duration: 4.340s, episode steps: 709, steps per second: 163, episode reward: 244.325, mean reward: 0.345 [-19.109, 100.000], mean action: 0.685 [0.000, 3.000], mean observation: 0.154 [-0.684, 1.393], loss: 6.602541, mae: 45.243282, mean_q: 60.734123
 1012552/1100000: episode: 2253, duration: 5.417s, episode steps: 826, steps per second: 152, episode reward: 233.545, mean reward: 0.283 [-19.388, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.035 [-0.918, 1.428], loss: 5.715661, mae: 45.579845, mean_q: 61.160885
 1012935/1100000: episode: 2254, duration: 2.325s, episode steps: 383, steps per second: 165, episode reward: 266.220, mean reward: 0.695 [-19.024, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.106 [-0.695, 1.540], loss: 5.027131, mae: 45.507984, mean_q: 61.014381
 1013303/1100000: episode: 2255, duration: 2.158s, episode steps: 368, steps per second: 171, episode reward: 274.854, mean reward: 0.747 [-14.273, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.115 [-0.850, 1.436], loss: 7.175110, mae: 45.138702, mean_q: 60.660866
 1013488/1100000: episode: 2256, duration: 1.061s, episode steps: 185, steps per second: 174, episode reward: 237.077, mean reward: 1.281 [-3.314, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: -0.046 [-0.692, 1.410], loss: 5.542158, mae: 45.612579, mean_q: 61.408798
 1013838/1100000: episode: 2257, duration: 2.132s, episode steps: 350, steps per second: 164, episode reward: 250.937, mean reward: 0.717 [-11.523, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.178 [-0.565, 1.389], loss: 5.137410, mae: 45.247097, mean_q: 60.702847
 1014838/1100000: episode: 2258, duration: 6.333s, episode steps: 1000, steps per second: 158, episode reward: 28.683, mean reward: 0.029 [-19.413, 27.098], mean action: 1.597 [0.000, 3.000], mean observation: 0.119 [-0.811, 1.391], loss: 5.054364, mae: 45.482601, mean_q: 61.069939
 1015189/1100000: episode: 2259, duration: 2.055s, episode steps: 351, steps per second: 171, episode reward: 273.077, mean reward: 0.778 [-11.029, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.022 [-0.738, 1.402], loss: 5.109197, mae: 45.105236, mean_q: 60.640079
 1015910/1100000: episode: 2260, duration: 4.409s, episode steps: 721, steps per second: 164, episode reward: 159.649, mean reward: 0.221 [-18.172, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.124 [-1.013, 1.401], loss: 6.373950, mae: 45.078213, mean_q: 60.427498
 1016383/1100000: episode: 2261, duration: 2.937s, episode steps: 473, steps per second: 161, episode reward: 255.775, mean reward: 0.541 [-19.097, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.132 [-0.934, 1.391], loss: 5.307086, mae: 45.062248, mean_q: 60.460506
 1016860/1100000: episode: 2262, duration: 2.944s, episode steps: 477, steps per second: 162, episode reward: 239.026, mean reward: 0.501 [-20.868, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.126 [-0.623, 1.453], loss: 5.274916, mae: 45.089687, mean_q: 60.394192
 1017193/1100000: episode: 2263, duration: 1.959s, episode steps: 333, steps per second: 170, episode reward: 290.103, mean reward: 0.871 [-19.547, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.177 [-0.856, 1.391], loss: 5.319173, mae: 45.442860, mean_q: 60.983055
 1017546/1100000: episode: 2264, duration: 2.114s, episode steps: 353, steps per second: 167, episode reward: 252.515, mean reward: 0.715 [-17.652, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.016 [-0.654, 1.481], loss: 4.684344, mae: 45.220039, mean_q: 60.861080
 1018374/1100000: episode: 2265, duration: 5.433s, episode steps: 828, steps per second: 152, episode reward: 231.616, mean reward: 0.280 [-19.950, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.151 [-0.631, 1.527], loss: 6.063198, mae: 45.069580, mean_q: 60.473450
 1018854/1100000: episode: 2266, duration: 3.056s, episode steps: 480, steps per second: 157, episode reward: 268.699, mean reward: 0.560 [-18.441, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: -0.011 [-0.731, 1.467], loss: 5.502962, mae: 45.523777, mean_q: 61.086494
 1019325/1100000: episode: 2267, duration: 2.952s, episode steps: 471, steps per second: 160, episode reward: 238.565, mean reward: 0.507 [-19.268, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.122 [-0.663, 1.401], loss: 5.220854, mae: 45.166901, mean_q: 60.611153
 1019743/1100000: episode: 2268, duration: 2.502s, episode steps: 418, steps per second: 167, episode reward: 269.207, mean reward: 0.644 [-9.911, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.040 [-0.705, 1.403], loss: 6.100653, mae: 44.955444, mean_q: 60.406540
 1020340/1100000: episode: 2269, duration: 3.859s, episode steps: 597, steps per second: 155, episode reward: 206.890, mean reward: 0.347 [-18.615, 100.000], mean action: 1.034 [0.000, 3.000], mean observation: 0.067 [-0.672, 1.404], loss: 5.209864, mae: 45.344429, mean_q: 60.900608
 1020691/1100000: episode: 2270, duration: 2.239s, episode steps: 351, steps per second: 157, episode reward: 275.656, mean reward: 0.785 [-10.513, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.022 [-0.741, 1.388], loss: 3.380982, mae: 44.998119, mean_q: 60.438969
 1021054/1100000: episode: 2271, duration: 2.173s, episode steps: 363, steps per second: 167, episode reward: 266.072, mean reward: 0.733 [-3.538, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.059 [-0.857, 1.518], loss: 5.621610, mae: 45.722004, mean_q: 61.260246
 1021634/1100000: episode: 2272, duration: 3.727s, episode steps: 580, steps per second: 156, episode reward: 196.923, mean reward: 0.340 [-18.190, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.131 [-0.632, 1.432], loss: 4.758843, mae: 45.546425, mean_q: 61.084415
 1022304/1100000: episode: 2273, duration: 4.208s, episode steps: 670, steps per second: 159, episode reward: -312.007, mean reward: -0.466 [-100.000, 21.858], mean action: 1.464 [0.000, 3.000], mean observation: 0.104 [-1.089, 1.784], loss: 5.368372, mae: 45.808926, mean_q: 61.484177
 1022857/1100000: episode: 2274, duration: 3.373s, episode steps: 553, steps per second: 164, episode reward: -29.445, mean reward: -0.053 [-100.000, 30.076], mean action: 1.532 [0.000, 3.000], mean observation: -0.033 [-0.600, 1.415], loss: 6.812807, mae: 45.702778, mean_q: 61.429432
 1023125/1100000: episode: 2275, duration: 1.571s, episode steps: 268, steps per second: 171, episode reward: 222.265, mean reward: 0.829 [-12.772, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: -0.024 [-0.650, 1.403], loss: 6.382181, mae: 45.687275, mean_q: 61.440548
 1023405/1100000: episode: 2276, duration: 1.641s, episode steps: 280, steps per second: 171, episode reward: 241.044, mean reward: 0.861 [-9.534, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.067 [-0.773, 1.399], loss: 6.324220, mae: 45.569427, mean_q: 61.274044
 1024405/1100000: episode: 2277, duration: 6.907s, episode steps: 1000, steps per second: 145, episode reward: -21.781, mean reward: -0.022 [-5.096, 6.562], mean action: 1.940 [0.000, 3.000], mean observation: 0.009 [-0.755, 1.521], loss: 6.595747, mae: 45.580242, mean_q: 61.123703
 1024788/1100000: episode: 2278, duration: 2.296s, episode steps: 383, steps per second: 167, episode reward: 286.446, mean reward: 0.748 [-17.400, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.105 [-0.656, 1.423], loss: 4.864897, mae: 45.684460, mean_q: 61.380657
 1025170/1100000: episode: 2279, duration: 2.284s, episode steps: 382, steps per second: 167, episode reward: 230.412, mean reward: 0.603 [-17.396, 100.000], mean action: 2.209 [0.000, 3.000], mean observation: 0.032 [-0.600, 1.422], loss: 5.014383, mae: 45.861217, mean_q: 61.613152
 1025466/1100000: episode: 2280, duration: 1.739s, episode steps: 296, steps per second: 170, episode reward: 238.469, mean reward: 0.806 [-12.354, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.092 [-0.768, 1.397], loss: 4.467042, mae: 45.984459, mean_q: 61.731365
 1025674/1100000: episode: 2281, duration: 1.197s, episode steps: 208, steps per second: 174, episode reward: -28.875, mean reward: -0.139 [-100.000, 21.554], mean action: 1.639 [0.000, 3.000], mean observation: 0.096 [-0.863, 1.411], loss: 3.573892, mae: 46.002754, mean_q: 61.499477
 1026409/1100000: episode: 2282, duration: 4.950s, episode steps: 735, steps per second: 148, episode reward: 221.540, mean reward: 0.301 [-18.979, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.025 [-0.730, 1.409], loss: 6.865039, mae: 45.967869, mean_q: 61.437775
 1026761/1100000: episode: 2283, duration: 2.081s, episode steps: 352, steps per second: 169, episode reward: 251.873, mean reward: 0.716 [-17.378, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.179 [-0.692, 1.405], loss: 3.401244, mae: 45.828819, mean_q: 61.482201
 1026818/1100000: episode: 2284, duration: 0.328s, episode steps: 57, steps per second: 174, episode reward: -139.655, mean reward: -2.450 [-100.000, 5.351], mean action: 1.404 [0.000, 3.000], mean observation: 0.167 [-3.729, 1.392], loss: 8.262464, mae: 45.895199, mean_q: 61.234016
 1026985/1100000: episode: 2285, duration: 0.948s, episode steps: 167, steps per second: 176, episode reward: -8.587, mean reward: -0.051 [-100.000, 16.055], mean action: 1.455 [0.000, 3.000], mean observation: -0.016 [-0.600, 1.438], loss: 3.998217, mae: 45.457870, mean_q: 60.999134
 1027319/1100000: episode: 2286, duration: 1.954s, episode steps: 334, steps per second: 171, episode reward: 235.156, mean reward: 0.704 [-15.514, 100.000], mean action: 1.985 [0.000, 3.000], mean observation: -0.024 [-0.687, 1.496], loss: 6.285639, mae: 45.834953, mean_q: 61.696693
 1028319/1100000: episode: 2287, duration: 7.203s, episode steps: 1000, steps per second: 139, episode reward: 142.009, mean reward: 0.142 [-20.634, 22.842], mean action: 1.311 [0.000, 3.000], mean observation: 0.050 [-0.771, 1.385], loss: 5.541468, mae: 45.639725, mean_q: 61.349403
 1028642/1100000: episode: 2288, duration: 1.898s, episode steps: 323, steps per second: 170, episode reward: 288.199, mean reward: 0.892 [-9.279, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.079 [-0.653, 1.514], loss: 6.470475, mae: 45.252014, mean_q: 60.650845
 1028975/1100000: episode: 2289, duration: 1.993s, episode steps: 333, steps per second: 167, episode reward: 237.503, mean reward: 0.713 [-9.777, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.096 [-0.647, 1.418], loss: 4.964577, mae: 45.761116, mean_q: 61.403198
 1029241/1100000: episode: 2290, duration: 1.533s, episode steps: 266, steps per second: 174, episode reward: 241.806, mean reward: 0.909 [-17.351, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: -0.033 [-0.689, 1.385], loss: 3.729165, mae: 45.474144, mean_q: 60.984726
 1030167/1100000: episode: 2291, duration: 6.412s, episode steps: 926, steps per second: 144, episode reward: 177.369, mean reward: 0.192 [-17.548, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.196 [-0.754, 1.430], loss: 5.465746, mae: 45.166988, mean_q: 60.654030
 1030527/1100000: episode: 2292, duration: 2.143s, episode steps: 360, steps per second: 168, episode reward: 251.817, mean reward: 0.699 [-17.454, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.208 [-0.858, 1.405], loss: 6.893133, mae: 44.409657, mean_q: 59.697056
 1030837/1100000: episode: 2293, duration: 1.825s, episode steps: 310, steps per second: 170, episode reward: 230.134, mean reward: 0.742 [-13.895, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: -0.017 [-0.750, 1.392], loss: 6.410763, mae: 44.567047, mean_q: 59.813236
 1030944/1100000: episode: 2294, duration: 0.604s, episode steps: 107, steps per second: 177, episode reward: -164.522, mean reward: -1.538 [-100.000, 3.979], mean action: 1.318 [0.000, 3.000], mean observation: -0.017 [-1.386, 1.398], loss: 9.676795, mae: 44.306973, mean_q: 59.586300
 1031277/1100000: episode: 2295, duration: 1.965s, episode steps: 333, steps per second: 169, episode reward: 251.728, mean reward: 0.756 [-10.323, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.088 [-0.627, 1.447], loss: 7.047038, mae: 44.480995, mean_q: 59.776070
 1031520/1100000: episode: 2296, duration: 1.423s, episode steps: 243, steps per second: 171, episode reward: 263.369, mean reward: 1.084 [-9.112, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: 0.051 [-0.723, 1.389], loss: 7.067679, mae: 44.893986, mean_q: 60.148308
 1031872/1100000: episode: 2297, duration: 2.065s, episode steps: 352, steps per second: 170, episode reward: 280.677, mean reward: 0.797 [-8.691, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.039 [-0.730, 1.416], loss: 3.685271, mae: 44.648361, mean_q: 60.030346
 1032806/1100000: episode: 2298, duration: 5.782s, episode steps: 934, steps per second: 162, episode reward: 239.058, mean reward: 0.256 [-22.820, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.241 [-0.651, 1.389], loss: 6.013683, mae: 44.822601, mean_q: 60.216728
 1033179/1100000: episode: 2299, duration: 2.384s, episode steps: 373, steps per second: 156, episode reward: 259.022, mean reward: 0.694 [-2.990, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.099 [-0.555, 1.392], loss: 6.854740, mae: 44.771709, mean_q: 60.024467
 1033349/1100000: episode: 2300, duration: 0.996s, episode steps: 170, steps per second: 171, episode reward: 24.619, mean reward: 0.145 [-100.000, 56.520], mean action: 1.482 [0.000, 3.000], mean observation: 0.110 [-1.280, 1.392], loss: 6.079750, mae: 44.622330, mean_q: 59.849632
 1033657/1100000: episode: 2301, duration: 1.828s, episode steps: 308, steps per second: 168, episode reward: 245.272, mean reward: 0.796 [-9.166, 100.000], mean action: 1.597 [0.000, 3.000], mean observation: 0.167 [-0.549, 1.397], loss: 5.922825, mae: 44.713833, mean_q: 60.038376
 1034148/1100000: episode: 2302, duration: 3.093s, episode steps: 491, steps per second: 159, episode reward: 273.287, mean reward: 0.557 [-19.408, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.122 [-0.611, 1.386], loss: 4.911150, mae: 44.767326, mean_q: 60.104977
 1034686/1100000: episode: 2303, duration: 3.179s, episode steps: 538, steps per second: 169, episode reward: 199.294, mean reward: 0.370 [-18.662, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.164 [-0.973, 1.410], loss: 6.101223, mae: 44.616600, mean_q: 59.633888
 1035282/1100000: episode: 2304, duration: 3.889s, episode steps: 596, steps per second: 153, episode reward: 188.584, mean reward: 0.316 [-18.584, 100.000], mean action: 1.549 [0.000, 3.000], mean observation: -0.018 [-0.790, 1.407], loss: 5.378216, mae: 44.697895, mean_q: 59.698498
 1035804/1100000: episode: 2305, duration: 3.121s, episode steps: 522, steps per second: 167, episode reward: 274.120, mean reward: 0.525 [-11.106, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.059 [-0.676, 1.510], loss: 6.076415, mae: 44.705311, mean_q: 59.633381
 1036072/1100000: episode: 2306, duration: 1.555s, episode steps: 268, steps per second: 172, episode reward: 240.541, mean reward: 0.898 [-8.205, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.162 [-0.706, 1.410], loss: 5.643975, mae: 45.151859, mean_q: 60.353268
 1036765/1100000: episode: 2307, duration: 4.486s, episode steps: 693, steps per second: 154, episode reward: 223.305, mean reward: 0.322 [-19.847, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.182 [-1.006, 1.396], loss: 6.225707, mae: 44.815754, mean_q: 59.799885
 1037103/1100000: episode: 2308, duration: 2.071s, episode steps: 338, steps per second: 163, episode reward: 258.602, mean reward: 0.765 [-10.853, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.089 [-0.607, 1.389], loss: 4.632304, mae: 44.671272, mean_q: 60.012878
 1037382/1100000: episode: 2309, duration: 1.693s, episode steps: 279, steps per second: 165, episode reward: 255.091, mean reward: 0.914 [-4.207, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.065 [-0.587, 1.405], loss: 6.282498, mae: 44.884426, mean_q: 60.288441
 1037967/1100000: episode: 2310, duration: 3.670s, episode steps: 585, steps per second: 159, episode reward: 273.330, mean reward: 0.467 [-17.378, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.128 [-0.665, 1.519], loss: 6.910271, mae: 44.538631, mean_q: 59.653126
 1038356/1100000: episode: 2311, duration: 2.335s, episode steps: 389, steps per second: 167, episode reward: 282.151, mean reward: 0.725 [-19.186, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.108 [-0.542, 1.388], loss: 5.455256, mae: 44.494759, mean_q: 59.696522
 1039175/1100000: episode: 2312, duration: 5.522s, episode steps: 819, steps per second: 148, episode reward: 211.014, mean reward: 0.258 [-20.023, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.150 [-0.602, 1.434], loss: 4.951453, mae: 44.815392, mean_q: 60.126846
 1039489/1100000: episode: 2313, duration: 1.872s, episode steps: 314, steps per second: 168, episode reward: 226.276, mean reward: 0.721 [-20.426, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.222 [-0.896, 1.447], loss: 4.834504, mae: 44.723179, mean_q: 60.005249
 1040240/1100000: episode: 2314, duration: 4.621s, episode steps: 751, steps per second: 163, episode reward: 231.221, mean reward: 0.308 [-18.784, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.074 [-0.726, 1.426], loss: 5.062702, mae: 44.653160, mean_q: 59.942566
 1040500/1100000: episode: 2315, duration: 1.511s, episode steps: 260, steps per second: 172, episode reward: 226.225, mean reward: 0.870 [-17.479, 100.000], mean action: 1.546 [0.000, 3.000], mean observation: -0.016 [-0.760, 1.423], loss: 6.933062, mae: 44.686417, mean_q: 59.918331
 1040908/1100000: episode: 2316, duration: 2.549s, episode steps: 408, steps per second: 160, episode reward: 239.902, mean reward: 0.588 [-9.380, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.084 [-0.555, 1.395], loss: 5.699647, mae: 44.954659, mean_q: 60.334908
 1041361/1100000: episode: 2317, duration: 2.836s, episode steps: 453, steps per second: 160, episode reward: 247.205, mean reward: 0.546 [-19.551, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.078 [-0.433, 1.406], loss: 5.826945, mae: 44.717651, mean_q: 59.942398
 1041759/1100000: episode: 2318, duration: 2.418s, episode steps: 398, steps per second: 165, episode reward: 260.637, mean reward: 0.655 [-17.641, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.080 [-0.464, 1.402], loss: 5.519686, mae: 44.519165, mean_q: 59.838371
 1042013/1100000: episode: 2319, duration: 1.478s, episode steps: 254, steps per second: 172, episode reward: 239.665, mean reward: 0.944 [-10.956, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.055 [-0.726, 1.459], loss: 6.547115, mae: 45.129269, mean_q: 60.529198
 1042478/1100000: episode: 2320, duration: 2.842s, episode steps: 465, steps per second: 164, episode reward: 263.351, mean reward: 0.566 [-19.218, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.034 [-0.718, 1.398], loss: 5.067704, mae: 44.708038, mean_q: 60.105888
 1042826/1100000: episode: 2321, duration: 2.056s, episode steps: 348, steps per second: 169, episode reward: 247.038, mean reward: 0.710 [-9.270, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.194 [-1.210, 1.488], loss: 6.316503, mae: 44.685471, mean_q: 60.033478
 1043233/1100000: episode: 2322, duration: 2.478s, episode steps: 407, steps per second: 164, episode reward: 264.417, mean reward: 0.650 [-20.261, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.205 [-0.857, 1.401], loss: 6.198986, mae: 44.463963, mean_q: 59.759567
 1043609/1100000: episode: 2323, duration: 2.321s, episode steps: 376, steps per second: 162, episode reward: 181.658, mean reward: 0.483 [-13.442, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: -0.022 [-0.625, 1.411], loss: 6.876343, mae: 44.607460, mean_q: 59.896408
 1044505/1100000: episode: 2324, duration: 5.796s, episode steps: 896, steps per second: 155, episode reward: 289.252, mean reward: 0.323 [-19.334, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.164 [-0.832, 1.494], loss: 4.634259, mae: 44.809570, mean_q: 60.238834
 1045062/1100000: episode: 2325, duration: 3.555s, episode steps: 557, steps per second: 157, episode reward: 220.935, mean reward: 0.397 [-23.946, 100.000], mean action: 2.533 [0.000, 3.000], mean observation: 0.040 [-0.770, 1.408], loss: 5.643663, mae: 44.937164, mean_q: 60.288811
 1045388/1100000: episode: 2326, duration: 1.922s, episode steps: 326, steps per second: 170, episode reward: 275.883, mean reward: 0.846 [-9.469, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.046 [-0.619, 1.403], loss: 5.612315, mae: 45.179749, mean_q: 60.705559
 1045720/1100000: episode: 2327, duration: 1.988s, episode steps: 332, steps per second: 167, episode reward: 278.784, mean reward: 0.840 [-10.119, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.051 [-0.587, 1.481], loss: 4.428293, mae: 45.244400, mean_q: 60.757725
 1046060/1100000: episode: 2328, duration: 2.016s, episode steps: 340, steps per second: 169, episode reward: 222.030, mean reward: 0.653 [-5.039, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.187 [-0.869, 1.398], loss: 4.250047, mae: 45.216141, mean_q: 60.751987
 1046165/1100000: episode: 2329, duration: 0.597s, episode steps: 105, steps per second: 176, episode reward: -32.551, mean reward: -0.310 [-100.000, 7.772], mean action: 1.286 [0.000, 3.000], mean observation: 0.088 [-1.051, 2.408], loss: 8.927247, mae: 45.398033, mean_q: 60.859093
 1046501/1100000: episode: 2330, duration: 2.062s, episode steps: 336, steps per second: 163, episode reward: 295.075, mean reward: 0.878 [-17.859, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.096 [-0.781, 1.392], loss: 5.671876, mae: 45.194626, mean_q: 60.731049
 1046877/1100000: episode: 2331, duration: 2.291s, episode steps: 376, steps per second: 164, episode reward: 223.059, mean reward: 0.593 [-17.676, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.100 [-0.585, 1.404], loss: 7.141227, mae: 45.043385, mean_q: 60.512707
 1047508/1100000: episode: 2332, duration: 3.938s, episode steps: 631, steps per second: 160, episode reward: 258.814, mean reward: 0.410 [-18.299, 100.000], mean action: 0.789 [0.000, 3.000], mean observation: 0.131 [-0.610, 1.457], loss: 4.505319, mae: 45.207581, mean_q: 60.744659
 1047844/1100000: episode: 2333, duration: 2.049s, episode steps: 336, steps per second: 164, episode reward: 281.124, mean reward: 0.837 [-3.244, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.079 [-0.507, 1.407], loss: 4.355020, mae: 45.369427, mean_q: 60.947441
 1048178/1100000: episode: 2334, duration: 1.996s, episode steps: 334, steps per second: 167, episode reward: 264.151, mean reward: 0.791 [-10.110, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.075 [-0.698, 1.521], loss: 3.915348, mae: 45.339897, mean_q: 60.943371
 1048343/1100000: episode: 2335, duration: 0.952s, episode steps: 165, steps per second: 173, episode reward: 34.859, mean reward: 0.211 [-100.000, 4.992], mean action: 1.739 [0.000, 3.000], mean observation: -0.035 [-0.642, 1.390], loss: 7.409081, mae: 45.360310, mean_q: 60.974674
 1048545/1100000: episode: 2336, duration: 1.164s, episode steps: 202, steps per second: 174, episode reward: 1.301, mean reward: 0.006 [-100.000, 10.826], mean action: 1.545 [0.000, 3.000], mean observation: -0.025 [-0.866, 1.509], loss: 6.233139, mae: 45.232170, mean_q: 60.802017
 1048976/1100000: episode: 2337, duration: 2.566s, episode steps: 431, steps per second: 168, episode reward: 246.515, mean reward: 0.572 [-21.520, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.260 [-1.281, 1.401], loss: 6.254636, mae: 45.346642, mean_q: 60.935009
 1049976/1100000: episode: 2338, duration: 7.042s, episode steps: 1000, steps per second: 142, episode reward: 88.919, mean reward: 0.089 [-17.976, 22.422], mean action: 1.262 [0.000, 3.000], mean observation: 0.015 [-0.613, 1.490], loss: 4.105998, mae: 45.231678, mean_q: 60.810837
 1050391/1100000: episode: 2339, duration: 2.501s, episode steps: 415, steps per second: 166, episode reward: 227.042, mean reward: 0.547 [-8.088, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.228 [-0.854, 1.437], loss: 5.262654, mae: 45.285545, mean_q: 60.917892
 1050872/1100000: episode: 2340, duration: 2.902s, episode steps: 481, steps per second: 166, episode reward: 214.428, mean reward: 0.446 [-13.753, 100.000], mean action: 1.769 [0.000, 3.000], mean observation: 0.028 [-0.711, 1.440], loss: 5.168036, mae: 45.437851, mean_q: 61.138279
 1051085/1100000: episode: 2341, duration: 1.243s, episode steps: 213, steps per second: 171, episode reward: 251.825, mean reward: 1.182 [-8.854, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: -0.028 [-0.699, 1.390], loss: 4.024544, mae: 45.033699, mean_q: 60.601395
 1051545/1100000: episode: 2342, duration: 2.767s, episode steps: 460, steps per second: 166, episode reward: -474.213, mean reward: -1.031 [-100.000, 23.202], mean action: 1.691 [0.000, 3.000], mean observation: 0.075 [-2.989, 1.805], loss: 6.733266, mae: 44.957909, mean_q: 60.518997
 1052262/1100000: episode: 2343, duration: 4.773s, episode steps: 717, steps per second: 150, episode reward: 149.887, mean reward: 0.209 [-13.285, 100.000], mean action: 1.958 [0.000, 3.000], mean observation: 0.158 [-1.539, 1.411], loss: 5.914220, mae: 45.318115, mean_q: 60.704075
 1052496/1100000: episode: 2344, duration: 1.372s, episode steps: 234, steps per second: 171, episode reward: 233.438, mean reward: 0.998 [-3.271, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.161 [-0.682, 1.408], loss: 4.792737, mae: 45.172585, mean_q: 60.763222
 1052887/1100000: episode: 2345, duration: 2.417s, episode steps: 391, steps per second: 162, episode reward: 260.951, mean reward: 0.667 [-20.971, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.120 [-0.838, 1.388], loss: 6.335009, mae: 45.243187, mean_q: 60.808979
 1053156/1100000: episode: 2346, duration: 1.572s, episode steps: 269, steps per second: 171, episode reward: 256.238, mean reward: 0.953 [-10.497, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: -0.032 [-0.741, 1.545], loss: 5.129688, mae: 45.312469, mean_q: 60.548672
 1053564/1100000: episode: 2347, duration: 2.489s, episode steps: 408, steps per second: 164, episode reward: 275.271, mean reward: 0.675 [-17.498, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.113 [-0.578, 1.394], loss: 4.591984, mae: 44.915234, mean_q: 60.172520
 1053920/1100000: episode: 2348, duration: 2.163s, episode steps: 356, steps per second: 165, episode reward: 263.447, mean reward: 0.740 [-2.832, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.083 [-0.559, 1.435], loss: 3.572079, mae: 44.750465, mean_q: 60.109779
 1054412/1100000: episode: 2349, duration: 3.013s, episode steps: 492, steps per second: 163, episode reward: 276.619, mean reward: 0.562 [-16.987, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.006 [-0.753, 1.519], loss: 6.849713, mae: 44.894684, mean_q: 60.159332
 1054770/1100000: episode: 2350, duration: 2.211s, episode steps: 358, steps per second: 162, episode reward: 297.368, mean reward: 0.831 [-9.153, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.095 [-0.732, 1.386], loss: 4.028825, mae: 45.329544, mean_q: 60.818565
 1055080/1100000: episode: 2351, duration: 1.869s, episode steps: 310, steps per second: 166, episode reward: 191.417, mean reward: 0.617 [-10.141, 100.000], mean action: 1.687 [0.000, 3.000], mean observation: 0.112 [-0.846, 1.434], loss: 5.026388, mae: 44.740746, mean_q: 60.190662
 1055488/1100000: episode: 2352, duration: 2.578s, episode steps: 408, steps per second: 158, episode reward: 243.619, mean reward: 0.597 [-17.659, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.084 [-0.600, 1.462], loss: 5.011292, mae: 45.031776, mean_q: 60.369427
 1055800/1100000: episode: 2353, duration: 1.848s, episode steps: 312, steps per second: 169, episode reward: 273.039, mean reward: 0.875 [-19.173, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.094 [-0.823, 1.393], loss: 7.205276, mae: 45.353535, mean_q: 60.993313
 1056474/1100000: episode: 2354, duration: 4.253s, episode steps: 674, steps per second: 158, episode reward: 191.711, mean reward: 0.284 [-19.361, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: -0.024 [-0.923, 1.459], loss: 4.464180, mae: 45.566460, mean_q: 60.876205
 1056961/1100000: episode: 2355, duration: 2.969s, episode steps: 487, steps per second: 164, episode reward: 238.558, mean reward: 0.490 [-18.042, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.226 [-1.081, 1.414], loss: 4.934887, mae: 45.513180, mean_q: 60.892517
 1057038/1100000: episode: 2356, duration: 0.444s, episode steps: 77, steps per second: 174, episode reward: -131.147, mean reward: -1.703 [-100.000, 7.818], mean action: 1.558 [0.000, 3.000], mean observation: 0.021 [-5.902, 1.401], loss: 7.700631, mae: 45.760361, mean_q: 61.555672
 1057480/1100000: episode: 2357, duration: 2.738s, episode steps: 442, steps per second: 161, episode reward: 254.986, mean reward: 0.577 [-11.279, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.189 [-0.634, 1.476], loss: 4.594309, mae: 45.784672, mean_q: 61.419807
 1057854/1100000: episode: 2358, duration: 2.200s, episode steps: 374, steps per second: 170, episode reward: 293.813, mean reward: 0.786 [-11.534, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.109 [-0.663, 1.389], loss: 4.468761, mae: 45.501129, mean_q: 61.111217
 1058180/1100000: episode: 2359, duration: 1.926s, episode steps: 326, steps per second: 169, episode reward: 275.890, mean reward: 0.846 [-7.945, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.188 [-0.579, 1.519], loss: 7.711618, mae: 45.539158, mean_q: 60.931473
 1058748/1100000: episode: 2360, duration: 3.607s, episode steps: 568, steps per second: 157, episode reward: 270.526, mean reward: 0.476 [-17.650, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.103 [-0.733, 1.392], loss: 5.864234, mae: 45.366436, mean_q: 60.740910
 1059004/1100000: episode: 2361, duration: 1.598s, episode steps: 256, steps per second: 160, episode reward: -60.237, mean reward: -0.235 [-100.000, 24.033], mean action: 1.766 [0.000, 3.000], mean observation: 0.187 [-1.704, 1.385], loss: 5.440773, mae: 45.273666, mean_q: 60.559036
 1059616/1100000: episode: 2362, duration: 3.916s, episode steps: 612, steps per second: 156, episode reward: 222.351, mean reward: 0.363 [-17.423, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.226 [-0.663, 1.429], loss: 5.459815, mae: 45.540504, mean_q: 60.918720
 1060020/1100000: episode: 2363, duration: 2.368s, episode steps: 404, steps per second: 171, episode reward: 289.794, mean reward: 0.717 [-9.928, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.082 [-0.691, 1.458], loss: 5.422367, mae: 45.468048, mean_q: 60.843334
 1060609/1100000: episode: 2364, duration: 3.659s, episode steps: 589, steps per second: 161, episode reward: 266.194, mean reward: 0.452 [-17.472, 100.000], mean action: 0.793 [0.000, 3.000], mean observation: 0.141 [-0.699, 1.525], loss: 5.282528, mae: 45.511055, mean_q: 60.985245
 1061006/1100000: episode: 2365, duration: 2.413s, episode steps: 397, steps per second: 165, episode reward: 248.589, mean reward: 0.626 [-15.498, 100.000], mean action: 2.008 [0.000, 3.000], mean observation: 0.140 [-0.758, 1.385], loss: 6.216943, mae: 45.614628, mean_q: 60.854073
 1061324/1100000: episode: 2366, duration: 1.911s, episode steps: 318, steps per second: 166, episode reward: 254.681, mean reward: 0.801 [-9.457, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.084 [-0.748, 1.477], loss: 6.358943, mae: 45.272007, mean_q: 60.694233
 1061608/1100000: episode: 2367, duration: 1.695s, episode steps: 284, steps per second: 168, episode reward: 278.518, mean reward: 0.981 [-2.631, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.091 [-0.678, 1.386], loss: 5.420883, mae: 45.489346, mean_q: 60.934269
 1061847/1100000: episode: 2368, duration: 1.374s, episode steps: 239, steps per second: 174, episode reward: 265.612, mean reward: 1.111 [-9.043, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: -0.054 [-0.754, 1.468], loss: 7.818547, mae: 45.372059, mean_q: 60.843391
 1062160/1100000: episode: 2369, duration: 1.827s, episode steps: 313, steps per second: 171, episode reward: 282.958, mean reward: 0.904 [-18.211, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.100 [-0.719, 1.489], loss: 5.173015, mae: 45.890129, mean_q: 61.565231
 1062627/1100000: episode: 2370, duration: 2.817s, episode steps: 467, steps per second: 166, episode reward: 267.154, mean reward: 0.572 [-20.849, 100.000], mean action: 0.912 [0.000, 3.000], mean observation: 0.119 [-0.556, 1.389], loss: 7.032048, mae: 45.712288, mean_q: 61.117119
 1062891/1100000: episode: 2371, duration: 1.526s, episode steps: 264, steps per second: 173, episode reward: 256.758, mean reward: 0.973 [-11.728, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: -0.014 [-0.623, 1.398], loss: 4.063318, mae: 46.402786, mean_q: 62.253365
 1063431/1100000: episode: 2372, duration: 3.261s, episode steps: 540, steps per second: 166, episode reward: 270.389, mean reward: 0.501 [-12.443, 100.000], mean action: 0.713 [0.000, 3.000], mean observation: 0.108 [-0.900, 1.535], loss: 4.176147, mae: 45.732906, mean_q: 61.406857
 1063769/1100000: episode: 2373, duration: 1.976s, episode steps: 338, steps per second: 171, episode reward: 256.809, mean reward: 0.760 [-10.721, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: -0.043 [-0.715, 1.438], loss: 4.722493, mae: 45.921021, mean_q: 61.679039
 1064080/1100000: episode: 2374, duration: 1.876s, episode steps: 311, steps per second: 166, episode reward: 286.510, mean reward: 0.921 [-17.565, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.100 [-0.954, 1.390], loss: 4.775242, mae: 45.984432, mean_q: 61.831924
 1064350/1100000: episode: 2375, duration: 1.590s, episode steps: 270, steps per second: 170, episode reward: -239.590, mean reward: -0.887 [-100.000, 12.734], mean action: 1.626 [0.000, 3.000], mean observation: 0.158 [-1.027, 2.057], loss: 4.901220, mae: 45.597900, mean_q: 61.207603
 1064839/1100000: episode: 2376, duration: 2.959s, episode steps: 489, steps per second: 165, episode reward: 245.045, mean reward: 0.501 [-14.973, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.176 [-0.978, 1.521], loss: 6.110032, mae: 46.007126, mean_q: 61.653637
 1065794/1100000: episode: 2377, duration: 6.605s, episode steps: 955, steps per second: 145, episode reward: -297.863, mean reward: -0.312 [-100.000, 38.009], mean action: 1.815 [0.000, 3.000], mean observation: 0.014 [-0.964, 1.406], loss: 5.320261, mae: 46.035408, mean_q: 61.795197
 1066092/1100000: episode: 2378, duration: 1.767s, episode steps: 298, steps per second: 169, episode reward: 213.460, mean reward: 0.716 [-14.431, 100.000], mean action: 2.154 [0.000, 3.000], mean observation: 0.057 [-0.694, 1.413], loss: 5.101524, mae: 45.988609, mean_q: 61.596031
 1066360/1100000: episode: 2379, duration: 1.597s, episode steps: 268, steps per second: 168, episode reward: 264.741, mean reward: 0.988 [-7.901, 100.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.118 [-0.607, 1.412], loss: 5.133122, mae: 46.257492, mean_q: 61.714508
 1066708/1100000: episode: 2380, duration: 2.117s, episode steps: 348, steps per second: 164, episode reward: 253.861, mean reward: 0.729 [-18.490, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.102 [-0.645, 1.451], loss: 5.118692, mae: 46.151127, mean_q: 61.676666
 1067605/1100000: episode: 2381, duration: 6.247s, episode steps: 897, steps per second: 144, episode reward: 139.256, mean reward: 0.155 [-14.272, 100.000], mean action: 1.739 [0.000, 3.000], mean observation: 0.002 [-1.184, 1.431], loss: 5.703833, mae: 46.398071, mean_q: 62.036030
 1067868/1100000: episode: 2382, duration: 1.525s, episode steps: 263, steps per second: 172, episode reward: 272.102, mean reward: 1.035 [-9.491, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.072 [-0.870, 1.475], loss: 3.210637, mae: 46.713718, mean_q: 62.565762
 1068497/1100000: episode: 2383, duration: 4.106s, episode steps: 629, steps per second: 153, episode reward: 175.685, mean reward: 0.279 [-15.690, 100.000], mean action: 1.838 [0.000, 3.000], mean observation: 0.025 [-0.686, 1.416], loss: 6.253043, mae: 46.523735, mean_q: 62.375259
 1069066/1100000: episode: 2384, duration: 3.427s, episode steps: 569, steps per second: 166, episode reward: 198.264, mean reward: 0.348 [-19.226, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.004 [-0.611, 1.410], loss: 5.567725, mae: 46.598190, mean_q: 62.537193
 1069495/1100000: episode: 2385, duration: 2.610s, episode steps: 429, steps per second: 164, episode reward: 255.788, mean reward: 0.596 [-18.167, 100.000], mean action: 1.751 [0.000, 3.000], mean observation: 0.215 [-0.640, 1.448], loss: 7.063462, mae: 46.780304, mean_q: 62.705990
 1069993/1100000: episode: 2386, duration: 2.968s, episode steps: 498, steps per second: 168, episode reward: 235.350, mean reward: 0.473 [-18.539, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.022 [-0.780, 1.404], loss: 6.739323, mae: 46.993004, mean_q: 62.907063
 1070181/1100000: episode: 2387, duration: 1.080s, episode steps: 188, steps per second: 174, episode reward: -4.872, mean reward: -0.026 [-100.000, 16.830], mean action: 1.691 [0.000, 3.000], mean observation: 0.118 [-0.994, 1.393], loss: 4.564328, mae: 46.894188, mean_q: 62.851788
 1070546/1100000: episode: 2388, duration: 2.124s, episode steps: 365, steps per second: 172, episode reward: 293.076, mean reward: 0.803 [-17.987, 100.000], mean action: 0.890 [0.000, 3.000], mean observation: -0.020 [-0.832, 1.388], loss: 5.124319, mae: 47.255104, mean_q: 63.259663
 1070804/1100000: episode: 2389, duration: 1.513s, episode steps: 258, steps per second: 171, episode reward: 247.603, mean reward: 0.960 [-14.806, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.171 [-0.889, 1.391], loss: 5.764981, mae: 47.047565, mean_q: 62.968666
 1070903/1100000: episode: 2390, duration: 0.571s, episode steps: 99, steps per second: 173, episode reward: -31.260, mean reward: -0.316 [-100.000, 11.108], mean action: 2.030 [1.000, 3.000], mean observation: 0.157 [-0.933, 1.400], loss: 4.029611, mae: 47.752975, mean_q: 63.820461
 1071103/1100000: episode: 2391, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: 246.907, mean reward: 1.235 [-10.889, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.053 [-0.826, 1.403], loss: 5.955885, mae: 47.761444, mean_q: 64.047180
 1071340/1100000: episode: 2392, duration: 1.368s, episode steps: 237, steps per second: 173, episode reward: 262.020, mean reward: 1.106 [-9.361, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: 0.094 [-0.965, 1.399], loss: 6.637307, mae: 47.832977, mean_q: 64.113976
 1071641/1100000: episode: 2393, duration: 1.756s, episode steps: 301, steps per second: 171, episode reward: -219.668, mean reward: -0.730 [-100.000, 11.827], mean action: 1.698 [0.000, 3.000], mean observation: 0.183 [-0.847, 2.065], loss: 6.849525, mae: 48.028465, mean_q: 64.192017
 1072020/1100000: episode: 2394, duration: 2.192s, episode steps: 379, steps per second: 173, episode reward: 252.133, mean reward: 0.665 [-18.568, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.171 [-0.665, 1.492], loss: 6.551125, mae: 47.645615, mean_q: 63.694427
 1072438/1100000: episode: 2395, duration: 2.554s, episode steps: 418, steps per second: 164, episode reward: 242.956, mean reward: 0.581 [-17.820, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.034 [-1.543, 1.409], loss: 4.835932, mae: 47.368114, mean_q: 63.305000
 1072702/1100000: episode: 2396, duration: 1.535s, episode steps: 264, steps per second: 172, episode reward: 286.223, mean reward: 1.084 [-10.794, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.093 [-0.852, 1.514], loss: 4.152777, mae: 47.596104, mean_q: 63.418243
 1072964/1100000: episode: 2397, duration: 1.522s, episode steps: 262, steps per second: 172, episode reward: 228.987, mean reward: 0.874 [-18.387, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: -0.042 [-0.635, 1.399], loss: 4.889297, mae: 47.445789, mean_q: 63.191711
 1073157/1100000: episode: 2398, duration: 1.111s, episode steps: 193, steps per second: 174, episode reward: 259.321, mean reward: 1.344 [-6.191, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.059 [-0.821, 1.415], loss: 7.626652, mae: 47.355221, mean_q: 63.227768
 1073308/1100000: episode: 2399, duration: 0.872s, episode steps: 151, steps per second: 173, episode reward: -126.344, mean reward: -0.837 [-100.000, 39.180], mean action: 1.987 [0.000, 3.000], mean observation: 0.053 [-0.942, 1.745], loss: 6.304506, mae: 47.837448, mean_q: 63.738754
 1073568/1100000: episode: 2400, duration: 1.516s, episode steps: 260, steps per second: 171, episode reward: 233.046, mean reward: 0.896 [-11.826, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.111 [-0.929, 1.408], loss: 5.203888, mae: 48.021980, mean_q: 64.058487
 1073858/1100000: episode: 2401, duration: 1.715s, episode steps: 290, steps per second: 169, episode reward: 264.239, mean reward: 0.911 [-14.509, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.024 [-0.967, 1.451], loss: 5.138343, mae: 48.004509, mean_q: 63.871727
 1074391/1100000: episode: 2402, duration: 3.239s, episode steps: 533, steps per second: 165, episode reward: 247.363, mean reward: 0.464 [-19.007, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.229 [-0.738, 1.545], loss: 7.228398, mae: 48.131947, mean_q: 64.029358
 1074930/1100000: episode: 2403, duration: 3.281s, episode steps: 539, steps per second: 164, episode reward: 266.944, mean reward: 0.495 [-19.758, 100.000], mean action: 1.796 [0.000, 3.000], mean observation: 0.121 [-0.761, 1.463], loss: 5.207605, mae: 48.369831, mean_q: 64.229530
 1075184/1100000: episode: 2404, duration: 1.474s, episode steps: 254, steps per second: 172, episode reward: 228.901, mean reward: 0.901 [-3.531, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.172 [-0.871, 1.411], loss: 4.678761, mae: 48.542377, mean_q: 64.218925
 1075489/1100000: episode: 2405, duration: 1.769s, episode steps: 305, steps per second: 172, episode reward: 230.531, mean reward: 0.756 [-9.054, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.187 [-0.848, 1.421], loss: 5.493772, mae: 48.459774, mean_q: 64.091560
 1075861/1100000: episode: 2406, duration: 2.190s, episode steps: 372, steps per second: 170, episode reward: 250.030, mean reward: 0.672 [-15.693, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.210 [-0.991, 1.385], loss: 5.987322, mae: 48.677052, mean_q: 64.227974
 1076395/1100000: episode: 2407, duration: 3.248s, episode steps: 534, steps per second: 164, episode reward: 222.966, mean reward: 0.418 [-22.769, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.234 [-0.671, 1.401], loss: 6.995928, mae: 48.804367, mean_q: 64.718079
 1076728/1100000: episode: 2408, duration: 2.078s, episode steps: 333, steps per second: 160, episode reward: 274.917, mean reward: 0.826 [-9.354, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.043 [-1.064, 1.475], loss: 5.458028, mae: 48.776775, mean_q: 64.737785
 1076956/1100000: episode: 2409, duration: 1.325s, episode steps: 228, steps per second: 172, episode reward: 283.565, mean reward: 1.244 [-2.653, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.033 [-0.862, 1.395], loss: 5.791373, mae: 49.150982, mean_q: 64.859383
 1077081/1100000: episode: 2410, duration: 0.711s, episode steps: 125, steps per second: 176, episode reward: -55.349, mean reward: -0.443 [-100.000, 28.744], mean action: 1.416 [0.000, 3.000], mean observation: 0.151 [-3.060, 1.523], loss: 3.420103, mae: 48.934471, mean_q: 64.987022
 1077153/1100000: episode: 2411, duration: 0.414s, episode steps: 72, steps per second: 174, episode reward: -154.842, mean reward: -2.151 [-100.000, 64.379], mean action: 0.778 [0.000, 3.000], mean observation: 0.142 [-1.507, 3.195], loss: 10.221861, mae: 50.041389, mean_q: 65.341118
 1077246/1100000: episode: 2412, duration: 0.529s, episode steps: 93, steps per second: 176, episode reward: 27.984, mean reward: 0.301 [-100.000, 18.699], mean action: 1.398 [0.000, 3.000], mean observation: 0.034 [-1.599, 1.397], loss: 8.427326, mae: 50.002438, mean_q: 66.261330
 1077816/1100000: episode: 2413, duration: 3.666s, episode steps: 570, steps per second: 155, episode reward: -43.510, mean reward: -0.076 [-100.000, 16.284], mean action: 1.563 [0.000, 3.000], mean observation: 0.036 [-0.952, 1.494], loss: 9.168335, mae: 49.497192, mean_q: 65.506020
 1077941/1100000: episode: 2414, duration: 0.698s, episode steps: 125, steps per second: 179, episode reward: -217.993, mean reward: -1.744 [-100.000, 81.883], mean action: 0.760 [0.000, 3.000], mean observation: 0.193 [-1.335, 2.643], loss: 7.704791, mae: 48.695541, mean_q: 64.367531
 1078289/1100000: episode: 2415, duration: 2.049s, episode steps: 348, steps per second: 170, episode reward: 242.970, mean reward: 0.698 [-14.969, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.147 [-0.786, 1.438], loss: 7.721013, mae: 49.504208, mean_q: 65.366859
 1078378/1100000: episode: 2416, duration: 0.508s, episode steps: 89, steps per second: 175, episode reward: 10.080, mean reward: 0.113 [-100.000, 18.705], mean action: 1.596 [0.000, 3.000], mean observation: 0.023 [-1.518, 1.396], loss: 7.240782, mae: 49.824898, mean_q: 66.199165
 1079022/1100000: episode: 2417, duration: 4.098s, episode steps: 644, steps per second: 157, episode reward: 239.991, mean reward: 0.373 [-19.834, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: -0.017 [-0.725, 1.455], loss: 6.884243, mae: 49.708839, mean_q: 65.674469
 1079286/1100000: episode: 2418, duration: 1.544s, episode steps: 264, steps per second: 171, episode reward: 244.294, mean reward: 0.925 [-9.425, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.051 [-0.854, 1.396], loss: 9.959061, mae: 49.516582, mean_q: 65.584000
 1079598/1100000: episode: 2419, duration: 1.878s, episode steps: 312, steps per second: 166, episode reward: 247.182, mean reward: 0.792 [-17.985, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: 0.188 [-0.887, 1.400], loss: 7.127042, mae: 49.826080, mean_q: 66.263374
 1079933/1100000: episode: 2420, duration: 1.978s, episode steps: 335, steps per second: 169, episode reward: 288.403, mean reward: 0.861 [-18.579, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.124 [-0.733, 1.492], loss: 7.331203, mae: 50.041813, mean_q: 66.318100
 1080457/1100000: episode: 2421, duration: 3.267s, episode steps: 524, steps per second: 160, episode reward: 242.558, mean reward: 0.463 [-17.801, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.172 [-0.771, 1.395], loss: 7.682933, mae: 49.664307, mean_q: 65.887405
 1080890/1100000: episode: 2422, duration: 2.567s, episode steps: 433, steps per second: 169, episode reward: 195.067, mean reward: 0.451 [-20.977, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.073 [-0.898, 1.509], loss: 7.628060, mae: 50.004742, mean_q: 66.177162
 1080992/1100000: episode: 2423, duration: 0.585s, episode steps: 102, steps per second: 174, episode reward: 7.355, mean reward: 0.072 [-100.000, 20.815], mean action: 1.931 [0.000, 3.000], mean observation: 0.088 [-1.881, 1.407], loss: 6.093513, mae: 49.522617, mean_q: 65.952034
 1081315/1100000: episode: 2424, duration: 1.931s, episode steps: 323, steps per second: 167, episode reward: 263.172, mean reward: 0.815 [-9.450, 100.000], mean action: 1.607 [0.000, 3.000], mean observation: 0.134 [-0.767, 1.392], loss: 7.809493, mae: 49.775276, mean_q: 66.149323
 1081697/1100000: episode: 2425, duration: 2.269s, episode steps: 382, steps per second: 168, episode reward: 209.812, mean reward: 0.549 [-18.398, 100.000], mean action: 1.526 [0.000, 3.000], mean observation: 0.180 [-0.619, 1.468], loss: 6.261899, mae: 50.527527, mean_q: 67.172531
 1082697/1100000: episode: 2426, duration: 6.341s, episode steps: 1000, steps per second: 158, episode reward: 8.552, mean reward: 0.009 [-18.619, 18.889], mean action: 1.456 [0.000, 3.000], mean observation: -0.030 [-1.007, 1.397], loss: 6.486222, mae: 50.151665, mean_q: 66.823570
 1083083/1100000: episode: 2427, duration: 2.423s, episode steps: 386, steps per second: 159, episode reward: 225.953, mean reward: 0.585 [-11.942, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.182 [-0.579, 1.386], loss: 8.676851, mae: 50.521172, mean_q: 67.120590
 1083745/1100000: episode: 2428, duration: 4.097s, episode steps: 662, steps per second: 162, episode reward: 236.363, mean reward: 0.357 [-21.657, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.023 [-0.716, 1.472], loss: 7.779539, mae: 50.276566, mean_q: 67.083191
 1084444/1100000: episode: 2429, duration: 4.620s, episode steps: 699, steps per second: 151, episode reward: 196.374, mean reward: 0.281 [-17.472, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: -0.016 [-1.021, 1.443], loss: 5.994366, mae: 50.199715, mean_q: 66.990128
 1084692/1100000: episode: 2430, duration: 1.429s, episode steps: 248, steps per second: 174, episode reward: 276.618, mean reward: 1.115 [-3.194, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.068 [-0.633, 1.442], loss: 6.344788, mae: 50.000416, mean_q: 66.793884
 1084999/1100000: episode: 2431, duration: 1.791s, episode steps: 307, steps per second: 171, episode reward: 243.340, mean reward: 0.793 [-9.882, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.040 [-0.681, 1.399], loss: 5.039499, mae: 49.983074, mean_q: 66.936028
 1085311/1100000: episode: 2432, duration: 1.834s, episode steps: 312, steps per second: 170, episode reward: 240.807, mean reward: 0.772 [-17.889, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.087 [-0.874, 1.403], loss: 8.007627, mae: 49.989914, mean_q: 66.637833
 1086311/1100000: episode: 2433, duration: 7.143s, episode steps: 1000, steps per second: 140, episode reward: -6.830, mean reward: -0.007 [-20.140, 18.145], mean action: 1.620 [0.000, 3.000], mean observation: -0.010 [-0.922, 1.415], loss: 7.464581, mae: 50.002861, mean_q: 66.451965
 1086610/1100000: episode: 2434, duration: 1.754s, episode steps: 299, steps per second: 170, episode reward: 266.525, mean reward: 0.891 [-11.602, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.049 [-0.628, 1.490], loss: 8.422247, mae: 49.592148, mean_q: 66.095032
 1087508/1100000: episode: 2435, duration: 5.755s, episode steps: 898, steps per second: 156, episode reward: -14.574, mean reward: -0.016 [-100.000, 22.573], mean action: 1.488 [0.000, 3.000], mean observation: 0.170 [-0.726, 1.392], loss: 5.030555, mae: 49.794956, mean_q: 66.077721
 1088029/1100000: episode: 2436, duration: 3.139s, episode steps: 521, steps per second: 166, episode reward: 218.137, mean reward: 0.419 [-22.066, 100.000], mean action: 2.436 [0.000, 3.000], mean observation: 0.062 [-0.701, 1.405], loss: 5.515105, mae: 49.880898, mean_q: 66.547066
 1088430/1100000: episode: 2437, duration: 2.480s, episode steps: 401, steps per second: 162, episode reward: 209.320, mean reward: 0.522 [-4.928, 100.000], mean action: 1.711 [0.000, 3.000], mean observation: 0.194 [-0.471, 1.408], loss: 7.858955, mae: 49.807648, mean_q: 66.110954
 1088790/1100000: episode: 2438, duration: 2.218s, episode steps: 360, steps per second: 162, episode reward: 266.938, mean reward: 0.741 [-17.996, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.090 [-0.962, 1.401], loss: 6.145257, mae: 49.691044, mean_q: 66.165665
 1089076/1100000: episode: 2439, duration: 1.684s, episode steps: 286, steps per second: 170, episode reward: 224.715, mean reward: 0.786 [-14.955, 100.000], mean action: 1.923 [0.000, 3.000], mean observation: 0.027 [-0.949, 1.404], loss: 7.498476, mae: 49.871777, mean_q: 66.488457
 1089297/1100000: episode: 2440, duration: 1.284s, episode steps: 221, steps per second: 172, episode reward: 290.509, mean reward: 1.315 [-9.510, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.004 [-0.712, 1.393], loss: 5.601488, mae: 50.267292, mean_q: 66.892639
 1090123/1100000: episode: 2441, duration: 5.172s, episode steps: 826, steps per second: 160, episode reward: 205.064, mean reward: 0.248 [-18.974, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.248 [-0.669, 1.398], loss: 6.354481, mae: 49.825504, mean_q: 66.331566
 1090748/1100000: episode: 2442, duration: 3.891s, episode steps: 625, steps per second: 161, episode reward: 193.060, mean reward: 0.309 [-19.577, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.178 [-0.632, 1.492], loss: 5.923302, mae: 49.543633, mean_q: 65.865974
 1090953/1100000: episode: 2443, duration: 1.187s, episode steps: 205, steps per second: 173, episode reward: 256.072, mean reward: 1.249 [-9.066, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: -0.011 [-0.980, 1.407], loss: 5.790673, mae: 49.330620, mean_q: 66.064209
 1091403/1100000: episode: 2444, duration: 2.770s, episode steps: 450, steps per second: 162, episode reward: 232.737, mean reward: 0.517 [-19.728, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: -0.031 [-1.018, 1.394], loss: 5.428392, mae: 49.480000, mean_q: 66.045631
 1091919/1100000: episode: 2445, duration: 3.234s, episode steps: 516, steps per second: 160, episode reward: 228.348, mean reward: 0.443 [-19.412, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.134 [-0.492, 1.393], loss: 5.912900, mae: 49.655682, mean_q: 66.324471
 1092154/1100000: episode: 2446, duration: 1.378s, episode steps: 235, steps per second: 171, episode reward: 252.308, mean reward: 1.074 [-13.006, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.063 [-0.599, 1.407], loss: 7.536816, mae: 49.762283, mean_q: 66.059395
 1092365/1100000: episode: 2447, duration: 1.232s, episode steps: 211, steps per second: 171, episode reward: 291.118, mean reward: 1.380 [-7.974, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.043 [-0.668, 1.397], loss: 7.349387, mae: 49.733116, mean_q: 66.499229
 1092570/1100000: episode: 2448, duration: 1.174s, episode steps: 205, steps per second: 175, episode reward: 258.719, mean reward: 1.262 [-9.533, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: -0.012 [-0.706, 1.443], loss: 5.829090, mae: 49.924217, mean_q: 66.791367
 1092835/1100000: episode: 2449, duration: 1.532s, episode steps: 265, steps per second: 173, episode reward: 235.727, mean reward: 0.890 [-19.590, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: -0.011 [-0.921, 1.423], loss: 5.924291, mae: 49.822136, mean_q: 66.677063
 1093200/1100000: episode: 2450, duration: 2.165s, episode steps: 365, steps per second: 169, episode reward: 214.920, mean reward: 0.589 [-18.353, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.181 [-0.770, 1.445], loss: 5.965798, mae: 49.992039, mean_q: 66.705605
 1093496/1100000: episode: 2451, duration: 1.734s, episode steps: 296, steps per second: 171, episode reward: 244.829, mean reward: 0.827 [-10.166, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.151 [-0.619, 1.497], loss: 4.210583, mae: 50.237617, mean_q: 67.008766
 1093656/1100000: episode: 2452, duration: 0.926s, episode steps: 160, steps per second: 173, episode reward: -360.137, mean reward: -2.251 [-100.000, 99.037], mean action: 2.194 [0.000, 3.000], mean observation: 0.059 [-4.229, 1.664], loss: 5.056245, mae: 50.003815, mean_q: 66.910233
 1094645/1100000: episode: 2453, duration: 6.761s, episode steps: 989, steps per second: 146, episode reward: 120.736, mean reward: 0.122 [-17.595, 100.000], mean action: 1.939 [0.000, 3.000], mean observation: 0.183 [-0.723, 1.437], loss: 6.549732, mae: 50.150311, mean_q: 67.017860
 1095117/1100000: episode: 2454, duration: 2.803s, episode steps: 472, steps per second: 168, episode reward: 239.913, mean reward: 0.508 [-20.251, 100.000], mean action: 0.890 [0.000, 3.000], mean observation: 0.007 [-0.823, 1.388], loss: 7.016358, mae: 50.080448, mean_q: 66.972107
 1095441/1100000: episode: 2455, duration: 1.932s, episode steps: 324, steps per second: 168, episode reward: 253.403, mean reward: 0.782 [-9.368, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.090 [-0.902, 1.452], loss: 6.123106, mae: 50.367908, mean_q: 67.420654
 1096034/1100000: episode: 2456, duration: 3.697s, episode steps: 593, steps per second: 160, episode reward: 174.917, mean reward: 0.295 [-19.784, 100.000], mean action: 1.811 [0.000, 3.000], mean observation: 0.213 [-0.563, 1.412], loss: 5.698002, mae: 50.197987, mean_q: 67.180664
 1096476/1100000: episode: 2457, duration: 2.625s, episode steps: 442, steps per second: 168, episode reward: 239.304, mean reward: 0.541 [-18.150, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.195 [-0.503, 1.395], loss: 6.348601, mae: 50.248074, mean_q: 67.334152
 1096752/1100000: episode: 2458, duration: 1.628s, episode steps: 276, steps per second: 169, episode reward: -228.232, mean reward: -0.827 [-100.000, 15.797], mean action: 1.710 [0.000, 3.000], mean observation: 0.122 [-0.811, 1.670], loss: 7.235690, mae: 50.237282, mean_q: 67.332764
 1096954/1100000: episode: 2459, duration: 1.161s, episode steps: 202, steps per second: 174, episode reward: -35.972, mean reward: -0.178 [-100.000, 11.367], mean action: 1.693 [0.000, 3.000], mean observation: 0.018 [-2.149, 1.471], loss: 5.489853, mae: 49.651947, mean_q: 66.524017
 1097528/1100000: episode: 2460, duration: 3.436s, episode steps: 574, steps per second: 167, episode reward: 210.715, mean reward: 0.367 [-18.340, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.188 [-0.708, 1.449], loss: 5.267884, mae: 49.784679, mean_q: 66.781540
 1097861/1100000: episode: 2461, duration: 1.974s, episode steps: 333, steps per second: 169, episode reward: 267.829, mean reward: 0.804 [-11.955, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.018 [-0.980, 1.481], loss: 6.721790, mae: 49.768158, mean_q: 66.501770
 1098246/1100000: episode: 2462, duration: 2.326s, episode steps: 385, steps per second: 166, episode reward: 273.342, mean reward: 0.710 [-7.753, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.129 [-0.772, 1.411], loss: 7.218156, mae: 49.878250, mean_q: 66.632050
 1099246/1100000: episode: 2463, duration: 6.667s, episode steps: 1000, steps per second: 150, episode reward: 122.789, mean reward: 0.123 [-23.421, 16.907], mean action: 2.048 [0.000, 3.000], mean observation: 0.240 [-0.570, 1.513], loss: 6.796439, mae: 50.078499, mean_q: 66.931122
 1099613/1100000: episode: 2464, duration: 2.186s, episode steps: 367, steps per second: 168, episode reward: 245.247, mean reward: 0.668 [-13.402, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.158 [-0.703, 1.512], loss: 5.364288, mae: 49.910236, mean_q: 66.589905
done, took 6793.625 seconds
Testing for 1000 episodes ...
Episode 1: reward: 220.737, steps: 456
Episode 2: reward: 211.118, steps: 245
Episode 3: reward: 133.189, steps: 1000
Episode 4: reward: 160.238, steps: 726
Episode 5: reward: 263.290, steps: 259
Episode 6: reward: 247.145, steps: 286
Episode 7: reward: 180.612, steps: 486
Episode 8: reward: -4.289, steps: 1000
Episode 9: reward: 272.482, steps: 216
Episode 10: reward: 206.337, steps: 341
Episode 11: reward: 297.481, steps: 259
Episode 12: reward: 209.877, steps: 443
Episode 13: reward: 255.052, steps: 265
Episode 14: reward: 260.141, steps: 323
Episode 15: reward: 233.543, steps: 251
Episode 16: reward: 293.646, steps: 278
Episode 17: reward: 250.513, steps: 759
Episode 18: reward: 232.938, steps: 403
Episode 19: reward: 201.089, steps: 445
Traceback (most recent call last):
  File "/home/matheus/tcc_implementation/agents/dqn/keras_rl_train_dqn_agent.py", line 41, in <module>
    dqn.test(env, nb_episodes=1000, visualize=True)
  File "/home/matheus/tcc_implementation/keras-rl/rl/core.py", line 353, in test
    callbacks.on_action_end(action)
  File "/home/matheus/tcc_implementation/keras-rl/rl/callbacks.py", line 101, in on_action_end
    callback.on_action_end(action, logs=logs)
  File "/home/matheus/tcc_implementation/keras-rl/rl/callbacks.py", line 366, in on_action_end
    self.env.render(mode='human')
  File "/home/matheus/tcc_implementation/gym/gym/core.py", line 233, in render
    return self.env.render(mode, **kwargs)
  File "/home/matheus/tcc_implementation/gym/gym/envs/box2d/lunar_lander.py", line 365, in render
    return self.viewer.render(return_rgb_array = mode=='rgb_array')
  File "/home/matheus/tcc_implementation/gym/gym/envs/classic_control/rendering.py", line 99, in render
    geom.render()
  File "/home/matheus/tcc_implementation/gym/gym/envs/classic_control/rendering.py", line 167, in render
    self.render1()
  File "/home/matheus/tcc_implementation/gym/gym/envs/classic_control/rendering.py", line 240, in render1
    glVertex3f(p[0], p[1],0)  # draw each vertex
  File "/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/pyglet/gl/lib.py", line 85, in errcheck
    def errcheck(result, func, arguments):
KeyboardInterrupt

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
