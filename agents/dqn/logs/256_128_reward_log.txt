/home/matheus/tcc_implementation/venv/bin/python /home/matheus/tcc_implementation/agents/dqn/keras_rl_train_dqn_agent.py
Using TensorFlow backend.
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2019-11-01 12:21:04.314549: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-01 12:21:04.335245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2019-11-01 12:21:04.335553: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15c97e0 executing computations on platform Host. Devices:
2019-11-01 12:21:04.335578: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-01 12:21:04.366313: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Training for 1100000 steps ...
WARNING:tensorflow:From /home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

     160/1100000: episode: 1, duration: 0.201s, episode steps: 160, steps per second: 795, episode reward: -987.103, mean reward: -6.169 [-100.000, 2.138], mean action: 1.956 [0.000, 3.000], mean observation: 0.449 [-3.266, 5.195], loss: --, mae: --, mean_q: --
     322/1100000: episode: 2, duration: 0.138s, episode steps: 162, steps per second: 1173, episode reward: -686.288, mean reward: -4.236 [-100.000, 4.006], mean action: 1.963 [0.000, 3.000], mean observation: 0.260 [-0.635, 3.977], loss: --, mae: --, mean_q: --
     432/1100000: episode: 3, duration: 0.081s, episode steps: 110, steps per second: 1360, episode reward: -851.360, mean reward: -7.740 [-100.000, 0.593], mean action: 1.955 [0.000, 3.000], mean observation: 0.516 [-0.701, 5.157], loss: --, mae: --, mean_q: --
     515/1100000: episode: 4, duration: 0.062s, episode steps: 83, steps per second: 1343, episode reward: -622.633, mean reward: -7.502 [-100.000, -0.228], mean action: 1.976 [1.000, 3.000], mean observation: 0.352 [-0.688, 3.366], loss: --, mae: --, mean_q: --
     620/1100000: episode: 5, duration: 0.081s, episode steps: 105, steps per second: 1299, episode reward: -747.842, mean reward: -7.122 [-100.000, -0.940], mean action: 1.952 [0.000, 3.000], mean observation: 0.541 [-0.308, 4.932], loss: --, mae: --, mean_q: --
     736/1100000: episode: 6, duration: 0.087s, episode steps: 116, steps per second: 1334, episode reward: -511.829, mean reward: -4.412 [-100.000, 2.834], mean action: 1.983 [0.000, 3.000], mean observation: 0.355 [-0.382, 3.114], loss: --, mae: --, mean_q: --
     813/1100000: episode: 7, duration: 0.053s, episode steps: 77, steps per second: 1453, episode reward: -475.919, mean reward: -6.181 [-100.000, 0.535], mean action: 1.948 [0.000, 3.000], mean observation: 0.344 [-0.631, 2.457], loss: --, mae: --, mean_q: --
     929/1100000: episode: 8, duration: 0.090s, episode steps: 116, steps per second: 1295, episode reward: -455.365, mean reward: -3.926 [-100.000, 1.276], mean action: 1.957 [0.000, 3.000], mean observation: 0.418 [-0.350, 2.864], loss: --, mae: --, mean_q: --
2019-11-01 12:21:05.907617: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-11-01 12:21:05.909152: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-11-01 12:21:05.923841: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-11-01 12:21:05.924937: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2019-11-01 12:21:05.925995: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2019-11-01 12:21:05.927495: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
    1033/1100000: episode: 9, duration: 0.888s, episode steps: 104, steps per second: 117, episode reward: -392.695, mean reward: -3.776 [-100.000, 0.979], mean action: 1.404 [0.000, 3.000], mean observation: 0.316 [-0.728, 2.428], loss: 49.171306, mae: 1.366951, mean_q: 0.042521
    1117/1100000: episode: 10, duration: 0.483s, episode steps: 84, steps per second: 174, episode reward: -269.250, mean reward: -3.205 [-100.000, 6.691], mean action: 1.048 [0.000, 3.000], mean observation: 0.096 [-1.624, 3.201], loss: 51.468658, mae: 2.449213, mean_q: -1.480340
    1204/1100000: episode: 11, duration: 0.496s, episode steps: 87, steps per second: 176, episode reward: -350.322, mean reward: -4.027 [-100.000, 2.297], mean action: 1.874 [0.000, 3.000], mean observation: -0.055 [-1.969, 4.254], loss: 43.146278, mae: 3.000560, mean_q: -2.184716
    1299/1100000: episode: 12, duration: 0.530s, episode steps: 95, steps per second: 179, episode reward: -157.778, mean reward: -1.661 [-100.000, 43.978], mean action: 0.211 [0.000, 3.000], mean observation: 0.059 [-3.138, 1.533], loss: 38.963367, mae: 3.521803, mean_q: -2.320995
    1417/1100000: episode: 13, duration: 0.656s, episode steps: 118, steps per second: 180, episode reward: -328.207, mean reward: -2.781 [-100.000, 5.640], mean action: 0.449 [0.000, 3.000], mean observation: -0.006 [-1.721, 5.111], loss: 34.701756, mae: 3.819299, mean_q: -3.219995
    1819/1100000: episode: 14, duration: 2.430s, episode steps: 402, steps per second: 165, episode reward: -187.310, mean reward: -0.466 [-100.000, 27.978], mean action: 1.540 [0.000, 3.000], mean observation: 0.022 [-3.585, 1.407], loss: 24.493422, mae: 5.443347, mean_q: -5.050339
    1958/1100000: episode: 15, duration: 0.795s, episode steps: 139, steps per second: 175, episode reward: -146.356, mean reward: -1.053 [-100.000, 9.009], mean action: 1.432 [0.000, 3.000], mean observation: 0.169 [-1.403, 5.500], loss: 18.460531, mae: 6.437275, mean_q: -6.177937
    2071/1100000: episode: 16, duration: 0.643s, episode steps: 113, steps per second: 176, episode reward: -158.550, mean reward: -1.403 [-100.000, 9.144], mean action: 1.407 [0.000, 3.000], mean observation: 0.035 [-4.577, 1.411], loss: 13.281793, mae: 6.542174, mean_q: -6.420674
    2128/1100000: episode: 17, duration: 0.321s, episode steps: 57, steps per second: 178, episode reward: -111.707, mean reward: -1.960 [-100.000, 9.281], mean action: 0.316 [0.000, 3.000], mean observation: 0.039 [-1.633, 1.392], loss: 19.711479, mae: 7.136155, mean_q: -6.988466
    2258/1100000: episode: 18, duration: 0.729s, episode steps: 130, steps per second: 178, episode reward: -125.360, mean reward: -0.964 [-100.000, 11.421], mean action: 1.254 [0.000, 3.000], mean observation: 0.188 [-1.524, 1.401], loss: 17.654373, mae: 7.271272, mean_q: -7.497848
    2343/1100000: episode: 19, duration: 0.483s, episode steps: 85, steps per second: 176, episode reward: -463.806, mean reward: -5.457 [-100.000, 0.623], mean action: 1.141 [0.000, 3.000], mean observation: 0.003 [-2.436, 1.635], loss: 25.070402, mae: 8.032317, mean_q: -8.025349
    2517/1100000: episode: 20, duration: 0.996s, episode steps: 174, steps per second: 175, episode reward: -126.599, mean reward: -0.728 [-100.000, 19.671], mean action: 1.707 [0.000, 3.000], mean observation: 0.137 [-0.867, 1.600], loss: 14.526191, mae: 8.442193, mean_q: -9.001408
    2680/1100000: episode: 21, duration: 0.925s, episode steps: 163, steps per second: 176, episode reward: -34.777, mean reward: -0.213 [-100.000, 17.103], mean action: 1.515 [0.000, 3.000], mean observation: 0.186 [-1.065, 1.457], loss: 18.034393, mae: 8.868082, mean_q: -9.554060
    2894/1100000: episode: 22, duration: 1.254s, episode steps: 214, steps per second: 171, episode reward: -208.109, mean reward: -0.972 [-100.000, 8.584], mean action: 1.883 [0.000, 3.000], mean observation: 0.003 [-1.794, 1.386], loss: 16.222561, mae: 9.682216, mean_q: -10.580783
    3027/1100000: episode: 23, duration: 0.757s, episode steps: 133, steps per second: 176, episode reward: -115.941, mean reward: -0.872 [-100.000, 11.985], mean action: 1.511 [0.000, 3.000], mean observation: 0.114 [-1.610, 2.096], loss: 14.202106, mae: 9.974844, mean_q: -10.807613
    3241/1100000: episode: 24, duration: 1.232s, episode steps: 214, steps per second: 174, episode reward: -99.524, mean reward: -0.465 [-100.000, 9.670], mean action: 1.486 [0.000, 3.000], mean observation: 0.102 [-1.624, 1.431], loss: 11.584432, mae: 10.322970, mean_q: -11.118239
    3561/1100000: episode: 25, duration: 1.876s, episode steps: 320, steps per second: 171, episode reward: -7.717, mean reward: -0.024 [-100.000, 17.147], mean action: 1.844 [0.000, 3.000], mean observation: 0.030 [-2.016, 1.432], loss: 8.488147, mae: 10.136966, mean_q: -10.670423
    3880/1100000: episode: 26, duration: 1.909s, episode steps: 319, steps per second: 167, episode reward: -195.807, mean reward: -0.614 [-100.000, 9.011], mean action: 1.602 [0.000, 3.000], mean observation: 0.155 [-1.010, 4.392], loss: 11.399435, mae: 10.177817, mean_q: -9.881560
    4867/1100000: episode: 27, duration: 6.408s, episode steps: 987, steps per second: 154, episode reward: -116.221, mean reward: -0.118 [-100.000, 14.712], mean action: 1.716 [0.000, 3.000], mean observation: -0.006 [-0.807, 1.473], loss: 8.103456, mae: 9.998762, mean_q: -5.992668
    5033/1100000: episode: 28, duration: 0.946s, episode steps: 166, steps per second: 175, episode reward: -301.833, mean reward: -1.818 [-100.000, 7.303], mean action: 1.416 [0.000, 3.000], mean observation: 0.147 [-1.145, 2.266], loss: 8.285121, mae: 10.659593, mean_q: -2.453474
    5586/1100000: episode: 29, duration: 3.599s, episode steps: 553, steps per second: 154, episode reward: -545.867, mean reward: -0.987 [-100.000, 4.871], mean action: 1.854 [0.000, 3.000], mean observation: 0.085 [-3.190, 1.403], loss: 7.311199, mae: 11.682350, mean_q: -0.160532
    6586/1100000: episode: 30, duration: 6.621s, episode steps: 1000, steps per second: 151, episode reward: -39.665, mean reward: -0.040 [-5.045, 5.670], mean action: 1.684 [0.000, 3.000], mean observation: 0.053 [-0.772, 1.545], loss: 7.820968, mae: 14.125660, mean_q: 4.920475
    6952/1100000: episode: 31, duration: 2.196s, episode steps: 366, steps per second: 167, episode reward: -126.819, mean reward: -0.347 [-100.000, 16.666], mean action: 1.544 [0.000, 3.000], mean observation: 0.154 [-1.387, 1.578], loss: 10.153870, mae: 15.968753, mean_q: 9.606512
    7952/1100000: episode: 32, duration: 6.514s, episode steps: 1000, steps per second: 154, episode reward: -35.228, mean reward: -0.035 [-5.184, 3.901], mean action: 1.561 [0.000, 3.000], mean observation: 0.032 [-0.656, 1.400], loss: 8.834330, mae: 17.835018, mean_q: 12.382213
    8952/1100000: episode: 33, duration: 6.935s, episode steps: 1000, steps per second: 144, episode reward: -13.807, mean reward: -0.014 [-4.140, 5.238], mean action: 1.548 [0.000, 3.000], mean observation: 0.172 [-0.708, 1.532], loss: 8.890990, mae: 19.336561, mean_q: 16.839539
    9952/1100000: episode: 34, duration: 7.080s, episode steps: 1000, steps per second: 141, episode reward: -54.451, mean reward: -0.054 [-5.147, 5.609], mean action: 1.612 [0.000, 3.000], mean observation: 0.152 [-0.606, 1.405], loss: 6.370182, mae: 20.512316, mean_q: 19.710669
   10952/1100000: episode: 35, duration: 7.222s, episode steps: 1000, steps per second: 138, episode reward: -88.223, mean reward: -0.088 [-5.488, 4.986], mean action: 1.813 [0.000, 3.000], mean observation: 0.124 [-0.560, 1.386], loss: 6.283830, mae: 21.299137, mean_q: 22.100101
   11952/1100000: episode: 36, duration: 6.997s, episode steps: 1000, steps per second: 143, episode reward: -81.168, mean reward: -0.081 [-4.373, 4.838], mean action: 1.616 [0.000, 3.000], mean observation: 0.135 [-0.312, 1.495], loss: 7.301972, mae: 22.572985, mean_q: 24.257141
   12952/1100000: episode: 37, duration: 7.166s, episode steps: 1000, steps per second: 140, episode reward: -21.769, mean reward: -0.022 [-5.107, 4.873], mean action: 1.692 [0.000, 3.000], mean observation: 0.163 [-0.371, 1.480], loss: 5.334026, mae: 23.545946, mean_q: 26.520809
   13952/1100000: episode: 38, duration: 6.454s, episode steps: 1000, steps per second: 155, episode reward: -34.588, mean reward: -0.035 [-4.768, 5.429], mean action: 1.659 [0.000, 3.000], mean observation: 0.155 [-0.280, 1.423], loss: 5.104383, mae: 23.978775, mean_q: 28.252666
   14952/1100000: episode: 39, duration: 7.507s, episode steps: 1000, steps per second: 133, episode reward: 5.377, mean reward: 0.005 [-23.761, 18.744], mean action: 1.684 [0.000, 3.000], mean observation: 0.005 [-1.122, 1.400], loss: 6.953913, mae: 24.989285, mean_q: 30.239445
   15952/1100000: episode: 40, duration: 6.520s, episode steps: 1000, steps per second: 153, episode reward: -36.758, mean reward: -0.037 [-4.556, 4.996], mean action: 1.836 [0.000, 3.000], mean observation: 0.141 [-0.435, 1.390], loss: 8.506981, mae: 25.907661, mean_q: 31.869623
   16952/1100000: episode: 41, duration: 6.793s, episode steps: 1000, steps per second: 147, episode reward: -8.226, mean reward: -0.008 [-7.777, 5.059], mean action: 1.651 [0.000, 3.000], mean observation: 0.166 [-0.800, 1.557], loss: 3.575826, mae: 26.889847, mean_q: 33.865105
   17539/1100000: episode: 42, duration: 3.800s, episode steps: 587, steps per second: 154, episode reward: -116.009, mean reward: -0.198 [-100.000, 19.781], mean action: 1.606 [0.000, 3.000], mean observation: 0.019 [-0.781, 2.876], loss: 4.473319, mae: 28.082111, mean_q: 35.570312
   18539/1100000: episode: 43, duration: 6.807s, episode steps: 1000, steps per second: 147, episode reward: -56.840, mean reward: -0.057 [-5.311, 4.184], mean action: 1.707 [0.000, 3.000], mean observation: 0.086 [-0.791, 1.417], loss: 5.792863, mae: 29.090105, mean_q: 37.402637
   19003/1100000: episode: 44, duration: 2.933s, episode steps: 464, steps per second: 158, episode reward: -120.338, mean reward: -0.259 [-100.000, 18.649], mean action: 1.580 [0.000, 3.000], mean observation: -0.009 [-0.808, 3.267], loss: 7.004775, mae: 29.934937, mean_q: 38.193779
   20003/1100000: episode: 45, duration: 6.792s, episode steps: 1000, steps per second: 147, episode reward: -59.548, mean reward: -0.060 [-5.236, 5.077], mean action: 1.746 [0.000, 3.000], mean observation: 0.096 [-0.488, 1.392], loss: 5.742800, mae: 30.383293, mean_q: 38.768642
   20210/1100000: episode: 46, duration: 1.248s, episode steps: 207, steps per second: 166, episode reward: -230.213, mean reward: -1.112 [-100.000, 32.666], mean action: 1.715 [0.000, 3.000], mean observation: -0.033 [-2.256, 1.558], loss: 7.116317, mae: 30.711329, mean_q: 39.356342
   21210/1100000: episode: 47, duration: 7.423s, episode steps: 1000, steps per second: 135, episode reward: -112.612, mean reward: -0.113 [-5.497, 4.499], mean action: 1.731 [0.000, 3.000], mean observation: 0.098 [-0.448, 1.552], loss: 4.592507, mae: 31.394560, mean_q: 40.164425
   22210/1100000: episode: 48, duration: 6.522s, episode steps: 1000, steps per second: 153, episode reward: -108.811, mean reward: -0.109 [-4.998, 4.968], mean action: 1.740 [0.000, 3.000], mean observation: 0.118 [-0.683, 1.416], loss: 9.948645, mae: 31.829679, mean_q: 40.896271
   23210/1100000: episode: 49, duration: 7.665s, episode steps: 1000, steps per second: 130, episode reward: -44.303, mean reward: -0.044 [-4.688, 5.509], mean action: 1.879 [0.000, 3.000], mean observation: 0.041 [-0.464, 1.470], loss: 5.852859, mae: 32.472984, mean_q: 41.801132
   24210/1100000: episode: 50, duration: 7.564s, episode steps: 1000, steps per second: 132, episode reward: -167.519, mean reward: -0.168 [-6.784, 14.212], mean action: 1.846 [0.000, 3.000], mean observation: 0.098 [-0.831, 1.457], loss: 3.972368, mae: 32.985603, mean_q: 42.774780
   24630/1100000: episode: 51, duration: 2.664s, episode steps: 420, steps per second: 158, episode reward: -411.235, mean reward: -0.979 [-100.000, 17.195], mean action: 1.893 [0.000, 3.000], mean observation: -0.086 [-2.332, 1.496], loss: 10.216768, mae: 33.639599, mean_q: 43.912807
   25028/1100000: episode: 52, duration: 2.517s, episode steps: 398, steps per second: 158, episode reward: -385.309, mean reward: -0.968 [-100.000, 45.962], mean action: 1.915 [0.000, 3.000], mean observation: -0.108 [-3.458, 1.392], loss: 5.877856, mae: 34.127850, mean_q: 44.694469
   26028/1100000: episode: 53, duration: 7.967s, episode steps: 1000, steps per second: 126, episode reward: -50.704, mean reward: -0.051 [-9.969, 16.435], mean action: 1.836 [0.000, 3.000], mean observation: 0.062 [-0.551, 1.395], loss: 7.731431, mae: 34.652538, mean_q: 44.787991
   26172/1100000: episode: 54, duration: 0.871s, episode steps: 144, steps per second: 165, episode reward: -145.141, mean reward: -1.008 [-100.000, 11.563], mean action: 1.556 [0.000, 3.000], mean observation: 0.018 [-1.811, 2.003], loss: 7.160454, mae: 35.145016, mean_q: 45.229145
   27172/1100000: episode: 55, duration: 6.742s, episode steps: 1000, steps per second: 148, episode reward: -80.714, mean reward: -0.081 [-4.842, 6.514], mean action: 1.770 [0.000, 3.000], mean observation: 0.083 [-0.726, 1.512], loss: 10.505146, mae: 35.035686, mean_q: 45.538624
   28172/1100000: episode: 56, duration: 7.025s, episode steps: 1000, steps per second: 142, episode reward: -60.637, mean reward: -0.061 [-4.821, 4.848], mean action: 1.778 [0.000, 3.000], mean observation: 0.097 [-0.812, 1.407], loss: 10.665631, mae: 34.649227, mean_q: 45.307327
   29089/1100000: episode: 57, duration: 6.707s, episode steps: 917, steps per second: 137, episode reward: -253.144, mean reward: -0.276 [-100.000, 4.095], mean action: 1.878 [0.000, 3.000], mean observation: -0.034 [-1.000, 1.402], loss: 5.901541, mae: 34.426117, mean_q: 44.804256
   30089/1100000: episode: 58, duration: 7.521s, episode steps: 1000, steps per second: 133, episode reward: -162.446, mean reward: -0.162 [-7.240, 11.023], mean action: 1.776 [0.000, 3.000], mean observation: 0.044 [-0.877, 1.453], loss: 8.983343, mae: 33.996025, mean_q: 44.415791
   31089/1100000: episode: 59, duration: 7.260s, episode steps: 1000, steps per second: 138, episode reward: -45.951, mean reward: -0.046 [-11.985, 19.454], mean action: 1.985 [0.000, 3.000], mean observation: -0.005 [-0.745, 1.390], loss: 10.950379, mae: 33.357914, mean_q: 43.893623
   31179/1100000: episode: 60, duration: 0.542s, episode steps: 90, steps per second: 166, episode reward: -144.799, mean reward: -1.609 [-100.000, 10.874], mean action: 0.933 [0.000, 3.000], mean observation: -0.137 [-5.965, 1.476], loss: 5.128670, mae: 33.198387, mean_q: 43.749249
   32179/1100000: episode: 61, duration: 7.068s, episode steps: 1000, steps per second: 141, episode reward: -43.541, mean reward: -0.044 [-5.013, 11.917], mean action: 1.813 [0.000, 3.000], mean observation: 0.083 [-0.624, 1.413], loss: 8.692412, mae: 32.930691, mean_q: 43.118774
   32610/1100000: episode: 62, duration: 2.810s, episode steps: 431, steps per second: 153, episode reward: -133.719, mean reward: -0.310 [-100.000, 4.595], mean action: 1.884 [0.000, 3.000], mean observation: -0.108 [-1.002, 1.442], loss: 5.850336, mae: 32.365833, mean_q: 42.570557
   33136/1100000: episode: 63, duration: 3.482s, episode steps: 526, steps per second: 151, episode reward: -371.807, mean reward: -0.707 [-100.000, 5.456], mean action: 1.781 [0.000, 3.000], mean observation: -0.087 [-3.528, 1.507], loss: 4.913357, mae: 32.969269, mean_q: 43.246185
   34136/1100000: episode: 64, duration: 7.818s, episode steps: 1000, steps per second: 128, episode reward: 32.805, mean reward: 0.033 [-26.878, 21.646], mean action: 1.780 [0.000, 3.000], mean observation: 0.100 [-0.648, 1.409], loss: 8.347274, mae: 32.724758, mean_q: 42.913010
   34452/1100000: episode: 65, duration: 2.013s, episode steps: 316, steps per second: 157, episode reward: -106.626, mean reward: -0.337 [-100.000, 10.231], mean action: 1.674 [0.000, 3.000], mean observation: 0.131 [-1.199, 2.143], loss: 6.862556, mae: 32.368183, mean_q: 42.394939
   34829/1100000: episode: 66, duration: 2.465s, episode steps: 377, steps per second: 153, episode reward: -94.126, mean reward: -0.250 [-100.000, 14.594], mean action: 1.645 [0.000, 3.000], mean observation: 0.232 [-1.104, 1.499], loss: 5.196752, mae: 32.466171, mean_q: 42.554321
   35550/1100000: episode: 67, duration: 4.949s, episode steps: 721, steps per second: 146, episode reward: -255.757, mean reward: -0.355 [-100.000, 25.846], mean action: 1.791 [0.000, 3.000], mean observation: 0.176 [-0.891, 1.720], loss: 10.156803, mae: 32.362671, mean_q: 42.435020
   35999/1100000: episode: 68, duration: 2.889s, episode steps: 449, steps per second: 155, episode reward: -278.012, mean reward: -0.619 [-100.000, 15.542], mean action: 1.837 [0.000, 3.000], mean observation: 0.067 [-0.868, 1.978], loss: 7.467755, mae: 32.059467, mean_q: 42.261749
   36817/1100000: episode: 69, duration: 5.673s, episode steps: 818, steps per second: 144, episode reward: -277.493, mean reward: -0.339 [-100.000, 16.440], mean action: 1.806 [0.000, 3.000], mean observation: 0.124 [-0.849, 1.585], loss: 5.394379, mae: 32.325165, mean_q: 42.583771
   36926/1100000: episode: 70, duration: 0.676s, episode steps: 109, steps per second: 161, episode reward: -151.389, mean reward: -1.389 [-100.000, 2.323], mean action: 1.688 [0.000, 3.000], mean observation: -0.171 [-1.089, 1.416], loss: 7.351817, mae: 32.133030, mean_q: 42.031719
   37063/1100000: episode: 71, duration: 0.849s, episode steps: 137, steps per second: 161, episode reward: -92.286, mean reward: -0.674 [-100.000, 16.791], mean action: 1.628 [0.000, 3.000], mean observation: 0.099 [-1.174, 1.806], loss: 5.194999, mae: 31.939930, mean_q: 41.740715
   37873/1100000: episode: 72, duration: 5.677s, episode steps: 810, steps per second: 143, episode reward: 129.078, mean reward: 0.159 [-20.972, 100.000], mean action: 1.707 [0.000, 3.000], mean observation: 0.074 [-0.521, 1.464], loss: 6.987585, mae: 32.144562, mean_q: 41.693447
   37986/1100000: episode: 73, duration: 0.704s, episode steps: 113, steps per second: 160, episode reward: -15.824, mean reward: -0.140 [-100.000, 97.575], mean action: 1.708 [0.000, 3.000], mean observation: -0.109 [-1.871, 1.399], loss: 13.598024, mae: 31.995043, mean_q: 41.127735
   38306/1100000: episode: 74, duration: 2.017s, episode steps: 320, steps per second: 159, episode reward: -71.608, mean reward: -0.224 [-100.000, 55.237], mean action: 1.756 [0.000, 3.000], mean observation: 0.023 [-0.802, 1.399], loss: 5.286908, mae: 31.916040, mean_q: 41.073975
   38618/1100000: episode: 75, duration: 2.056s, episode steps: 312, steps per second: 152, episode reward: -42.284, mean reward: -0.136 [-100.000, 25.961], mean action: 1.824 [0.000, 3.000], mean observation: 0.111 [-1.421, 1.487], loss: 11.081927, mae: 31.925533, mean_q: 41.746956
   39389/1100000: episode: 76, duration: 5.241s, episode steps: 771, steps per second: 147, episode reward: 170.149, mean reward: 0.221 [-19.416, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.024 [-1.178, 1.503], loss: 4.783340, mae: 32.179947, mean_q: 41.445015
   40389/1100000: episode: 77, duration: 7.216s, episode steps: 1000, steps per second: 139, episode reward: -150.592, mean reward: -0.151 [-5.292, 4.084], mean action: 1.819 [0.000, 3.000], mean observation: 0.185 [-0.509, 1.558], loss: 5.701706, mae: 32.201698, mean_q: 41.655853
   41178/1100000: episode: 78, duration: 5.566s, episode steps: 789, steps per second: 142, episode reward: 141.652, mean reward: 0.180 [-18.112, 100.000], mean action: 2.076 [0.000, 3.000], mean observation: 0.144 [-0.579, 1.453], loss: 6.998685, mae: 32.271236, mean_q: 41.657398
   41734/1100000: episode: 79, duration: 3.742s, episode steps: 556, steps per second: 149, episode reward: -329.498, mean reward: -0.593 [-100.000, 5.540], mean action: 1.734 [0.000, 3.000], mean observation: 0.055 [-0.980, 3.608], loss: 5.037479, mae: 32.326038, mean_q: 42.070621
   42554/1100000: episode: 80, duration: 5.952s, episode steps: 820, steps per second: 138, episode reward: 113.428, mean reward: 0.138 [-17.404, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: -0.071 [-0.769, 1.396], loss: 5.398610, mae: 32.381310, mean_q: 41.965401
   43554/1100000: episode: 81, duration: 7.328s, episode steps: 1000, steps per second: 136, episode reward: -13.165, mean reward: -0.013 [-4.650, 5.690], mean action: 1.864 [0.000, 3.000], mean observation: 0.008 [-0.600, 1.420], loss: 5.467237, mae: 32.976727, mean_q: 42.823139
   44554/1100000: episode: 82, duration: 7.092s, episode steps: 1000, steps per second: 141, episode reward: 13.245, mean reward: 0.013 [-21.884, 22.269], mean action: 1.586 [0.000, 3.000], mean observation: 0.096 [-0.925, 1.387], loss: 6.184773, mae: 33.633373, mean_q: 43.716095
   45554/1100000: episode: 83, duration: 6.906s, episode steps: 1000, steps per second: 145, episode reward: 3.835, mean reward: 0.004 [-20.754, 13.938], mean action: 2.016 [0.000, 3.000], mean observation: 0.004 [-0.715, 1.464], loss: 7.205550, mae: 34.307117, mean_q: 44.474560
   46318/1100000: episode: 84, duration: 5.340s, episode steps: 764, steps per second: 143, episode reward: -236.086, mean reward: -0.309 [-100.000, 32.416], mean action: 1.709 [0.000, 3.000], mean observation: 0.094 [-0.700, 1.477], loss: 9.453314, mae: 35.770782, mean_q: 46.229332
   47318/1100000: episode: 85, duration: 7.690s, episode steps: 1000, steps per second: 130, episode reward: -57.422, mean reward: -0.057 [-4.491, 5.709], mean action: 1.665 [0.000, 3.000], mean observation: 0.090 [-0.371, 1.472], loss: 8.954676, mae: 35.825512, mean_q: 46.452549
   47417/1100000: episode: 86, duration: 0.629s, episode steps: 99, steps per second: 157, episode reward: -191.163, mean reward: -1.931 [-100.000, 48.335], mean action: 1.636 [0.000, 3.000], mean observation: 0.088 [-1.217, 2.398], loss: 7.431798, mae: 36.057472, mean_q: 46.581863
   48417/1100000: episode: 87, duration: 7.686s, episode steps: 1000, steps per second: 130, episode reward: -61.753, mean reward: -0.062 [-4.981, 5.163], mean action: 1.678 [0.000, 3.000], mean observation: 0.107 [-0.413, 1.484], loss: 9.999521, mae: 35.644608, mean_q: 46.645008
   49417/1100000: episode: 88, duration: 7.574s, episode steps: 1000, steps per second: 132, episode reward: -67.212, mean reward: -0.067 [-5.185, 5.381], mean action: 1.866 [0.000, 3.000], mean observation: 0.128 [-0.748, 1.432], loss: 6.329305, mae: 35.730812, mean_q: 46.891369
   49794/1100000: episode: 89, duration: 2.522s, episode steps: 377, steps per second: 149, episode reward: 154.915, mean reward: 0.411 [-7.431, 100.000], mean action: 1.626 [0.000, 3.000], mean observation: -0.057 [-1.349, 1.408], loss: 5.418262, mae: 35.965004, mean_q: 47.395844
   50794/1100000: episode: 90, duration: 7.802s, episode steps: 1000, steps per second: 128, episode reward: -117.851, mean reward: -0.118 [-5.306, 4.288], mean action: 1.730 [0.000, 3.000], mean observation: 0.131 [-0.427, 1.391], loss: 6.746494, mae: 36.175041, mean_q: 47.787006
   51794/1100000: episode: 91, duration: 7.470s, episode steps: 1000, steps per second: 134, episode reward: -12.349, mean reward: -0.012 [-23.878, 21.198], mean action: 1.764 [0.000, 3.000], mean observation: -0.015 [-1.139, 1.498], loss: 8.450370, mae: 36.217091, mean_q: 47.809147
   52210/1100000: episode: 92, duration: 2.805s, episode steps: 416, steps per second: 148, episode reward: -347.995, mean reward: -0.837 [-100.000, 5.070], mean action: 1.868 [0.000, 3.000], mean observation: 0.091 [-2.939, 2.794], loss: 4.236802, mae: 36.421238, mean_q: 48.160786
   53210/1100000: episode: 93, duration: 7.595s, episode steps: 1000, steps per second: 132, episode reward: -119.966, mean reward: -0.120 [-4.871, 4.218], mean action: 1.758 [0.000, 3.000], mean observation: 0.170 [-0.404, 1.428], loss: 6.105212, mae: 37.118706, mean_q: 49.006432
   54210/1100000: episode: 94, duration: 7.618s, episode steps: 1000, steps per second: 131, episode reward: -71.889, mean reward: -0.072 [-13.505, 21.895], mean action: 1.660 [0.000, 3.000], mean observation: 0.109 [-1.001, 1.415], loss: 7.232750, mae: 37.393349, mean_q: 49.374619
   54333/1100000: episode: 95, duration: 0.792s, episode steps: 123, steps per second: 155, episode reward: -93.183, mean reward: -0.758 [-100.000, 13.390], mean action: 1.268 [0.000, 3.000], mean observation: 0.084 [-5.296, 1.391], loss: 3.206523, mae: 37.873402, mean_q: 50.091724
   55333/1100000: episode: 96, duration: 7.632s, episode steps: 1000, steps per second: 131, episode reward: -123.937, mean reward: -0.124 [-5.964, 4.610], mean action: 1.817 [0.000, 3.000], mean observation: 0.183 [-0.310, 1.399], loss: 5.499018, mae: 37.609924, mean_q: 49.250587
   55524/1100000: episode: 97, duration: 1.239s, episode steps: 191, steps per second: 154, episode reward: -192.761, mean reward: -1.009 [-100.000, 5.330], mean action: 1.853 [0.000, 3.000], mean observation: 0.013 [-1.315, 2.152], loss: 15.489437, mae: 38.296909, mean_q: 50.363617
   56042/1100000: episode: 98, duration: 3.686s, episode steps: 518, steps per second: 141, episode reward: 185.636, mean reward: 0.358 [-19.315, 100.000], mean action: 1.660 [0.000, 3.000], mean observation: -0.009 [-0.778, 1.394], loss: 7.954920, mae: 37.414551, mean_q: 49.564468
   57042/1100000: episode: 99, duration: 7.598s, episode steps: 1000, steps per second: 132, episode reward: -2.847, mean reward: -0.003 [-14.693, 24.702], mean action: 1.717 [0.000, 3.000], mean observation: 0.093 [-1.287, 1.415], loss: 6.328666, mae: 37.293938, mean_q: 49.251743
   58008/1100000: episode: 100, duration: 6.935s, episode steps: 966, steps per second: 139, episode reward: 137.146, mean reward: 0.142 [-18.047, 100.000], mean action: 2.130 [0.000, 3.000], mean observation: 0.073 [-1.191, 1.481], loss: 7.830586, mae: 36.889709, mean_q: 48.788036
   58140/1100000: episode: 101, duration: 0.853s, episode steps: 132, steps per second: 155, episode reward: -130.097, mean reward: -0.986 [-100.000, 10.661], mean action: 1.258 [0.000, 3.000], mean observation: 0.061 [-4.687, 1.409], loss: 4.936501, mae: 37.367310, mean_q: 49.473396
   58288/1100000: episode: 102, duration: 0.956s, episode steps: 148, steps per second: 155, episode reward: -141.629, mean reward: -0.957 [-100.000, 9.048], mean action: 1.108 [0.000, 3.000], mean observation: 0.110 [-6.044, 1.507], loss: 8.012587, mae: 37.194618, mean_q: 49.262695
   58996/1100000: episode: 103, duration: 4.963s, episode steps: 708, steps per second: 143, episode reward: -167.650, mean reward: -0.237 [-100.000, 11.444], mean action: 1.944 [0.000, 3.000], mean observation: 0.042 [-0.967, 1.405], loss: 6.214943, mae: 37.262684, mean_q: 49.311550
   59307/1100000: episode: 104, duration: 2.071s, episode steps: 311, steps per second: 150, episode reward: -26.673, mean reward: -0.086 [-100.000, 10.502], mean action: 1.540 [0.000, 3.000], mean observation: 0.050 [-1.701, 1.425], loss: 6.179619, mae: 37.080872, mean_q: 48.802235
   59470/1100000: episode: 105, duration: 1.051s, episode steps: 163, steps per second: 155, episode reward: -120.343, mean reward: -0.738 [-100.000, 9.478], mean action: 1.239 [0.000, 3.000], mean observation: 0.174 [-5.023, 1.525], loss: 6.564667, mae: 36.655155, mean_q: 48.567223
   59732/1100000: episode: 106, duration: 1.748s, episode steps: 262, steps per second: 150, episode reward: -105.359, mean reward: -0.402 [-100.000, 13.105], mean action: 1.550 [0.000, 3.000], mean observation: 0.089 [-3.771, 1.398], loss: 5.809426, mae: 37.017071, mean_q: 48.719418
   59993/1100000: episode: 107, duration: 1.754s, episode steps: 261, steps per second: 149, episode reward: -148.971, mean reward: -0.571 [-100.000, 3.613], mean action: 1.732 [0.000, 3.000], mean observation: -0.078 [-1.000, 1.397], loss: 8.013556, mae: 36.941872, mean_q: 48.666862
   60544/1100000: episode: 108, duration: 3.966s, episode steps: 551, steps per second: 139, episode reward: -81.689, mean reward: -0.148 [-100.000, 18.800], mean action: 1.657 [0.000, 3.000], mean observation: 0.097 [-0.777, 1.401], loss: 6.720534, mae: 36.772511, mean_q: 48.191269
   60703/1100000: episode: 109, duration: 1.051s, episode steps: 159, steps per second: 151, episode reward: -153.363, mean reward: -0.965 [-100.000, 9.854], mean action: 1.730 [0.000, 3.000], mean observation: -0.049 [-1.377, 1.405], loss: 6.870689, mae: 36.938084, mean_q: 48.530708
   61341/1100000: episode: 110, duration: 4.751s, episode steps: 638, steps per second: 134, episode reward: 168.892, mean reward: 0.265 [-18.879, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: -0.041 [-0.858, 1.406], loss: 5.608785, mae: 36.678703, mean_q: 47.938728
   61641/1100000: episode: 111, duration: 2.038s, episode steps: 300, steps per second: 147, episode reward: -84.054, mean reward: -0.280 [-100.000, 11.491], mean action: 1.650 [0.000, 3.000], mean observation: 0.078 [-2.292, 1.434], loss: 12.310135, mae: 37.083084, mean_q: 48.236610
   61720/1100000: episode: 112, duration: 0.519s, episode steps: 79, steps per second: 152, episode reward: -116.163, mean reward: -1.470 [-100.000, 5.731], mean action: 0.962 [0.000, 3.000], mean observation: 0.081 [-1.590, 4.841], loss: 6.909449, mae: 36.433327, mean_q: 48.003620
   61858/1100000: episode: 113, duration: 0.906s, episode steps: 138, steps per second: 152, episode reward: -220.567, mean reward: -1.598 [-100.000, 2.845], mean action: 1.964 [0.000, 3.000], mean observation: -0.113 [-1.581, 1.436], loss: 11.277662, mae: 36.415585, mean_q: 47.628254
   62282/1100000: episode: 114, duration: 2.976s, episode steps: 424, steps per second: 142, episode reward: 139.798, mean reward: 0.330 [-17.783, 100.000], mean action: 1.774 [0.000, 3.000], mean observation: -0.077 [-1.288, 1.479], loss: 9.748247, mae: 36.803177, mean_q: 48.059883
   63023/1100000: episode: 115, duration: 5.238s, episode steps: 741, steps per second: 141, episode reward: -160.582, mean reward: -0.217 [-100.000, 19.354], mean action: 1.653 [0.000, 3.000], mean observation: 0.142 [-1.245, 1.408], loss: 8.250546, mae: 36.682449, mean_q: 47.907856
   64023/1100000: episode: 116, duration: 7.763s, episode steps: 1000, steps per second: 129, episode reward: -11.005, mean reward: -0.011 [-11.723, 12.363], mean action: 1.812 [0.000, 3.000], mean observation: -0.085 [-0.828, 1.452], loss: 8.060636, mae: 37.213017, mean_q: 48.589520
   64558/1100000: episode: 117, duration: 3.747s, episode steps: 535, steps per second: 143, episode reward: -162.602, mean reward: -0.304 [-100.000, 5.575], mean action: 1.871 [0.000, 3.000], mean observation: -0.084 [-1.003, 1.515], loss: 6.625678, mae: 36.986172, mean_q: 48.749889
   65558/1100000: episode: 118, duration: 7.778s, episode steps: 1000, steps per second: 129, episode reward: -73.869, mean reward: -0.074 [-4.938, 5.021], mean action: 1.709 [0.000, 3.000], mean observation: 0.112 [-0.496, 1.393], loss: 6.969313, mae: 36.769638, mean_q: 48.140820
   66358/1100000: episode: 119, duration: 5.853s, episode steps: 800, steps per second: 137, episode reward: 192.067, mean reward: 0.240 [-19.940, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.073 [-0.925, 1.395], loss: 7.042119, mae: 36.997456, mean_q: 48.734589
   66993/1100000: episode: 120, duration: 4.577s, episode steps: 635, steps per second: 139, episode reward: 132.235, mean reward: 0.208 [-21.058, 100.000], mean action: 2.033 [0.000, 3.000], mean observation: 0.004 [-1.584, 1.391], loss: 8.694638, mae: 36.720455, mean_q: 48.701492
   67789/1100000: episode: 121, duration: 6.006s, episode steps: 796, steps per second: 133, episode reward: -267.198, mean reward: -0.336 [-100.000, 16.943], mean action: 1.715 [0.000, 3.000], mean observation: 0.031 [-1.008, 1.400], loss: 7.288527, mae: 36.573433, mean_q: 48.465698
   68789/1100000: episode: 122, duration: 7.266s, episode steps: 1000, steps per second: 138, episode reward: -110.505, mean reward: -0.111 [-8.601, 36.735], mean action: 1.844 [0.000, 3.000], mean observation: 0.099 [-0.857, 1.404], loss: 6.234130, mae: 36.421570, mean_q: 48.293468
   69789/1100000: episode: 123, duration: 7.890s, episode steps: 1000, steps per second: 127, episode reward: 86.453, mean reward: 0.086 [-19.957, 22.905], mean action: 1.259 [0.000, 3.000], mean observation: 0.007 [-0.764, 1.400], loss: 6.172752, mae: 36.248962, mean_q: 48.169060
   70789/1100000: episode: 124, duration: 7.875s, episode steps: 1000, steps per second: 127, episode reward: 24.218, mean reward: 0.024 [-4.541, 4.984], mean action: 1.657 [0.000, 3.000], mean observation: 0.064 [-0.411, 1.399], loss: 4.974508, mae: 36.351860, mean_q: 48.350960
   71511/1100000: episode: 125, duration: 5.422s, episode steps: 722, steps per second: 133, episode reward: 146.074, mean reward: 0.202 [-19.428, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.018 [-0.799, 1.419], loss: 5.672352, mae: 36.328892, mean_q: 48.290474
   72335/1100000: episode: 126, duration: 6.374s, episode steps: 824, steps per second: 129, episode reward: 87.740, mean reward: 0.106 [-17.805, 100.000], mean action: 1.631 [0.000, 3.000], mean observation: 0.027 [-0.684, 1.429], loss: 6.362943, mae: 36.099857, mean_q: 48.306427
   73335/1100000: episode: 127, duration: 7.872s, episode steps: 1000, steps per second: 127, episode reward: -143.601, mean reward: -0.144 [-5.061, 5.190], mean action: 1.760 [0.000, 3.000], mean observation: 0.090 [-0.873, 1.432], loss: 6.072055, mae: 36.086414, mean_q: 48.245953
   73479/1100000: episode: 128, duration: 0.959s, episode steps: 144, steps per second: 150, episode reward: -24.073, mean reward: -0.167 [-100.000, 10.444], mean action: 1.389 [0.000, 3.000], mean observation: 0.048 [-3.097, 1.455], loss: 4.268978, mae: 35.964790, mean_q: 47.706753
   73904/1100000: episode: 129, duration: 2.990s, episode steps: 425, steps per second: 142, episode reward: -146.490, mean reward: -0.345 [-100.000, 4.118], mean action: 1.885 [0.000, 3.000], mean observation: 0.055 [-1.352, 1.484], loss: 4.732884, mae: 35.728870, mean_q: 47.739735
   74904/1100000: episode: 130, duration: 7.413s, episode steps: 1000, steps per second: 135, episode reward: 35.528, mean reward: 0.036 [-21.685, 23.033], mean action: 1.577 [0.000, 3.000], mean observation: 0.038 [-0.600, 1.413], loss: 4.788695, mae: 35.482098, mean_q: 47.347118
   75690/1100000: episode: 131, duration: 5.781s, episode steps: 786, steps per second: 136, episode reward: 150.163, mean reward: 0.191 [-19.818, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: -0.020 [-0.817, 1.401], loss: 5.825645, mae: 35.688358, mean_q: 47.692589
   76192/1100000: episode: 132, duration: 3.555s, episode steps: 502, steps per second: 141, episode reward: -352.797, mean reward: -0.703 [-100.000, 27.179], mean action: 1.574 [0.000, 3.000], mean observation: 0.093 [-1.842, 1.425], loss: 7.385174, mae: 35.684238, mean_q: 47.645218
   76612/1100000: episode: 133, duration: 2.909s, episode steps: 420, steps per second: 144, episode reward: 261.057, mean reward: 0.622 [-12.165, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.111 [-0.672, 1.864], loss: 5.160286, mae: 35.359402, mean_q: 47.296558
   77612/1100000: episode: 134, duration: 7.618s, episode steps: 1000, steps per second: 131, episode reward: -54.384, mean reward: -0.054 [-4.547, 5.368], mean action: 1.862 [0.000, 3.000], mean observation: 0.059 [-0.600, 1.564], loss: 6.337896, mae: 35.519382, mean_q: 47.308220
   78612/1100000: episode: 135, duration: 7.514s, episode steps: 1000, steps per second: 133, episode reward: -116.635, mean reward: -0.117 [-4.518, 4.603], mean action: 1.866 [0.000, 3.000], mean observation: 0.078 [-0.600, 1.389], loss: 5.872027, mae: 35.635345, mean_q: 47.347836
   78714/1100000: episode: 136, duration: 0.680s, episode steps: 102, steps per second: 150, episode reward: 7.654, mean reward: 0.075 [-100.000, 14.080], mean action: 1.373 [0.000, 3.000], mean observation: 0.011 [-1.019, 1.397], loss: 8.705659, mae: 35.847244, mean_q: 47.570911
   79037/1100000: episode: 137, duration: 2.260s, episode steps: 323, steps per second: 143, episode reward: 240.989, mean reward: 0.746 [-11.869, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.083 [-1.315, 1.509], loss: 7.318495, mae: 36.042931, mean_q: 47.895615
   79563/1100000: episode: 138, duration: 3.907s, episode steps: 526, steps per second: 135, episode reward: -31.864, mean reward: -0.061 [-100.000, 21.519], mean action: 1.437 [0.000, 3.000], mean observation: 0.128 [-0.751, 1.451], loss: 5.830313, mae: 35.687389, mean_q: 47.574986
   79656/1100000: episode: 139, duration: 0.623s, episode steps: 93, steps per second: 149, episode reward: -92.087, mean reward: -0.990 [-100.000, 8.712], mean action: 1.925 [0.000, 3.000], mean observation: 0.166 [-3.094, 1.389], loss: 3.293502, mae: 36.040657, mean_q: 48.184322
   79746/1100000: episode: 140, duration: 0.591s, episode steps: 90, steps per second: 152, episode reward: -115.274, mean reward: -1.281 [-100.000, 8.761], mean action: 1.167 [0.000, 3.000], mean observation: 0.097 [-5.558, 1.537], loss: 4.579209, mae: 36.208145, mean_q: 48.223854
   79904/1100000: episode: 141, duration: 1.047s, episode steps: 158, steps per second: 151, episode reward: -205.639, mean reward: -1.302 [-100.000, 8.242], mean action: 1.310 [0.000, 3.000], mean observation: -0.005 [-1.911, 1.598], loss: 3.650461, mae: 35.888870, mean_q: 47.871704
   80031/1100000: episode: 142, duration: 0.847s, episode steps: 127, steps per second: 150, episode reward: -211.194, mean reward: -1.663 [-100.000, 44.179], mean action: 1.866 [0.000, 3.000], mean observation: 0.223 [-1.080, 2.743], loss: 10.070978, mae: 36.603661, mean_q: 48.905727
   80099/1100000: episode: 143, duration: 0.449s, episode steps: 68, steps per second: 151, episode reward: -38.850, mean reward: -0.571 [-100.000, 9.977], mean action: 1.191 [0.000, 3.000], mean observation: 0.008 [-1.173, 2.720], loss: 3.911421, mae: 36.436909, mean_q: 47.917343
   80601/1100000: episode: 144, duration: 3.563s, episode steps: 502, steps per second: 141, episode reward: 217.643, mean reward: 0.434 [-19.143, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.044 [-1.021, 1.410], loss: 5.154886, mae: 36.205406, mean_q: 48.096512
   81152/1100000: episode: 145, duration: 3.959s, episode steps: 551, steps per second: 139, episode reward: -48.933, mean reward: -0.089 [-100.000, 14.032], mean action: 1.719 [0.000, 3.000], mean observation: 0.013 [-0.624, 1.472], loss: 7.440773, mae: 36.307106, mean_q: 48.215687
   82152/1100000: episode: 146, duration: 7.629s, episode steps: 1000, steps per second: 131, episode reward: -133.479, mean reward: -0.133 [-5.054, 4.269], mean action: 1.784 [0.000, 3.000], mean observation: 0.097 [-0.600, 1.410], loss: 5.689700, mae: 36.450890, mean_q: 48.481407
   82909/1100000: episode: 147, duration: 5.559s, episode steps: 757, steps per second: 136, episode reward: -111.468, mean reward: -0.147 [-100.000, 21.951], mean action: 1.793 [0.000, 3.000], mean observation: 0.165 [-0.501, 1.398], loss: 4.506981, mae: 36.341679, mean_q: 48.289135
   82999/1100000: episode: 148, duration: 0.595s, episode steps: 90, steps per second: 151, episode reward: -93.846, mean reward: -1.043 [-100.000, 5.626], mean action: 1.222 [0.000, 3.000], mean observation: 0.138 [-3.761, 1.403], loss: 10.148335, mae: 36.957485, mean_q: 48.623955
   83568/1100000: episode: 149, duration: 4.061s, episode steps: 569, steps per second: 140, episode reward: -172.254, mean reward: -0.303 [-100.000, 8.806], mean action: 1.641 [0.000, 3.000], mean observation: 0.064 [-1.123, 4.255], loss: 6.817111, mae: 36.521729, mean_q: 48.493656
   84568/1100000: episode: 150, duration: 8.059s, episode steps: 1000, steps per second: 124, episode reward: -70.689, mean reward: -0.071 [-24.725, 23.820], mean action: 1.547 [0.000, 3.000], mean observation: 0.037 [-0.600, 1.412], loss: 7.384502, mae: 36.198196, mean_q: 47.946117
   84713/1100000: episode: 151, duration: 0.968s, episode steps: 145, steps per second: 150, episode reward: -140.674, mean reward: -0.970 [-100.000, 4.325], mean action: 1.386 [0.000, 3.000], mean observation: 0.059 [-0.728, 1.399], loss: 5.585127, mae: 36.452229, mean_q: 48.100605
   85384/1100000: episode: 152, duration: 4.865s, episode steps: 671, steps per second: 138, episode reward: 137.861, mean reward: 0.205 [-17.139, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: -0.048 [-1.129, 1.394], loss: 10.672472, mae: 36.294418, mean_q: 47.933578
   86384/1100000: episode: 153, duration: 7.363s, episode steps: 1000, steps per second: 136, episode reward: 69.546, mean reward: 0.070 [-20.969, 19.569], mean action: 1.796 [0.000, 3.000], mean observation: 0.024 [-0.952, 1.464], loss: 5.906935, mae: 36.045700, mean_q: 47.716709
   86474/1100000: episode: 154, duration: 0.591s, episode steps: 90, steps per second: 152, episode reward: -159.245, mean reward: -1.769 [-100.000, 47.656], mean action: 1.122 [0.000, 3.000], mean observation: 0.203 [-1.060, 2.655], loss: 12.441381, mae: 36.132328, mean_q: 47.589367
   86758/1100000: episode: 155, duration: 1.948s, episode steps: 284, steps per second: 146, episode reward: 281.990, mean reward: 0.993 [-17.772, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.066 [-1.408, 1.393], loss: 4.186916, mae: 36.075592, mean_q: 47.763878
   87292/1100000: episode: 156, duration: 3.704s, episode steps: 534, steps per second: 144, episode reward: 220.522, mean reward: 0.413 [-18.531, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.179 [-1.314, 1.408], loss: 6.123311, mae: 36.248871, mean_q: 47.928452
   87474/1100000: episode: 157, duration: 1.226s, episode steps: 182, steps per second: 148, episode reward: -127.879, mean reward: -0.703 [-100.000, 3.805], mean action: 1.489 [0.000, 3.000], mean observation: 0.174 [-0.518, 1.426], loss: 8.131930, mae: 36.209183, mean_q: 48.031357
   87676/1100000: episode: 158, duration: 1.383s, episode steps: 202, steps per second: 146, episode reward: -155.851, mean reward: -0.772 [-100.000, 3.904], mean action: 1.713 [0.000, 3.000], mean observation: 0.160 [-0.484, 1.406], loss: 9.621736, mae: 36.597015, mean_q: 48.222027
   87855/1100000: episode: 159, duration: 1.231s, episode steps: 179, steps per second: 145, episode reward: -129.893, mean reward: -0.726 [-100.000, 11.863], mean action: 1.866 [0.000, 3.000], mean observation: 0.066 [-0.887, 1.529], loss: 5.090431, mae: 36.099461, mean_q: 47.559761
   88115/1100000: episode: 160, duration: 1.823s, episode steps: 260, steps per second: 143, episode reward: -352.191, mean reward: -1.355 [-100.000, 8.562], mean action: 1.719 [0.000, 3.000], mean observation: 0.100 [-2.675, 1.484], loss: 8.685211, mae: 36.532822, mean_q: 48.609505
   88266/1100000: episode: 161, duration: 0.998s, episode steps: 151, steps per second: 151, episode reward: -43.725, mean reward: -0.290 [-100.000, 20.735], mean action: 1.430 [0.000, 3.000], mean observation: 0.200 [-1.765, 1.538], loss: 10.386658, mae: 36.398518, mean_q: 48.007572
   88417/1100000: episode: 162, duration: 1.014s, episode steps: 151, steps per second: 149, episode reward: -99.544, mean reward: -0.659 [-100.000, 3.518], mean action: 1.523 [0.000, 3.000], mean observation: 0.197 [-0.485, 1.396], loss: 4.163119, mae: 36.346279, mean_q: 48.283455
   89012/1100000: episode: 163, duration: 4.310s, episode steps: 595, steps per second: 138, episode reward: 237.227, mean reward: 0.399 [-8.462, 100.000], mean action: 1.607 [0.000, 3.000], mean observation: 0.049 [-1.083, 1.490], loss: 6.710783, mae: 36.655647, mean_q: 48.253052
   89517/1100000: episode: 164, duration: 3.499s, episode steps: 505, steps per second: 144, episode reward: -67.681, mean reward: -0.134 [-100.000, 10.336], mean action: 1.580 [0.000, 3.000], mean observation: -0.012 [-0.830, 1.388], loss: 6.773339, mae: 36.983570, mean_q: 48.874622
   89844/1100000: episode: 165, duration: 2.250s, episode steps: 327, steps per second: 145, episode reward: 243.785, mean reward: 0.746 [-9.503, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: -0.031 [-0.800, 1.396], loss: 6.222929, mae: 37.471493, mean_q: 49.534256
   90605/1100000: episode: 166, duration: 5.599s, episode steps: 761, steps per second: 136, episode reward: 156.139, mean reward: 0.205 [-20.507, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.153 [-0.643, 1.392], loss: 6.715486, mae: 36.985863, mean_q: 48.980495
   90825/1100000: episode: 167, duration: 1.476s, episode steps: 220, steps per second: 149, episode reward: -168.336, mean reward: -0.765 [-100.000, 27.647], mean action: 1.777 [0.000, 3.000], mean observation: 0.171 [-0.827, 1.980], loss: 7.393968, mae: 37.574841, mean_q: 49.685432
   91127/1100000: episode: 168, duration: 2.089s, episode steps: 302, steps per second: 145, episode reward: -211.476, mean reward: -0.700 [-100.000, 45.346], mean action: 1.619 [0.000, 3.000], mean observation: 0.012 [-0.627, 1.779], loss: 4.786105, mae: 37.339855, mean_q: 49.476082
   91487/1100000: episode: 169, duration: 2.463s, episode steps: 360, steps per second: 146, episode reward: 3.376, mean reward: 0.009 [-100.000, 14.365], mean action: 1.778 [0.000, 3.000], mean observation: 0.124 [-0.568, 1.442], loss: 9.256330, mae: 37.022018, mean_q: 48.953358
   92487/1100000: episode: 170, duration: 7.770s, episode steps: 1000, steps per second: 129, episode reward: -50.228, mean reward: -0.050 [-21.405, 12.917], mean action: 1.479 [0.000, 3.000], mean observation: 0.008 [-0.600, 1.492], loss: 8.096038, mae: 37.628616, mean_q: 49.689728
   92828/1100000: episode: 171, duration: 2.334s, episode steps: 341, steps per second: 146, episode reward: 252.413, mean reward: 0.740 [-7.996, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.100 [-0.998, 1.497], loss: 7.911532, mae: 37.836258, mean_q: 50.047939
   93810/1100000: episode: 172, duration: 7.830s, episode steps: 982, steps per second: 125, episode reward: 155.793, mean reward: 0.159 [-17.973, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.071 [-1.185, 1.438], loss: 6.308603, mae: 37.995419, mean_q: 49.993668
   94810/1100000: episode: 173, duration: 8.295s, episode steps: 1000, steps per second: 121, episode reward: 46.948, mean reward: 0.047 [-21.195, 15.144], mean action: 2.161 [0.000, 3.000], mean observation: 0.189 [-0.557, 1.395], loss: 8.156191, mae: 38.262352, mean_q: 50.321354
   95810/1100000: episode: 174, duration: 8.027s, episode steps: 1000, steps per second: 125, episode reward: -25.938, mean reward: -0.026 [-22.557, 23.133], mean action: 1.769 [0.000, 3.000], mean observation: 0.192 [-0.836, 1.450], loss: 8.037564, mae: 38.392757, mean_q: 50.598740
   96484/1100000: episode: 175, duration: 4.899s, episode steps: 674, steps per second: 138, episode reward: 166.203, mean reward: 0.247 [-22.478, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: -0.029 [-0.987, 1.422], loss: 6.167453, mae: 38.221214, mean_q: 50.322063
   97027/1100000: episode: 176, duration: 3.796s, episode steps: 543, steps per second: 143, episode reward: 202.636, mean reward: 0.373 [-17.991, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.157 [-0.588, 1.407], loss: 7.548194, mae: 38.635899, mean_q: 50.773235
   97700/1100000: episode: 177, duration: 4.807s, episode steps: 673, steps per second: 140, episode reward: 199.926, mean reward: 0.297 [-25.027, 100.000], mean action: 1.746 [0.000, 3.000], mean observation: 0.137 [-0.578, 1.387], loss: 6.579604, mae: 38.759541, mean_q: 51.178207
   98310/1100000: episode: 178, duration: 4.255s, episode steps: 610, steps per second: 143, episode reward: 271.611, mean reward: 0.445 [-10.416, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.050 [-0.547, 1.388], loss: 6.995055, mae: 38.599079, mean_q: 50.991020
   98866/1100000: episode: 179, duration: 4.055s, episode steps: 556, steps per second: 137, episode reward: 239.655, mean reward: 0.431 [-9.320, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.106 [-1.056, 1.440], loss: 8.086396, mae: 39.019325, mean_q: 51.227329
   99422/1100000: episode: 180, duration: 3.835s, episode steps: 556, steps per second: 145, episode reward: 223.031, mean reward: 0.401 [-10.389, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: -0.076 [-0.720, 1.428], loss: 6.258979, mae: 39.387085, mean_q: 52.109329
  100030/1100000: episode: 181, duration: 4.441s, episode steps: 608, steps per second: 137, episode reward: 197.702, mean reward: 0.325 [-21.248, 100.000], mean action: 2.016 [0.000, 3.000], mean observation: 0.146 [-0.536, 1.401], loss: 6.790640, mae: 39.239792, mean_q: 51.918968
  101030/1100000: episode: 182, duration: 7.489s, episode steps: 1000, steps per second: 134, episode reward: -32.959, mean reward: -0.033 [-23.620, 22.620], mean action: 1.934 [0.000, 3.000], mean observation: 0.129 [-0.750, 1.403], loss: 7.721139, mae: 39.401516, mean_q: 52.174755
  101495/1100000: episode: 183, duration: 3.206s, episode steps: 465, steps per second: 145, episode reward: 175.516, mean reward: 0.377 [-21.127, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: -0.012 [-0.761, 1.408], loss: 6.964121, mae: 39.474319, mean_q: 52.018497
  101694/1100000: episode: 184, duration: 1.333s, episode steps: 199, steps per second: 149, episode reward: 2.435, mean reward: 0.012 [-100.000, 16.942], mean action: 1.588 [0.000, 3.000], mean observation: -0.088 [-1.941, 1.435], loss: 6.587134, mae: 39.729988, mean_q: 52.612534
  102694/1100000: episode: 185, duration: 7.825s, episode steps: 1000, steps per second: 128, episode reward: -123.988, mean reward: -0.124 [-5.219, 5.197], mean action: 1.873 [0.000, 3.000], mean observation: 0.069 [-0.691, 1.412], loss: 7.063974, mae: 39.796421, mean_q: 52.552692
  103427/1100000: episode: 186, duration: 5.471s, episode steps: 733, steps per second: 134, episode reward: 133.750, mean reward: 0.182 [-11.872, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.143 [-0.678, 1.408], loss: 5.313385, mae: 39.611225, mean_q: 52.460522
  103630/1100000: episode: 187, duration: 1.356s, episode steps: 203, steps per second: 150, episode reward: -58.686, mean reward: -0.289 [-100.000, 9.042], mean action: 1.690 [0.000, 3.000], mean observation: -0.134 [-0.807, 1.422], loss: 6.342165, mae: 39.710857, mean_q: 52.610348
  104630/1100000: episode: 188, duration: 7.627s, episode steps: 1000, steps per second: 131, episode reward: 18.348, mean reward: 0.018 [-22.897, 21.991], mean action: 1.829 [0.000, 3.000], mean observation: 0.150 [-0.623, 1.412], loss: 6.417929, mae: 39.850132, mean_q: 52.479027
  105372/1100000: episode: 189, duration: 5.300s, episode steps: 742, steps per second: 140, episode reward: 252.436, mean reward: 0.340 [-19.705, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.106 [-0.598, 1.413], loss: 4.643644, mae: 39.694130, mean_q: 52.545055
  105487/1100000: episode: 190, duration: 0.759s, episode steps: 115, steps per second: 152, episode reward: 19.733, mean reward: 0.172 [-100.000, 15.983], mean action: 1.322 [0.000, 3.000], mean observation: -0.071 [-0.982, 1.473], loss: 4.326212, mae: 39.677670, mean_q: 52.452690
  106026/1100000: episode: 191, duration: 3.869s, episode steps: 539, steps per second: 139, episode reward: 252.077, mean reward: 0.468 [-4.067, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.050 [-0.731, 1.400], loss: 5.775209, mae: 39.864979, mean_q: 52.864838
  106497/1100000: episode: 192, duration: 3.350s, episode steps: 471, steps per second: 141, episode reward: 206.618, mean reward: 0.439 [-19.465, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.070 [-0.846, 1.429], loss: 6.383352, mae: 39.886063, mean_q: 52.989948
  106584/1100000: episode: 193, duration: 0.579s, episode steps: 87, steps per second: 150, episode reward: -120.645, mean reward: -1.387 [-100.000, 7.793], mean action: 1.241 [0.000, 3.000], mean observation: 0.065 [-1.209, 2.687], loss: 5.782485, mae: 39.596962, mean_q: 52.368057
  107584/1100000: episode: 194, duration: 7.663s, episode steps: 1000, steps per second: 131, episode reward: -12.481, mean reward: -0.012 [-22.697, 24.648], mean action: 1.929 [0.000, 3.000], mean observation: 0.186 [-0.502, 1.396], loss: 7.244483, mae: 39.774719, mean_q: 52.768501
  107828/1100000: episode: 195, duration: 1.632s, episode steps: 244, steps per second: 150, episode reward: 263.545, mean reward: 1.080 [-10.523, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.073 [-1.170, 1.465], loss: 5.803477, mae: 39.387959, mean_q: 52.156246
  108089/1100000: episode: 196, duration: 1.749s, episode steps: 261, steps per second: 149, episode reward: -18.576, mean reward: -0.071 [-100.000, 39.408], mean action: 1.701 [0.000, 3.000], mean observation: 0.048 [-2.048, 1.404], loss: 4.486067, mae: 39.844933, mean_q: 52.864113
  108649/1100000: episode: 197, duration: 3.881s, episode steps: 560, steps per second: 144, episode reward: -189.244, mean reward: -0.338 [-100.000, 13.542], mean action: 1.721 [0.000, 3.000], mean observation: -0.021 [-1.000, 1.397], loss: 5.698540, mae: 39.995914, mean_q: 52.761955
  109088/1100000: episode: 198, duration: 2.977s, episode steps: 439, steps per second: 147, episode reward: 243.801, mean reward: 0.555 [-20.412, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.140 [-1.068, 1.409], loss: 6.384398, mae: 39.988396, mean_q: 52.752617
  109179/1100000: episode: 199, duration: 0.604s, episode steps: 91, steps per second: 151, episode reward: -162.247, mean reward: -1.783 [-100.000, 8.643], mean action: 1.791 [0.000, 3.000], mean observation: -0.110 [-5.076, 1.481], loss: 12.470918, mae: 40.140915, mean_q: 52.338245
  110025/1100000: episode: 200, duration: 6.104s, episode steps: 846, steps per second: 139, episode reward: 233.576, mean reward: 0.276 [-22.570, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.119 [-0.620, 1.392], loss: 7.520459, mae: 40.054504, mean_q: 52.625435
  110188/1100000: episode: 201, duration: 1.093s, episode steps: 163, steps per second: 149, episode reward: -1.831, mean reward: -0.011 [-100.000, 9.831], mean action: 1.718 [0.000, 3.000], mean observation: 0.034 [-1.395, 1.402], loss: 6.665292, mae: 39.799240, mean_q: 52.388493
  110581/1100000: episode: 202, duration: 2.767s, episode steps: 393, steps per second: 142, episode reward: 222.420, mean reward: 0.566 [-13.544, 100.000], mean action: 2.117 [0.000, 3.000], mean observation: -0.015 [-0.727, 1.444], loss: 7.188497, mae: 39.856434, mean_q: 52.490776
  111240/1100000: episode: 203, duration: 4.780s, episode steps: 659, steps per second: 138, episode reward: 177.809, mean reward: 0.270 [-19.838, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.156 [-0.665, 1.402], loss: 7.709165, mae: 39.849270, mean_q: 52.316235
  111707/1100000: episode: 204, duration: 3.256s, episode steps: 467, steps per second: 143, episode reward: 164.212, mean reward: 0.352 [-19.045, 100.000], mean action: 1.869 [0.000, 3.000], mean observation: 0.158 [-0.680, 1.406], loss: 6.356852, mae: 39.885433, mean_q: 52.550789
  111983/1100000: episode: 205, duration: 1.876s, episode steps: 276, steps per second: 147, episode reward: -143.681, mean reward: -0.521 [-100.000, 9.347], mean action: 1.844 [0.000, 3.000], mean observation: -0.122 [-1.003, 1.413], loss: 6.558578, mae: 40.320339, mean_q: 53.443272
  112540/1100000: episode: 206, duration: 3.991s, episode steps: 557, steps per second: 140, episode reward: -252.971, mean reward: -0.454 [-100.000, 25.997], mean action: 1.312 [0.000, 3.000], mean observation: -0.052 [-1.541, 1.401], loss: 5.547219, mae: 40.402679, mean_q: 53.075718
  113397/1100000: episode: 207, duration: 6.278s, episode steps: 857, steps per second: 137, episode reward: 200.941, mean reward: 0.234 [-18.854, 100.000], mean action: 1.978 [0.000, 3.000], mean observation: 0.121 [-0.751, 1.521], loss: 6.468191, mae: 40.796219, mean_q: 53.665562
  114106/1100000: episode: 208, duration: 5.031s, episode steps: 709, steps per second: 141, episode reward: -207.400, mean reward: -0.293 [-100.000, 12.730], mean action: 1.588 [0.000, 3.000], mean observation: -0.004 [-1.005, 1.410], loss: 7.395023, mae: 41.249298, mean_q: 54.064934
  114517/1100000: episode: 209, duration: 2.839s, episode steps: 411, steps per second: 145, episode reward: 252.730, mean reward: 0.615 [-9.077, 100.000], mean action: 1.903 [0.000, 3.000], mean observation: 0.001 [-0.861, 1.406], loss: 7.354439, mae: 42.045921, mean_q: 55.422035
  115517/1100000: episode: 210, duration: 7.735s, episode steps: 1000, steps per second: 129, episode reward: 128.964, mean reward: 0.129 [-20.322, 15.711], mean action: 2.568 [0.000, 3.000], mean observation: 0.193 [-0.749, 1.411], loss: 6.937136, mae: 42.064438, mean_q: 55.581635
  115940/1100000: episode: 211, duration: 2.889s, episode steps: 423, steps per second: 146, episode reward: 300.450, mean reward: 0.710 [-18.036, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.126 [-0.811, 1.472], loss: 6.296795, mae: 42.739372, mean_q: 56.273327
  116475/1100000: episode: 212, duration: 3.850s, episode steps: 535, steps per second: 139, episode reward: 178.138, mean reward: 0.333 [-13.174, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.053 [-0.896, 1.427], loss: 7.722219, mae: 43.202160, mean_q: 57.081142
  117022/1100000: episode: 213, duration: 3.953s, episode steps: 547, steps per second: 138, episode reward: 265.544, mean reward: 0.485 [-17.503, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.121 [-0.810, 1.403], loss: 6.194379, mae: 43.470654, mean_q: 57.503174
  117164/1100000: episode: 214, duration: 0.968s, episode steps: 142, steps per second: 147, episode reward: 3.019, mean reward: 0.021 [-100.000, 17.346], mean action: 1.887 [0.000, 3.000], mean observation: -0.060 [-1.687, 1.416], loss: 4.369537, mae: 43.782013, mean_q: 57.906601
  118164/1100000: episode: 215, duration: 7.634s, episode steps: 1000, steps per second: 131, episode reward: -26.662, mean reward: -0.027 [-21.929, 24.628], mean action: 1.864 [0.000, 3.000], mean observation: -0.005 [-0.886, 1.390], loss: 5.347739, mae: 44.209099, mean_q: 58.179688
  119164/1100000: episode: 216, duration: 7.665s, episode steps: 1000, steps per second: 130, episode reward: 9.429, mean reward: 0.009 [-22.919, 15.291], mean action: 1.985 [0.000, 3.000], mean observation: 0.000 [-0.910, 1.396], loss: 9.186322, mae: 44.807037, mean_q: 58.879478
  119805/1100000: episode: 217, duration: 4.658s, episode steps: 641, steps per second: 138, episode reward: 180.252, mean reward: 0.281 [-22.138, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.176 [-0.661, 1.496], loss: 7.914236, mae: 45.415222, mean_q: 59.618481
  120530/1100000: episode: 218, duration: 5.256s, episode steps: 725, steps per second: 138, episode reward: 206.857, mean reward: 0.285 [-19.962, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.028 [-0.647, 1.473], loss: 5.367928, mae: 45.478409, mean_q: 60.104408
  120920/1100000: episode: 219, duration: 2.804s, episode steps: 390, steps per second: 139, episode reward: -80.775, mean reward: -0.207 [-100.000, 30.982], mean action: 1.692 [0.000, 3.000], mean observation: -0.028 [-1.089, 1.470], loss: 9.650419, mae: 45.764423, mean_q: 60.490601
  121330/1100000: episode: 220, duration: 2.793s, episode steps: 410, steps per second: 147, episode reward: 266.535, mean reward: 0.650 [-20.715, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.183 [-0.886, 1.527], loss: 6.641046, mae: 46.230080, mean_q: 61.239983
  122330/1100000: episode: 221, duration: 7.499s, episode steps: 1000, steps per second: 133, episode reward: 72.305, mean reward: 0.072 [-23.847, 23.431], mean action: 1.559 [0.000, 3.000], mean observation: 0.250 [-0.987, 1.386], loss: 6.508684, mae: 46.570454, mean_q: 61.586868
  123076/1100000: episode: 222, duration: 5.635s, episode steps: 746, steps per second: 132, episode reward: 233.387, mean reward: 0.313 [-19.651, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.052 [-0.873, 1.388], loss: 6.998391, mae: 46.609772, mean_q: 61.521584
  123423/1100000: episode: 223, duration: 2.330s, episode steps: 347, steps per second: 149, episode reward: 270.954, mean reward: 0.781 [-13.123, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.029 [-0.702, 1.521], loss: 7.007062, mae: 46.874382, mean_q: 61.988670
  123809/1100000: episode: 224, duration: 2.657s, episode steps: 386, steps per second: 145, episode reward: 199.208, mean reward: 0.516 [-3.552, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.148 [-0.439, 1.408], loss: 4.536073, mae: 46.971397, mean_q: 62.342648
  123958/1100000: episode: 225, duration: 0.998s, episode steps: 149, steps per second: 149, episode reward: 31.342, mean reward: 0.210 [-100.000, 13.332], mean action: 1.805 [0.000, 3.000], mean observation: 0.028 [-0.769, 1.691], loss: 9.408531, mae: 47.658463, mean_q: 62.904022
  124701/1100000: episode: 226, duration: 5.424s, episode steps: 743, steps per second: 137, episode reward: 183.067, mean reward: 0.246 [-21.079, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.186 [-0.573, 1.473], loss: 7.327531, mae: 47.034672, mean_q: 62.201000
  124894/1100000: episode: 227, duration: 1.301s, episode steps: 193, steps per second: 148, episode reward: 82.916, mean reward: 0.430 [-100.000, 16.528], mean action: 1.746 [0.000, 3.000], mean observation: 0.009 [-1.417, 1.486], loss: 7.629044, mae: 47.316315, mean_q: 62.271271
  125313/1100000: episode: 228, duration: 2.879s, episode steps: 419, steps per second: 146, episode reward: 243.540, mean reward: 0.581 [-20.340, 100.000], mean action: 0.864 [0.000, 3.000], mean observation: 0.114 [-0.891, 1.410], loss: 5.619212, mae: 47.107780, mean_q: 62.262936
  125614/1100000: episode: 229, duration: 2.057s, episode steps: 301, steps per second: 146, episode reward: 231.331, mean reward: 0.769 [-9.633, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.195 [-0.866, 1.390], loss: 8.342910, mae: 47.268970, mean_q: 62.266827
  125884/1100000: episode: 230, duration: 1.837s, episode steps: 270, steps per second: 147, episode reward: 235.158, mean reward: 0.871 [-8.499, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.074 [-0.858, 1.396], loss: 5.694617, mae: 47.574356, mean_q: 62.837994
  126154/1100000: episode: 231, duration: 1.847s, episode steps: 270, steps per second: 146, episode reward: 266.967, mean reward: 0.989 [-9.648, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.082 [-0.923, 1.394], loss: 5.242553, mae: 47.718121, mean_q: 62.951244
  127154/1100000: episode: 232, duration: 7.292s, episode steps: 1000, steps per second: 137, episode reward: 89.155, mean reward: 0.089 [-18.981, 23.852], mean action: 1.294 [0.000, 3.000], mean observation: 0.256 [-0.673, 1.393], loss: 5.131845, mae: 47.533981, mean_q: 62.832069
  128154/1100000: episode: 233, duration: 7.494s, episode steps: 1000, steps per second: 133, episode reward: 12.732, mean reward: 0.013 [-20.281, 18.428], mean action: 1.639 [0.000, 3.000], mean observation: 0.211 [-0.566, 1.499], loss: 6.077178, mae: 46.763046, mean_q: 61.899014
  128642/1100000: episode: 234, duration: 3.353s, episode steps: 488, steps per second: 146, episode reward: 202.679, mean reward: 0.415 [-17.868, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.205 [-0.829, 1.397], loss: 8.009507, mae: 46.633717, mean_q: 61.550140
  128954/1100000: episode: 235, duration: 2.253s, episode steps: 312, steps per second: 138, episode reward: -41.475, mean reward: -0.133 [-100.000, 8.198], mean action: 1.734 [0.000, 3.000], mean observation: 0.155 [-1.322, 1.400], loss: 3.708560, mae: 46.295376, mean_q: 60.965496
  129268/1100000: episode: 236, duration: 2.146s, episode steps: 314, steps per second: 146, episode reward: 209.818, mean reward: 0.668 [-11.090, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.155 [-0.612, 1.405], loss: 7.577877, mae: 46.545551, mean_q: 61.428097
  130192/1100000: episode: 237, duration: 6.588s, episode steps: 924, steps per second: 140, episode reward: 164.586, mean reward: 0.178 [-20.168, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.185 [-0.891, 1.517], loss: 6.315303, mae: 46.763756, mean_q: 61.791771
  130600/1100000: episode: 238, duration: 2.843s, episode steps: 408, steps per second: 144, episode reward: 242.042, mean reward: 0.593 [-17.952, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.163 [-0.594, 1.428], loss: 6.171434, mae: 47.230099, mean_q: 62.385555
  131212/1100000: episode: 239, duration: 4.308s, episode steps: 612, steps per second: 142, episode reward: 202.367, mean reward: 0.331 [-18.418, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.237 [-0.816, 1.405], loss: 5.991028, mae: 47.053799, mean_q: 62.173191
  131806/1100000: episode: 240, duration: 4.306s, episode steps: 594, steps per second: 138, episode reward: 156.305, mean reward: 0.263 [-23.296, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.144 [-0.764, 1.412], loss: 7.232558, mae: 46.864033, mean_q: 62.031513
  132148/1100000: episode: 241, duration: 2.381s, episode steps: 342, steps per second: 144, episode reward: 226.935, mean reward: 0.664 [-18.262, 100.000], mean action: 2.187 [0.000, 3.000], mean observation: 0.161 [-0.799, 1.444], loss: 6.333602, mae: 47.156815, mean_q: 62.186825
  132764/1100000: episode: 242, duration: 4.411s, episode steps: 616, steps per second: 140, episode reward: 180.891, mean reward: 0.294 [-19.744, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.126 [-0.812, 1.405], loss: 6.986975, mae: 47.430035, mean_q: 62.754795
  133157/1100000: episode: 243, duration: 2.652s, episode steps: 393, steps per second: 148, episode reward: 248.788, mean reward: 0.633 [-19.878, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.223 [-1.044, 1.442], loss: 7.230539, mae: 47.281551, mean_q: 62.687588
  133473/1100000: episode: 244, duration: 2.171s, episode steps: 316, steps per second: 146, episode reward: 230.335, mean reward: 0.729 [-18.914, 100.000], mean action: 2.234 [0.000, 3.000], mean observation: 0.052 [-0.676, 1.391], loss: 5.483329, mae: 47.271404, mean_q: 62.809498
  133755/1100000: episode: 245, duration: 1.930s, episode steps: 282, steps per second: 146, episode reward: 245.707, mean reward: 0.871 [-9.702, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.079 [-0.771, 1.403], loss: 4.126908, mae: 47.482571, mean_q: 63.041237
  134755/1100000: episode: 246, duration: 7.923s, episode steps: 1000, steps per second: 126, episode reward: -84.716, mean reward: -0.085 [-19.714, 20.299], mean action: 1.620 [0.000, 3.000], mean observation: -0.024 [-0.824, 1.388], loss: 8.477140, mae: 47.534798, mean_q: 63.118053
  135383/1100000: episode: 247, duration: 4.296s, episode steps: 628, steps per second: 146, episode reward: 228.946, mean reward: 0.365 [-17.950, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.219 [-0.793, 1.396], loss: 4.340082, mae: 47.960190, mean_q: 63.634689
  135897/1100000: episode: 248, duration: 3.580s, episode steps: 514, steps per second: 144, episode reward: 201.248, mean reward: 0.392 [-13.270, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: -0.045 [-0.869, 1.404], loss: 6.381930, mae: 48.176891, mean_q: 63.616669
  136006/1100000: episode: 249, duration: 0.734s, episode steps: 109, steps per second: 149, episode reward: -169.814, mean reward: -1.558 [-100.000, 2.388], mean action: 1.716 [0.000, 3.000], mean observation: 0.225 [-0.735, 1.389], loss: 7.953519, mae: 48.632175, mean_q: 64.386215
  136278/1100000: episode: 250, duration: 1.837s, episode steps: 272, steps per second: 148, episode reward: 251.686, mean reward: 0.925 [-8.450, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.033 [-0.869, 1.492], loss: 7.375390, mae: 48.079060, mean_q: 63.587330
  136473/1100000: episode: 251, duration: 1.300s, episode steps: 195, steps per second: 150, episode reward: -33.224, mean reward: -0.170 [-100.000, 12.228], mean action: 1.441 [0.000, 3.000], mean observation: 0.123 [-1.321, 1.513], loss: 3.992919, mae: 48.101311, mean_q: 63.734333
  136823/1100000: episode: 252, duration: 2.379s, episode steps: 350, steps per second: 147, episode reward: 219.941, mean reward: 0.628 [-18.957, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.205 [-0.826, 1.438], loss: 4.123115, mae: 48.321243, mean_q: 64.041649
  137202/1100000: episode: 253, duration: 2.640s, episode steps: 379, steps per second: 144, episode reward: 214.393, mean reward: 0.566 [-9.820, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.198 [-0.662, 1.442], loss: 6.432542, mae: 48.437840, mean_q: 64.125114
  138202/1100000: episode: 254, duration: 7.578s, episode steps: 1000, steps per second: 132, episode reward: 120.686, mean reward: 0.121 [-20.526, 23.077], mean action: 1.133 [0.000, 3.000], mean observation: 0.054 [-0.600, 1.408], loss: 6.185944, mae: 48.525749, mean_q: 64.345322
  139009/1100000: episode: 255, duration: 5.605s, episode steps: 807, steps per second: 144, episode reward: 284.853, mean reward: 0.353 [-20.165, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.137 [-0.929, 1.409], loss: 5.412275, mae: 48.713318, mean_q: 64.690582
  139324/1100000: episode: 256, duration: 2.176s, episode steps: 315, steps per second: 145, episode reward: 270.561, mean reward: 0.859 [-9.012, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.108 [-0.747, 1.401], loss: 7.953435, mae: 48.526585, mean_q: 64.515694
  139700/1100000: episode: 257, duration: 2.584s, episode steps: 376, steps per second: 146, episode reward: 310.793, mean reward: 0.827 [-17.474, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.093 [-1.031, 1.456], loss: 7.178766, mae: 48.730156, mean_q: 64.657494
  140132/1100000: episode: 258, duration: 3.024s, episode steps: 432, steps per second: 143, episode reward: 159.720, mean reward: 0.370 [-11.419, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.133 [-1.469, 1.405], loss: 5.503037, mae: 48.835625, mean_q: 64.945152
  141132/1100000: episode: 259, duration: 7.068s, episode steps: 1000, steps per second: 141, episode reward: 76.313, mean reward: 0.076 [-20.881, 22.643], mean action: 1.443 [0.000, 3.000], mean observation: 0.105 [-0.878, 1.412], loss: 7.429422, mae: 48.679577, mean_q: 64.455368
  141417/1100000: episode: 260, duration: 1.953s, episode steps: 285, steps per second: 146, episode reward: 253.600, mean reward: 0.890 [-8.237, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.060 [-0.964, 1.435], loss: 5.894642, mae: 48.388599, mean_q: 64.361809
  141716/1100000: episode: 261, duration: 2.031s, episode steps: 299, steps per second: 147, episode reward: 289.995, mean reward: 0.970 [-19.329, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.095 [-0.641, 1.450], loss: 5.468462, mae: 48.060513, mean_q: 63.878384
  141858/1100000: episode: 262, duration: 0.948s, episode steps: 142, steps per second: 150, episode reward: 15.402, mean reward: 0.108 [-100.000, 19.974], mean action: 1.803 [0.000, 3.000], mean observation: -0.088 [-2.420, 1.443], loss: 7.572620, mae: 48.512016, mean_q: 64.534866
  142027/1100000: episode: 263, duration: 1.124s, episode steps: 169, steps per second: 150, episode reward: 278.730, mean reward: 1.649 [-12.963, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.045 [-1.002, 1.418], loss: 4.670616, mae: 47.931133, mean_q: 63.694378
  142376/1100000: episode: 264, duration: 2.385s, episode steps: 349, steps per second: 146, episode reward: 253.473, mean reward: 0.726 [-17.112, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.051 [-0.867, 1.441], loss: 7.071340, mae: 48.645775, mean_q: 64.603172
  142583/1100000: episode: 265, duration: 1.397s, episode steps: 207, steps per second: 148, episode reward: 255.193, mean reward: 1.233 [-10.256, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.036 [-0.839, 1.391], loss: 5.033848, mae: 48.398117, mean_q: 64.281792
  143100/1100000: episode: 266, duration: 3.646s, episode steps: 517, steps per second: 142, episode reward: -274.506, mean reward: -0.531 [-100.000, 13.180], mean action: 1.497 [0.000, 3.000], mean observation: 0.100 [-0.768, 1.445], loss: 4.369876, mae: 48.563179, mean_q: 64.639648
  143504/1100000: episode: 267, duration: 2.856s, episode steps: 404, steps per second: 141, episode reward: 243.997, mean reward: 0.604 [-17.503, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.238 [-0.581, 1.490], loss: 7.594380, mae: 48.732258, mean_q: 64.645729
  144347/1100000: episode: 268, duration: 6.440s, episode steps: 843, steps per second: 131, episode reward: -249.538, mean reward: -0.296 [-100.000, 25.382], mean action: 1.279 [0.000, 3.000], mean observation: 0.144 [-0.623, 1.780], loss: 5.821713, mae: 48.900486, mean_q: 65.133736
  144755/1100000: episode: 269, duration: 2.818s, episode steps: 408, steps per second: 145, episode reward: 271.428, mean reward: 0.665 [-10.882, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.076 [-0.718, 1.422], loss: 6.770830, mae: 48.425457, mean_q: 64.416580
  145171/1100000: episode: 270, duration: 2.935s, episode steps: 416, steps per second: 142, episode reward: 272.340, mean reward: 0.655 [-18.412, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.047 [-1.290, 1.390], loss: 6.762324, mae: 48.656082, mean_q: 64.609833
  145513/1100000: episode: 271, duration: 2.341s, episode steps: 342, steps per second: 146, episode reward: -208.395, mean reward: -0.609 [-100.000, 22.929], mean action: 1.301 [0.000, 3.000], mean observation: 0.085 [-1.551, 1.497], loss: 5.345272, mae: 48.684063, mean_q: 64.943840
  145863/1100000: episode: 272, duration: 2.478s, episode steps: 350, steps per second: 141, episode reward: 293.031, mean reward: 0.837 [-9.035, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.088 [-0.691, 1.395], loss: 6.139495, mae: 48.565144, mean_q: 64.813553
  146115/1100000: episode: 273, duration: 1.711s, episode steps: 252, steps per second: 147, episode reward: 273.480, mean reward: 1.085 [-12.081, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.084 [-0.902, 1.388], loss: 4.539715, mae: 48.772552, mean_q: 64.564034
  146372/1100000: episode: 274, duration: 1.752s, episode steps: 257, steps per second: 147, episode reward: 243.152, mean reward: 0.946 [-7.424, 100.000], mean action: 1.774 [0.000, 3.000], mean observation: 0.042 [-1.098, 1.412], loss: 4.802510, mae: 48.595005, mean_q: 64.756004
  146874/1100000: episode: 275, duration: 3.502s, episode steps: 502, steps per second: 143, episode reward: 276.353, mean reward: 0.551 [-17.819, 100.000], mean action: 0.699 [0.000, 3.000], mean observation: 0.129 [-0.706, 1.486], loss: 6.546659, mae: 48.688801, mean_q: 64.696739
  147031/1100000: episode: 276, duration: 1.060s, episode steps: 157, steps per second: 148, episode reward: -50.236, mean reward: -0.320 [-100.000, 12.901], mean action: 1.656 [0.000, 3.000], mean observation: 0.043 [-0.668, 1.405], loss: 6.352292, mae: 49.076729, mean_q: 64.712654
  147497/1100000: episode: 277, duration: 3.172s, episode steps: 466, steps per second: 147, episode reward: 250.639, mean reward: 0.538 [-10.188, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.030 [-0.600, 1.392], loss: 5.695327, mae: 48.786270, mean_q: 64.861649
  147823/1100000: episode: 278, duration: 2.226s, episode steps: 326, steps per second: 146, episode reward: 247.483, mean reward: 0.759 [-10.509, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.039 [-1.088, 1.423], loss: 6.362475, mae: 49.149071, mean_q: 65.197113
  147940/1100000: episode: 279, duration: 0.777s, episode steps: 117, steps per second: 151, episode reward: 38.257, mean reward: 0.327 [-100.000, 14.882], mean action: 1.812 [0.000, 3.000], mean observation: 0.049 [-0.832, 1.401], loss: 4.407411, mae: 49.475727, mean_q: 65.614899
  148505/1100000: episode: 280, duration: 4.186s, episode steps: 565, steps per second: 135, episode reward: 223.331, mean reward: 0.395 [-19.016, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: -0.028 [-0.600, 1.389], loss: 5.553706, mae: 49.127098, mean_q: 65.105377
  149505/1100000: episode: 281, duration: 7.793s, episode steps: 1000, steps per second: 128, episode reward: -18.656, mean reward: -0.019 [-22.065, 30.438], mean action: 1.459 [0.000, 3.000], mean observation: -0.032 [-0.922, 1.422], loss: 6.703197, mae: 49.158085, mean_q: 65.216499
  149667/1100000: episode: 282, duration: 1.087s, episode steps: 162, steps per second: 149, episode reward: -1157.005, mean reward: -7.142 [-100.000, 0.957], mean action: 1.944 [0.000, 3.000], mean observation: 0.591 [-2.074, 7.982], loss: 5.522175, mae: 49.308315, mean_q: 65.790817
  149804/1100000: episode: 283, duration: 0.913s, episode steps: 137, steps per second: 150, episode reward: -31.343, mean reward: -0.229 [-100.000, 13.719], mean action: 1.628 [0.000, 3.000], mean observation: -0.056 [-1.689, 1.457], loss: 10.385879, mae: 49.874989, mean_q: 65.920532
  149950/1100000: episode: 284, duration: 0.970s, episode steps: 146, steps per second: 150, episode reward: 39.313, mean reward: 0.269 [-100.000, 17.913], mean action: 1.801 [0.000, 3.000], mean observation: -0.082 [-1.858, 1.421], loss: 6.042388, mae: 49.324608, mean_q: 65.338554
  150680/1100000: episode: 285, duration: 5.126s, episode steps: 730, steps per second: 142, episode reward: 190.912, mean reward: 0.262 [-18.087, 100.000], mean action: 1.034 [0.000, 3.000], mean observation: 0.129 [-0.935, 1.390], loss: 5.582998, mae: 49.549881, mean_q: 65.715462
  150791/1100000: episode: 286, duration: 0.734s, episode steps: 111, steps per second: 151, episode reward: -188.398, mean reward: -1.697 [-100.000, 38.361], mean action: 1.288 [0.000, 3.000], mean observation: 0.068 [-1.113, 2.204], loss: 6.880636, mae: 48.952457, mean_q: 65.199310
  151010/1100000: episode: 287, duration: 1.481s, episode steps: 219, steps per second: 148, episode reward: 290.419, mean reward: 1.326 [-9.691, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.045 [-0.726, 1.425], loss: 5.968789, mae: 49.993134, mean_q: 66.582314
  151499/1100000: episode: 288, duration: 3.389s, episode steps: 489, steps per second: 144, episode reward: 219.356, mean reward: 0.449 [-19.911, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.198 [-0.872, 1.409], loss: 8.946858, mae: 49.665024, mean_q: 65.822487
  152148/1100000: episode: 289, duration: 4.609s, episode steps: 649, steps per second: 141, episode reward: 203.433, mean reward: 0.313 [-19.583, 100.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.145 [-0.634, 1.438], loss: 6.650457, mae: 49.409386, mean_q: 65.502213
  152255/1100000: episode: 290, duration: 0.723s, episode steps: 107, steps per second: 148, episode reward: -10.207, mean reward: -0.095 [-100.000, 27.553], mean action: 1.925 [0.000, 3.000], mean observation: -0.063 [-1.306, 1.387], loss: 6.591076, mae: 49.502911, mean_q: 65.643173
  152659/1100000: episode: 291, duration: 2.805s, episode steps: 404, steps per second: 144, episode reward: 243.355, mean reward: 0.602 [-18.724, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.208 [-0.898, 1.465], loss: 8.910131, mae: 49.725021, mean_q: 65.698433
  153028/1100000: episode: 292, duration: 2.533s, episode steps: 369, steps per second: 146, episode reward: 231.627, mean reward: 0.628 [-11.051, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.163 [-0.569, 1.398], loss: 7.378695, mae: 49.524078, mean_q: 65.607674
  153521/1100000: episode: 293, duration: 3.382s, episode steps: 493, steps per second: 146, episode reward: 243.692, mean reward: 0.494 [-24.288, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.220 [-1.098, 1.389], loss: 5.786103, mae: 49.566017, mean_q: 65.732643
  153624/1100000: episode: 294, duration: 0.696s, episode steps: 103, steps per second: 148, episode reward: 53.649, mean reward: 0.521 [-100.000, 13.525], mean action: 1.913 [0.000, 3.000], mean observation: -0.020 [-0.943, 1.385], loss: 11.304541, mae: 49.856541, mean_q: 66.761307
  153753/1100000: episode: 295, duration: 0.859s, episode steps: 129, steps per second: 150, episode reward: -144.137, mean reward: -1.117 [-100.000, 29.966], mean action: 1.798 [0.000, 3.000], mean observation: 0.105 [-0.899, 1.623], loss: 5.476633, mae: 49.818314, mean_q: 66.375458
  154379/1100000: episode: 296, duration: 4.377s, episode steps: 626, steps per second: 143, episode reward: 236.954, mean reward: 0.379 [-18.487, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.237 [-0.597, 1.409], loss: 7.297304, mae: 49.766594, mean_q: 65.956718
  154781/1100000: episode: 297, duration: 2.856s, episode steps: 402, steps per second: 141, episode reward: 266.898, mean reward: 0.664 [-20.415, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.203 [-1.059, 1.388], loss: 5.441257, mae: 49.911255, mean_q: 66.406036
  155763/1100000: episode: 298, duration: 7.118s, episode steps: 982, steps per second: 138, episode reward: 201.041, mean reward: 0.205 [-18.953, 100.000], mean action: 1.933 [0.000, 3.000], mean observation: 0.073 [-0.726, 1.510], loss: 8.370433, mae: 49.981987, mean_q: 66.521286
  156458/1100000: episode: 299, duration: 5.039s, episode steps: 695, steps per second: 138, episode reward: 158.456, mean reward: 0.228 [-13.767, 100.000], mean action: 1.694 [0.000, 3.000], mean observation: -0.005 [-0.711, 1.451], loss: 8.026632, mae: 49.864941, mean_q: 66.488945
  156946/1100000: episode: 300, duration: 3.378s, episode steps: 488, steps per second: 144, episode reward: 213.493, mean reward: 0.437 [-19.544, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.020 [-0.600, 1.410], loss: 7.033560, mae: 50.028610, mean_q: 66.654808
  157435/1100000: episode: 301, duration: 3.462s, episode steps: 489, steps per second: 141, episode reward: 250.944, mean reward: 0.513 [-8.404, 100.000], mean action: 1.697 [0.000, 3.000], mean observation: 0.063 [-0.865, 1.393], loss: 7.019557, mae: 50.277939, mean_q: 66.865585
  157783/1100000: episode: 302, duration: 2.359s, episode steps: 348, steps per second: 148, episode reward: 283.347, mean reward: 0.814 [-7.078, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.140 [-0.802, 1.446], loss: 7.955651, mae: 50.082638, mean_q: 66.320465
  157884/1100000: episode: 303, duration: 0.674s, episode steps: 101, steps per second: 150, episode reward: 5.079, mean reward: 0.050 [-100.000, 18.109], mean action: 2.000 [0.000, 3.000], mean observation: -0.039 [-0.815, 1.831], loss: 4.030482, mae: 50.557129, mean_q: 67.002853
  158194/1100000: episode: 304, duration: 2.120s, episode steps: 310, steps per second: 146, episode reward: 277.618, mean reward: 0.896 [-11.579, 100.000], mean action: 1.677 [0.000, 3.000], mean observation: 0.069 [-0.964, 1.408], loss: 4.982407, mae: 50.159248, mean_q: 66.627052
  158472/1100000: episode: 305, duration: 1.890s, episode steps: 278, steps per second: 147, episode reward: -139.040, mean reward: -0.500 [-100.000, 5.071], mean action: 1.777 [0.000, 3.000], mean observation: 0.060 [-0.778, 1.464], loss: 9.738056, mae: 50.037979, mean_q: 66.356247
  158864/1100000: episode: 306, duration: 2.668s, episode steps: 392, steps per second: 147, episode reward: 294.653, mean reward: 0.752 [-9.814, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.100 [-0.743, 1.401], loss: 4.725097, mae: 50.225754, mean_q: 66.598000
  158953/1100000: episode: 307, duration: 0.592s, episode steps: 89, steps per second: 150, episode reward: -74.546, mean reward: -0.838 [-100.000, 11.952], mean action: 1.708 [0.000, 3.000], mean observation: -0.010 [-1.019, 1.414], loss: 4.878961, mae: 50.505272, mean_q: 67.168625
  159953/1100000: episode: 308, duration: 7.719s, episode steps: 1000, steps per second: 130, episode reward: 77.937, mean reward: 0.078 [-13.018, 22.907], mean action: 1.594 [0.000, 3.000], mean observation: 0.011 [-0.839, 1.385], loss: 7.828315, mae: 49.877090, mean_q: 66.229019
  160942/1100000: episode: 309, duration: 7.604s, episode steps: 989, steps per second: 130, episode reward: 59.592, mean reward: 0.060 [-23.750, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.100 [-0.731, 1.422], loss: 6.119306, mae: 49.298653, mean_q: 65.585327
  161430/1100000: episode: 310, duration: 3.458s, episode steps: 488, steps per second: 141, episode reward: 249.192, mean reward: 0.511 [-19.624, 100.000], mean action: 0.869 [0.000, 3.000], mean observation: 0.234 [-0.967, 1.399], loss: 7.305535, mae: 49.350864, mean_q: 65.744186
  161689/1100000: episode: 311, duration: 1.741s, episode steps: 259, steps per second: 149, episode reward: 257.823, mean reward: 0.995 [-13.454, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.153 [-0.836, 1.420], loss: 5.292258, mae: 49.364960, mean_q: 65.702919
  162045/1100000: episode: 312, duration: 2.410s, episode steps: 356, steps per second: 148, episode reward: 189.019, mean reward: 0.531 [-10.552, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.188 [-0.546, 1.423], loss: 6.934636, mae: 48.950123, mean_q: 65.323242
  162456/1100000: episode: 313, duration: 2.835s, episode steps: 411, steps per second: 145, episode reward: 246.282, mean reward: 0.599 [-23.320, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.122 [-0.838, 1.388], loss: 4.287672, mae: 49.093597, mean_q: 65.368004
  162801/1100000: episode: 314, duration: 2.449s, episode steps: 345, steps per second: 141, episode reward: 279.056, mean reward: 0.809 [-10.419, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: -0.025 [-1.146, 1.481], loss: 7.690408, mae: 49.178333, mean_q: 65.227684
  163237/1100000: episode: 315, duration: 3.124s, episode steps: 436, steps per second: 140, episode reward: -199.653, mean reward: -0.458 [-100.000, 13.849], mean action: 1.647 [0.000, 3.000], mean observation: 0.108 [-1.593, 1.402], loss: 4.980119, mae: 49.119160, mean_q: 65.339676
  163397/1100000: episode: 316, duration: 1.065s, episode steps: 160, steps per second: 150, episode reward: 48.277, mean reward: 0.302 [-100.000, 17.103], mean action: 1.850 [0.000, 3.000], mean observation: 0.097 [-0.869, 1.866], loss: 4.174648, mae: 49.096447, mean_q: 65.116333
  163933/1100000: episode: 317, duration: 3.901s, episode steps: 536, steps per second: 137, episode reward: 263.892, mean reward: 0.492 [-19.819, 100.000], mean action: 2.506 [0.000, 3.000], mean observation: 0.025 [-0.896, 1.392], loss: 4.472021, mae: 48.993202, mean_q: 65.209221
  164241/1100000: episode: 318, duration: 2.107s, episode steps: 308, steps per second: 146, episode reward: 218.907, mean reward: 0.711 [-6.045, 100.000], mean action: 1.588 [0.000, 3.000], mean observation: -0.045 [-0.799, 1.420], loss: 6.103696, mae: 49.402344, mean_q: 65.667511
  164488/1100000: episode: 319, duration: 1.672s, episode steps: 247, steps per second: 148, episode reward: -142.877, mean reward: -0.578 [-100.000, 5.292], mean action: 1.794 [0.000, 3.000], mean observation: 0.085 [-0.600, 1.477], loss: 4.307917, mae: 49.364075, mean_q: 65.648117
  164718/1100000: episode: 320, duration: 1.572s, episode steps: 230, steps per second: 146, episode reward: 246.200, mean reward: 1.070 [-18.309, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: 0.162 [-1.378, 1.389], loss: 8.094708, mae: 49.190964, mean_q: 65.337830
  165718/1100000: episode: 321, duration: 7.387s, episode steps: 1000, steps per second: 135, episode reward: 163.985, mean reward: 0.164 [-19.775, 36.687], mean action: 1.026 [0.000, 3.000], mean observation: 0.253 [-0.896, 1.440], loss: 5.226718, mae: 49.494598, mean_q: 65.931770
  166156/1100000: episode: 322, duration: 2.999s, episode steps: 438, steps per second: 146, episode reward: 253.142, mean reward: 0.578 [-19.858, 100.000], mean action: 0.836 [0.000, 3.000], mean observation: 0.151 [-0.796, 1.444], loss: 4.501204, mae: 49.556728, mean_q: 66.078888
  166575/1100000: episode: 323, duration: 2.923s, episode steps: 419, steps per second: 143, episode reward: 246.317, mean reward: 0.588 [-7.331, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: 0.174 [-0.891, 1.413], loss: 6.276260, mae: 49.613255, mean_q: 65.757286
  167283/1100000: episode: 324, duration: 5.083s, episode steps: 708, steps per second: 139, episode reward: 198.045, mean reward: 0.280 [-23.762, 100.000], mean action: 1.756 [0.000, 3.000], mean observation: 0.006 [-0.619, 1.401], loss: 5.987427, mae: 49.805393, mean_q: 66.062592
  168092/1100000: episode: 325, duration: 5.661s, episode steps: 809, steps per second: 143, episode reward: 214.428, mean reward: 0.265 [-18.721, 100.000], mean action: 0.555 [0.000, 3.000], mean observation: 0.172 [-0.789, 1.406], loss: 6.244389, mae: 50.003326, mean_q: 66.399193
  168494/1100000: episode: 326, duration: 2.695s, episode steps: 402, steps per second: 149, episode reward: 264.720, mean reward: 0.659 [-20.266, 100.000], mean action: 0.565 [0.000, 3.000], mean observation: 0.207 [-1.499, 1.388], loss: 7.361159, mae: 49.863270, mean_q: 66.200432
  168821/1100000: episode: 327, duration: 2.254s, episode steps: 327, steps per second: 145, episode reward: 242.515, mean reward: 0.742 [-15.053, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.181 [-1.463, 1.454], loss: 4.664686, mae: 50.412460, mean_q: 67.224983
  169218/1100000: episode: 328, duration: 2.740s, episode steps: 397, steps per second: 145, episode reward: 237.580, mean reward: 0.598 [-19.723, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: -0.001 [-0.710, 1.398], loss: 5.323112, mae: 50.277824, mean_q: 66.975227
  169569/1100000: episode: 329, duration: 2.400s, episode steps: 351, steps per second: 146, episode reward: 232.855, mean reward: 0.663 [-17.738, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.127 [-0.737, 1.415], loss: 5.120613, mae: 50.937912, mean_q: 67.681435
  170060/1100000: episode: 330, duration: 3.384s, episode steps: 491, steps per second: 145, episode reward: 250.542, mean reward: 0.510 [-19.030, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.223 [-1.000, 1.489], loss: 5.473591, mae: 50.436737, mean_q: 66.973312
  170826/1100000: episode: 331, duration: 5.275s, episode steps: 766, steps per second: 145, episode reward: -287.325, mean reward: -0.375 [-100.000, 23.316], mean action: 0.982 [0.000, 3.000], mean observation: 0.164 [-1.091, 1.913], loss: 7.637824, mae: 50.847157, mean_q: 67.627480
  171070/1100000: episode: 332, duration: 1.672s, episode steps: 244, steps per second: 146, episode reward: 249.645, mean reward: 1.023 [-19.924, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.135 [-1.272, 1.395], loss: 7.314993, mae: 51.373730, mean_q: 68.157944
  171348/1100000: episode: 333, duration: 1.945s, episode steps: 278, steps per second: 143, episode reward: 234.858, mean reward: 0.845 [-7.475, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: -0.001 [-0.808, 1.395], loss: 6.541221, mae: 51.536819, mean_q: 68.441841
  171683/1100000: episode: 334, duration: 2.260s, episode steps: 335, steps per second: 148, episode reward: 260.856, mean reward: 0.779 [-20.778, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.237 [-1.172, 1.525], loss: 6.575325, mae: 51.222919, mean_q: 68.050491
  172113/1100000: episode: 335, duration: 2.944s, episode steps: 430, steps per second: 146, episode reward: 255.461, mean reward: 0.594 [-10.380, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.129 [-1.274, 1.411], loss: 7.641128, mae: 51.352280, mean_q: 68.164917
  172522/1100000: episode: 336, duration: 2.843s, episode steps: 409, steps per second: 144, episode reward: 143.160, mean reward: 0.350 [-14.435, 100.000], mean action: 1.836 [0.000, 3.000], mean observation: 0.087 [-0.724, 1.509], loss: 7.801789, mae: 51.814716, mean_q: 68.614731
  172700/1100000: episode: 337, duration: 1.197s, episode steps: 178, steps per second: 149, episode reward: 302.533, mean reward: 1.700 [-9.265, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: 0.096 [-0.901, 1.387], loss: 5.414975, mae: 51.546436, mean_q: 68.480446
  173065/1100000: episode: 338, duration: 2.513s, episode steps: 365, steps per second: 145, episode reward: 316.730, mean reward: 0.868 [-17.505, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.078 [-1.340, 1.483], loss: 6.115390, mae: 51.676327, mean_q: 68.333572
  173276/1100000: episode: 339, duration: 1.416s, episode steps: 211, steps per second: 149, episode reward: -33.132, mean reward: -0.157 [-100.000, 11.617], mean action: 1.882 [0.000, 3.000], mean observation: -0.052 [-0.651, 1.413], loss: 9.321152, mae: 52.195137, mean_q: 68.699440
  173691/1100000: episode: 340, duration: 2.920s, episode steps: 415, steps per second: 142, episode reward: 194.486, mean reward: 0.469 [-13.189, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: -0.018 [-0.656, 1.421], loss: 7.840178, mae: 51.708618, mean_q: 68.780602
  173869/1100000: episode: 341, duration: 1.212s, episode steps: 178, steps per second: 147, episode reward: 245.655, mean reward: 1.380 [-6.139, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.077 [-0.868, 1.411], loss: 4.635888, mae: 51.865257, mean_q: 69.241859
  174002/1100000: episode: 342, duration: 0.887s, episode steps: 133, steps per second: 150, episode reward: -38.223, mean reward: -0.287 [-100.000, 11.622], mean action: 1.549 [0.000, 3.000], mean observation: 0.123 [-1.425, 1.409], loss: 6.581260, mae: 52.170151, mean_q: 69.162857
  174140/1100000: episode: 343, duration: 0.928s, episode steps: 138, steps per second: 149, episode reward: 54.723, mean reward: 0.397 [-100.000, 15.773], mean action: 1.833 [0.000, 3.000], mean observation: 0.080 [-1.036, 1.444], loss: 10.786286, mae: 52.299801, mean_q: 69.745766
  174626/1100000: episode: 344, duration: 3.393s, episode steps: 486, steps per second: 143, episode reward: 243.324, mean reward: 0.501 [-21.615, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.238 [-0.686, 1.406], loss: 6.452934, mae: 52.027561, mean_q: 69.224312
  175021/1100000: episode: 345, duration: 2.693s, episode steps: 395, steps per second: 147, episode reward: 303.136, mean reward: 0.767 [-17.808, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.144 [-0.668, 1.479], loss: 6.250092, mae: 51.995163, mean_q: 69.384583
  175824/1100000: episode: 346, duration: 5.551s, episode steps: 803, steps per second: 145, episode reward: 227.462, mean reward: 0.283 [-18.249, 100.000], mean action: 0.746 [0.000, 3.000], mean observation: 0.213 [-0.901, 1.400], loss: 6.181895, mae: 51.992237, mean_q: 69.276306
  176166/1100000: episode: 347, duration: 2.382s, episode steps: 342, steps per second: 144, episode reward: 190.636, mean reward: 0.557 [-12.725, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: -0.040 [-0.911, 1.421], loss: 6.078075, mae: 51.755386, mean_q: 68.920227
  176428/1100000: episode: 348, duration: 1.787s, episode steps: 262, steps per second: 147, episode reward: 259.342, mean reward: 0.990 [-17.715, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.114 [-0.884, 1.488], loss: 4.684799, mae: 52.105946, mean_q: 69.760735
  176668/1100000: episode: 349, duration: 1.616s, episode steps: 240, steps per second: 149, episode reward: 267.404, mean reward: 1.114 [-8.693, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.042 [-0.908, 1.424], loss: 6.583226, mae: 51.764450, mean_q: 69.367790
  177343/1100000: episode: 350, duration: 4.788s, episode steps: 675, steps per second: 141, episode reward: 226.082, mean reward: 0.335 [-18.970, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.078 [-0.703, 1.462], loss: 5.714513, mae: 52.050972, mean_q: 69.637070
  177443/1100000: episode: 351, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 0.793, mean reward: 0.008 [-100.000, 15.154], mean action: 1.890 [0.000, 3.000], mean observation: 0.123 [-0.927, 1.401], loss: 5.034439, mae: 51.950371, mean_q: 69.487328
  177960/1100000: episode: 352, duration: 3.573s, episode steps: 517, steps per second: 145, episode reward: 35.155, mean reward: 0.068 [-100.000, 17.784], mean action: 1.168 [0.000, 3.000], mean observation: 0.170 [-0.681, 1.418], loss: 8.868732, mae: 51.853687, mean_q: 69.396805
  178172/1100000: episode: 353, duration: 1.425s, episode steps: 212, steps per second: 149, episode reward: -126.546, mean reward: -0.597 [-100.000, 19.104], mean action: 1.608 [0.000, 3.000], mean observation: 0.098 [-1.668, 1.515], loss: 6.101775, mae: 52.041622, mean_q: 69.924850
  178299/1100000: episode: 354, duration: 0.854s, episode steps: 127, steps per second: 149, episode reward: 6.234, mean reward: 0.049 [-100.000, 14.166], mean action: 1.622 [0.000, 3.000], mean observation: 0.108 [-0.846, 1.404], loss: 9.356069, mae: 51.713387, mean_q: 69.377007
  179278/1100000: episode: 355, duration: 8.150s, episode steps: 979, steps per second: 120, episode reward: 126.444, mean reward: 0.129 [-14.518, 100.000], mean action: 1.638 [0.000, 3.000], mean observation: 0.010 [-0.705, 1.480], loss: 6.705810, mae: 52.197300, mean_q: 69.757858
  179385/1100000: episode: 356, duration: 0.713s, episode steps: 107, steps per second: 150, episode reward: 27.457, mean reward: 0.257 [-100.000, 12.266], mean action: 1.701 [0.000, 3.000], mean observation: 0.043 [-0.992, 1.639], loss: 10.700403, mae: 51.211304, mean_q: 68.646698
  179815/1100000: episode: 357, duration: 3.054s, episode steps: 430, steps per second: 141, episode reward: 279.085, mean reward: 0.649 [-18.706, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.129 [-0.961, 1.415], loss: 7.258787, mae: 51.771999, mean_q: 69.321060
  180510/1100000: episode: 358, duration: 4.993s, episode steps: 695, steps per second: 139, episode reward: 127.130, mean reward: 0.183 [-11.612, 100.000], mean action: 1.812 [0.000, 3.000], mean observation: 0.093 [-0.538, 1.393], loss: 8.488105, mae: 51.875439, mean_q: 69.565369
  180686/1100000: episode: 359, duration: 1.183s, episode steps: 176, steps per second: 149, episode reward: -273.301, mean reward: -1.553 [-100.000, 12.258], mean action: 1.784 [0.000, 3.000], mean observation: 0.108 [-1.131, 2.210], loss: 4.628027, mae: 51.794067, mean_q: 69.257225
  180929/1100000: episode: 360, duration: 1.656s, episode steps: 243, steps per second: 147, episode reward: -177.218, mean reward: -0.729 [-100.000, 3.886], mean action: 1.930 [0.000, 3.000], mean observation: 0.055 [-0.665, 1.412], loss: 8.573200, mae: 52.234310, mean_q: 69.880302
  181171/1100000: episode: 361, duration: 1.626s, episode steps: 242, steps per second: 149, episode reward: -125.398, mean reward: -0.518 [-100.000, 8.308], mean action: 1.851 [0.000, 3.000], mean observation: 0.018 [-3.223, 1.449], loss: 3.825636, mae: 52.210503, mean_q: 70.083061
  181478/1100000: episode: 362, duration: 2.087s, episode steps: 307, steps per second: 147, episode reward: 284.675, mean reward: 0.927 [-17.988, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.105 [-0.816, 1.394], loss: 7.699492, mae: 52.240238, mean_q: 69.821213
  181944/1100000: episode: 363, duration: 3.258s, episode steps: 466, steps per second: 143, episode reward: 126.925, mean reward: 0.272 [-9.260, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.063 [-0.799, 1.425], loss: 8.902534, mae: 52.020840, mean_q: 69.508125
  182790/1100000: episode: 364, duration: 6.250s, episode steps: 846, steps per second: 135, episode reward: 205.696, mean reward: 0.243 [-19.549, 100.000], mean action: 1.992 [0.000, 3.000], mean observation: 0.104 [-1.182, 1.523], loss: 7.257287, mae: 51.883865, mean_q: 69.418503
  183189/1100000: episode: 365, duration: 2.766s, episode steps: 399, steps per second: 144, episode reward: 246.461, mean reward: 0.618 [-11.100, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.085 [-0.872, 1.407], loss: 10.411236, mae: 51.744503, mean_q: 69.088226
  183705/1100000: episode: 366, duration: 3.640s, episode steps: 516, steps per second: 142, episode reward: 167.467, mean reward: 0.325 [-15.920, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: -0.000 [-0.820, 1.391], loss: 6.123017, mae: 51.936001, mean_q: 69.523239
  184105/1100000: episode: 367, duration: 2.845s, episode steps: 400, steps per second: 141, episode reward: 203.388, mean reward: 0.508 [-18.708, 100.000], mean action: 1.810 [0.000, 3.000], mean observation: 0.053 [-0.681, 1.413], loss: 6.319473, mae: 52.024639, mean_q: 69.820717
  184627/1100000: episode: 368, duration: 3.802s, episode steps: 522, steps per second: 137, episode reward: 167.226, mean reward: 0.320 [-24.090, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: -0.018 [-0.918, 1.402], loss: 6.066531, mae: 51.848740, mean_q: 69.562988
  184898/1100000: episode: 369, duration: 1.874s, episode steps: 271, steps per second: 145, episode reward: 244.553, mean reward: 0.902 [-13.874, 100.000], mean action: 1.565 [0.000, 3.000], mean observation: 0.127 [-0.858, 1.393], loss: 5.901585, mae: 52.220104, mean_q: 69.909836
  185244/1100000: episode: 370, duration: 2.377s, episode steps: 346, steps per second: 146, episode reward: 222.386, mean reward: 0.643 [-10.419, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: -0.052 [-0.657, 1.459], loss: 8.543003, mae: 52.393593, mean_q: 70.353119
  185434/1100000: episode: 371, duration: 1.278s, episode steps: 190, steps per second: 149, episode reward: 299.505, mean reward: 1.576 [-8.918, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.183 [-0.762, 1.394], loss: 5.019183, mae: 52.457314, mean_q: 70.155533
  185666/1100000: episode: 372, duration: 1.575s, episode steps: 232, steps per second: 147, episode reward: 273.122, mean reward: 1.177 [-9.277, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.044 [-1.221, 1.456], loss: 4.638243, mae: 52.447262, mean_q: 70.297440
  186019/1100000: episode: 373, duration: 2.431s, episode steps: 353, steps per second: 145, episode reward: 244.162, mean reward: 0.692 [-9.964, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.105 [-1.367, 1.423], loss: 6.892073, mae: 52.762814, mean_q: 70.748543
  186381/1100000: episode: 374, duration: 2.477s, episode steps: 362, steps per second: 146, episode reward: 226.426, mean reward: 0.625 [-18.173, 100.000], mean action: 0.912 [0.000, 3.000], mean observation: 0.236 [-0.873, 1.408], loss: 5.851830, mae: 52.575062, mean_q: 70.417908
  186706/1100000: episode: 375, duration: 2.283s, episode steps: 325, steps per second: 142, episode reward: 199.357, mean reward: 0.613 [-19.742, 100.000], mean action: 2.098 [0.000, 3.000], mean observation: 0.107 [-0.709, 1.404], loss: 7.168386, mae: 52.847706, mean_q: 70.899757
  187067/1100000: episode: 376, duration: 2.480s, episode steps: 361, steps per second: 146, episode reward: 242.249, mean reward: 0.671 [-20.238, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.016 [-0.916, 1.394], loss: 5.009043, mae: 52.678471, mean_q: 70.588570
  187834/1100000: episode: 377, duration: 5.428s, episode steps: 767, steps per second: 141, episode reward: 210.106, mean reward: 0.274 [-17.583, 100.000], mean action: 0.658 [0.000, 3.000], mean observation: 0.031 [-0.782, 1.404], loss: 6.342677, mae: 52.941502, mean_q: 71.081268
  188314/1100000: episode: 378, duration: 3.463s, episode steps: 480, steps per second: 139, episode reward: 238.253, mean reward: 0.496 [-17.484, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.045 [-0.659, 1.412], loss: 6.571470, mae: 53.121799, mean_q: 71.255775
  188445/1100000: episode: 379, duration: 0.870s, episode steps: 131, steps per second: 151, episode reward: -29.655, mean reward: -0.226 [-100.000, 14.268], mean action: 1.557 [0.000, 3.000], mean observation: 0.083 [-0.745, 1.400], loss: 9.803996, mae: 52.660530, mean_q: 70.555283
  188765/1100000: episode: 380, duration: 2.193s, episode steps: 320, steps per second: 146, episode reward: 218.322, mean reward: 0.682 [-17.947, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.126 [-0.868, 1.481], loss: 5.731823, mae: 52.300316, mean_q: 70.130554
  189064/1100000: episode: 381, duration: 2.055s, episode steps: 299, steps per second: 146, episode reward: 268.161, mean reward: 0.897 [-12.299, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: 0.003 [-0.796, 1.497], loss: 4.289061, mae: 52.421829, mean_q: 70.261200
  189305/1100000: episode: 382, duration: 1.642s, episode steps: 241, steps per second: 147, episode reward: 258.396, mean reward: 1.072 [-9.387, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.105 [-0.800, 1.405], loss: 4.299352, mae: 52.752365, mean_q: 70.667290
  189571/1100000: episode: 383, duration: 1.823s, episode steps: 266, steps per second: 146, episode reward: 269.616, mean reward: 1.014 [-10.287, 100.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.078 [-0.782, 1.399], loss: 10.502131, mae: 52.834522, mean_q: 70.593941
  189893/1100000: episode: 384, duration: 2.181s, episode steps: 322, steps per second: 148, episode reward: 222.488, mean reward: 0.691 [-18.603, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: -0.035 [-0.727, 1.411], loss: 7.492623, mae: 52.982773, mean_q: 70.948997
  190274/1100000: episode: 385, duration: 2.634s, episode steps: 381, steps per second: 145, episode reward: 283.402, mean reward: 0.744 [-12.238, 100.000], mean action: 0.761 [0.000, 3.000], mean observation: 0.119 [-0.799, 1.513], loss: 6.431829, mae: 53.105827, mean_q: 71.211838
  190448/1100000: episode: 386, duration: 1.179s, episode steps: 174, steps per second: 148, episode reward: -30.735, mean reward: -0.177 [-100.000, 12.331], mean action: 1.948 [0.000, 3.000], mean observation: 0.068 [-0.902, 1.486], loss: 12.208600, mae: 53.128021, mean_q: 70.901657
  190727/1100000: episode: 387, duration: 1.892s, episode steps: 279, steps per second: 147, episode reward: 226.631, mean reward: 0.812 [-21.673, 100.000], mean action: 2.011 [0.000, 3.000], mean observation: 0.025 [-0.744, 1.423], loss: 4.830229, mae: 52.531342, mean_q: 70.388062
  191242/1100000: episode: 388, duration: 3.635s, episode steps: 515, steps per second: 142, episode reward: 219.897, mean reward: 0.427 [-18.976, 100.000], mean action: 2.190 [0.000, 3.000], mean observation: 0.058 [-0.744, 1.408], loss: 8.392418, mae: 52.727215, mean_q: 70.677925
  191626/1100000: episode: 389, duration: 2.658s, episode steps: 384, steps per second: 144, episode reward: 245.676, mean reward: 0.640 [-17.131, 100.000], mean action: 2.047 [0.000, 3.000], mean observation: 0.113 [-0.581, 1.433], loss: 7.246262, mae: 52.769489, mean_q: 70.705154
  191954/1100000: episode: 390, duration: 2.217s, episode steps: 328, steps per second: 148, episode reward: 280.327, mean reward: 0.855 [-19.308, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.071 [-0.766, 1.438], loss: 7.562684, mae: 52.472031, mean_q: 70.416107
  192678/1100000: episode: 391, duration: 5.236s, episode steps: 724, steps per second: 138, episode reward: 191.354, mean reward: 0.264 [-20.191, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.155 [-0.663, 1.478], loss: 8.963175, mae: 53.171852, mean_q: 71.254242
  193235/1100000: episode: 392, duration: 4.034s, episode steps: 557, steps per second: 138, episode reward: 185.565, mean reward: 0.333 [-23.168, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.005 [-0.895, 1.396], loss: 6.244398, mae: 53.353291, mean_q: 71.596207
  193550/1100000: episode: 393, duration: 2.177s, episode steps: 315, steps per second: 145, episode reward: 274.340, mean reward: 0.871 [-11.194, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.042 [-0.712, 1.404], loss: 5.093567, mae: 53.078957, mean_q: 71.282959
  193969/1100000: episode: 394, duration: 2.904s, episode steps: 419, steps per second: 144, episode reward: 237.008, mean reward: 0.566 [-20.501, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.189 [-0.702, 1.391], loss: 6.284607, mae: 53.439468, mean_q: 71.723457
  194384/1100000: episode: 395, duration: 2.834s, episode steps: 415, steps per second: 146, episode reward: 215.406, mean reward: 0.519 [-18.294, 100.000], mean action: 0.870 [0.000, 3.000], mean observation: 0.025 [-0.731, 1.409], loss: 4.509486, mae: 53.048191, mean_q: 71.250458
  194822/1100000: episode: 396, duration: 3.137s, episode steps: 438, steps per second: 140, episode reward: 196.030, mean reward: 0.448 [-9.502, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.130 [-0.752, 1.404], loss: 6.873253, mae: 52.890934, mean_q: 71.032722
  195198/1100000: episode: 397, duration: 2.557s, episode steps: 376, steps per second: 147, episode reward: 233.904, mean reward: 0.622 [-17.775, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.019 [-0.882, 1.385], loss: 6.062398, mae: 52.530289, mean_q: 70.267525
  195569/1100000: episode: 398, duration: 2.603s, episode steps: 371, steps per second: 143, episode reward: 234.605, mean reward: 0.632 [-10.864, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.126 [-0.729, 1.396], loss: 8.013705, mae: 52.381065, mean_q: 70.303574
  195795/1100000: episode: 399, duration: 1.522s, episode steps: 226, steps per second: 148, episode reward: -213.350, mean reward: -0.944 [-100.000, 10.018], mean action: 1.894 [0.000, 3.000], mean observation: 0.223 [-0.653, 2.058], loss: 5.743501, mae: 52.329491, mean_q: 70.163017
  196093/1100000: episode: 400, duration: 2.064s, episode steps: 298, steps per second: 144, episode reward: 301.662, mean reward: 1.012 [-18.072, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.115 [-0.684, 1.408], loss: 8.277689, mae: 52.725365, mean_q: 70.595955
  196347/1100000: episode: 401, duration: 1.745s, episode steps: 254, steps per second: 146, episode reward: 245.997, mean reward: 0.968 [-8.361, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: 0.070 [-0.693, 1.390], loss: 7.177177, mae: 52.953648, mean_q: 70.745819
  196781/1100000: episode: 402, duration: 3.033s, episode steps: 434, steps per second: 143, episode reward: 22.824, mean reward: 0.053 [-100.000, 17.953], mean action: 1.177 [0.000, 3.000], mean observation: 0.025 [-1.044, 1.471], loss: 8.732753, mae: 52.176826, mean_q: 70.017090
  196902/1100000: episode: 403, duration: 0.856s, episode steps: 121, steps per second: 141, episode reward: 54.262, mean reward: 0.448 [-100.000, 24.304], mean action: 1.793 [0.000, 3.000], mean observation: 0.155 [-0.877, 1.402], loss: 9.442665, mae: 51.702427, mean_q: 69.107788
  197414/1100000: episode: 404, duration: 3.648s, episode steps: 512, steps per second: 140, episode reward: 243.114, mean reward: 0.475 [-12.642, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.107 [-0.808, 1.419], loss: 6.427793, mae: 52.416557, mean_q: 70.287140
  197718/1100000: episode: 405, duration: 2.080s, episode steps: 304, steps per second: 146, episode reward: 239.844, mean reward: 0.789 [-13.649, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: -0.011 [-0.733, 1.452], loss: 9.371438, mae: 52.936150, mean_q: 70.707390
  198075/1100000: episode: 406, duration: 2.445s, episode steps: 357, steps per second: 146, episode reward: -43.588, mean reward: -0.122 [-100.000, 22.076], mean action: 1.311 [0.000, 3.000], mean observation: -0.014 [-0.795, 1.406], loss: 7.468535, mae: 52.261532, mean_q: 69.980209
  198200/1100000: episode: 407, duration: 0.834s, episode steps: 125, steps per second: 150, episode reward: -14.184, mean reward: -0.113 [-100.000, 8.172], mean action: 1.424 [0.000, 3.000], mean observation: 0.113 [-0.929, 1.456], loss: 10.799355, mae: 51.919456, mean_q: 69.511658
  198703/1100000: episode: 408, duration: 3.640s, episode steps: 503, steps per second: 138, episode reward: 209.621, mean reward: 0.417 [-18.102, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.012 [-0.812, 1.398], loss: 9.273063, mae: 52.215557, mean_q: 69.836014
  199335/1100000: episode: 409, duration: 4.496s, episode steps: 632, steps per second: 141, episode reward: 242.194, mean reward: 0.383 [-20.234, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.055 [-0.714, 1.525], loss: 8.528481, mae: 52.492386, mean_q: 70.367981
  199691/1100000: episode: 410, duration: 2.461s, episode steps: 356, steps per second: 145, episode reward: 192.564, mean reward: 0.541 [-18.802, 100.000], mean action: 2.362 [0.000, 3.000], mean observation: -0.012 [-0.810, 1.410], loss: 8.528254, mae: 51.673248, mean_q: 69.315636
  199817/1100000: episode: 411, duration: 0.837s, episode steps: 126, steps per second: 150, episode reward: 15.995, mean reward: 0.127 [-100.000, 11.983], mean action: 1.500 [0.000, 3.000], mean observation: -0.098 [-1.965, 1.440], loss: 5.284117, mae: 51.881466, mean_q: 69.367455
  199948/1100000: episode: 412, duration: 0.874s, episode steps: 131, steps per second: 150, episode reward: -297.079, mean reward: -2.268 [-100.000, 7.920], mean action: 1.626 [0.000, 3.000], mean observation: -0.121 [-4.213, 2.157], loss: 7.306062, mae: 52.323055, mean_q: 69.824005
  200149/1100000: episode: 413, duration: 1.365s, episode steps: 201, steps per second: 147, episode reward: 250.481, mean reward: 1.246 [-13.564, 100.000], mean action: 2.090 [0.000, 3.000], mean observation: 0.049 [-0.829, 1.408], loss: 6.231259, mae: 52.444847, mean_q: 70.216660
  200758/1100000: episode: 414, duration: 4.432s, episode steps: 609, steps per second: 137, episode reward: 143.212, mean reward: 0.235 [-15.084, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.112 [-1.230, 1.411], loss: 10.108437, mae: 52.696709, mean_q: 70.181572
  201012/1100000: episode: 415, duration: 1.737s, episode steps: 254, steps per second: 146, episode reward: 291.249, mean reward: 1.147 [-19.628, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.029 [-0.844, 1.390], loss: 7.340655, mae: 52.771778, mean_q: 70.408630
  201429/1100000: episode: 416, duration: 2.931s, episode steps: 417, steps per second: 142, episode reward: -237.791, mean reward: -0.570 [-100.000, 14.442], mean action: 1.453 [0.000, 3.000], mean observation: 0.167 [-0.931, 1.411], loss: 10.098559, mae: 52.099472, mean_q: 69.492447
  201948/1100000: episode: 417, duration: 3.540s, episode steps: 519, steps per second: 147, episode reward: 288.054, mean reward: 0.555 [-20.383, 100.000], mean action: 0.501 [0.000, 3.000], mean observation: 0.045 [-0.816, 1.394], loss: 21.458525, mae: 52.358376, mean_q: 69.841568
  202817/1100000: episode: 418, duration: 6.534s, episode steps: 869, steps per second: 133, episode reward: 224.534, mean reward: 0.258 [-22.492, 100.000], mean action: 2.334 [0.000, 3.000], mean observation: 0.137 [-0.857, 1.388], loss: 7.205173, mae: 52.016487, mean_q: 69.407867
  203205/1100000: episode: 419, duration: 2.706s, episode steps: 388, steps per second: 143, episode reward: 273.336, mean reward: 0.704 [-7.410, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.020 [-1.042, 1.453], loss: 9.691955, mae: 52.420155, mean_q: 70.098160
  203983/1100000: episode: 420, duration: 5.816s, episode steps: 778, steps per second: 134, episode reward: 166.288, mean reward: 0.214 [-17.533, 100.000], mean action: 1.829 [0.000, 3.000], mean observation: 0.164 [-0.729, 1.419], loss: 10.266329, mae: 52.573437, mean_q: 69.990868
  204323/1100000: episode: 421, duration: 2.337s, episode steps: 340, steps per second: 146, episode reward: 234.498, mean reward: 0.690 [-18.612, 100.000], mean action: 2.194 [0.000, 3.000], mean observation: -0.024 [-0.751, 1.410], loss: 8.737948, mae: 53.069504, mean_q: 70.921791
  204631/1100000: episode: 422, duration: 2.068s, episode steps: 308, steps per second: 149, episode reward: 247.919, mean reward: 0.805 [-18.508, 100.000], mean action: 0.805 [0.000, 3.000], mean observation: -0.007 [-0.867, 1.395], loss: 9.196454, mae: 52.355209, mean_q: 69.879829
  204806/1100000: episode: 423, duration: 1.181s, episode steps: 175, steps per second: 148, episode reward: 6.387, mean reward: 0.036 [-100.000, 11.511], mean action: 1.983 [0.000, 3.000], mean observation: 0.061 [-0.766, 1.408], loss: 6.706106, mae: 52.903622, mean_q: 70.660980
  205480/1100000: episode: 424, duration: 4.764s, episode steps: 674, steps per second: 141, episode reward: 261.071, mean reward: 0.387 [-18.170, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.048 [-0.817, 1.393], loss: 8.388123, mae: 52.921978, mean_q: 70.725845
  205575/1100000: episode: 425, duration: 0.635s, episode steps: 95, steps per second: 150, episode reward: -100.032, mean reward: -1.053 [-100.000, 15.156], mean action: 1.642 [0.000, 3.000], mean observation: -0.062 [-3.476, 1.420], loss: 4.297840, mae: 52.688637, mean_q: 70.449211
  205912/1100000: episode: 426, duration: 2.300s, episode steps: 337, steps per second: 146, episode reward: 305.624, mean reward: 0.907 [-17.872, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.127 [-0.690, 1.498], loss: 9.173700, mae: 53.553001, mean_q: 71.681458
  206136/1100000: episode: 427, duration: 1.507s, episode steps: 224, steps per second: 149, episode reward: 262.580, mean reward: 1.172 [-7.912, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.070 [-0.741, 1.407], loss: 5.320148, mae: 53.238792, mean_q: 71.161438
  206380/1100000: episode: 428, duration: 1.679s, episode steps: 244, steps per second: 145, episode reward: 261.317, mean reward: 1.071 [-13.232, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.143 [-0.804, 1.387], loss: 8.483526, mae: 53.400661, mean_q: 71.556801
  206628/1100000: episode: 429, duration: 1.671s, episode steps: 248, steps per second: 148, episode reward: 255.366, mean reward: 1.030 [-10.108, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: -0.078 [-1.167, 1.408], loss: 8.592063, mae: 53.032906, mean_q: 71.052193
  206867/1100000: episode: 430, duration: 1.634s, episode steps: 239, steps per second: 146, episode reward: 280.418, mean reward: 1.173 [-9.585, 100.000], mean action: 1.657 [0.000, 3.000], mean observation: 0.086 [-0.776, 1.389], loss: 8.333586, mae: 52.905697, mean_q: 70.702736
  207645/1100000: episode: 431, duration: 5.474s, episode steps: 778, steps per second: 142, episode reward: 241.573, mean reward: 0.311 [-21.001, 100.000], mean action: 1.871 [0.000, 3.000], mean observation: 0.046 [-0.876, 1.386], loss: 8.435555, mae: 53.071037, mean_q: 70.971420
  208007/1100000: episode: 432, duration: 2.516s, episode steps: 362, steps per second: 144, episode reward: 280.441, mean reward: 0.775 [-17.360, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.112 [-0.828, 1.409], loss: 8.919170, mae: 52.816189, mean_q: 70.708168
  208617/1100000: episode: 433, duration: 4.197s, episode steps: 610, steps per second: 145, episode reward: 255.215, mean reward: 0.418 [-19.609, 100.000], mean action: 0.869 [0.000, 3.000], mean observation: 0.252 [-0.849, 1.402], loss: 7.134447, mae: 52.486267, mean_q: 70.258080
  208869/1100000: episode: 434, duration: 1.705s, episode steps: 252, steps per second: 148, episode reward: 4.343, mean reward: 0.017 [-100.000, 10.263], mean action: 1.500 [0.000, 3.000], mean observation: -0.014 [-0.844, 1.386], loss: 8.050049, mae: 52.249371, mean_q: 70.105980
  209495/1100000: episode: 435, duration: 4.438s, episode steps: 626, steps per second: 141, episode reward: 200.939, mean reward: 0.321 [-21.105, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.270 [-0.859, 1.406], loss: 7.078750, mae: 52.660786, mean_q: 70.358246
  209828/1100000: episode: 436, duration: 2.237s, episode steps: 333, steps per second: 149, episode reward: 232.446, mean reward: 0.698 [-13.171, 100.000], mean action: 0.859 [0.000, 3.000], mean observation: 0.209 [-0.777, 1.392], loss: 9.564549, mae: 52.361984, mean_q: 69.913521
  210587/1100000: episode: 437, duration: 5.611s, episode steps: 759, steps per second: 135, episode reward: 208.654, mean reward: 0.275 [-21.082, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.208 [-0.669, 1.406], loss: 8.125589, mae: 52.525482, mean_q: 70.201080
  211402/1100000: episode: 438, duration: 5.960s, episode steps: 815, steps per second: 137, episode reward: 116.212, mean reward: 0.143 [-22.308, 100.000], mean action: 1.897 [0.000, 3.000], mean observation: 0.048 [-0.840, 1.405], loss: 7.201996, mae: 52.480503, mean_q: 70.055908
  211589/1100000: episode: 439, duration: 1.263s, episode steps: 187, steps per second: 148, episode reward: -139.026, mean reward: -0.743 [-100.000, 22.790], mean action: 1.738 [0.000, 3.000], mean observation: -0.098 [-1.576, 1.393], loss: 10.666480, mae: 52.250061, mean_q: 69.558899
  212005/1100000: episode: 440, duration: 2.894s, episode steps: 416, steps per second: 144, episode reward: 217.578, mean reward: 0.523 [-18.728, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: -0.001 [-0.700, 1.464], loss: 8.612340, mae: 52.383018, mean_q: 69.950142
  212312/1100000: episode: 441, duration: 2.128s, episode steps: 307, steps per second: 144, episode reward: 227.656, mean reward: 0.742 [-11.048, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: 0.181 [-0.651, 1.433], loss: 6.784880, mae: 52.089661, mean_q: 69.488762
  212575/1100000: episode: 442, duration: 1.792s, episode steps: 263, steps per second: 147, episode reward: 259.955, mean reward: 0.988 [-9.791, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: -0.067 [-0.671, 1.399], loss: 9.692563, mae: 52.140564, mean_q: 69.735977
  212739/1100000: episode: 443, duration: 1.098s, episode steps: 164, steps per second: 149, episode reward: 18.667, mean reward: 0.114 [-100.000, 12.329], mean action: 1.726 [0.000, 3.000], mean observation: -0.035 [-1.057, 1.400], loss: 7.624004, mae: 52.215191, mean_q: 69.610580
  213126/1100000: episode: 444, duration: 2.668s, episode steps: 387, steps per second: 145, episode reward: 160.274, mean reward: 0.414 [-17.573, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.118 [-0.965, 1.411], loss: 10.881461, mae: 52.337471, mean_q: 69.868462
  213546/1100000: episode: 445, duration: 2.991s, episode steps: 420, steps per second: 140, episode reward: 250.865, mean reward: 0.597 [-20.136, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: -0.002 [-0.664, 1.388], loss: 9.522194, mae: 52.195415, mean_q: 69.624626
  213902/1100000: episode: 446, duration: 2.537s, episode steps: 356, steps per second: 140, episode reward: 249.544, mean reward: 0.701 [-20.565, 100.000], mean action: 0.857 [0.000, 3.000], mean observation: 0.109 [-0.729, 1.404], loss: 7.663770, mae: 51.826996, mean_q: 69.390854
  214015/1100000: episode: 447, duration: 0.770s, episode steps: 113, steps per second: 147, episode reward: -137.938, mean reward: -1.221 [-100.000, 2.762], mean action: 2.062 [0.000, 3.000], mean observation: 0.018 [-1.087, 1.453], loss: 5.104374, mae: 52.042370, mean_q: 69.514786
  214118/1100000: episode: 448, duration: 0.711s, episode steps: 103, steps per second: 145, episode reward: -132.711, mean reward: -1.288 [-100.000, 4.350], mean action: 2.068 [0.000, 3.000], mean observation: -0.028 [-1.219, 1.389], loss: 8.340167, mae: 52.027847, mean_q: 69.623604
  214671/1100000: episode: 449, duration: 4.044s, episode steps: 553, steps per second: 137, episode reward: 122.674, mean reward: 0.222 [-21.905, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.085 [-0.908, 1.486], loss: 8.693011, mae: 52.238735, mean_q: 69.870110
  214801/1100000: episode: 450, duration: 0.865s, episode steps: 130, steps per second: 150, episode reward: 42.010, mean reward: 0.323 [-100.000, 12.289], mean action: 1.408 [0.000, 3.000], mean observation: 0.118 [-0.929, 1.506], loss: 5.467400, mae: 52.190723, mean_q: 70.195343
  215801/1100000: episode: 451, duration: 7.533s, episode steps: 1000, steps per second: 133, episode reward: 132.161, mean reward: 0.132 [-21.166, 21.966], mean action: 2.427 [0.000, 3.000], mean observation: 0.072 [-0.689, 1.407], loss: 7.936642, mae: 52.230301, mean_q: 70.017845
  215937/1100000: episode: 452, duration: 0.905s, episode steps: 136, steps per second: 150, episode reward: 30.581, mean reward: 0.225 [-100.000, 11.248], mean action: 1.809 [0.000, 3.000], mean observation: -0.029 [-0.769, 1.393], loss: 4.295749, mae: 51.917324, mean_q: 69.552055
  216460/1100000: episode: 453, duration: 3.720s, episode steps: 523, steps per second: 141, episode reward: 224.339, mean reward: 0.429 [-18.120, 100.000], mean action: 1.658 [0.000, 3.000], mean observation: 0.174 [-0.799, 1.451], loss: 10.868041, mae: 52.235600, mean_q: 70.041557
  216915/1100000: episode: 454, duration: 3.148s, episode steps: 455, steps per second: 145, episode reward: 188.207, mean reward: 0.414 [-13.125, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.166 [-1.197, 1.420], loss: 8.008000, mae: 51.799503, mean_q: 69.552910
  217221/1100000: episode: 455, duration: 2.071s, episode steps: 306, steps per second: 148, episode reward: 240.262, mean reward: 0.785 [-17.543, 100.000], mean action: 1.765 [0.000, 3.000], mean observation: 0.036 [-0.791, 1.495], loss: 10.456335, mae: 51.288326, mean_q: 68.981956
  217811/1100000: episode: 456, duration: 4.447s, episode steps: 590, steps per second: 133, episode reward: 161.641, mean reward: 0.274 [-19.646, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: -0.028 [-0.714, 1.402], loss: 7.459346, mae: 51.302605, mean_q: 68.851822
  218010/1100000: episode: 457, duration: 1.327s, episode steps: 199, steps per second: 150, episode reward: 261.401, mean reward: 1.314 [-2.835, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.096 [-0.919, 1.436], loss: 8.116014, mae: 51.103497, mean_q: 68.632423
  218397/1100000: episode: 458, duration: 2.683s, episode steps: 387, steps per second: 144, episode reward: 273.145, mean reward: 0.706 [-17.865, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.128 [-0.722, 1.521], loss: 10.755844, mae: 51.686455, mean_q: 69.557343
  218533/1100000: episode: 459, duration: 0.900s, episode steps: 136, steps per second: 151, episode reward: -113.557, mean reward: -0.835 [-100.000, 3.382], mean action: 1.382 [0.000, 3.000], mean observation: 0.147 [-0.600, 1.409], loss: 6.965104, mae: 51.423145, mean_q: 69.376999
  218902/1100000: episode: 460, duration: 2.556s, episode steps: 369, steps per second: 144, episode reward: -203.297, mean reward: -0.551 [-100.000, 9.455], mean action: 1.705 [0.000, 3.000], mean observation: -0.016 [-0.663, 1.506], loss: 8.445992, mae: 51.286217, mean_q: 68.850189
  219182/1100000: episode: 461, duration: 1.890s, episode steps: 280, steps per second: 148, episode reward: 283.783, mean reward: 1.014 [-18.595, 100.000], mean action: 0.896 [0.000, 3.000], mean observation: 0.082 [-0.917, 1.539], loss: 7.179267, mae: 51.733105, mean_q: 69.222473
  220044/1100000: episode: 462, duration: 6.182s, episode steps: 862, steps per second: 139, episode reward: 234.167, mean reward: 0.272 [-19.356, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.272 [-0.736, 1.445], loss: 10.340242, mae: 51.758961, mean_q: 69.335472
  220327/1100000: episode: 463, duration: 1.884s, episode steps: 283, steps per second: 150, episode reward: 227.813, mean reward: 0.805 [-22.367, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.188 [-0.916, 1.435], loss: 5.186809, mae: 51.157394, mean_q: 68.586266
  220566/1100000: episode: 464, duration: 1.603s, episode steps: 239, steps per second: 149, episode reward: 269.156, mean reward: 1.126 [-2.741, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: -0.049 [-0.686, 1.424], loss: 4.203147, mae: 51.053829, mean_q: 68.589256
  220963/1100000: episode: 465, duration: 2.768s, episode steps: 397, steps per second: 143, episode reward: 237.769, mean reward: 0.599 [-17.424, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: -0.005 [-0.600, 1.434], loss: 9.963701, mae: 51.469547, mean_q: 69.045029
  221111/1100000: episode: 466, duration: 0.998s, episode steps: 148, steps per second: 148, episode reward: 63.227, mean reward: 0.427 [-100.000, 16.319], mean action: 1.736 [0.000, 3.000], mean observation: -0.061 [-0.705, 1.385], loss: 5.313533, mae: 51.686695, mean_q: 69.556168
  221377/1100000: episode: 467, duration: 1.807s, episode steps: 266, steps per second: 147, episode reward: 294.135, mean reward: 1.106 [-10.885, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.097 [-0.571, 1.460], loss: 14.369497, mae: 51.673805, mean_q: 69.291870
  221835/1100000: episode: 468, duration: 3.262s, episode steps: 458, steps per second: 140, episode reward: 217.069, mean reward: 0.474 [-19.227, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.129 [-0.931, 1.402], loss: 10.392590, mae: 51.463985, mean_q: 69.212029
  222389/1100000: episode: 469, duration: 3.956s, episode steps: 554, steps per second: 140, episode reward: 249.882, mean reward: 0.451 [-18.212, 100.000], mean action: 1.857 [0.000, 3.000], mean observation: 0.145 [-0.784, 1.404], loss: 9.661233, mae: 51.115993, mean_q: 68.711754
  222589/1100000: episode: 470, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: 314.418, mean reward: 1.572 [-7.768, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: 0.071 [-0.871, 1.386], loss: 8.832980, mae: 50.720234, mean_q: 68.129608
  222942/1100000: episode: 471, duration: 2.386s, episode steps: 353, steps per second: 148, episode reward: 207.558, mean reward: 0.588 [-10.684, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.205 [-0.734, 1.418], loss: 5.780500, mae: 50.551456, mean_q: 67.976158
  223238/1100000: episode: 472, duration: 2.045s, episode steps: 296, steps per second: 145, episode reward: 4.558, mean reward: 0.015 [-100.000, 14.091], mean action: 1.828 [0.000, 3.000], mean observation: -0.010 [-1.005, 1.498], loss: 7.191463, mae: 50.957737, mean_q: 68.506493
  223702/1100000: episode: 473, duration: 3.240s, episode steps: 464, steps per second: 143, episode reward: -316.424, mean reward: -0.682 [-100.000, 20.450], mean action: 1.248 [0.000, 3.000], mean observation: -0.023 [-2.192, 1.433], loss: 8.378615, mae: 51.202080, mean_q: 68.826385
  224133/1100000: episode: 474, duration: 2.976s, episode steps: 431, steps per second: 145, episode reward: 259.545, mean reward: 0.602 [-17.838, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.222 [-0.804, 1.456], loss: 5.980771, mae: 50.884651, mean_q: 68.429367
  224660/1100000: episode: 475, duration: 3.643s, episode steps: 527, steps per second: 145, episode reward: 286.338, mean reward: 0.543 [-17.379, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.086 [-0.765, 1.498], loss: 7.375204, mae: 51.018463, mean_q: 68.724457
  224861/1100000: episode: 476, duration: 1.343s, episode steps: 201, steps per second: 150, episode reward: 253.988, mean reward: 1.264 [-3.358, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.238 [-1.181, 1.391], loss: 5.833266, mae: 51.036644, mean_q: 68.733376
  225185/1100000: episode: 477, duration: 2.238s, episode steps: 324, steps per second: 145, episode reward: 218.653, mean reward: 0.675 [-9.732, 100.000], mean action: 1.827 [0.000, 3.000], mean observation: 0.045 [-0.938, 1.413], loss: 8.118808, mae: 51.922050, mean_q: 69.932404
  225373/1100000: episode: 478, duration: 1.271s, episode steps: 188, steps per second: 148, episode reward: 244.575, mean reward: 1.301 [-12.507, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.133 [-1.067, 1.401], loss: 11.844107, mae: 51.495472, mean_q: 69.203384
  225603/1100000: episode: 479, duration: 1.546s, episode steps: 230, steps per second: 149, episode reward: 281.403, mean reward: 1.223 [-19.399, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.244 [-0.767, 1.389], loss: 5.964488, mae: 51.352165, mean_q: 69.114525
  225715/1100000: episode: 480, duration: 0.751s, episode steps: 112, steps per second: 149, episode reward: -23.935, mean reward: -0.214 [-100.000, 11.443], mean action: 1.973 [0.000, 3.000], mean observation: 0.059 [-1.639, 1.399], loss: 7.921527, mae: 52.032829, mean_q: 69.916428
  226314/1100000: episode: 481, duration: 4.197s, episode steps: 599, steps per second: 143, episode reward: 241.216, mean reward: 0.403 [-20.228, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.049 [-0.719, 1.511], loss: 10.283255, mae: 51.863403, mean_q: 69.716278
  226658/1100000: episode: 482, duration: 2.397s, episode steps: 344, steps per second: 143, episode reward: 259.412, mean reward: 0.754 [-18.770, 100.000], mean action: 2.203 [0.000, 3.000], mean observation: 0.028 [-0.858, 1.458], loss: 11.681714, mae: 52.005928, mean_q: 69.875999
  226787/1100000: episode: 483, duration: 0.854s, episode steps: 129, steps per second: 151, episode reward: -18.646, mean reward: -0.145 [-100.000, 11.822], mean action: 1.388 [0.000, 3.000], mean observation: -0.024 [-1.015, 1.509], loss: 12.044456, mae: 52.460659, mean_q: 70.545135
  227159/1100000: episode: 484, duration: 2.590s, episode steps: 372, steps per second: 144, episode reward: 263.236, mean reward: 0.708 [-17.715, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.135 [-0.828, 1.476], loss: 7.448707, mae: 51.895512, mean_q: 69.875031
  227272/1100000: episode: 485, duration: 0.743s, episode steps: 113, steps per second: 152, episode reward: -58.245, mean reward: -0.515 [-100.000, 10.002], mean action: 1.690 [0.000, 3.000], mean observation: -0.022 [-3.080, 1.473], loss: 6.012524, mae: 52.509544, mean_q: 70.796143
  227683/1100000: episode: 486, duration: 2.798s, episode steps: 411, steps per second: 147, episode reward: 252.553, mean reward: 0.614 [-18.876, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.041 [-0.960, 1.397], loss: 8.050832, mae: 51.999432, mean_q: 70.032631
  228364/1100000: episode: 487, duration: 4.774s, episode steps: 681, steps per second: 143, episode reward: 212.687, mean reward: 0.312 [-18.819, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.024 [-0.711, 1.411], loss: 9.719069, mae: 52.219345, mean_q: 70.209595
  228682/1100000: episode: 488, duration: 2.182s, episode steps: 318, steps per second: 146, episode reward: 255.918, mean reward: 0.805 [-18.587, 100.000], mean action: 0.695 [0.000, 3.000], mean observation: 0.230 [-1.438, 1.393], loss: 11.556698, mae: 52.042858, mean_q: 69.895226
  228795/1100000: episode: 489, duration: 0.750s, episode steps: 113, steps per second: 151, episode reward: 51.408, mean reward: 0.455 [-100.000, 18.769], mean action: 1.832 [0.000, 3.000], mean observation: -0.009 [-0.830, 1.411], loss: 14.740870, mae: 52.157661, mean_q: 70.046661
  229383/1100000: episode: 490, duration: 4.096s, episode steps: 588, steps per second: 144, episode reward: 255.846, mean reward: 0.435 [-19.710, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: 0.036 [-0.616, 1.446], loss: 8.218868, mae: 52.699677, mean_q: 70.851448
  229618/1100000: episode: 491, duration: 1.573s, episode steps: 235, steps per second: 149, episode reward: 264.038, mean reward: 1.124 [-16.998, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.151 [-0.936, 1.400], loss: 6.121723, mae: 52.832737, mean_q: 71.089958
  230394/1100000: episode: 492, duration: 5.540s, episode steps: 776, steps per second: 140, episode reward: 242.706, mean reward: 0.313 [-18.977, 100.000], mean action: 2.117 [0.000, 3.000], mean observation: 0.146 [-0.700, 1.456], loss: 8.869944, mae: 52.807117, mean_q: 70.942909
  230975/1100000: episode: 493, duration: 4.050s, episode steps: 581, steps per second: 143, episode reward: 276.832, mean reward: 0.476 [-18.385, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.051 [-0.710, 1.410], loss: 9.385944, mae: 52.876553, mean_q: 70.996925
  231644/1100000: episode: 494, duration: 4.921s, episode steps: 669, steps per second: 136, episode reward: 199.095, mean reward: 0.298 [-19.856, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.020 [-0.803, 1.421], loss: 9.556794, mae: 52.559673, mean_q: 70.709885
  231821/1100000: episode: 495, duration: 1.180s, episode steps: 177, steps per second: 150, episode reward: -99.638, mean reward: -0.563 [-100.000, 6.078], mean action: 1.712 [0.000, 3.000], mean observation: 0.023 [-0.893, 1.524], loss: 9.041652, mae: 52.435005, mean_q: 70.474098
  232201/1100000: episode: 496, duration: 2.632s, episode steps: 380, steps per second: 144, episode reward: 240.085, mean reward: 0.632 [-12.709, 100.000], mean action: 2.103 [0.000, 3.000], mean observation: 0.029 [-0.779, 1.389], loss: 6.647650, mae: 52.603733, mean_q: 70.850960
  232411/1100000: episode: 497, duration: 1.465s, episode steps: 210, steps per second: 143, episode reward: 29.194, mean reward: 0.139 [-100.000, 11.246], mean action: 1.867 [0.000, 3.000], mean observation: -0.019 [-0.629, 1.477], loss: 8.237282, mae: 53.012455, mean_q: 71.406960
  232616/1100000: episode: 498, duration: 1.416s, episode steps: 205, steps per second: 145, episode reward: 285.827, mean reward: 1.394 [-3.375, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.108 [-1.016, 1.520], loss: 6.511302, mae: 52.142654, mean_q: 70.271782
  232804/1100000: episode: 499, duration: 1.266s, episode steps: 188, steps per second: 149, episode reward: 53.204, mean reward: 0.283 [-100.000, 11.924], mean action: 1.915 [0.000, 3.000], mean observation: -0.024 [-0.681, 1.394], loss: 7.118242, mae: 53.182404, mean_q: 71.576569
  232987/1100000: episode: 500, duration: 1.222s, episode steps: 183, steps per second: 150, episode reward: 270.281, mean reward: 1.477 [-10.175, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.118 [-0.903, 1.388], loss: 14.416481, mae: 52.782536, mean_q: 70.906631
  233331/1100000: episode: 501, duration: 2.363s, episode steps: 344, steps per second: 146, episode reward: 214.811, mean reward: 0.624 [-12.408, 100.000], mean action: 1.613 [0.000, 3.000], mean observation: 0.252 [-0.976, 1.407], loss: 7.184826, mae: 53.207638, mean_q: 71.733025
  233787/1100000: episode: 502, duration: 3.227s, episode steps: 456, steps per second: 141, episode reward: 225.541, mean reward: 0.495 [-17.495, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.025 [-1.324, 1.439], loss: 7.826272, mae: 53.162941, mean_q: 71.755379
  234042/1100000: episode: 503, duration: 1.724s, episode steps: 255, steps per second: 148, episode reward: -278.423, mean reward: -1.092 [-100.000, 32.366], mean action: 1.698 [0.000, 3.000], mean observation: 0.039 [-0.823, 2.248], loss: 10.212807, mae: 52.962345, mean_q: 71.270859
  234210/1100000: episode: 504, duration: 1.129s, episode steps: 168, steps per second: 149, episode reward: 48.380, mean reward: 0.288 [-100.000, 18.639], mean action: 1.994 [0.000, 3.000], mean observation: -0.035 [-0.826, 1.489], loss: 6.499915, mae: 53.245827, mean_q: 71.740547
  234510/1100000: episode: 505, duration: 2.050s, episode steps: 300, steps per second: 146, episode reward: 256.383, mean reward: 0.855 [-11.984, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.109 [-0.772, 1.412], loss: 11.508944, mae: 53.387936, mean_q: 71.926735
  234792/1100000: episode: 506, duration: 1.906s, episode steps: 282, steps per second: 148, episode reward: 283.041, mean reward: 1.004 [-17.719, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.161 [-0.743, 1.405], loss: 6.667987, mae: 53.283070, mean_q: 71.893967
  235472/1100000: episode: 507, duration: 5.261s, episode steps: 680, steps per second: 129, episode reward: 218.397, mean reward: 0.321 [-20.478, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.023 [-0.687, 1.389], loss: 10.108912, mae: 53.683491, mean_q: 72.202835
  235808/1100000: episode: 508, duration: 2.351s, episode steps: 336, steps per second: 143, episode reward: 186.314, mean reward: 0.555 [-17.282, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: 0.033 [-0.958, 1.410], loss: 7.534879, mae: 53.796734, mean_q: 72.276550
  236087/1100000: episode: 509, duration: 1.897s, episode steps: 279, steps per second: 147, episode reward: 255.107, mean reward: 0.914 [-18.671, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.131 [-0.959, 1.416], loss: 12.615983, mae: 54.060802, mean_q: 72.896523
  236505/1100000: episode: 510, duration: 2.883s, episode steps: 418, steps per second: 145, episode reward: 272.304, mean reward: 0.651 [-19.836, 100.000], mean action: 0.828 [0.000, 3.000], mean observation: 0.123 [-0.770, 1.386], loss: 9.075279, mae: 54.075397, mean_q: 72.892380
  236863/1100000: episode: 511, duration: 2.456s, episode steps: 358, steps per second: 146, episode reward: 269.407, mean reward: 0.753 [-11.649, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: -0.030 [-0.782, 1.420], loss: 7.951326, mae: 53.891224, mean_q: 72.579453
  236963/1100000: episode: 512, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -89.391, mean reward: -0.894 [-100.000, 40.657], mean action: 1.520 [0.000, 3.000], mean observation: 0.148 [-0.864, 2.036], loss: 6.500115, mae: 54.125977, mean_q: 72.925850
  237060/1100000: episode: 513, duration: 0.649s, episode steps: 97, steps per second: 150, episode reward: 6.857, mean reward: 0.071 [-100.000, 18.363], mean action: 1.814 [0.000, 3.000], mean observation: 0.036 [-1.625, 1.391], loss: 7.407262, mae: 54.648735, mean_q: 73.578835
  238060/1100000: episode: 514, duration: 7.468s, episode steps: 1000, steps per second: 134, episode reward: 54.981, mean reward: 0.055 [-20.870, 24.735], mean action: 1.133 [0.000, 3.000], mean observation: 0.164 [-0.908, 1.419], loss: 8.398925, mae: 54.459633, mean_q: 73.219284
  238331/1100000: episode: 515, duration: 1.832s, episode steps: 271, steps per second: 148, episode reward: 295.942, mean reward: 1.092 [-18.244, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.131 [-0.719, 1.453], loss: 13.782115, mae: 54.672886, mean_q: 73.660858
  238423/1100000: episode: 516, duration: 0.607s, episode steps: 92, steps per second: 152, episode reward: -215.670, mean reward: -2.344 [-100.000, 5.275], mean action: 1.641 [0.000, 3.000], mean observation: 0.167 [-4.445, 1.477], loss: 12.844113, mae: 53.872684, mean_q: 72.644943
  238513/1100000: episode: 517, duration: 0.605s, episode steps: 90, steps per second: 149, episode reward: -19.720, mean reward: -0.219 [-100.000, 20.074], mean action: 1.767 [0.000, 3.000], mean observation: 0.152 [-0.840, 2.272], loss: 10.646364, mae: 53.795258, mean_q: 72.146645
  238725/1100000: episode: 518, duration: 1.430s, episode steps: 212, steps per second: 148, episode reward: 267.324, mean reward: 1.261 [-17.562, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.082 [-0.782, 1.406], loss: 11.283820, mae: 54.470192, mean_q: 73.404579
  239021/1100000: episode: 519, duration: 2.032s, episode steps: 296, steps per second: 146, episode reward: 255.231, mean reward: 0.862 [-9.537, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.260 [-0.729, 1.421], loss: 8.681203, mae: 54.013344, mean_q: 72.769020
  239100/1100000: episode: 520, duration: 0.535s, episode steps: 79, steps per second: 148, episode reward: -127.332, mean reward: -1.612 [-100.000, 9.146], mean action: 1.722 [0.000, 3.000], mean observation: 0.143 [-1.013, 2.927], loss: 5.305715, mae: 54.194477, mean_q: 73.060806
  239588/1100000: episode: 521, duration: 3.404s, episode steps: 488, steps per second: 143, episode reward: 246.472, mean reward: 0.505 [-17.367, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.031 [-0.745, 1.430], loss: 12.280849, mae: 53.983864, mean_q: 72.719826
  240192/1100000: episode: 522, duration: 4.276s, episode steps: 604, steps per second: 141, episode reward: 205.039, mean reward: 0.339 [-19.357, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.186 [-0.778, 1.399], loss: 8.834759, mae: 53.600941, mean_q: 72.310196
  240598/1100000: episode: 523, duration: 2.818s, episode steps: 406, steps per second: 144, episode reward: 226.788, mean reward: 0.559 [-17.637, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: -0.014 [-0.731, 1.500], loss: 9.870098, mae: 53.750515, mean_q: 72.580086
  240861/1100000: episode: 524, duration: 1.795s, episode steps: 263, steps per second: 147, episode reward: 281.876, mean reward: 1.072 [-19.641, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.084 [-0.904, 1.390], loss: 13.897861, mae: 53.887424, mean_q: 72.651634
  241408/1100000: episode: 525, duration: 3.801s, episode steps: 547, steps per second: 144, episode reward: 256.432, mean reward: 0.469 [-20.114, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.014 [-0.764, 1.386], loss: 9.750928, mae: 54.161011, mean_q: 73.062111
  241525/1100000: episode: 526, duration: 0.782s, episode steps: 117, steps per second: 150, episode reward: -7.354, mean reward: -0.063 [-100.000, 10.195], mean action: 1.718 [0.000, 3.000], mean observation: 0.052 [-1.271, 1.387], loss: 13.523716, mae: 54.004940, mean_q: 73.112892
  241819/1100000: episode: 527, duration: 1.993s, episode steps: 294, steps per second: 148, episode reward: 219.241, mean reward: 0.746 [-13.431, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: -0.014 [-0.737, 1.424], loss: 10.573710, mae: 54.123009, mean_q: 73.089226
  241930/1100000: episode: 528, duration: 0.748s, episode steps: 111, steps per second: 148, episode reward: -34.425, mean reward: -0.310 [-100.000, 14.605], mean action: 1.685 [0.000, 3.000], mean observation: 0.172 [-0.722, 1.388], loss: 7.530806, mae: 54.494709, mean_q: 73.350334
  242074/1100000: episode: 529, duration: 0.964s, episode steps: 144, steps per second: 149, episode reward: -81.682, mean reward: -0.567 [-100.000, 8.690], mean action: 1.917 [0.000, 3.000], mean observation: 0.095 [-0.838, 3.196], loss: 12.582151, mae: 54.615597, mean_q: 73.229790
  242415/1100000: episode: 530, duration: 2.368s, episode steps: 341, steps per second: 144, episode reward: 253.922, mean reward: 0.745 [-4.204, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.190 [-0.615, 1.405], loss: 7.798325, mae: 54.517723, mean_q: 73.425034
  242628/1100000: episode: 531, duration: 1.427s, episode steps: 213, steps per second: 149, episode reward: 253.551, mean reward: 1.190 [-9.037, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: -0.031 [-0.855, 1.404], loss: 8.467311, mae: 54.439087, mean_q: 73.296318
  242894/1100000: episode: 532, duration: 1.819s, episode steps: 266, steps per second: 146, episode reward: 264.162, mean reward: 0.993 [-9.450, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.074 [-0.746, 1.397], loss: 7.477899, mae: 55.005867, mean_q: 73.790543
  243243/1100000: episode: 533, duration: 2.391s, episode steps: 349, steps per second: 146, episode reward: 237.475, mean reward: 0.680 [-13.856, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.005 [-0.626, 1.407], loss: 9.347119, mae: 54.759945, mean_q: 73.603943
  243709/1100000: episode: 534, duration: 3.227s, episode steps: 466, steps per second: 144, episode reward: 273.421, mean reward: 0.587 [-20.251, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.105 [-0.925, 1.460], loss: 9.506413, mae: 54.425915, mean_q: 73.158440
  243979/1100000: episode: 535, duration: 1.839s, episode steps: 270, steps per second: 147, episode reward: 262.689, mean reward: 0.973 [-9.595, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.072 [-0.696, 1.446], loss: 9.555753, mae: 54.816032, mean_q: 74.058380
  244392/1100000: episode: 536, duration: 2.847s, episode steps: 413, steps per second: 145, episode reward: 196.293, mean reward: 0.475 [-19.693, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: -0.032 [-0.770, 1.461], loss: 8.463565, mae: 54.746708, mean_q: 73.893654
  244861/1100000: episode: 537, duration: 3.266s, episode steps: 469, steps per second: 144, episode reward: 234.709, mean reward: 0.500 [-20.375, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.211 [-0.638, 1.390], loss: 8.334090, mae: 55.259987, mean_q: 74.426636
  245289/1100000: episode: 538, duration: 2.895s, episode steps: 428, steps per second: 148, episode reward: 258.507, mean reward: 0.604 [-18.204, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.048 [-0.824, 1.390], loss: 14.173369, mae: 55.010433, mean_q: 73.925636
  245935/1100000: episode: 539, duration: 4.657s, episode steps: 646, steps per second: 139, episode reward: 167.731, mean reward: 0.260 [-21.739, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: 0.100 [-1.297, 1.397], loss: 8.865239, mae: 54.822128, mean_q: 73.873131
  246301/1100000: episode: 540, duration: 2.508s, episode steps: 366, steps per second: 146, episode reward: 240.215, mean reward: 0.656 [-10.027, 100.000], mean action: 1.757 [0.000, 3.000], mean observation: 0.020 [-0.771, 1.388], loss: 7.264626, mae: 55.211014, mean_q: 74.328873
  246576/1100000: episode: 541, duration: 1.878s, episode steps: 275, steps per second: 146, episode reward: 232.439, mean reward: 0.845 [-12.606, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: -0.028 [-0.811, 1.393], loss: 6.213455, mae: 55.054981, mean_q: 74.181992
  247061/1100000: episode: 542, duration: 3.416s, episode steps: 485, steps per second: 142, episode reward: 273.676, mean reward: 0.564 [-19.309, 100.000], mean action: 1.002 [0.000, 3.000], mean observation: 0.027 [-0.701, 1.438], loss: 8.334429, mae: 55.294754, mean_q: 74.554352
  247516/1100000: episode: 543, duration: 3.157s, episode steps: 455, steps per second: 144, episode reward: 235.299, mean reward: 0.517 [-17.368, 100.000], mean action: 2.145 [0.000, 3.000], mean observation: 0.245 [-0.693, 1.489], loss: 10.601328, mae: 54.951355, mean_q: 74.018700
  247738/1100000: episode: 544, duration: 1.505s, episode steps: 222, steps per second: 147, episode reward: 244.861, mean reward: 1.103 [-17.950, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.081 [-0.763, 1.399], loss: 9.915097, mae: 55.027599, mean_q: 74.212311
  248080/1100000: episode: 545, duration: 2.426s, episode steps: 342, steps per second: 141, episode reward: 248.073, mean reward: 0.725 [-21.389, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.147 [-0.612, 1.406], loss: 9.590426, mae: 55.051537, mean_q: 74.187515
  248238/1100000: episode: 546, duration: 1.096s, episode steps: 158, steps per second: 144, episode reward: -61.879, mean reward: -0.392 [-100.000, 12.091], mean action: 1.696 [0.000, 3.000], mean observation: 0.105 [-0.687, 1.397], loss: 7.431292, mae: 54.311123, mean_q: 73.431946
  248467/1100000: episode: 547, duration: 1.561s, episode steps: 229, steps per second: 147, episode reward: 284.463, mean reward: 1.242 [-7.400, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.093 [-0.685, 1.461], loss: 9.106108, mae: 55.044117, mean_q: 74.230522
  249186/1100000: episode: 548, duration: 5.002s, episode steps: 719, steps per second: 144, episode reward: 247.317, mean reward: 0.344 [-18.148, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.014 [-0.801, 1.391], loss: 8.122846, mae: 55.175575, mean_q: 74.590958
  249289/1100000: episode: 549, duration: 0.686s, episode steps: 103, steps per second: 150, episode reward: 28.722, mean reward: 0.279 [-100.000, 43.428], mean action: 1.583 [0.000, 3.000], mean observation: 0.133 [-1.454, 1.400], loss: 13.617269, mae: 55.144051, mean_q: 74.471436
  249620/1100000: episode: 550, duration: 2.302s, episode steps: 331, steps per second: 144, episode reward: 277.384, mean reward: 0.838 [-17.558, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.180 [-0.653, 1.389], loss: 7.372592, mae: 54.905487, mean_q: 74.188423
  249837/1100000: episode: 551, duration: 1.447s, episode steps: 217, steps per second: 150, episode reward: 245.099, mean reward: 1.129 [-17.452, 100.000], mean action: 0.857 [0.000, 3.000], mean observation: 0.097 [-0.953, 1.415], loss: 17.330412, mae: 54.958092, mean_q: 74.257050
  250407/1100000: episode: 552, duration: 4.057s, episode steps: 570, steps per second: 140, episode reward: 202.089, mean reward: 0.355 [-18.831, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.216 [-0.729, 1.416], loss: 9.949030, mae: 54.787796, mean_q: 73.931732
  251114/1100000: episode: 553, duration: 5.086s, episode steps: 707, steps per second: 139, episode reward: 202.540, mean reward: 0.286 [-23.773, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.000 [-0.840, 1.470], loss: 8.662098, mae: 55.043003, mean_q: 74.252640
  251301/1100000: episode: 554, duration: 1.264s, episode steps: 187, steps per second: 148, episode reward: -132.727, mean reward: -0.710 [-100.000, 25.703], mean action: 1.722 [0.000, 3.000], mean observation: 0.152 [-0.676, 1.594], loss: 10.004784, mae: 55.864567, mean_q: 75.222458
  251791/1100000: episode: 555, duration: 3.517s, episode steps: 490, steps per second: 139, episode reward: 252.429, mean reward: 0.515 [-20.079, 100.000], mean action: 1.551 [0.000, 3.000], mean observation: 0.053 [-0.715, 1.396], loss: 9.431166, mae: 55.251793, mean_q: 74.436966
  251877/1100000: episode: 556, duration: 0.582s, episode steps: 86, steps per second: 148, episode reward: -25.311, mean reward: -0.294 [-100.000, 10.443], mean action: 2.012 [0.000, 3.000], mean observation: 0.064 [-0.754, 1.387], loss: 7.932146, mae: 54.512287, mean_q: 73.678741
  252526/1100000: episode: 557, duration: 4.814s, episode steps: 649, steps per second: 135, episode reward: 202.050, mean reward: 0.311 [-18.752, 100.000], mean action: 1.763 [0.000, 3.000], mean observation: 0.100 [-0.997, 1.397], loss: 8.130536, mae: 55.293144, mean_q: 74.592148
  252838/1100000: episode: 558, duration: 2.160s, episode steps: 312, steps per second: 144, episode reward: 239.349, mean reward: 0.767 [-6.523, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.157 [-0.623, 1.401], loss: 10.542286, mae: 55.039120, mean_q: 74.432533
  253035/1100000: episode: 559, duration: 1.339s, episode steps: 197, steps per second: 147, episode reward: -25.083, mean reward: -0.127 [-100.000, 9.596], mean action: 1.944 [0.000, 3.000], mean observation: -0.078 [-0.791, 1.390], loss: 10.186400, mae: 54.880249, mean_q: 74.147469
  253258/1100000: episode: 560, duration: 1.488s, episode steps: 223, steps per second: 150, episode reward: 294.008, mean reward: 1.318 [-9.546, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.094 [-0.791, 1.503], loss: 7.197378, mae: 55.228451, mean_q: 74.364151
  253617/1100000: episode: 561, duration: 2.504s, episode steps: 359, steps per second: 143, episode reward: 251.580, mean reward: 0.701 [-10.324, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.069 [-0.948, 1.565], loss: 9.949443, mae: 55.271648, mean_q: 74.408211
  253970/1100000: episode: 562, duration: 2.407s, episode steps: 353, steps per second: 147, episode reward: 199.481, mean reward: 0.565 [-19.853, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: -0.056 [-0.768, 1.398], loss: 10.784255, mae: 55.717522, mean_q: 74.983223
  254196/1100000: episode: 563, duration: 1.507s, episode steps: 226, steps per second: 150, episode reward: 251.949, mean reward: 1.115 [-11.365, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.185 [-0.697, 1.482], loss: 8.603664, mae: 55.716061, mean_q: 75.002151
  254681/1100000: episode: 564, duration: 3.420s, episode steps: 485, steps per second: 142, episode reward: 235.890, mean reward: 0.486 [-19.400, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.085 [-0.863, 1.455], loss: 9.010837, mae: 55.521675, mean_q: 74.607697
  254777/1100000: episode: 565, duration: 0.648s, episode steps: 96, steps per second: 148, episode reward: -69.215, mean reward: -0.721 [-100.000, 17.855], mean action: 1.896 [0.000, 3.000], mean observation: 0.053 [-1.172, 3.442], loss: 9.560596, mae: 55.491486, mean_q: 74.531624
  255162/1100000: episode: 566, duration: 2.675s, episode steps: 385, steps per second: 144, episode reward: -310.004, mean reward: -0.805 [-100.000, 11.091], mean action: 1.868 [0.000, 3.000], mean observation: -0.091 [-2.836, 1.421], loss: 5.967884, mae: 55.018475, mean_q: 74.093819
  255526/1100000: episode: 567, duration: 2.473s, episode steps: 364, steps per second: 147, episode reward: 252.372, mean reward: 0.693 [-18.352, 100.000], mean action: 0.769 [0.000, 3.000], mean observation: 0.229 [-0.775, 1.482], loss: 9.183301, mae: 55.300201, mean_q: 74.241257
  255743/1100000: episode: 568, duration: 1.470s, episode steps: 217, steps per second: 148, episode reward: 252.602, mean reward: 1.164 [-11.195, 100.000], mean action: 1.719 [0.000, 3.000], mean observation: 0.070 [-0.816, 1.403], loss: 10.348783, mae: 55.115101, mean_q: 73.990341
  255999/1100000: episode: 569, duration: 1.736s, episode steps: 256, steps per second: 147, episode reward: 286.943, mean reward: 1.121 [-11.346, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.050 [-0.807, 1.401], loss: 7.649172, mae: 55.287228, mean_q: 74.228035
  256518/1100000: episode: 570, duration: 3.732s, episode steps: 519, steps per second: 139, episode reward: 253.155, mean reward: 0.488 [-18.778, 100.000], mean action: 2.106 [0.000, 3.000], mean observation: 0.137 [-0.737, 1.401], loss: 11.715136, mae: 55.411308, mean_q: 74.433723
  257428/1100000: episode: 571, duration: 6.647s, episode steps: 910, steps per second: 137, episode reward: 187.665, mean reward: 0.206 [-18.386, 100.000], mean action: 0.848 [0.000, 3.000], mean observation: 0.037 [-0.708, 1.471], loss: 8.900239, mae: 55.200947, mean_q: 74.190308
  257720/1100000: episode: 572, duration: 1.991s, episode steps: 292, steps per second: 147, episode reward: 255.238, mean reward: 0.874 [-3.038, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.076 [-0.678, 1.427], loss: 10.884283, mae: 55.536156, mean_q: 74.720078
  258008/1100000: episode: 573, duration: 1.986s, episode steps: 288, steps per second: 145, episode reward: 237.746, mean reward: 0.826 [-15.239, 100.000], mean action: 2.188 [0.000, 3.000], mean observation: 0.027 [-0.881, 1.441], loss: 6.596311, mae: 55.764412, mean_q: 74.949493
  258190/1100000: episode: 574, duration: 1.216s, episode steps: 182, steps per second: 150, episode reward: 278.869, mean reward: 1.532 [-10.290, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.073 [-0.836, 1.390], loss: 8.376930, mae: 55.664204, mean_q: 74.841339
  258335/1100000: episode: 575, duration: 0.967s, episode steps: 145, steps per second: 150, episode reward: -22.001, mean reward: -0.152 [-100.000, 9.065], mean action: 1.186 [0.000, 3.000], mean observation: 0.114 [-1.166, 1.428], loss: 7.528443, mae: 54.676025, mean_q: 73.331848
  258547/1100000: episode: 576, duration: 1.419s, episode steps: 212, steps per second: 149, episode reward: 272.523, mean reward: 1.285 [-9.527, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.059 [-0.917, 1.478], loss: 4.671773, mae: 55.506454, mean_q: 74.527138
  258657/1100000: episode: 577, duration: 0.741s, episode steps: 110, steps per second: 148, episode reward: 23.203, mean reward: 0.211 [-100.000, 16.385], mean action: 1.618 [0.000, 3.000], mean observation: 0.004 [-0.952, 1.405], loss: 9.536710, mae: 55.380890, mean_q: 73.800423
  259246/1100000: episode: 578, duration: 4.181s, episode steps: 589, steps per second: 141, episode reward: 218.535, mean reward: 0.371 [-17.792, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.028 [-0.907, 1.489], loss: 6.905439, mae: 55.568588, mean_q: 74.650391
  259487/1100000: episode: 579, duration: 1.657s, episode steps: 241, steps per second: 145, episode reward: 259.136, mean reward: 1.075 [-9.525, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.081 [-0.671, 1.410], loss: 5.558408, mae: 56.015011, mean_q: 75.158989
  259961/1100000: episode: 580, duration: 3.292s, episode steps: 474, steps per second: 144, episode reward: 248.207, mean reward: 0.524 [-18.045, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.019 [-0.738, 1.485], loss: 10.061978, mae: 56.248371, mean_q: 75.452843
  260604/1100000: episode: 581, duration: 4.478s, episode steps: 643, steps per second: 144, episode reward: 175.855, mean reward: 0.273 [-18.485, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.134 [-0.733, 1.459], loss: 8.025680, mae: 56.531601, mean_q: 76.186127
  261037/1100000: episode: 582, duration: 2.993s, episode steps: 433, steps per second: 145, episode reward: 280.969, mean reward: 0.649 [-17.945, 100.000], mean action: 0.891 [0.000, 3.000], mean observation: 0.225 [-0.751, 1.386], loss: 11.299111, mae: 56.402138, mean_q: 75.905006
  261200/1100000: episode: 583, duration: 1.084s, episode steps: 163, steps per second: 150, episode reward: 282.357, mean reward: 1.732 [-2.525, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.070 [-0.813, 1.388], loss: 9.151213, mae: 56.521233, mean_q: 75.976524
  261800/1100000: episode: 584, duration: 4.251s, episode steps: 600, steps per second: 141, episode reward: 241.274, mean reward: 0.402 [-17.508, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.234 [-0.644, 1.405], loss: 10.405781, mae: 56.352215, mean_q: 75.585815
  262482/1100000: episode: 585, duration: 4.954s, episode steps: 682, steps per second: 138, episode reward: 218.440, mean reward: 0.320 [-20.247, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.211 [-0.785, 1.482], loss: 7.797200, mae: 56.770756, mean_q: 76.021393
  262830/1100000: episode: 586, duration: 2.371s, episode steps: 348, steps per second: 147, episode reward: 256.697, mean reward: 0.738 [-8.567, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.143 [-0.585, 1.402], loss: 5.400848, mae: 56.604015, mean_q: 76.151543
  262948/1100000: episode: 587, duration: 0.801s, episode steps: 118, steps per second: 147, episode reward: -37.225, mean reward: -0.315 [-100.000, 8.071], mean action: 2.008 [0.000, 3.000], mean observation: 0.057 [-0.843, 1.395], loss: 4.434278, mae: 55.957127, mean_q: 75.366722
  263341/1100000: episode: 588, duration: 2.689s, episode steps: 393, steps per second: 146, episode reward: 186.544, mean reward: 0.475 [-18.824, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.212 [-0.694, 1.412], loss: 17.216043, mae: 56.439548, mean_q: 75.638687
  263603/1100000: episode: 589, duration: 1.822s, episode steps: 262, steps per second: 144, episode reward: 258.341, mean reward: 0.986 [-11.289, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.107 [-0.742, 1.406], loss: 7.145014, mae: 55.807880, mean_q: 74.927750
  263960/1100000: episode: 590, duration: 2.439s, episode steps: 357, steps per second: 146, episode reward: 277.252, mean reward: 0.777 [-19.133, 100.000], mean action: 0.739 [0.000, 3.000], mean observation: 0.115 [-0.795, 1.405], loss: 9.114571, mae: 56.637177, mean_q: 76.204903
  264462/1100000: episode: 591, duration: 3.567s, episode steps: 502, steps per second: 141, episode reward: 198.614, mean reward: 0.396 [-14.500, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: -0.025 [-0.618, 1.451], loss: 8.700732, mae: 56.110203, mean_q: 75.394058
  264756/1100000: episode: 592, duration: 1.989s, episode steps: 294, steps per second: 148, episode reward: 252.562, mean reward: 0.859 [-12.231, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: -0.027 [-0.713, 1.422], loss: 7.505489, mae: 56.872776, mean_q: 76.515388
  265664/1100000: episode: 593, duration: 6.849s, episode steps: 908, steps per second: 133, episode reward: 189.413, mean reward: 0.209 [-19.330, 100.000], mean action: 1.905 [0.000, 3.000], mean observation: 0.106 [-0.751, 1.428], loss: 7.433939, mae: 56.543304, mean_q: 76.058701
  265760/1100000: episode: 594, duration: 0.639s, episode steps: 96, steps per second: 150, episode reward: -54.212, mean reward: -0.565 [-100.000, 12.238], mean action: 1.292 [0.000, 3.000], mean observation: 0.033 [-1.237, 1.682], loss: 5.012557, mae: 56.918201, mean_q: 76.471985
  266165/1100000: episode: 595, duration: 2.859s, episode steps: 405, steps per second: 142, episode reward: 210.149, mean reward: 0.519 [-19.643, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.011 [-0.844, 1.402], loss: 10.506071, mae: 55.961418, mean_q: 74.956444
  266413/1100000: episode: 596, duration: 1.683s, episode steps: 248, steps per second: 147, episode reward: 297.357, mean reward: 1.199 [-17.425, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.107 [-0.827, 1.386], loss: 7.351060, mae: 56.327320, mean_q: 75.620155
  267413/1100000: episode: 597, duration: 7.749s, episode steps: 1000, steps per second: 129, episode reward: 81.357, mean reward: 0.081 [-19.772, 22.587], mean action: 1.584 [0.000, 3.000], mean observation: -0.000 [-0.846, 1.387], loss: 9.596764, mae: 55.851799, mean_q: 74.931923
  267710/1100000: episode: 598, duration: 2.044s, episode steps: 297, steps per second: 145, episode reward: 249.831, mean reward: 0.841 [-7.737, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.119 [-0.728, 1.397], loss: 7.965623, mae: 55.786335, mean_q: 74.871521
  267897/1100000: episode: 599, duration: 1.282s, episode steps: 187, steps per second: 146, episode reward: 251.235, mean reward: 1.344 [-3.050, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.197 [-0.729, 1.413], loss: 9.336007, mae: 55.307011, mean_q: 74.331314
  268412/1100000: episode: 600, duration: 3.655s, episode steps: 515, steps per second: 141, episode reward: 269.611, mean reward: 0.524 [-20.171, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.080 [-0.856, 1.408], loss: 7.253223, mae: 55.941555, mean_q: 75.014793
  268616/1100000: episode: 601, duration: 1.366s, episode steps: 204, steps per second: 149, episode reward: 282.347, mean reward: 1.384 [-8.934, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.202 [-0.824, 1.387], loss: 12.557787, mae: 55.176903, mean_q: 73.946999
  269140/1100000: episode: 602, duration: 3.594s, episode steps: 524, steps per second: 146, episode reward: 229.434, mean reward: 0.438 [-17.922, 100.000], mean action: 0.782 [0.000, 3.000], mean observation: 0.243 [-1.041, 1.437], loss: 8.606728, mae: 55.703091, mean_q: 74.822968
  269385/1100000: episode: 603, duration: 1.655s, episode steps: 245, steps per second: 148, episode reward: 254.530, mean reward: 1.039 [-9.952, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.112 [-0.777, 1.407], loss: 8.884991, mae: 56.167889, mean_q: 75.381744
  269494/1100000: episode: 604, duration: 0.725s, episode steps: 109, steps per second: 150, episode reward: -264.835, mean reward: -2.430 [-100.000, 1.905], mean action: 1.330 [0.000, 3.000], mean observation: 0.303 [-2.744, 1.775], loss: 17.825745, mae: 55.301476, mean_q: 73.910194
  269851/1100000: episode: 605, duration: 2.511s, episode steps: 357, steps per second: 142, episode reward: 226.274, mean reward: 0.634 [-14.534, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.118 [-0.886, 1.385], loss: 7.877117, mae: 55.713467, mean_q: 74.806229
  270213/1100000: episode: 606, duration: 2.465s, episode steps: 362, steps per second: 147, episode reward: 233.283, mean reward: 0.644 [-18.936, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.006 [-0.865, 1.399], loss: 7.312692, mae: 55.596298, mean_q: 74.654785
  270581/1100000: episode: 607, duration: 2.500s, episode steps: 368, steps per second: 147, episode reward: 270.293, mean reward: 0.734 [-10.924, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: -0.018 [-0.600, 1.501], loss: 5.566157, mae: 55.691433, mean_q: 74.720535
  270838/1100000: episode: 608, duration: 1.771s, episode steps: 257, steps per second: 145, episode reward: 220.892, mean reward: 0.860 [-18.106, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.102 [-0.754, 1.422], loss: 13.245155, mae: 56.330559, mean_q: 75.490334
  271031/1100000: episode: 609, duration: 1.295s, episode steps: 193, steps per second: 149, episode reward: 228.630, mean reward: 1.185 [-15.556, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: -0.027 [-0.853, 1.392], loss: 9.480983, mae: 55.902130, mean_q: 74.935059
  271269/1100000: episode: 610, duration: 1.599s, episode steps: 238, steps per second: 149, episode reward: 234.070, mean reward: 0.983 [-9.116, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: -0.042 [-0.730, 1.403], loss: 6.718429, mae: 56.241756, mean_q: 75.799286
  271479/1100000: episode: 611, duration: 1.415s, episode steps: 210, steps per second: 148, episode reward: 265.216, mean reward: 1.263 [-9.930, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.101 [-0.731, 1.395], loss: 9.297493, mae: 56.028748, mean_q: 75.123123
  272479/1100000: episode: 612, duration: 7.185s, episode steps: 1000, steps per second: 139, episode reward: 121.202, mean reward: 0.121 [-20.578, 22.529], mean action: 1.178 [0.000, 3.000], mean observation: 0.290 [-0.797, 1.443], loss: 7.728642, mae: 56.077370, mean_q: 75.443359
  273096/1100000: episode: 613, duration: 4.491s, episode steps: 617, steps per second: 137, episode reward: 196.196, mean reward: 0.318 [-20.037, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.270 [-0.633, 1.408], loss: 8.015080, mae: 56.203636, mean_q: 75.590218
  273351/1100000: episode: 614, duration: 1.715s, episode steps: 255, steps per second: 149, episode reward: 260.793, mean reward: 1.023 [-19.119, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.099 [-0.696, 1.415], loss: 10.157270, mae: 55.879097, mean_q: 75.162567
  273441/1100000: episode: 615, duration: 0.598s, episode steps: 90, steps per second: 151, episode reward: -248.196, mean reward: -2.758 [-100.000, 1.021], mean action: 1.078 [0.000, 3.000], mean observation: -0.016 [-1.869, 1.403], loss: 4.061539, mae: 56.313232, mean_q: 75.893074
  273748/1100000: episode: 616, duration: 2.110s, episode steps: 307, steps per second: 145, episode reward: 239.686, mean reward: 0.781 [-17.447, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.164 [-1.255, 1.428], loss: 6.465013, mae: 56.685535, mean_q: 76.329979
  274230/1100000: episode: 617, duration: 3.425s, episode steps: 482, steps per second: 141, episode reward: 264.144, mean reward: 0.548 [-18.493, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.009 [-0.632, 1.415], loss: 6.762264, mae: 56.444275, mean_q: 75.962090
  274502/1100000: episode: 618, duration: 1.837s, episode steps: 272, steps per second: 148, episode reward: 264.045, mean reward: 0.971 [-19.768, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.088 [-0.719, 1.422], loss: 5.782855, mae: 56.499935, mean_q: 75.962158
  274954/1100000: episode: 619, duration: 3.249s, episode steps: 452, steps per second: 139, episode reward: 250.424, mean reward: 0.554 [-11.709, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.146 [-1.103, 1.405], loss: 7.261865, mae: 56.532185, mean_q: 75.939232
  275132/1100000: episode: 620, duration: 1.186s, episode steps: 178, steps per second: 150, episode reward: 278.767, mean reward: 1.566 [-9.823, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.079 [-0.718, 1.395], loss: 7.321251, mae: 56.847229, mean_q: 76.547279
  275375/1100000: episode: 621, duration: 1.636s, episode steps: 243, steps per second: 149, episode reward: 271.408, mean reward: 1.117 [-9.560, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.098 [-0.677, 1.398], loss: 8.334555, mae: 56.527313, mean_q: 76.153198
  275768/1100000: episode: 622, duration: 2.698s, episode steps: 393, steps per second: 146, episode reward: 145.219, mean reward: 0.370 [-14.970, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.101 [-0.802, 1.398], loss: 5.564531, mae: 57.456097, mean_q: 77.173012
  276153/1100000: episode: 623, duration: 2.646s, episode steps: 385, steps per second: 145, episode reward: 250.414, mean reward: 0.650 [-17.965, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.125 [-0.718, 1.411], loss: 8.347636, mae: 56.853783, mean_q: 76.328484
  276351/1100000: episode: 624, duration: 1.337s, episode steps: 198, steps per second: 148, episode reward: -192.878, mean reward: -0.974 [-100.000, 5.263], mean action: 1.707 [0.000, 3.000], mean observation: 0.027 [-0.945, 1.447], loss: 4.284909, mae: 56.713486, mean_q: 76.211411
  276858/1100000: episode: 625, duration: 3.581s, episode steps: 507, steps per second: 142, episode reward: 233.291, mean reward: 0.460 [-20.615, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.122 [-0.732, 1.407], loss: 9.607524, mae: 56.508869, mean_q: 75.788460
  277312/1100000: episode: 626, duration: 3.158s, episode steps: 454, steps per second: 144, episode reward: 280.486, mean reward: 0.618 [-17.049, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.028 [-1.110, 1.432], loss: 7.962020, mae: 56.334747, mean_q: 75.724014
  277649/1100000: episode: 627, duration: 2.288s, episode steps: 337, steps per second: 147, episode reward: -213.083, mean reward: -0.632 [-100.000, 19.850], mean action: 1.142 [0.000, 3.000], mean observation: 0.121 [-1.115, 1.501], loss: 8.367731, mae: 56.567612, mean_q: 76.072784
  277819/1100000: episode: 628, duration: 1.134s, episode steps: 170, steps per second: 150, episode reward: 49.317, mean reward: 0.290 [-100.000, 15.008], mean action: 1.771 [0.000, 3.000], mean observation: 0.083 [-0.717, 1.404], loss: 11.010763, mae: 56.776505, mean_q: 76.196129
  277908/1100000: episode: 629, duration: 0.591s, episode steps: 89, steps per second: 151, episode reward: -330.562, mean reward: -3.714 [-100.000, -0.249], mean action: 0.809 [0.000, 3.000], mean observation: 0.050 [-1.356, 1.450], loss: 9.718830, mae: 57.232708, mean_q: 76.989845
  278431/1100000: episode: 630, duration: 3.784s, episode steps: 523, steps per second: 138, episode reward: 202.165, mean reward: 0.387 [-17.615, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.124 [-1.346, 1.388], loss: 7.499582, mae: 56.679962, mean_q: 76.269821
  279410/1100000: episode: 631, duration: 7.183s, episode steps: 979, steps per second: 136, episode reward: 236.321, mean reward: 0.241 [-19.900, 100.000], mean action: 0.869 [0.000, 3.000], mean observation: 0.214 [-0.711, 1.411], loss: 8.954393, mae: 56.671650, mean_q: 76.067398
  279738/1100000: episode: 632, duration: 2.225s, episode steps: 328, steps per second: 147, episode reward: 279.732, mean reward: 0.853 [-17.638, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.201 [-0.887, 1.527], loss: 8.570690, mae: 55.965874, mean_q: 75.223000
  280034/1100000: episode: 633, duration: 2.017s, episode steps: 296, steps per second: 147, episode reward: 271.741, mean reward: 0.918 [-17.954, 100.000], mean action: 0.733 [0.000, 3.000], mean observation: 0.134 [-0.867, 1.397], loss: 7.317270, mae: 56.345078, mean_q: 75.574181
  280175/1100000: episode: 634, duration: 0.940s, episode steps: 141, steps per second: 150, episode reward: 6.504, mean reward: 0.046 [-100.000, 14.605], mean action: 1.652 [0.000, 3.000], mean observation: 0.082 [-1.063, 1.423], loss: 8.354061, mae: 55.856499, mean_q: 74.587189
  280480/1100000: episode: 635, duration: 2.081s, episode steps: 305, steps per second: 147, episode reward: 278.350, mean reward: 0.913 [-9.438, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.094 [-0.830, 1.505], loss: 9.085379, mae: 56.742329, mean_q: 76.109810
  280635/1100000: episode: 636, duration: 1.036s, episode steps: 155, steps per second: 150, episode reward: 34.499, mean reward: 0.223 [-100.000, 24.771], mean action: 1.690 [0.000, 3.000], mean observation: -0.034 [-1.027, 1.401], loss: 8.039435, mae: 56.164932, mean_q: 74.955582
  280736/1100000: episode: 637, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: 35.329, mean reward: 0.350 [-100.000, 13.677], mean action: 1.881 [0.000, 3.000], mean observation: 0.099 [-1.419, 1.389], loss: 10.951449, mae: 56.565010, mean_q: 76.023865
  280929/1100000: episode: 638, duration: 1.312s, episode steps: 193, steps per second: 147, episode reward: 241.926, mean reward: 1.254 [-15.882, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: 0.058 [-0.959, 1.385], loss: 5.392172, mae: 56.648674, mean_q: 75.930595
  281157/1100000: episode: 639, duration: 1.532s, episode steps: 228, steps per second: 149, episode reward: 284.131, mean reward: 1.246 [-20.179, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.094 [-0.695, 1.412], loss: 12.441848, mae: 56.367771, mean_q: 75.637024
  281629/1100000: episode: 640, duration: 3.337s, episode steps: 472, steps per second: 141, episode reward: 219.266, mean reward: 0.465 [-18.231, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.170 [-0.926, 1.419], loss: 6.808803, mae: 56.352425, mean_q: 75.591957
  281962/1100000: episode: 641, duration: 2.283s, episode steps: 333, steps per second: 146, episode reward: 238.419, mean reward: 0.716 [-19.907, 100.000], mean action: 1.766 [0.000, 3.000], mean observation: 0.080 [-1.009, 1.479], loss: 7.103283, mae: 57.097145, mean_q: 76.482407
  282092/1100000: episode: 642, duration: 0.871s, episode steps: 130, steps per second: 149, episode reward: -231.363, mean reward: -1.780 [-100.000, 6.320], mean action: 1.508 [0.000, 3.000], mean observation: 0.001 [-3.440, 1.550], loss: 9.620335, mae: 56.640327, mean_q: 75.833519
  282295/1100000: episode: 643, duration: 1.363s, episode steps: 203, steps per second: 149, episode reward: 282.320, mean reward: 1.391 [-6.203, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: -0.028 [-0.827, 1.445], loss: 12.783964, mae: 57.103233, mean_q: 76.487740
  282380/1100000: episode: 644, duration: 0.578s, episode steps: 85, steps per second: 147, episode reward: -57.633, mean reward: -0.678 [-100.000, 9.907], mean action: 1.576 [0.000, 3.000], mean observation: -0.026 [-0.886, 1.390], loss: 6.824455, mae: 56.498749, mean_q: 75.765938
  282692/1100000: episode: 645, duration: 2.229s, episode steps: 312, steps per second: 140, episode reward: 220.588, mean reward: 0.707 [-18.034, 100.000], mean action: 2.353 [0.000, 3.000], mean observation: 0.014 [-0.888, 1.388], loss: 9.523691, mae: 56.760975, mean_q: 76.123711
  282769/1100000: episode: 646, duration: 0.515s, episode steps: 77, steps per second: 149, episode reward: -6.215, mean reward: -0.081 [-100.000, 10.934], mean action: 1.883 [0.000, 3.000], mean observation: -0.078 [-1.921, 1.392], loss: 7.197554, mae: 56.675896, mean_q: 76.073547
  283254/1100000: episode: 647, duration: 3.350s, episode steps: 485, steps per second: 145, episode reward: 237.798, mean reward: 0.490 [-18.257, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.142 [-0.801, 1.406], loss: 9.139085, mae: 57.282349, mean_q: 76.897804
  283690/1100000: episode: 648, duration: 2.948s, episode steps: 436, steps per second: 148, episode reward: 269.771, mean reward: 0.619 [-17.399, 100.000], mean action: 0.933 [0.000, 3.000], mean observation: 0.191 [-0.737, 1.402], loss: 9.140631, mae: 56.995674, mean_q: 76.602135
  283827/1100000: episode: 649, duration: 0.912s, episode steps: 137, steps per second: 150, episode reward: 53.567, mean reward: 0.391 [-100.000, 17.673], mean action: 1.664 [0.000, 3.000], mean observation: 0.078 [-0.635, 1.408], loss: 6.697386, mae: 56.972507, mean_q: 76.488930
  284157/1100000: episode: 650, duration: 2.252s, episode steps: 330, steps per second: 147, episode reward: 217.677, mean reward: 0.660 [-9.451, 100.000], mean action: 1.645 [0.000, 3.000], mean observation: 0.109 [-0.904, 1.401], loss: 4.578195, mae: 57.155006, mean_q: 76.791077
  284255/1100000: episode: 651, duration: 0.652s, episode steps: 98, steps per second: 150, episode reward: -146.758, mean reward: -1.498 [-100.000, 6.365], mean action: 1.510 [0.000, 3.000], mean observation: -0.048 [-1.770, 1.426], loss: 20.509319, mae: 56.066460, mean_q: 75.298363
  284665/1100000: episode: 652, duration: 2.792s, episode steps: 410, steps per second: 147, episode reward: 291.868, mean reward: 0.712 [-10.652, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.064 [-0.910, 1.465], loss: 7.511981, mae: 57.033966, mean_q: 76.765823
  284910/1100000: episode: 653, duration: 1.664s, episode steps: 245, steps per second: 147, episode reward: 260.166, mean reward: 1.062 [-17.412, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.084 [-0.794, 1.398], loss: 5.868986, mae: 56.936184, mean_q: 76.610657
  285289/1100000: episode: 654, duration: 2.644s, episode steps: 379, steps per second: 143, episode reward: 266.473, mean reward: 0.703 [-18.862, 100.000], mean action: 0.781 [0.000, 3.000], mean observation: 0.122 [-0.891, 1.401], loss: 6.579636, mae: 56.854057, mean_q: 76.422081
  285390/1100000: episode: 655, duration: 0.665s, episode steps: 101, steps per second: 152, episode reward: -67.635, mean reward: -0.670 [-100.000, 18.988], mean action: 1.168 [0.000, 3.000], mean observation: 0.212 [-1.029, 1.503], loss: 10.670414, mae: 56.484596, mean_q: 75.907974
  285926/1100000: episode: 656, duration: 3.751s, episode steps: 536, steps per second: 143, episode reward: 223.549, mean reward: 0.417 [-19.152, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.147 [-0.984, 1.426], loss: 10.945975, mae: 56.719749, mean_q: 76.271278
  286134/1100000: episode: 657, duration: 1.390s, episode steps: 208, steps per second: 150, episode reward: 290.336, mean reward: 1.396 [-19.519, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.044 [-0.692, 1.421], loss: 4.094170, mae: 56.970158, mean_q: 76.593987
  286512/1100000: episode: 658, duration: 2.669s, episode steps: 378, steps per second: 142, episode reward: 299.956, mean reward: 0.794 [-10.138, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: -0.030 [-0.796, 1.441], loss: 9.497905, mae: 56.888588, mean_q: 76.422791
  286589/1100000: episode: 659, duration: 0.516s, episode steps: 77, steps per second: 149, episode reward: -94.647, mean reward: -1.229 [-100.000, 10.476], mean action: 1.675 [0.000, 3.000], mean observation: 0.003 [-0.896, 1.665], loss: 14.646346, mae: 57.074409, mean_q: 76.516945
  286814/1100000: episode: 660, duration: 1.521s, episode steps: 225, steps per second: 148, episode reward: 235.402, mean reward: 1.046 [-19.155, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.189 [-1.421, 1.441], loss: 11.507125, mae: 57.192131, mean_q: 76.665001
  287122/1100000: episode: 661, duration: 2.102s, episode steps: 308, steps per second: 147, episode reward: 292.916, mean reward: 0.951 [-3.437, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.137 [-0.704, 1.476], loss: 6.049653, mae: 57.026722, mean_q: 76.544312
  287272/1100000: episode: 662, duration: 1.002s, episode steps: 150, steps per second: 150, episode reward: -175.931, mean reward: -1.173 [-100.000, 11.423], mean action: 1.507 [0.000, 3.000], mean observation: 0.028 [-0.809, 1.406], loss: 7.051334, mae: 57.560757, mean_q: 77.032562
  287765/1100000: episode: 663, duration: 3.394s, episode steps: 493, steps per second: 145, episode reward: 260.162, mean reward: 0.528 [-20.777, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.210 [-0.684, 1.437], loss: 9.097224, mae: 56.963940, mean_q: 76.446861
  288055/1100000: episode: 664, duration: 1.959s, episode steps: 290, steps per second: 148, episode reward: 270.370, mean reward: 0.932 [-9.105, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.228 [-0.563, 1.477], loss: 4.378725, mae: 57.268078, mean_q: 77.054337
  288368/1100000: episode: 665, duration: 2.112s, episode steps: 313, steps per second: 148, episode reward: 219.369, mean reward: 0.701 [-18.335, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.189 [-0.922, 1.400], loss: 6.621387, mae: 56.379593, mean_q: 75.613091
  288658/1100000: episode: 666, duration: 1.983s, episode steps: 290, steps per second: 146, episode reward: -17.988, mean reward: -0.062 [-100.000, 15.633], mean action: 1.728 [0.000, 3.000], mean observation: -0.069 [-1.475, 1.415], loss: 6.196141, mae: 56.572536, mean_q: 75.905762
  288931/1100000: episode: 667, duration: 1.873s, episode steps: 273, steps per second: 146, episode reward: 254.380, mean reward: 0.932 [-14.127, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.196 [-0.817, 1.395], loss: 8.049766, mae: 55.919655, mean_q: 75.208748
  289544/1100000: episode: 668, duration: 4.183s, episode steps: 613, steps per second: 147, episode reward: 241.623, mean reward: 0.394 [-19.358, 100.000], mean action: 0.871 [0.000, 3.000], mean observation: 0.036 [-0.726, 1.450], loss: 7.791769, mae: 55.843739, mean_q: 74.975426
  289703/1100000: episode: 669, duration: 1.066s, episode steps: 159, steps per second: 149, episode reward: 43.500, mean reward: 0.274 [-100.000, 17.400], mean action: 1.648 [0.000, 3.000], mean observation: 0.088 [-0.720, 1.447], loss: 7.181162, mae: 55.515823, mean_q: 74.688324
  289894/1100000: episode: 670, duration: 1.281s, episode steps: 191, steps per second: 149, episode reward: 262.791, mean reward: 1.376 [-9.636, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.082 [-0.772, 1.391], loss: 9.118051, mae: 56.297817, mean_q: 75.454330
  290298/1100000: episode: 671, duration: 2.832s, episode steps: 404, steps per second: 143, episode reward: 219.954, mean reward: 0.544 [-6.539, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: 0.116 [-0.684, 1.518], loss: 6.740529, mae: 55.593033, mean_q: 74.785873
  290655/1100000: episode: 672, duration: 2.461s, episode steps: 357, steps per second: 145, episode reward: 262.834, mean reward: 0.736 [-21.042, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.122 [-0.872, 1.410], loss: 7.551275, mae: 55.920486, mean_q: 74.977402
  290761/1100000: episode: 673, duration: 0.708s, episode steps: 106, steps per second: 150, episode reward: -73.226, mean reward: -0.691 [-100.000, 16.192], mean action: 1.736 [0.000, 3.000], mean observation: 0.008 [-1.334, 1.393], loss: 9.351417, mae: 55.186375, mean_q: 74.371574
  291035/1100000: episode: 674, duration: 1.904s, episode steps: 274, steps per second: 144, episode reward: 252.582, mean reward: 0.922 [-7.534, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.075 [-0.830, 1.387], loss: 10.929954, mae: 55.883160, mean_q: 74.960121
  291669/1100000: episode: 675, duration: 4.709s, episode steps: 634, steps per second: 135, episode reward: 188.084, mean reward: 0.297 [-19.675, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.161 [-0.814, 1.409], loss: 8.973975, mae: 55.750481, mean_q: 74.885880
  291767/1100000: episode: 676, duration: 0.657s, episode steps: 98, steps per second: 149, episode reward: -50.586, mean reward: -0.516 [-100.000, 9.707], mean action: 1.622 [0.000, 3.000], mean observation: -0.007 [-0.814, 1.398], loss: 9.588024, mae: 55.314617, mean_q: 74.433273
  292040/1100000: episode: 677, duration: 1.831s, episode steps: 273, steps per second: 149, episode reward: 248.136, mean reward: 0.909 [-19.725, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.216 [-0.896, 1.426], loss: 9.183063, mae: 55.454884, mean_q: 74.553841
  292232/1100000: episode: 678, duration: 1.286s, episode steps: 192, steps per second: 149, episode reward: 41.899, mean reward: 0.218 [-100.000, 17.699], mean action: 1.417 [0.000, 3.000], mean observation: 0.088 [-0.754, 1.413], loss: 4.990361, mae: 55.666500, mean_q: 74.873650
  292465/1100000: episode: 679, duration: 1.567s, episode steps: 233, steps per second: 149, episode reward: 260.433, mean reward: 1.118 [-9.318, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.071 [-0.769, 1.395], loss: 9.921887, mae: 55.309608, mean_q: 73.986290
  292611/1100000: episode: 680, duration: 0.987s, episode steps: 146, steps per second: 148, episode reward: -69.354, mean reward: -0.475 [-100.000, 7.289], mean action: 1.897 [0.000, 3.000], mean observation: -0.059 [-0.855, 1.593], loss: 11.276238, mae: 55.656002, mean_q: 74.370117
  292896/1100000: episode: 681, duration: 1.906s, episode steps: 285, steps per second: 150, episode reward: 268.317, mean reward: 0.941 [-18.978, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.201 [-1.160, 1.390], loss: 8.582361, mae: 55.452972, mean_q: 74.441544
  293031/1100000: episode: 682, duration: 0.907s, episode steps: 135, steps per second: 149, episode reward: -5.668, mean reward: -0.042 [-100.000, 16.137], mean action: 1.652 [0.000, 3.000], mean observation: -0.003 [-0.879, 1.414], loss: 8.337931, mae: 56.021339, mean_q: 75.343117
  293255/1100000: episode: 683, duration: 1.500s, episode steps: 224, steps per second: 149, episode reward: 285.423, mean reward: 1.274 [-9.888, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.154 [-0.709, 1.392], loss: 7.564710, mae: 55.804329, mean_q: 74.635117
  293673/1100000: episode: 684, duration: 2.866s, episode steps: 418, steps per second: 146, episode reward: 219.088, mean reward: 0.524 [-21.078, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.015 [-0.737, 1.409], loss: 10.597322, mae: 55.694592, mean_q: 74.575615
  294034/1100000: episode: 685, duration: 2.482s, episode steps: 361, steps per second: 145, episode reward: 277.540, mean reward: 0.769 [-12.573, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.201 [-0.757, 1.394], loss: 7.642489, mae: 55.654839, mean_q: 74.593231
  294258/1100000: episode: 686, duration: 1.512s, episode steps: 224, steps per second: 148, episode reward: 278.472, mean reward: 1.243 [-2.805, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.196 [-0.645, 1.411], loss: 7.173269, mae: 55.818394, mean_q: 75.001762
  294407/1100000: episode: 687, duration: 0.998s, episode steps: 149, steps per second: 149, episode reward: 43.324, mean reward: 0.291 [-100.000, 19.058], mean action: 1.940 [0.000, 3.000], mean observation: -0.100 [-0.657, 1.424], loss: 16.874832, mae: 55.258480, mean_q: 73.944458
  294833/1100000: episode: 688, duration: 2.912s, episode steps: 426, steps per second: 146, episode reward: 278.086, mean reward: 0.653 [-19.641, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: 0.102 [-0.857, 1.446], loss: 9.670386, mae: 55.701168, mean_q: 74.712212
  295175/1100000: episode: 689, duration: 2.387s, episode steps: 342, steps per second: 143, episode reward: 227.596, mean reward: 0.665 [-17.478, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.116 [-0.907, 1.404], loss: 8.853890, mae: 55.939316, mean_q: 74.604988
  295331/1100000: episode: 690, duration: 1.043s, episode steps: 156, steps per second: 150, episode reward: 53.642, mean reward: 0.344 [-100.000, 15.722], mean action: 1.673 [0.000, 3.000], mean observation: -0.004 [-0.714, 1.427], loss: 5.719740, mae: 56.206291, mean_q: 75.149254
  295610/1100000: episode: 691, duration: 1.883s, episode steps: 279, steps per second: 148, episode reward: 247.349, mean reward: 0.887 [-17.804, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.097 [-0.820, 1.422], loss: 9.671964, mae: 56.155548, mean_q: 75.036156
  295732/1100000: episode: 692, duration: 0.814s, episode steps: 122, steps per second: 150, episode reward: 44.864, mean reward: 0.368 [-100.000, 16.272], mean action: 1.697 [0.000, 3.000], mean observation: -0.017 [-0.999, 1.398], loss: 11.326658, mae: 56.389793, mean_q: 75.507866
  295876/1100000: episode: 693, duration: 0.967s, episode steps: 144, steps per second: 149, episode reward: 19.415, mean reward: 0.135 [-100.000, 12.828], mean action: 1.736 [0.000, 3.000], mean observation: 0.134 [-1.257, 1.510], loss: 6.125491, mae: 56.381187, mean_q: 75.549927
  296167/1100000: episode: 694, duration: 1.941s, episode steps: 291, steps per second: 150, episode reward: 245.640, mean reward: 0.844 [-2.934, 100.000], mean action: 0.687 [0.000, 3.000], mean observation: 0.207 [-0.826, 1.403], loss: 8.044157, mae: 56.501068, mean_q: 75.796280
  296497/1100000: episode: 695, duration: 2.265s, episode steps: 330, steps per second: 146, episode reward: 265.327, mean reward: 0.804 [-19.500, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.125 [-0.720, 1.394], loss: 6.220311, mae: 56.314133, mean_q: 75.331856
  296725/1100000: episode: 696, duration: 1.540s, episode steps: 228, steps per second: 148, episode reward: 275.271, mean reward: 1.207 [-2.602, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.060 [-0.535, 1.445], loss: 9.000358, mae: 56.417763, mean_q: 75.532104
  297041/1100000: episode: 697, duration: 2.189s, episode steps: 316, steps per second: 144, episode reward: -195.522, mean reward: -0.619 [-100.000, 24.486], mean action: 1.785 [0.000, 3.000], mean observation: 0.056 [-1.752, 1.406], loss: 7.586076, mae: 56.307705, mean_q: 75.377853
  297292/1100000: episode: 698, duration: 1.717s, episode steps: 251, steps per second: 146, episode reward: 275.043, mean reward: 1.096 [-11.081, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.051 [-0.894, 1.404], loss: 7.584259, mae: 55.878574, mean_q: 74.665138
  297586/1100000: episode: 699, duration: 1.987s, episode steps: 294, steps per second: 148, episode reward: 257.357, mean reward: 0.875 [-8.306, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.219 [-0.753, 1.416], loss: 12.667285, mae: 56.307968, mean_q: 75.142334
  297899/1100000: episode: 700, duration: 2.156s, episode steps: 313, steps per second: 145, episode reward: 246.678, mean reward: 0.788 [-9.227, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.042 [-0.785, 1.460], loss: 8.837082, mae: 56.226040, mean_q: 75.364075
  298071/1100000: episode: 701, duration: 1.158s, episode steps: 172, steps per second: 149, episode reward: 43.784, mean reward: 0.255 [-100.000, 20.772], mean action: 1.750 [0.000, 3.000], mean observation: 0.085 [-0.984, 1.387], loss: 9.131931, mae: 56.255768, mean_q: 75.103539
  298359/1100000: episode: 702, duration: 1.977s, episode steps: 288, steps per second: 146, episode reward: 235.511, mean reward: 0.818 [-11.009, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.050 [-0.732, 1.428], loss: 7.362805, mae: 56.028732, mean_q: 74.764107
  298468/1100000: episode: 703, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: 10.848, mean reward: 0.100 [-100.000, 17.728], mean action: 1.862 [0.000, 3.000], mean observation: 0.042 [-1.731, 1.388], loss: 8.816389, mae: 56.635014, mean_q: 75.312210
  298732/1100000: episode: 704, duration: 1.792s, episode steps: 264, steps per second: 147, episode reward: 239.629, mean reward: 0.908 [-18.734, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.178 [-0.771, 1.511], loss: 9.200216, mae: 56.504417, mean_q: 75.259804
  299266/1100000: episode: 705, duration: 3.835s, episode steps: 534, steps per second: 139, episode reward: 289.214, mean reward: 0.542 [-17.539, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.112 [-0.934, 1.395], loss: 10.547790, mae: 56.102242, mean_q: 75.021156
  299385/1100000: episode: 706, duration: 0.797s, episode steps: 119, steps per second: 149, episode reward: 37.582, mean reward: 0.316 [-100.000, 14.186], mean action: 1.731 [0.000, 3.000], mean observation: -0.067 [-0.807, 1.388], loss: 10.388029, mae: 55.735023, mean_q: 74.694504
  299607/1100000: episode: 707, duration: 1.490s, episode steps: 222, steps per second: 149, episode reward: 288.878, mean reward: 1.301 [-9.900, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.170 [-0.786, 1.387], loss: 11.936544, mae: 56.023670, mean_q: 74.447853
  299808/1100000: episode: 708, duration: 1.357s, episode steps: 201, steps per second: 148, episode reward: -15.734, mean reward: -0.078 [-100.000, 16.904], mean action: 1.398 [0.000, 3.000], mean observation: 0.117 [-0.821, 1.418], loss: 11.044820, mae: 55.939510, mean_q: 74.944328
  300102/1100000: episode: 709, duration: 2.057s, episode steps: 294, steps per second: 143, episode reward: 274.124, mean reward: 0.932 [-17.636, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.110 [-0.697, 1.492], loss: 9.925199, mae: 55.909466, mean_q: 74.998558
  300381/1100000: episode: 710, duration: 1.910s, episode steps: 279, steps per second: 146, episode reward: 290.392, mean reward: 1.041 [-18.149, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.088 [-0.784, 1.389], loss: 7.554705, mae: 55.899498, mean_q: 74.579651
  300659/1100000: episode: 711, duration: 1.899s, episode steps: 278, steps per second: 146, episode reward: 258.582, mean reward: 0.930 [-11.050, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.101 [-1.108, 1.450], loss: 7.302171, mae: 55.864906, mean_q: 74.579254
  301322/1100000: episode: 712, duration: 4.787s, episode steps: 663, steps per second: 139, episode reward: 214.429, mean reward: 0.323 [-19.931, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.139 [-1.263, 1.407], loss: 7.298170, mae: 56.522160, mean_q: 75.514885
  301721/1100000: episode: 713, duration: 2.715s, episode steps: 399, steps per second: 147, episode reward: 252.003, mean reward: 0.632 [-17.808, 100.000], mean action: 0.659 [0.000, 3.000], mean observation: 0.134 [-0.974, 1.404], loss: 9.959298, mae: 56.104950, mean_q: 75.143745
  301917/1100000: episode: 714, duration: 1.304s, episode steps: 196, steps per second: 150, episode reward: -97.482, mean reward: -0.497 [-100.000, 3.807], mean action: 1.184 [0.000, 3.000], mean observation: 0.083 [-0.600, 1.417], loss: 10.262349, mae: 56.088921, mean_q: 74.919685
  302917/1100000: episode: 715, duration: 7.159s, episode steps: 1000, steps per second: 140, episode reward: 104.272, mean reward: 0.104 [-18.949, 20.147], mean action: 1.261 [0.000, 3.000], mean observation: 0.271 [-0.790, 1.404], loss: 7.801246, mae: 56.270710, mean_q: 75.260727
  303129/1100000: episode: 716, duration: 1.434s, episode steps: 212, steps per second: 148, episode reward: 228.232, mean reward: 1.077 [-15.228, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.133 [-0.745, 1.393], loss: 5.857264, mae: 56.358696, mean_q: 75.516006
  304129/1100000: episode: 717, duration: 7.310s, episode steps: 1000, steps per second: 137, episode reward: 135.493, mean reward: 0.135 [-19.261, 20.304], mean action: 1.036 [0.000, 3.000], mean observation: 0.274 [-0.759, 1.515], loss: 9.006231, mae: 56.724155, mean_q: 75.815575
  304221/1100000: episode: 718, duration: 0.621s, episode steps: 92, steps per second: 148, episode reward: -22.842, mean reward: -0.248 [-100.000, 6.510], mean action: 1.957 [0.000, 3.000], mean observation: -0.075 [-1.050, 1.385], loss: 8.577552, mae: 56.824902, mean_q: 76.139122
  304578/1100000: episode: 719, duration: 2.483s, episode steps: 357, steps per second: 144, episode reward: 268.157, mean reward: 0.751 [-19.770, 100.000], mean action: 1.826 [0.000, 3.000], mean observation: -0.005 [-1.257, 1.400], loss: 10.799796, mae: 56.662983, mean_q: 75.922424
  304835/1100000: episode: 720, duration: 1.744s, episode steps: 257, steps per second: 147, episode reward: 14.183, mean reward: 0.055 [-100.000, 10.043], mean action: 1.498 [0.000, 3.000], mean observation: -0.012 [-0.772, 1.424], loss: 8.068124, mae: 56.233089, mean_q: 75.278595
  304925/1100000: episode: 721, duration: 0.605s, episode steps: 90, steps per second: 149, episode reward: -33.129, mean reward: -0.368 [-100.000, 7.294], mean action: 1.900 [0.000, 3.000], mean observation: -0.068 [-2.196, 1.401], loss: 8.631842, mae: 57.439144, mean_q: 76.252167
  305127/1100000: episode: 722, duration: 1.362s, episode steps: 202, steps per second: 148, episode reward: 19.203, mean reward: 0.095 [-100.000, 11.562], mean action: 1.738 [0.000, 3.000], mean observation: 0.030 [-1.899, 1.492], loss: 7.989472, mae: 56.052280, mean_q: 74.968575
  305976/1100000: episode: 723, duration: 5.995s, episode steps: 849, steps per second: 142, episode reward: 235.866, mean reward: 0.278 [-20.185, 100.000], mean action: 0.667 [0.000, 3.000], mean observation: 0.182 [-0.864, 1.427], loss: 9.077111, mae: 56.710617, mean_q: 75.936905
  306215/1100000: episode: 724, duration: 1.604s, episode steps: 239, steps per second: 149, episode reward: 267.804, mean reward: 1.121 [-10.583, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.191 [-0.726, 1.456], loss: 16.548326, mae: 55.898781, mean_q: 75.421120
  306337/1100000: episode: 725, duration: 0.821s, episode steps: 122, steps per second: 149, episode reward: 21.066, mean reward: 0.173 [-100.000, 19.471], mean action: 1.795 [0.000, 3.000], mean observation: -0.026 [-0.786, 1.396], loss: 12.978175, mae: 56.230450, mean_q: 75.607246
  306489/1100000: episode: 726, duration: 1.010s, episode steps: 152, steps per second: 151, episode reward: 33.767, mean reward: 0.222 [-100.000, 17.485], mean action: 1.217 [0.000, 3.000], mean observation: 0.071 [-0.879, 1.452], loss: 9.689934, mae: 56.177479, mean_q: 75.424278
  306783/1100000: episode: 727, duration: 2.010s, episode steps: 294, steps per second: 146, episode reward: 267.510, mean reward: 0.910 [-9.549, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.083 [-0.781, 1.482], loss: 7.717045, mae: 56.575619, mean_q: 75.936592
  306918/1100000: episode: 728, duration: 0.906s, episode steps: 135, steps per second: 149, episode reward: 22.112, mean reward: 0.164 [-100.000, 16.402], mean action: 1.600 [0.000, 3.000], mean observation: 0.180 [-1.210, 1.414], loss: 10.477894, mae: 56.197388, mean_q: 75.405960
  307204/1100000: episode: 729, duration: 1.972s, episode steps: 286, steps per second: 145, episode reward: 267.729, mean reward: 0.936 [-8.327, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.096 [-0.700, 1.400], loss: 8.017274, mae: 56.606850, mean_q: 75.863541
  307733/1100000: episode: 730, duration: 3.815s, episode steps: 529, steps per second: 139, episode reward: 257.631, mean reward: 0.487 [-18.583, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.156 [-0.691, 1.388], loss: 10.679344, mae: 56.633816, mean_q: 75.919853
  308316/1100000: episode: 731, duration: 4.256s, episode steps: 583, steps per second: 137, episode reward: 219.331, mean reward: 0.376 [-24.895, 100.000], mean action: 1.909 [0.000, 3.000], mean observation: 0.015 [-1.119, 1.491], loss: 10.990328, mae: 56.498371, mean_q: 75.772690
  308785/1100000: episode: 732, duration: 3.481s, episode steps: 469, steps per second: 135, episode reward: 265.704, mean reward: 0.567 [-17.767, 100.000], mean action: 0.864 [0.000, 3.000], mean observation: 0.093 [-0.832, 1.386], loss: 8.081457, mae: 56.457584, mean_q: 75.866737
  309135/1100000: episode: 733, duration: 2.412s, episode steps: 350, steps per second: 145, episode reward: 227.851, mean reward: 0.651 [-21.643, 100.000], mean action: 1.826 [0.000, 3.000], mean observation: 0.028 [-0.798, 1.406], loss: 8.909348, mae: 56.392727, mean_q: 75.679062
  309697/1100000: episode: 734, duration: 4.053s, episode steps: 562, steps per second: 139, episode reward: 244.931, mean reward: 0.436 [-18.390, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.018 [-1.044, 1.412], loss: 8.443530, mae: 56.607101, mean_q: 75.715904
  309998/1100000: episode: 735, duration: 2.040s, episode steps: 301, steps per second: 148, episode reward: 264.265, mean reward: 0.878 [-9.607, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.125 [-0.602, 1.398], loss: 9.159977, mae: 56.897774, mean_q: 76.069817
  310300/1100000: episode: 736, duration: 2.090s, episode steps: 302, steps per second: 145, episode reward: 299.162, mean reward: 0.991 [-17.362, 100.000], mean action: 0.844 [0.000, 3.000], mean observation: 0.114 [-1.381, 1.388], loss: 11.145741, mae: 56.036823, mean_q: 75.225014
  310803/1100000: episode: 737, duration: 3.522s, episode steps: 503, steps per second: 143, episode reward: 275.491, mean reward: 0.548 [-21.695, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.102 [-1.682, 1.504], loss: 11.291456, mae: 55.855946, mean_q: 74.969177
  311189/1100000: episode: 738, duration: 2.705s, episode steps: 386, steps per second: 143, episode reward: 268.774, mean reward: 0.696 [-18.893, 100.000], mean action: 0.847 [0.000, 3.000], mean observation: 0.112 [-0.825, 1.388], loss: 9.558404, mae: 55.665283, mean_q: 74.572258
  311523/1100000: episode: 739, duration: 2.298s, episode steps: 334, steps per second: 145, episode reward: 300.038, mean reward: 0.898 [-12.581, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.036 [-0.771, 1.396], loss: 5.365171, mae: 56.241081, mean_q: 75.464508
  312113/1100000: episode: 740, duration: 4.135s, episode steps: 590, steps per second: 143, episode reward: 222.868, mean reward: 0.378 [-22.777, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.239 [-0.870, 1.439], loss: 7.175430, mae: 56.070610, mean_q: 75.110603
  312297/1100000: episode: 741, duration: 1.238s, episode steps: 184, steps per second: 149, episode reward: -76.227, mean reward: -0.414 [-100.000, 28.572], mean action: 1.625 [0.000, 3.000], mean observation: 0.130 [-0.811, 1.411], loss: 8.057827, mae: 55.849361, mean_q: 74.932014
  312495/1100000: episode: 742, duration: 1.338s, episode steps: 198, steps per second: 148, episode reward: 282.495, mean reward: 1.427 [-10.218, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.063 [-1.114, 1.396], loss: 12.777369, mae: 56.574440, mean_q: 76.058922
  312673/1100000: episode: 743, duration: 1.201s, episode steps: 178, steps per second: 148, episode reward: 278.854, mean reward: 1.567 [-5.212, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.166 [-0.825, 1.391], loss: 6.954243, mae: 55.313168, mean_q: 74.369850
  312905/1100000: episode: 744, duration: 1.578s, episode steps: 232, steps per second: 147, episode reward: 283.218, mean reward: 1.221 [-4.930, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.075 [-1.138, 1.390], loss: 9.278790, mae: 55.566898, mean_q: 74.450256
  313643/1100000: episode: 745, duration: 5.354s, episode steps: 738, steps per second: 138, episode reward: 246.138, mean reward: 0.334 [-19.209, 100.000], mean action: 1.683 [0.000, 3.000], mean observation: 0.229 [-0.781, 1.636], loss: 8.571757, mae: 55.727070, mean_q: 74.739029
  313775/1100000: episode: 746, duration: 0.888s, episode steps: 132, steps per second: 149, episode reward: 19.905, mean reward: 0.151 [-100.000, 8.044], mean action: 1.583 [0.000, 3.000], mean observation: 0.081 [-0.940, 1.394], loss: 9.921414, mae: 56.205830, mean_q: 75.163544
  314107/1100000: episode: 747, duration: 2.304s, episode steps: 332, steps per second: 144, episode reward: -300.465, mean reward: -0.905 [-100.000, 7.371], mean action: 1.825 [0.000, 3.000], mean observation: -0.057 [-2.578, 1.503], loss: 9.002254, mae: 55.674816, mean_q: 74.652893
  314293/1100000: episode: 748, duration: 1.244s, episode steps: 186, steps per second: 150, episode reward: 54.311, mean reward: 0.292 [-100.000, 11.804], mean action: 1.231 [0.000, 3.000], mean observation: 0.160 [-0.855, 1.481], loss: 5.046577, mae: 55.281059, mean_q: 74.232613
  314648/1100000: episode: 749, duration: 2.484s, episode steps: 355, steps per second: 143, episode reward: 270.808, mean reward: 0.763 [-9.797, 100.000], mean action: 0.986 [0.000, 3.000], mean observation: 0.109 [-0.767, 1.394], loss: 8.887334, mae: 55.928303, mean_q: 74.913170
  314961/1100000: episode: 750, duration: 2.145s, episode steps: 313, steps per second: 146, episode reward: -42.038, mean reward: -0.134 [-100.000, 16.745], mean action: 1.633 [0.000, 3.000], mean observation: 0.099 [-1.277, 1.396], loss: 8.069105, mae: 55.823509, mean_q: 74.735138
  315380/1100000: episode: 751, duration: 2.876s, episode steps: 419, steps per second: 146, episode reward: 279.479, mean reward: 0.667 [-17.895, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.109 [-0.601, 1.486], loss: 7.851448, mae: 55.612091, mean_q: 74.301285
  315716/1100000: episode: 752, duration: 2.308s, episode steps: 336, steps per second: 146, episode reward: 205.693, mean reward: 0.612 [-19.455, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: -0.061 [-0.654, 1.420], loss: 8.451332, mae: 56.300503, mean_q: 75.105263
  316321/1100000: episode: 753, duration: 4.196s, episode steps: 605, steps per second: 144, episode reward: 230.867, mean reward: 0.382 [-18.486, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.218 [-1.115, 1.488], loss: 8.920792, mae: 56.056820, mean_q: 74.936684
  316552/1100000: episode: 754, duration: 1.569s, episode steps: 231, steps per second: 147, episode reward: 281.688, mean reward: 1.219 [-3.124, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.045 [-0.648, 1.413], loss: 7.401073, mae: 56.559307, mean_q: 75.799858
  317252/1100000: episode: 755, duration: 4.993s, episode steps: 700, steps per second: 140, episode reward: 256.188, mean reward: 0.366 [-18.825, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.028 [-0.793, 1.412], loss: 7.179419, mae: 56.076931, mean_q: 75.203629
  317804/1100000: episode: 756, duration: 3.906s, episode steps: 552, steps per second: 141, episode reward: 276.634, mean reward: 0.501 [-20.167, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.047 [-0.695, 1.535], loss: 9.714553, mae: 56.514523, mean_q: 75.498726
  318356/1100000: episode: 757, duration: 3.868s, episode steps: 552, steps per second: 143, episode reward: 271.634, mean reward: 0.492 [-17.557, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.204 [-1.092, 1.459], loss: 8.185344, mae: 56.049778, mean_q: 75.001183
  319038/1100000: episode: 758, duration: 4.859s, episode steps: 682, steps per second: 140, episode reward: 203.041, mean reward: 0.298 [-18.713, 100.000], mean action: 2.054 [0.000, 3.000], mean observation: 0.207 [-0.854, 1.398], loss: 7.848167, mae: 55.810375, mean_q: 74.576645
  319215/1100000: episode: 759, duration: 1.193s, episode steps: 177, steps per second: 148, episode reward: 295.254, mean reward: 1.668 [-2.636, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.090 [-1.174, 1.389], loss: 7.994263, mae: 55.789803, mean_q: 74.622208
  319377/1100000: episode: 760, duration: 1.093s, episode steps: 162, steps per second: 148, episode reward: 53.665, mean reward: 0.331 [-100.000, 20.528], mean action: 1.722 [0.000, 3.000], mean observation: 0.098 [-0.737, 1.411], loss: 13.163490, mae: 55.546551, mean_q: 73.981544
  319919/1100000: episode: 761, duration: 3.862s, episode steps: 542, steps per second: 140, episode reward: 249.462, mean reward: 0.460 [-17.789, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.193 [-0.819, 1.439], loss: 7.948048, mae: 55.664494, mean_q: 74.465858
  320225/1100000: episode: 762, duration: 2.096s, episode steps: 306, steps per second: 146, episode reward: 252.757, mean reward: 0.826 [-16.969, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.166 [-1.007, 1.409], loss: 9.753113, mae: 56.093391, mean_q: 75.181435
  320940/1100000: episode: 763, duration: 5.170s, episode steps: 715, steps per second: 138, episode reward: 226.667, mean reward: 0.317 [-17.620, 100.000], mean action: 1.924 [0.000, 3.000], mean observation: 0.047 [-0.600, 1.594], loss: 8.228047, mae: 55.950912, mean_q: 74.920059
  321141/1100000: episode: 764, duration: 1.352s, episode steps: 201, steps per second: 149, episode reward: -133.583, mean reward: -0.665 [-100.000, 10.074], mean action: 1.756 [0.000, 3.000], mean observation: 0.020 [-0.857, 1.650], loss: 9.448603, mae: 55.922401, mean_q: 75.047501
  321540/1100000: episode: 765, duration: 2.822s, episode steps: 399, steps per second: 141, episode reward: 127.493, mean reward: 0.320 [-18.333, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: -0.032 [-1.592, 1.417], loss: 8.672762, mae: 56.051208, mean_q: 75.046173
  321946/1100000: episode: 766, duration: 2.822s, episode steps: 406, steps per second: 144, episode reward: -138.519, mean reward: -0.341 [-100.000, 72.692], mean action: 1.554 [0.000, 3.000], mean observation: -0.134 [-1.245, 1.412], loss: 8.502958, mae: 55.795242, mean_q: 74.722542
  322096/1100000: episode: 767, duration: 1.002s, episode steps: 150, steps per second: 150, episode reward: 31.153, mean reward: 0.208 [-100.000, 11.609], mean action: 1.560 [0.000, 3.000], mean observation: 0.037 [-0.749, 1.408], loss: 17.799173, mae: 56.068268, mean_q: 75.199608
  322715/1100000: episode: 768, duration: 4.662s, episode steps: 619, steps per second: 133, episode reward: -167.337, mean reward: -0.270 [-100.000, 20.893], mean action: 1.727 [0.000, 3.000], mean observation: -0.036 [-1.247, 1.417], loss: 10.354345, mae: 55.853241, mean_q: 74.623482
  323125/1100000: episode: 769, duration: 2.838s, episode steps: 410, steps per second: 144, episode reward: -68.241, mean reward: -0.166 [-100.000, 11.982], mean action: 1.568 [0.000, 3.000], mean observation: -0.044 [-1.313, 1.463], loss: 8.540174, mae: 55.784779, mean_q: 74.793991
  323377/1100000: episode: 770, duration: 1.717s, episode steps: 252, steps per second: 147, episode reward: 273.807, mean reward: 1.087 [-15.217, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.157 [-1.136, 1.514], loss: 9.359836, mae: 56.182827, mean_q: 75.336403
  323659/1100000: episode: 771, duration: 1.909s, episode steps: 282, steps per second: 148, episode reward: 269.201, mean reward: 0.955 [-13.981, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.046 [-1.041, 1.431], loss: 8.129745, mae: 55.766098, mean_q: 74.790878
  324063/1100000: episode: 772, duration: 2.769s, episode steps: 404, steps per second: 146, episode reward: 261.200, mean reward: 0.647 [-10.466, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.187 [-0.668, 1.399], loss: 7.685803, mae: 56.199104, mean_q: 75.419655
  324411/1100000: episode: 773, duration: 2.394s, episode steps: 348, steps per second: 145, episode reward: 284.450, mean reward: 0.817 [-14.944, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.029 [-1.007, 1.413], loss: 12.294181, mae: 56.414680, mean_q: 75.653610
  325354/1100000: episode: 774, duration: 7.140s, episode steps: 943, steps per second: 132, episode reward: 184.539, mean reward: 0.196 [-19.634, 100.000], mean action: 2.118 [0.000, 3.000], mean observation: 0.274 [-0.880, 1.404], loss: 10.770588, mae: 56.328903, mean_q: 75.584953
  325808/1100000: episode: 775, duration: 3.232s, episode steps: 454, steps per second: 140, episode reward: 270.527, mean reward: 0.596 [-17.406, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.095 [-0.837, 1.410], loss: 7.697298, mae: 56.281086, mean_q: 75.581223
  326118/1100000: episode: 776, duration: 2.122s, episode steps: 310, steps per second: 146, episode reward: 274.834, mean reward: 0.887 [-5.801, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.088 [-1.130, 1.526], loss: 11.179737, mae: 56.025253, mean_q: 75.062225
  326419/1100000: episode: 777, duration: 2.040s, episode steps: 301, steps per second: 148, episode reward: 234.832, mean reward: 0.780 [-20.125, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.197 [-0.824, 1.472], loss: 6.044481, mae: 56.961296, mean_q: 76.198883
  326825/1100000: episode: 778, duration: 2.784s, episode steps: 406, steps per second: 146, episode reward: 299.384, mean reward: 0.737 [-19.697, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.013 [-0.861, 1.387], loss: 8.392817, mae: 56.549351, mean_q: 75.748108
  327825/1100000: episode: 779, duration: 7.453s, episode steps: 1000, steps per second: 134, episode reward: 13.207, mean reward: 0.013 [-13.322, 19.723], mean action: 1.093 [0.000, 3.000], mean observation: -0.022 [-0.935, 1.404], loss: 11.284214, mae: 56.708599, mean_q: 75.847755
  328038/1100000: episode: 780, duration: 1.434s, episode steps: 213, steps per second: 148, episode reward: 250.778, mean reward: 1.177 [-8.094, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.077 [-0.919, 1.403], loss: 8.715080, mae: 56.677891, mean_q: 76.148972
  328479/1100000: episode: 781, duration: 3.053s, episode steps: 441, steps per second: 144, episode reward: 284.775, mean reward: 0.646 [-18.383, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.163 [-0.769, 1.541], loss: 10.101448, mae: 56.516361, mean_q: 75.755898
  328846/1100000: episode: 782, duration: 2.452s, episode steps: 367, steps per second: 150, episode reward: 246.798, mean reward: 0.672 [-19.066, 100.000], mean action: 0.774 [0.000, 3.000], mean observation: 0.051 [-0.933, 1.524], loss: 10.272142, mae: 56.991219, mean_q: 76.487831
  329016/1100000: episode: 783, duration: 1.145s, episode steps: 170, steps per second: 148, episode reward: -30.457, mean reward: -0.179 [-100.000, 8.824], mean action: 1.441 [0.000, 3.000], mean observation: -0.077 [-0.936, 1.395], loss: 8.160800, mae: 56.811863, mean_q: 76.401474
  329292/1100000: episode: 784, duration: 1.870s, episode steps: 276, steps per second: 148, episode reward: 259.815, mean reward: 0.941 [-9.962, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.099 [-0.806, 1.416], loss: 10.043390, mae: 56.988705, mean_q: 76.716240
  329432/1100000: episode: 785, duration: 0.934s, episode steps: 140, steps per second: 150, episode reward: 34.860, mean reward: 0.249 [-100.000, 19.912], mean action: 2.007 [0.000, 3.000], mean observation: 0.179 [-0.892, 1.439], loss: 8.875079, mae: 57.092278, mean_q: 76.832230
  329939/1100000: episode: 786, duration: 3.422s, episode steps: 507, steps per second: 148, episode reward: 269.112, mean reward: 0.531 [-18.929, 100.000], mean action: 0.698 [0.000, 3.000], mean observation: 0.044 [-0.720, 1.581], loss: 9.927694, mae: 57.622719, mean_q: 77.472031
  330271/1100000: episode: 787, duration: 2.252s, episode steps: 332, steps per second: 147, episode reward: 277.998, mean reward: 0.837 [-19.933, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.023 [-0.706, 1.460], loss: 10.884995, mae: 57.345619, mean_q: 76.960320
  330386/1100000: episode: 788, duration: 0.763s, episode steps: 115, steps per second: 151, episode reward: -2.476, mean reward: -0.022 [-100.000, 15.826], mean action: 1.800 [0.000, 3.000], mean observation: 0.180 [-2.217, 1.408], loss: 6.157485, mae: 58.220188, mean_q: 78.163101
  330742/1100000: episode: 789, duration: 2.476s, episode steps: 356, steps per second: 144, episode reward: 290.745, mean reward: 0.817 [-21.139, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.067 [-0.732, 1.416], loss: 10.228507, mae: 57.911121, mean_q: 77.665344
  331043/1100000: episode: 790, duration: 2.082s, episode steps: 301, steps per second: 145, episode reward: 285.948, mean reward: 0.950 [-5.475, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.075 [-0.659, 1.473], loss: 8.462441, mae: 57.515820, mean_q: 77.469742
  331218/1100000: episode: 791, duration: 1.183s, episode steps: 175, steps per second: 148, episode reward: -186.864, mean reward: -1.068 [-100.000, 11.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.124 [-0.866, 1.941], loss: 5.419445, mae: 57.944733, mean_q: 77.894073
  331804/1100000: episode: 792, duration: 4.086s, episode steps: 586, steps per second: 143, episode reward: 282.078, mean reward: 0.481 [-19.977, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.078 [-0.723, 1.413], loss: 9.651586, mae: 57.534969, mean_q: 77.245674
  332058/1100000: episode: 793, duration: 1.700s, episode steps: 254, steps per second: 149, episode reward: 277.790, mean reward: 1.094 [-10.919, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.107 [-0.813, 1.530], loss: 8.797834, mae: 57.195820, mean_q: 76.520805
  332263/1100000: episode: 794, duration: 1.385s, episode steps: 205, steps per second: 148, episode reward: 33.109, mean reward: 0.162 [-100.000, 12.367], mean action: 1.737 [0.000, 3.000], mean observation: -0.011 [-0.777, 1.504], loss: 14.152165, mae: 57.953526, mean_q: 77.553291
  332820/1100000: episode: 795, duration: 4.012s, episode steps: 557, steps per second: 139, episode reward: 183.596, mean reward: 0.330 [-17.839, 100.000], mean action: 2.201 [0.000, 3.000], mean observation: 0.226 [-0.751, 1.409], loss: 12.993453, mae: 57.694393, mean_q: 77.464592
  332955/1100000: episode: 796, duration: 0.896s, episode steps: 135, steps per second: 151, episode reward: -43.318, mean reward: -0.321 [-100.000, 10.595], mean action: 1.489 [0.000, 3.000], mean observation: -0.032 [-0.676, 1.409], loss: 12.502837, mae: 57.367123, mean_q: 77.189415
  333155/1100000: episode: 797, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -17.481, mean reward: -0.087 [-100.000, 20.236], mean action: 1.565 [0.000, 3.000], mean observation: 0.146 [-1.519, 1.410], loss: 8.192299, mae: 57.490955, mean_q: 77.452782
  333332/1100000: episode: 798, duration: 1.194s, episode steps: 177, steps per second: 148, episode reward: 21.745, mean reward: 0.123 [-100.000, 15.076], mean action: 1.678 [0.000, 3.000], mean observation: 0.043 [-0.713, 1.413], loss: 9.730496, mae: 56.778404, mean_q: 76.127846
  334332/1100000: episode: 799, duration: 7.089s, episode steps: 1000, steps per second: 141, episode reward: 131.503, mean reward: 0.132 [-22.986, 22.235], mean action: 0.806 [0.000, 3.000], mean observation: 0.072 [-0.705, 1.425], loss: 12.533312, mae: 57.180389, mean_q: 76.804871
  334411/1100000: episode: 800, duration: 0.533s, episode steps: 79, steps per second: 148, episode reward: -148.292, mean reward: -1.877 [-100.000, 3.941], mean action: 1.810 [0.000, 3.000], mean observation: -0.014 [-3.678, 1.405], loss: 13.597596, mae: 57.036404, mean_q: 76.710167
  334959/1100000: episode: 801, duration: 3.920s, episode steps: 548, steps per second: 140, episode reward: 217.088, mean reward: 0.396 [-21.748, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: -0.005 [-0.857, 1.468], loss: 11.058155, mae: 56.908932, mean_q: 76.207504
  335218/1100000: episode: 802, duration: 1.775s, episode steps: 259, steps per second: 146, episode reward: -182.802, mean reward: -0.706 [-100.000, 13.888], mean action: 1.475 [0.000, 3.000], mean observation: -0.034 [-2.031, 1.420], loss: 8.272615, mae: 57.127853, mean_q: 76.386482
  335648/1100000: episode: 803, duration: 2.960s, episode steps: 430, steps per second: 145, episode reward: 246.997, mean reward: 0.574 [-10.152, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.014 [-0.754, 1.389], loss: 10.559084, mae: 57.063999, mean_q: 76.439934
  335979/1100000: episode: 804, duration: 2.285s, episode steps: 331, steps per second: 145, episode reward: -242.630, mean reward: -0.733 [-100.000, 14.119], mean action: 1.420 [0.000, 3.000], mean observation: -0.096 [-1.368, 2.595], loss: 8.378876, mae: 56.768463, mean_q: 76.008606
  336244/1100000: episode: 805, duration: 1.817s, episode steps: 265, steps per second: 146, episode reward: 303.934, mean reward: 1.147 [-9.068, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.094 [-0.639, 1.401], loss: 11.795067, mae: 57.008953, mean_q: 76.396637
  336444/1100000: episode: 806, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: 13.562, mean reward: 0.068 [-100.000, 8.511], mean action: 1.520 [0.000, 3.000], mean observation: 0.008 [-1.020, 1.487], loss: 9.031515, mae: 57.521172, mean_q: 76.924782
  336691/1100000: episode: 807, duration: 1.648s, episode steps: 247, steps per second: 150, episode reward: -26.462, mean reward: -0.107 [-100.000, 13.250], mean action: 1.336 [0.000, 3.000], mean observation: 0.004 [-0.979, 1.523], loss: 16.468441, mae: 57.625111, mean_q: 76.946007
  336806/1100000: episode: 808, duration: 0.777s, episode steps: 115, steps per second: 148, episode reward: 48.082, mean reward: 0.418 [-100.000, 12.638], mean action: 1.774 [0.000, 3.000], mean observation: 0.014 [-0.813, 1.393], loss: 12.356118, mae: 57.152752, mean_q: 76.452797
  337029/1100000: episode: 809, duration: 1.532s, episode steps: 223, steps per second: 146, episode reward: 263.772, mean reward: 1.183 [-6.527, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.070 [-0.713, 1.407], loss: 12.409262, mae: 56.930202, mean_q: 76.118744
  337305/1100000: episode: 810, duration: 1.889s, episode steps: 276, steps per second: 146, episode reward: 300.741, mean reward: 1.090 [-10.079, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.049 [-0.957, 1.386], loss: 12.447384, mae: 57.494648, mean_q: 76.928215
  337493/1100000: episode: 811, duration: 1.262s, episode steps: 188, steps per second: 149, episode reward: -2.389, mean reward: -0.013 [-100.000, 45.701], mean action: 1.723 [0.000, 3.000], mean observation: 0.199 [-2.135, 1.522], loss: 12.565687, mae: 56.935528, mean_q: 76.223259
  338010/1100000: episode: 812, duration: 3.637s, episode steps: 517, steps per second: 142, episode reward: -332.041, mean reward: -0.642 [-100.000, 13.911], mean action: 1.551 [0.000, 3.000], mean observation: -0.063 [-0.940, 1.969], loss: 10.883074, mae: 57.357533, mean_q: 76.649567
  338241/1100000: episode: 813, duration: 1.544s, episode steps: 231, steps per second: 150, episode reward: 267.815, mean reward: 1.159 [-18.990, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: -0.035 [-0.737, 1.421], loss: 13.434914, mae: 57.421799, mean_q: 76.776619
  338897/1100000: episode: 814, duration: 4.589s, episode steps: 656, steps per second: 143, episode reward: 230.434, mean reward: 0.351 [-18.155, 100.000], mean action: 1.680 [0.000, 3.000], mean observation: 0.190 [-0.813, 1.393], loss: 12.460281, mae: 57.584156, mean_q: 76.638580
  339020/1100000: episode: 815, duration: 0.821s, episode steps: 123, steps per second: 150, episode reward: -4.497, mean reward: -0.037 [-100.000, 15.455], mean action: 1.764 [0.000, 3.000], mean observation: -0.006 [-0.877, 1.726], loss: 8.238536, mae: 57.847340, mean_q: 77.616562
  339186/1100000: episode: 816, duration: 1.108s, episode steps: 166, steps per second: 150, episode reward: -314.432, mean reward: -1.894 [-100.000, 11.820], mean action: 1.729 [0.000, 3.000], mean observation: 0.145 [-1.612, 3.274], loss: 8.351779, mae: 58.249928, mean_q: 78.057991
  339430/1100000: episode: 817, duration: 1.649s, episode steps: 244, steps per second: 148, episode reward: 254.100, mean reward: 1.041 [-7.788, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.079 [-0.794, 1.413], loss: 7.931911, mae: 57.405346, mean_q: 76.845833
  339904/1100000: episode: 818, duration: 3.279s, episode steps: 474, steps per second: 145, episode reward: -342.932, mean reward: -0.723 [-100.000, 12.550], mean action: 1.568 [0.000, 3.000], mean observation: -0.061 [-1.196, 1.454], loss: 11.671599, mae: 57.684460, mean_q: 77.152893
  340033/1100000: episode: 819, duration: 0.859s, episode steps: 129, steps per second: 150, episode reward: 14.429, mean reward: 0.112 [-100.000, 16.225], mean action: 1.612 [0.000, 3.000], mean observation: 0.045 [-1.628, 1.401], loss: 4.501583, mae: 58.341496, mean_q: 77.860100
  340293/1100000: episode: 820, duration: 1.775s, episode steps: 260, steps per second: 146, episode reward: -132.140, mean reward: -0.508 [-100.000, 26.763], mean action: 1.604 [0.000, 3.000], mean observation: 0.103 [-0.728, 1.394], loss: 12.079096, mae: 58.207943, mean_q: 77.525223
  340797/1100000: episode: 821, duration: 3.472s, episode steps: 504, steps per second: 145, episode reward: 283.311, mean reward: 0.562 [-20.535, 100.000], mean action: 0.627 [0.000, 3.000], mean observation: 0.146 [-1.119, 1.388], loss: 13.073670, mae: 58.303478, mean_q: 77.931274
  341681/1100000: episode: 822, duration: 6.570s, episode steps: 884, steps per second: 135, episode reward: 270.588, mean reward: 0.306 [-23.835, 100.000], mean action: 0.896 [0.000, 3.000], mean observation: 0.125 [-0.698, 1.389], loss: 13.357339, mae: 58.401562, mean_q: 77.805916
  341828/1100000: episode: 823, duration: 0.987s, episode steps: 147, steps per second: 149, episode reward: -9.215, mean reward: -0.063 [-100.000, 12.197], mean action: 1.796 [0.000, 3.000], mean observation: 0.185 [-0.965, 1.504], loss: 16.289963, mae: 58.779243, mean_q: 78.325867
  341959/1100000: episode: 824, duration: 0.866s, episode steps: 131, steps per second: 151, episode reward: -124.833, mean reward: -0.953 [-100.000, 5.085], mean action: 1.450 [0.000, 3.000], mean observation: 0.233 [-0.809, 1.414], loss: 7.737513, mae: 58.999516, mean_q: 78.673340
  342142/1100000: episode: 825, duration: 1.241s, episode steps: 183, steps per second: 147, episode reward: -25.538, mean reward: -0.140 [-100.000, 4.756], mean action: 1.617 [0.000, 3.000], mean observation: -0.010 [-0.664, 1.409], loss: 16.863043, mae: 58.960312, mean_q: 78.664787
  342709/1100000: episode: 826, duration: 3.947s, episode steps: 567, steps per second: 144, episode reward: 261.647, mean reward: 0.461 [-18.397, 100.000], mean action: 0.614 [0.000, 3.000], mean observation: 0.255 [-1.086, 1.390], loss: 11.167733, mae: 58.902431, mean_q: 78.545845
  342857/1100000: episode: 827, duration: 1.038s, episode steps: 148, steps per second: 143, episode reward: -110.711, mean reward: -0.748 [-100.000, 3.968], mean action: 1.412 [0.000, 3.000], mean observation: 0.224 [-0.790, 1.390], loss: 14.980363, mae: 58.893883, mean_q: 78.664856
  343210/1100000: episode: 828, duration: 2.446s, episode steps: 353, steps per second: 144, episode reward: -275.663, mean reward: -0.781 [-100.000, 19.357], mean action: 1.416 [0.000, 3.000], mean observation: -0.081 [-1.265, 1.413], loss: 11.656054, mae: 59.231503, mean_q: 79.042999
  343822/1100000: episode: 829, duration: 4.111s, episode steps: 612, steps per second: 149, episode reward: 242.279, mean reward: 0.396 [-19.038, 100.000], mean action: 0.523 [0.000, 3.000], mean observation: 0.036 [-0.897, 1.424], loss: 13.516562, mae: 59.276207, mean_q: 78.991348
  344015/1100000: episode: 830, duration: 1.293s, episode steps: 193, steps per second: 149, episode reward: 258.624, mean reward: 1.340 [-8.091, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: -0.021 [-0.766, 1.396], loss: 10.771605, mae: 59.604958, mean_q: 79.222260
  344350/1100000: episode: 831, duration: 2.299s, episode steps: 335, steps per second: 146, episode reward: 233.172, mean reward: 0.696 [-5.576, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.041 [-0.750, 1.402], loss: 13.992664, mae: 59.547382, mean_q: 79.220024
  344666/1100000: episode: 832, duration: 2.140s, episode steps: 316, steps per second: 148, episode reward: 32.627, mean reward: 0.103 [-100.000, 15.063], mean action: 1.506 [0.000, 3.000], mean observation: -0.077 [-0.825, 1.489], loss: 11.899945, mae: 59.101463, mean_q: 78.528595
  345577/1100000: episode: 833, duration: 6.684s, episode steps: 911, steps per second: 136, episode reward: 188.801, mean reward: 0.207 [-18.193, 100.000], mean action: 2.221 [0.000, 3.000], mean observation: 0.270 [-0.837, 1.413], loss: 10.811246, mae: 60.094116, mean_q: 79.967934
  346075/1100000: episode: 834, duration: 3.508s, episode steps: 498, steps per second: 142, episode reward: 265.157, mean reward: 0.532 [-20.195, 100.000], mean action: 1.104 [0.000, 3.000], mean observation: 0.128 [-0.638, 1.407], loss: 12.135338, mae: 59.739201, mean_q: 79.376915
  346395/1100000: episode: 835, duration: 2.181s, episode steps: 320, steps per second: 147, episode reward: 255.553, mean reward: 0.799 [-12.254, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.106 [-0.696, 1.409], loss: 17.750000, mae: 59.989655, mean_q: 79.476181
  346550/1100000: episode: 836, duration: 1.035s, episode steps: 155, steps per second: 150, episode reward: -24.370, mean reward: -0.157 [-100.000, 17.810], mean action: 1.361 [0.000, 3.000], mean observation: -0.040 [-1.520, 1.405], loss: 11.233309, mae: 60.463268, mean_q: 80.179016
  347217/1100000: episode: 837, duration: 4.828s, episode steps: 667, steps per second: 138, episode reward: 212.601, mean reward: 0.319 [-21.036, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.002 [-0.625, 1.431], loss: 13.422906, mae: 60.247448, mean_q: 80.261909
  348078/1100000: episode: 838, duration: 6.308s, episode steps: 861, steps per second: 136, episode reward: 261.674, mean reward: 0.304 [-21.693, 100.000], mean action: 2.057 [0.000, 3.000], mean observation: 0.014 [-1.027, 1.385], loss: 13.309466, mae: 59.709820, mean_q: 79.309258
  348927/1100000: episode: 839, duration: 6.326s, episode steps: 849, steps per second: 134, episode reward: 204.691, mean reward: 0.241 [-24.323, 100.000], mean action: 2.351 [0.000, 3.000], mean observation: -0.010 [-0.868, 1.416], loss: 14.100376, mae: 59.204819, mean_q: 78.936935
  349294/1100000: episode: 840, duration: 2.518s, episode steps: 367, steps per second: 146, episode reward: 288.108, mean reward: 0.785 [-22.434, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.034 [-1.052, 1.473], loss: 15.065430, mae: 59.675556, mean_q: 79.096657
  349733/1100000: episode: 841, duration: 3.025s, episode steps: 439, steps per second: 145, episode reward: 284.970, mean reward: 0.649 [-18.183, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.125 [-0.757, 1.473], loss: 11.279734, mae: 59.984089, mean_q: 79.843979
  350056/1100000: episode: 842, duration: 2.204s, episode steps: 323, steps per second: 147, episode reward: 237.058, mean reward: 0.734 [-18.902, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.204 [-0.786, 1.388], loss: 11.085056, mae: 59.828259, mean_q: 79.706268
  350220/1100000: episode: 843, duration: 1.110s, episode steps: 164, steps per second: 148, episode reward: -13.640, mean reward: -0.083 [-100.000, 14.901], mean action: 1.671 [0.000, 3.000], mean observation: 0.032 [-1.141, 1.466], loss: 9.867216, mae: 59.274029, mean_q: 79.356354
  350469/1100000: episode: 844, duration: 1.690s, episode steps: 249, steps per second: 147, episode reward: 230.173, mean reward: 0.924 [-16.681, 100.000], mean action: 1.996 [0.000, 3.000], mean observation: 0.106 [-0.710, 1.401], loss: 8.929878, mae: 59.785908, mean_q: 79.957375
  350768/1100000: episode: 845, duration: 2.035s, episode steps: 299, steps per second: 147, episode reward: 238.173, mean reward: 0.797 [-16.008, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: -0.054 [-0.974, 1.409], loss: 10.686746, mae: 60.279251, mean_q: 80.324577
  351001/1100000: episode: 846, duration: 1.551s, episode steps: 233, steps per second: 150, episode reward: 240.951, mean reward: 1.034 [-4.936, 100.000], mean action: 0.897 [0.000, 3.000], mean observation: 0.090 [-1.138, 1.410], loss: 10.344497, mae: 59.765724, mean_q: 79.908546
  351621/1100000: episode: 847, duration: 4.581s, episode steps: 620, steps per second: 135, episode reward: 255.762, mean reward: 0.413 [-18.145, 100.000], mean action: 0.671 [0.000, 3.000], mean observation: 0.121 [-1.143, 1.404], loss: 8.830881, mae: 60.007423, mean_q: 80.083870
  351856/1100000: episode: 848, duration: 1.584s, episode steps: 235, steps per second: 148, episode reward: 227.341, mean reward: 0.967 [-8.403, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.186 [-0.673, 1.419], loss: 8.510176, mae: 59.569626, mean_q: 79.315460
  352184/1100000: episode: 849, duration: 2.269s, episode steps: 328, steps per second: 145, episode reward: -43.763, mean reward: -0.133 [-100.000, 13.214], mean action: 1.784 [0.000, 3.000], mean observation: 0.197 [-0.899, 1.425], loss: 18.411324, mae: 59.209949, mean_q: 78.885826
  352545/1100000: episode: 850, duration: 2.533s, episode steps: 361, steps per second: 142, episode reward: 281.354, mean reward: 0.779 [-17.632, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.225 [-0.772, 1.386], loss: 14.942506, mae: 60.081600, mean_q: 79.926979
  352785/1100000: episode: 851, duration: 1.613s, episode steps: 240, steps per second: 149, episode reward: 266.594, mean reward: 1.111 [-11.665, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.087 [-0.829, 1.454], loss: 11.371876, mae: 59.321182, mean_q: 78.914711
  353014/1100000: episode: 852, duration: 1.546s, episode steps: 229, steps per second: 148, episode reward: 289.656, mean reward: 1.265 [-19.516, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.074 [-0.733, 1.393], loss: 9.139226, mae: 59.523941, mean_q: 79.565590
  353429/1100000: episode: 853, duration: 2.870s, episode steps: 415, steps per second: 145, episode reward: 223.473, mean reward: 0.538 [-8.359, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: -0.029 [-1.001, 1.388], loss: 11.877346, mae: 59.653873, mean_q: 79.457756
  354429/1100000: episode: 854, duration: 7.552s, episode steps: 1000, steps per second: 132, episode reward: 126.471, mean reward: 0.126 [-18.624, 23.427], mean action: 1.321 [0.000, 3.000], mean observation: 0.249 [-0.920, 1.501], loss: 13.906228, mae: 59.149860, mean_q: 78.887878
  354575/1100000: episode: 855, duration: 0.975s, episode steps: 146, steps per second: 150, episode reward: 15.127, mean reward: 0.104 [-100.000, 12.027], mean action: 1.562 [0.000, 3.000], mean observation: 0.025 [-0.848, 1.434], loss: 7.506001, mae: 59.299706, mean_q: 79.237244
  354949/1100000: episode: 856, duration: 2.555s, episode steps: 374, steps per second: 146, episode reward: 266.014, mean reward: 0.711 [-17.531, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.102 [-0.661, 1.403], loss: 11.618765, mae: 58.895042, mean_q: 78.466858
  355800/1100000: episode: 857, duration: 5.885s, episode steps: 851, steps per second: 145, episode reward: 210.577, mean reward: 0.247 [-19.404, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.278 [-0.810, 1.410], loss: 9.071217, mae: 58.455898, mean_q: 77.895874
  356154/1100000: episode: 858, duration: 2.458s, episode steps: 354, steps per second: 144, episode reward: 245.066, mean reward: 0.692 [-13.249, 100.000], mean action: 1.887 [0.000, 3.000], mean observation: 0.221 [-1.159, 1.513], loss: 9.284592, mae: 58.017616, mean_q: 77.514671
  356871/1100000: episode: 859, duration: 5.434s, episode steps: 717, steps per second: 132, episode reward: -221.314, mean reward: -0.309 [-100.000, 32.282], mean action: 1.321 [0.000, 3.000], mean observation: 0.069 [-1.516, 1.774], loss: 9.509143, mae: 58.178020, mean_q: 77.374535
  357210/1100000: episode: 860, duration: 2.341s, episode steps: 339, steps per second: 145, episode reward: 231.096, mean reward: 0.682 [-24.452, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: -0.036 [-0.854, 1.395], loss: 9.602612, mae: 58.259338, mean_q: 77.880447
  357862/1100000: episode: 861, duration: 4.695s, episode steps: 652, steps per second: 139, episode reward: 221.597, mean reward: 0.340 [-19.214, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.010 [-0.873, 1.437], loss: 11.779937, mae: 58.320137, mean_q: 77.535110
  358075/1100000: episode: 862, duration: 1.446s, episode steps: 213, steps per second: 147, episode reward: 17.041, mean reward: 0.080 [-100.000, 12.059], mean action: 1.826 [0.000, 3.000], mean observation: 0.164 [-1.246, 1.396], loss: 11.819865, mae: 58.102203, mean_q: 77.410744
  358182/1100000: episode: 863, duration: 0.712s, episode steps: 107, steps per second: 150, episode reward: 27.885, mean reward: 0.261 [-100.000, 11.602], mean action: 1.916 [0.000, 3.000], mean observation: 0.158 [-0.788, 1.395], loss: 11.501589, mae: 58.222012, mean_q: 77.817261
  358311/1100000: episode: 864, duration: 0.863s, episode steps: 129, steps per second: 149, episode reward: -267.087, mean reward: -2.070 [-100.000, 18.977], mean action: 1.558 [0.000, 3.000], mean observation: -0.043 [-2.291, 1.424], loss: 13.502687, mae: 58.623623, mean_q: 77.876839
  358627/1100000: episode: 865, duration: 2.196s, episode steps: 316, steps per second: 144, episode reward: 207.580, mean reward: 0.657 [-10.986, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.172 [-0.687, 1.391], loss: 10.401308, mae: 57.884964, mean_q: 77.159142
  358776/1100000: episode: 866, duration: 0.994s, episode steps: 149, steps per second: 150, episode reward: 19.123, mean reward: 0.128 [-100.000, 9.460], mean action: 1.664 [0.000, 3.000], mean observation: -0.031 [-0.891, 1.399], loss: 12.267442, mae: 58.431522, mean_q: 77.713425
  359118/1100000: episode: 867, duration: 2.366s, episode steps: 342, steps per second: 145, episode reward: -338.448, mean reward: -0.990 [-100.000, 5.212], mean action: 1.968 [0.000, 3.000], mean observation: 0.242 [-1.159, 1.561], loss: 10.013673, mae: 58.001778, mean_q: 77.242661
  359410/1100000: episode: 868, duration: 2.014s, episode steps: 292, steps per second: 145, episode reward: 191.214, mean reward: 0.655 [-10.760, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: -0.100 [-1.008, 1.403], loss: 14.977415, mae: 58.190937, mean_q: 77.470879
  359603/1100000: episode: 869, duration: 1.299s, episode steps: 193, steps per second: 149, episode reward: 300.780, mean reward: 1.558 [-8.551, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.035 [-0.689, 1.399], loss: 11.325595, mae: 57.533962, mean_q: 76.333519
  359845/1100000: episode: 870, duration: 1.665s, episode steps: 242, steps per second: 145, episode reward: 222.108, mean reward: 0.918 [-9.508, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.138 [-0.652, 1.422], loss: 11.985025, mae: 57.924519, mean_q: 76.644943
  360051/1100000: episode: 871, duration: 1.406s, episode steps: 206, steps per second: 147, episode reward: -175.622, mean reward: -0.853 [-100.000, 9.649], mean action: 1.641 [0.000, 3.000], mean observation: 0.171 [-0.784, 1.412], loss: 12.829966, mae: 57.527298, mean_q: 76.164894
  360200/1100000: episode: 872, duration: 1.002s, episode steps: 149, steps per second: 149, episode reward: 1.584, mean reward: 0.011 [-100.000, 18.259], mean action: 1.859 [0.000, 3.000], mean observation: 0.091 [-1.029, 1.698], loss: 9.529105, mae: 58.102127, mean_q: 76.760292
  361024/1100000: episode: 873, duration: 6.019s, episode steps: 824, steps per second: 137, episode reward: 234.053, mean reward: 0.284 [-19.788, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.243 [-1.236, 1.416], loss: 9.235155, mae: 57.944752, mean_q: 76.549950
  361394/1100000: episode: 874, duration: 2.535s, episode steps: 370, steps per second: 146, episode reward: 216.471, mean reward: 0.585 [-18.412, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.225 [-0.800, 1.422], loss: 7.529899, mae: 57.641850, mean_q: 76.011688
  361684/1100000: episode: 875, duration: 1.970s, episode steps: 290, steps per second: 147, episode reward: 301.460, mean reward: 1.040 [-18.040, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.095 [-0.851, 1.520], loss: 6.896451, mae: 58.162556, mean_q: 77.192917
  362113/1100000: episode: 876, duration: 3.031s, episode steps: 429, steps per second: 142, episode reward: 217.550, mean reward: 0.507 [-22.004, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.208 [-0.767, 1.430], loss: 11.868430, mae: 58.055634, mean_q: 76.789444
  362245/1100000: episode: 877, duration: 0.878s, episode steps: 132, steps per second: 150, episode reward: -40.833, mean reward: -0.309 [-100.000, 14.121], mean action: 1.492 [0.000, 3.000], mean observation: 0.016 [-0.699, 1.687], loss: 16.536686, mae: 57.516907, mean_q: 75.820328
  362444/1100000: episode: 878, duration: 1.339s, episode steps: 199, steps per second: 149, episode reward: -157.288, mean reward: -0.790 [-100.000, 27.151], mean action: 1.382 [0.000, 3.000], mean observation: 0.027 [-0.883, 1.706], loss: 10.814591, mae: 58.184223, mean_q: 76.955254
  362830/1100000: episode: 879, duration: 2.801s, episode steps: 386, steps per second: 138, episode reward: 251.685, mean reward: 0.652 [-18.022, 100.000], mean action: 2.332 [0.000, 3.000], mean observation: 0.209 [-0.798, 1.385], loss: 9.051068, mae: 58.045437, mean_q: 76.917076
  363131/1100000: episode: 880, duration: 2.087s, episode steps: 301, steps per second: 144, episode reward: 3.371, mean reward: 0.011 [-100.000, 26.154], mean action: 1.568 [0.000, 3.000], mean observation: -0.085 [-0.877, 1.640], loss: 11.022413, mae: 58.386063, mean_q: 76.943558
  363329/1100000: episode: 881, duration: 1.335s, episode steps: 198, steps per second: 148, episode reward: 5.027, mean reward: 0.025 [-100.000, 16.615], mean action: 1.581 [0.000, 3.000], mean observation: -0.085 [-1.009, 1.549], loss: 10.309243, mae: 59.076794, mean_q: 78.081802
  363506/1100000: episode: 882, duration: 1.184s, episode steps: 177, steps per second: 149, episode reward: -1.446, mean reward: -0.008 [-100.000, 10.633], mean action: 1.780 [0.000, 3.000], mean observation: -0.082 [-0.916, 1.705], loss: 15.004676, mae: 58.993313, mean_q: 78.489220
  364111/1100000: episode: 883, duration: 4.372s, episode steps: 605, steps per second: 138, episode reward: 197.161, mean reward: 0.326 [-18.879, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.147 [-1.312, 1.415], loss: 11.380227, mae: 58.781651, mean_q: 77.727592
  364284/1100000: episode: 884, duration: 1.152s, episode steps: 173, steps per second: 150, episode reward: -51.807, mean reward: -0.299 [-100.000, 16.297], mean action: 1.792 [0.000, 3.000], mean observation: -0.090 [-0.929, 1.489], loss: 9.833852, mae: 58.888660, mean_q: 77.806526
  364952/1100000: episode: 885, duration: 4.719s, episode steps: 668, steps per second: 142, episode reward: 172.598, mean reward: 0.258 [-24.116, 100.000], mean action: 1.891 [0.000, 3.000], mean observation: 0.063 [-0.775, 1.409], loss: 11.716744, mae: 59.119083, mean_q: 77.924721
  365255/1100000: episode: 886, duration: 2.084s, episode steps: 303, steps per second: 145, episode reward: 249.137, mean reward: 0.822 [-10.975, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.022 [-0.973, 1.481], loss: 14.037335, mae: 58.369907, mean_q: 77.142387
  365424/1100000: episode: 887, duration: 1.132s, episode steps: 169, steps per second: 149, episode reward: -33.604, mean reward: -0.199 [-100.000, 15.517], mean action: 1.609 [0.000, 3.000], mean observation: -0.078 [-0.651, 1.452], loss: 16.377127, mae: 58.937824, mean_q: 78.059494
  365583/1100000: episode: 888, duration: 1.068s, episode steps: 159, steps per second: 149, episode reward: -24.464, mean reward: -0.154 [-100.000, 17.718], mean action: 1.698 [0.000, 3.000], mean observation: -0.010 [-1.795, 1.397], loss: 14.094272, mae: 58.585999, mean_q: 77.424210
  365779/1100000: episode: 889, duration: 1.330s, episode steps: 196, steps per second: 147, episode reward: -370.137, mean reward: -1.888 [-100.000, 5.191], mean action: 1.867 [0.000, 3.000], mean observation: -0.048 [-2.722, 1.432], loss: 13.797601, mae: 58.560661, mean_q: 77.498917
  365925/1100000: episode: 890, duration: 0.976s, episode steps: 146, steps per second: 150, episode reward: 56.248, mean reward: 0.385 [-100.000, 11.987], mean action: 1.390 [0.000, 3.000], mean observation: -0.063 [-0.964, 1.397], loss: 11.973909, mae: 58.224350, mean_q: 76.682045
  366251/1100000: episode: 891, duration: 2.199s, episode steps: 326, steps per second: 148, episode reward: 302.133, mean reward: 0.927 [-18.640, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.109 [-0.802, 1.410], loss: 11.290418, mae: 59.032406, mean_q: 78.264191
  366407/1100000: episode: 892, duration: 1.048s, episode steps: 156, steps per second: 149, episode reward: 0.416, mean reward: 0.003 [-100.000, 13.026], mean action: 1.897 [0.000, 3.000], mean observation: 0.098 [-1.587, 1.424], loss: 14.211864, mae: 59.342930, mean_q: 78.279839
  366572/1100000: episode: 893, duration: 1.103s, episode steps: 165, steps per second: 150, episode reward: 31.400, mean reward: 0.190 [-100.000, 17.937], mean action: 1.612 [0.000, 3.000], mean observation: -0.096 [-0.910, 1.405], loss: 14.908080, mae: 58.387203, mean_q: 76.645073
  366778/1100000: episode: 894, duration: 1.376s, episode steps: 206, steps per second: 150, episode reward: -199.774, mean reward: -0.970 [-100.000, 5.589], mean action: 1.248 [0.000, 3.000], mean observation: 0.088 [-0.883, 1.539], loss: 16.010929, mae: 58.940151, mean_q: 78.022095
  367189/1100000: episode: 895, duration: 2.913s, episode steps: 411, steps per second: 141, episode reward: 226.841, mean reward: 0.552 [-17.476, 100.000], mean action: 2.331 [0.000, 3.000], mean observation: 0.213 [-0.767, 1.389], loss: 13.383136, mae: 59.027637, mean_q: 77.575005
  368189/1100000: episode: 896, duration: 7.411s, episode steps: 1000, steps per second: 135, episode reward: 41.497, mean reward: 0.041 [-23.851, 23.595], mean action: 1.165 [0.000, 3.000], mean observation: -0.005 [-0.874, 1.426], loss: 11.458531, mae: 59.249649, mean_q: 78.053337
  368328/1100000: episode: 897, duration: 0.930s, episode steps: 139, steps per second: 150, episode reward: -95.046, mean reward: -0.684 [-100.000, 5.382], mean action: 1.532 [0.000, 3.000], mean observation: -0.173 [-1.003, 1.431], loss: 14.266603, mae: 59.000717, mean_q: 78.101776
  368679/1100000: episode: 898, duration: 2.546s, episode steps: 351, steps per second: 138, episode reward: 212.598, mean reward: 0.606 [-19.793, 100.000], mean action: 1.983 [0.000, 3.000], mean observation: 0.113 [-0.929, 1.467], loss: 11.651163, mae: 59.549774, mean_q: 78.795372
  368970/1100000: episode: 899, duration: 2.010s, episode steps: 291, steps per second: 145, episode reward: 294.989, mean reward: 1.014 [-17.373, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.095 [-0.728, 1.408], loss: 13.400415, mae: 59.855953, mean_q: 78.687378
  369081/1100000: episode: 900, duration: 0.738s, episode steps: 111, steps per second: 150, episode reward: -224.393, mean reward: -2.022 [-100.000, 2.878], mean action: 1.937 [0.000, 3.000], mean observation: 0.055 [-1.090, 1.532], loss: 17.655228, mae: 59.697952, mean_q: 77.951248
  369526/1100000: episode: 901, duration: 3.120s, episode steps: 445, steps per second: 143, episode reward: 238.581, mean reward: 0.536 [-11.145, 100.000], mean action: 0.807 [0.000, 3.000], mean observation: 0.189 [-0.804, 1.395], loss: 13.307595, mae: 59.667828, mean_q: 78.412376
  369681/1100000: episode: 902, duration: 1.036s, episode steps: 155, steps per second: 150, episode reward: -22.602, mean reward: -0.146 [-100.000, 16.641], mean action: 1.361 [0.000, 3.000], mean observation: 0.084 [-2.600, 1.506], loss: 26.121233, mae: 59.488358, mean_q: 77.883049
  369907/1100000: episode: 903, duration: 1.525s, episode steps: 226, steps per second: 148, episode reward: -4.947, mean reward: -0.022 [-100.000, 8.440], mean action: 1.730 [0.000, 3.000], mean observation: 0.061 [-1.018, 1.575], loss: 14.702051, mae: 58.398308, mean_q: 77.392845
  370270/1100000: episode: 904, duration: 2.509s, episode steps: 363, steps per second: 145, episode reward: 196.724, mean reward: 0.542 [-19.199, 100.000], mean action: 2.196 [0.000, 3.000], mean observation: 0.142 [-0.805, 1.480], loss: 10.463202, mae: 59.099464, mean_q: 77.704803
  370829/1100000: episode: 905, duration: 4.021s, episode steps: 559, steps per second: 139, episode reward: -136.174, mean reward: -0.244 [-100.000, 14.274], mean action: 1.621 [0.000, 3.000], mean observation: 0.175 [-1.258, 1.470], loss: 10.630888, mae: 59.473991, mean_q: 78.546890
  370999/1100000: episode: 906, duration: 1.136s, episode steps: 170, steps per second: 150, episode reward: -30.411, mean reward: -0.179 [-100.000, 19.421], mean action: 1.859 [0.000, 3.000], mean observation: 0.010 [-1.629, 1.545], loss: 12.222389, mae: 60.015781, mean_q: 78.925095
  371092/1100000: episode: 907, duration: 0.653s, episode steps: 93, steps per second: 142, episode reward: -14.664, mean reward: -0.158 [-100.000, 8.816], mean action: 1.355 [0.000, 3.000], mean observation: 0.087 [-0.813, 1.398], loss: 18.973673, mae: 60.035648, mean_q: 78.262047
  371249/1100000: episode: 908, duration: 1.057s, episode steps: 157, steps per second: 148, episode reward: 10.696, mean reward: 0.068 [-100.000, 15.152], mean action: 1.605 [0.000, 3.000], mean observation: 0.055 [-0.971, 1.404], loss: 24.135733, mae: 60.274929, mean_q: 79.111725
  371569/1100000: episode: 909, duration: 2.193s, episode steps: 320, steps per second: 146, episode reward: 268.346, mean reward: 0.839 [-12.988, 100.000], mean action: 2.203 [0.000, 3.000], mean observation: 0.185 [-0.977, 1.452], loss: 16.836573, mae: 59.570709, mean_q: 78.727158
  371714/1100000: episode: 910, duration: 0.969s, episode steps: 145, steps per second: 150, episode reward: -13.360, mean reward: -0.092 [-100.000, 17.484], mean action: 1.634 [0.000, 3.000], mean observation: 0.073 [-1.523, 1.441], loss: 11.165672, mae: 59.399315, mean_q: 78.673195
  372180/1100000: episode: 911, duration: 3.310s, episode steps: 466, steps per second: 141, episode reward: 273.838, mean reward: 0.588 [-10.893, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.103 [-1.282, 1.395], loss: 16.824022, mae: 59.428947, mean_q: 77.885880
  372751/1100000: episode: 912, duration: 3.948s, episode steps: 571, steps per second: 145, episode reward: 262.677, mean reward: 0.460 [-18.472, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.128 [-0.704, 1.389], loss: 17.733274, mae: 59.507580, mean_q: 78.511566
  373234/1100000: episode: 913, duration: 3.348s, episode steps: 483, steps per second: 144, episode reward: 242.106, mean reward: 0.501 [-18.049, 100.000], mean action: 1.905 [0.000, 3.000], mean observation: 0.052 [-0.855, 1.390], loss: 18.107906, mae: 59.009575, mean_q: 78.013931
  373388/1100000: episode: 914, duration: 1.053s, episode steps: 154, steps per second: 146, episode reward: -133.879, mean reward: -0.869 [-100.000, 5.920], mean action: 2.000 [0.000, 3.000], mean observation: -0.168 [-1.009, 1.420], loss: 15.918083, mae: 58.803493, mean_q: 77.897705
  373463/1100000: episode: 915, duration: 0.505s, episode steps: 75, steps per second: 148, episode reward: 6.777, mean reward: 0.090 [-100.000, 20.151], mean action: 1.933 [0.000, 3.000], mean observation: 0.164 [-3.070, 1.387], loss: 12.204557, mae: 58.440189, mean_q: 77.401611
  373689/1100000: episode: 916, duration: 1.544s, episode steps: 226, steps per second: 146, episode reward: -226.830, mean reward: -1.004 [-100.000, 5.866], mean action: 1.544 [0.000, 3.000], mean observation: -0.021 [-0.755, 1.781], loss: 11.871768, mae: 59.075291, mean_q: 78.276413
  373978/1100000: episode: 917, duration: 1.986s, episode steps: 289, steps per second: 146, episode reward: 24.584, mean reward: 0.085 [-100.000, 15.398], mean action: 1.644 [0.000, 3.000], mean observation: -0.043 [-0.889, 1.495], loss: 10.538881, mae: 58.695042, mean_q: 77.917572
  374625/1100000: episode: 918, duration: 4.723s, episode steps: 647, steps per second: 137, episode reward: -255.422, mean reward: -0.395 [-100.000, 18.067], mean action: 1.620 [0.000, 3.000], mean observation: -0.068 [-0.881, 1.579], loss: 11.374432, mae: 59.238831, mean_q: 78.464653
  374983/1100000: episode: 919, duration: 2.501s, episode steps: 358, steps per second: 143, episode reward: 237.853, mean reward: 0.664 [-19.076, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.104 [-0.934, 1.395], loss: 11.492599, mae: 59.594864, mean_q: 78.874367
  375346/1100000: episode: 920, duration: 2.464s, episode steps: 363, steps per second: 147, episode reward: 253.042, mean reward: 0.697 [-18.728, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.225 [-0.909, 1.396], loss: 14.005544, mae: 59.775631, mean_q: 79.137657
  375818/1100000: episode: 921, duration: 3.301s, episode steps: 472, steps per second: 143, episode reward: 199.231, mean reward: 0.422 [-20.388, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.157 [-0.873, 1.414], loss: 16.513762, mae: 59.929234, mean_q: 79.477989
  376132/1100000: episode: 922, duration: 2.187s, episode steps: 314, steps per second: 144, episode reward: 237.638, mean reward: 0.757 [-18.341, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.200 [-0.682, 1.433], loss: 14.091315, mae: 60.023464, mean_q: 79.744614
  376228/1100000: episode: 923, duration: 0.648s, episode steps: 96, steps per second: 148, episode reward: 46.757, mean reward: 0.487 [-100.000, 22.735], mean action: 1.771 [0.000, 3.000], mean observation: 0.171 [-1.912, 1.389], loss: 7.904447, mae: 60.711597, mean_q: 80.592537
  376667/1100000: episode: 924, duration: 3.131s, episode steps: 439, steps per second: 140, episode reward: -229.577, mean reward: -0.523 [-100.000, 5.615], mean action: 1.692 [0.000, 3.000], mean observation: 0.194 [-0.833, 1.415], loss: 10.001430, mae: 59.689014, mean_q: 78.986015
  376841/1100000: episode: 925, duration: 1.198s, episode steps: 174, steps per second: 145, episode reward: -31.647, mean reward: -0.182 [-100.000, 19.229], mean action: 1.718 [0.000, 3.000], mean observation: -0.079 [-0.751, 1.535], loss: 9.232163, mae: 59.623989, mean_q: 79.157661
  376960/1100000: episode: 926, duration: 0.847s, episode steps: 119, steps per second: 140, episode reward: -46.956, mean reward: -0.395 [-100.000, 14.263], mean action: 1.689 [0.000, 3.000], mean observation: -0.033 [-1.451, 1.404], loss: 8.123843, mae: 60.173824, mean_q: 78.799255
  377678/1100000: episode: 927, duration: 5.445s, episode steps: 718, steps per second: 132, episode reward: 254.364, mean reward: 0.354 [-21.285, 100.000], mean action: 2.464 [0.000, 3.000], mean observation: 0.263 [-0.893, 1.523], loss: 12.347791, mae: 60.779480, mean_q: 80.288834
  377888/1100000: episode: 928, duration: 1.419s, episode steps: 210, steps per second: 148, episode reward: -179.606, mean reward: -0.855 [-100.000, 13.511], mean action: 1.805 [0.000, 3.000], mean observation: 0.109 [-5.253, 2.142], loss: 14.695178, mae: 60.526493, mean_q: 80.124184
  378316/1100000: episode: 929, duration: 3.043s, episode steps: 428, steps per second: 141, episode reward: 204.317, mean reward: 0.477 [-11.697, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: 0.160 [-0.720, 1.412], loss: 18.965225, mae: 60.598957, mean_q: 79.718056
  378785/1100000: episode: 930, duration: 3.284s, episode steps: 469, steps per second: 143, episode reward: 197.018, mean reward: 0.420 [-17.337, 100.000], mean action: 2.094 [0.000, 3.000], mean observation: 0.220 [-0.739, 1.402], loss: 10.782624, mae: 60.379345, mean_q: 79.499191
  379280/1100000: episode: 931, duration: 3.566s, episode steps: 495, steps per second: 139, episode reward: -144.010, mean reward: -0.291 [-100.000, 4.692], mean action: 1.388 [0.000, 3.000], mean observation: 0.204 [-0.730, 1.387], loss: 15.110325, mae: 59.971882, mean_q: 78.761948
  379562/1100000: episode: 932, duration: 1.981s, episode steps: 282, steps per second: 142, episode reward: -737.501, mean reward: -2.615 [-100.000, 5.566], mean action: 1.823 [0.000, 3.000], mean observation: 0.043 [-2.922, 2.672], loss: 9.814121, mae: 60.218624, mean_q: 78.877892
  379706/1100000: episode: 933, duration: 0.976s, episode steps: 144, steps per second: 148, episode reward: 34.047, mean reward: 0.236 [-100.000, 16.233], mean action: 1.750 [0.000, 3.000], mean observation: 0.098 [-0.577, 1.408], loss: 14.650416, mae: 60.667629, mean_q: 79.829826
  380202/1100000: episode: 934, duration: 3.508s, episode steps: 496, steps per second: 141, episode reward: 223.237, mean reward: 0.450 [-17.471, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.224 [-0.763, 1.462], loss: 10.630342, mae: 60.281048, mean_q: 79.195717
  380489/1100000: episode: 935, duration: 1.961s, episode steps: 287, steps per second: 146, episode reward: 232.634, mean reward: 0.811 [-11.031, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.197 [-0.731, 1.454], loss: 12.408918, mae: 60.339954, mean_q: 78.859222
  380737/1100000: episode: 936, duration: 1.695s, episode steps: 248, steps per second: 146, episode reward: 62.069, mean reward: 0.250 [-100.000, 14.764], mean action: 1.806 [0.000, 3.000], mean observation: 0.105 [-1.008, 1.466], loss: 25.554008, mae: 60.227219, mean_q: 78.910896
  381124/1100000: episode: 937, duration: 2.661s, episode steps: 387, steps per second: 145, episode reward: 244.237, mean reward: 0.631 [-17.792, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.226 [-0.819, 1.509], loss: 9.225974, mae: 60.497734, mean_q: 79.752457
  381397/1100000: episode: 938, duration: 1.915s, episode steps: 273, steps per second: 143, episode reward: 202.255, mean reward: 0.741 [-17.061, 100.000], mean action: 2.136 [0.000, 3.000], mean observation: 0.159 [-0.783, 1.398], loss: 14.550644, mae: 60.068001, mean_q: 78.443390
  381656/1100000: episode: 939, duration: 1.761s, episode steps: 259, steps per second: 147, episode reward: 260.364, mean reward: 1.005 [-12.680, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.192 [-0.821, 1.518], loss: 8.752731, mae: 60.773724, mean_q: 79.826599
  381776/1100000: episode: 940, duration: 0.805s, episode steps: 120, steps per second: 149, episode reward: 19.564, mean reward: 0.163 [-100.000, 8.742], mean action: 1.650 [0.000, 3.000], mean observation: 0.016 [-0.824, 1.391], loss: 12.912240, mae: 60.341961, mean_q: 79.184914
  381903/1100000: episode: 941, duration: 0.860s, episode steps: 127, steps per second: 148, episode reward: -55.747, mean reward: -0.439 [-100.000, 14.258], mean action: 1.835 [0.000, 3.000], mean observation: 0.071 [-0.783, 1.720], loss: 10.311748, mae: 61.131157, mean_q: 80.229507
  382903/1100000: episode: 942, duration: 7.703s, episode steps: 1000, steps per second: 130, episode reward: 105.130, mean reward: 0.105 [-19.998, 22.268], mean action: 1.204 [0.000, 3.000], mean observation: 0.169 [-0.775, 1.393], loss: 12.039191, mae: 60.826595, mean_q: 80.083878
  383002/1100000: episode: 943, duration: 0.668s, episode steps: 99, steps per second: 148, episode reward: -5.694, mean reward: -0.058 [-100.000, 11.427], mean action: 1.879 [0.000, 3.000], mean observation: 0.131 [-0.805, 1.742], loss: 9.708520, mae: 60.517128, mean_q: 79.439613
  383209/1100000: episode: 944, duration: 1.396s, episode steps: 207, steps per second: 148, episode reward: 323.535, mean reward: 1.563 [-9.213, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.036 [-0.898, 1.386], loss: 15.795875, mae: 60.867027, mean_q: 80.474411
  384209/1100000: episode: 945, duration: 7.400s, episode steps: 1000, steps per second: 135, episode reward: 50.936, mean reward: 0.051 [-19.731, 25.347], mean action: 1.527 [0.000, 3.000], mean observation: -0.007 [-1.005, 1.573], loss: 13.811528, mae: 60.970989, mean_q: 80.410103
  385209/1100000: episode: 946, duration: 7.424s, episode steps: 1000, steps per second: 135, episode reward: 57.395, mean reward: 0.057 [-21.500, 22.679], mean action: 1.805 [0.000, 3.000], mean observation: 0.005 [-0.946, 1.402], loss: 13.092199, mae: 61.672104, mean_q: 81.355064
  385338/1100000: episode: 947, duration: 0.924s, episode steps: 129, steps per second: 140, episode reward: 32.451, mean reward: 0.252 [-100.000, 10.986], mean action: 1.915 [0.000, 3.000], mean observation: 0.147 [-0.881, 1.403], loss: 11.979513, mae: 61.804737, mean_q: 80.871773
  385605/1100000: episode: 948, duration: 1.856s, episode steps: 267, steps per second: 144, episode reward: 258.825, mean reward: 0.969 [-10.121, 100.000], mean action: 1.970 [0.000, 3.000], mean observation: 0.107 [-0.678, 1.390], loss: 20.391800, mae: 61.422432, mean_q: 80.675735
  385866/1100000: episode: 949, duration: 1.755s, episode steps: 261, steps per second: 149, episode reward: 248.659, mean reward: 0.953 [-9.562, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.055 [-0.824, 1.637], loss: 14.460638, mae: 61.181538, mean_q: 80.183502
  386121/1100000: episode: 950, duration: 1.727s, episode steps: 255, steps per second: 148, episode reward: 265.728, mean reward: 1.042 [-17.973, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.051 [-0.571, 1.418], loss: 11.128939, mae: 61.642609, mean_q: 81.481003
  386999/1100000: episode: 951, duration: 6.544s, episode steps: 878, steps per second: 134, episode reward: 134.647, mean reward: 0.153 [-19.782, 100.000], mean action: 1.918 [0.000, 3.000], mean observation: -0.038 [-0.822, 1.397], loss: 13.003746, mae: 61.353947, mean_q: 80.707588
  387098/1100000: episode: 952, duration: 0.656s, episode steps: 99, steps per second: 151, episode reward: -76.722, mean reward: -0.775 [-100.000, 9.703], mean action: 1.444 [0.000, 3.000], mean observation: -0.073 [-0.813, 2.866], loss: 8.579259, mae: 60.420204, mean_q: 80.114906
  387372/1100000: episode: 953, duration: 1.856s, episode steps: 274, steps per second: 148, episode reward: 258.882, mean reward: 0.945 [-3.213, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.130 [-0.776, 1.474], loss: 13.134419, mae: 60.763714, mean_q: 80.249947
  387738/1100000: episode: 954, duration: 2.585s, episode steps: 366, steps per second: 142, episode reward: 186.392, mean reward: 0.509 [-20.923, 100.000], mean action: 1.978 [0.000, 3.000], mean observation: 0.135 [-0.996, 1.407], loss: 13.837687, mae: 60.435001, mean_q: 80.080177
  387880/1100000: episode: 955, duration: 0.955s, episode steps: 142, steps per second: 149, episode reward: -26.157, mean reward: -0.184 [-100.000, 11.533], mean action: 1.599 [0.000, 3.000], mean observation: 0.109 [-0.856, 1.415], loss: 17.610254, mae: 60.011169, mean_q: 79.104202
  388473/1100000: episode: 956, duration: 4.214s, episode steps: 593, steps per second: 141, episode reward: 251.356, mean reward: 0.424 [-18.325, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.098 [-0.688, 1.396], loss: 12.102536, mae: 60.205681, mean_q: 79.501030
  389037/1100000: episode: 957, duration: 4.201s, episode steps: 564, steps per second: 134, episode reward: 243.885, mean reward: 0.432 [-19.665, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.076 [-0.568, 1.421], loss: 12.339582, mae: 60.030224, mean_q: 79.421539
  389849/1100000: episode: 958, duration: 5.816s, episode steps: 812, steps per second: 140, episode reward: 214.661, mean reward: 0.264 [-20.273, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.028 [-0.671, 1.393], loss: 15.846476, mae: 60.449463, mean_q: 79.866402
  390185/1100000: episode: 959, duration: 2.388s, episode steps: 336, steps per second: 141, episode reward: 224.795, mean reward: 0.669 [-17.532, 100.000], mean action: 2.122 [0.000, 3.000], mean observation: 0.129 [-0.609, 1.399], loss: 12.392168, mae: 60.947136, mean_q: 80.597008
  390381/1100000: episode: 960, duration: 1.329s, episode steps: 196, steps per second: 147, episode reward: -95.844, mean reward: -0.489 [-100.000, 29.481], mean action: 1.699 [0.000, 3.000], mean observation: -0.007 [-1.703, 1.412], loss: 17.592861, mae: 60.323730, mean_q: 79.740097
  390801/1100000: episode: 961, duration: 2.965s, episode steps: 420, steps per second: 142, episode reward: -184.054, mean reward: -0.438 [-100.000, 13.121], mean action: 1.605 [0.000, 3.000], mean observation: 0.028 [-0.726, 1.466], loss: 16.746246, mae: 60.764751, mean_q: 80.904968
  390974/1100000: episode: 962, duration: 1.162s, episode steps: 173, steps per second: 149, episode reward: -138.279, mean reward: -0.799 [-100.000, 13.925], mean action: 1.954 [0.000, 3.000], mean observation: -0.113 [-1.256, 1.386], loss: 11.331656, mae: 59.868847, mean_q: 78.920532
  391643/1100000: episode: 963, duration: 4.912s, episode steps: 669, steps per second: 136, episode reward: 199.963, mean reward: 0.299 [-21.201, 100.000], mean action: 2.036 [0.000, 3.000], mean observation: 0.170 [-0.728, 1.469], loss: 18.242315, mae: 61.366894, mean_q: 81.306923
  391911/1100000: episode: 964, duration: 1.840s, episode steps: 268, steps per second: 146, episode reward: 229.154, mean reward: 0.855 [-18.360, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.072 [-0.874, 1.435], loss: 19.979853, mae: 61.493660, mean_q: 81.679573
  392911/1100000: episode: 965, duration: 8.129s, episode steps: 1000, steps per second: 123, episode reward: 13.986, mean reward: 0.014 [-19.807, 21.142], mean action: 1.498 [0.000, 3.000], mean observation: -0.043 [-0.942, 1.393], loss: 14.457398, mae: 60.459747, mean_q: 80.339600
  393050/1100000: episode: 966, duration: 0.937s, episode steps: 139, steps per second: 148, episode reward: 34.183, mean reward: 0.246 [-100.000, 19.092], mean action: 1.849 [0.000, 3.000], mean observation: 0.128 [-0.842, 1.398], loss: 10.228315, mae: 59.638088, mean_q: 78.503502
  393517/1100000: episode: 967, duration: 3.338s, episode steps: 467, steps per second: 140, episode reward: 251.623, mean reward: 0.539 [-19.434, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.097 [-1.060, 1.425], loss: 11.120071, mae: 59.844574, mean_q: 79.677460
  393961/1100000: episode: 968, duration: 3.158s, episode steps: 444, steps per second: 141, episode reward: -49.505, mean reward: -0.111 [-100.000, 13.559], mean action: 1.680 [0.000, 3.000], mean observation: -0.051 [-0.815, 1.471], loss: 10.459446, mae: 60.342388, mean_q: 80.215828
  394349/1100000: episode: 969, duration: 2.716s, episode steps: 388, steps per second: 143, episode reward: 285.167, mean reward: 0.735 [-17.468, 100.000], mean action: 2.327 [0.000, 3.000], mean observation: 0.033 [-0.949, 1.627], loss: 10.864456, mae: 59.841866, mean_q: 79.365936
  395349/1100000: episode: 970, duration: 7.574s, episode steps: 1000, steps per second: 132, episode reward: 20.220, mean reward: 0.020 [-24.460, 24.878], mean action: 1.629 [0.000, 3.000], mean observation: 0.230 [-0.777, 1.402], loss: 15.860090, mae: 60.444931, mean_q: 80.310516
  395475/1100000: episode: 971, duration: 0.853s, episode steps: 126, steps per second: 148, episode reward: 50.575, mean reward: 0.401 [-100.000, 22.666], mean action: 1.873 [0.000, 3.000], mean observation: 0.185 [-0.740, 1.390], loss: 8.296422, mae: 60.245930, mean_q: 80.387238
  395742/1100000: episode: 972, duration: 1.822s, episode steps: 267, steps per second: 147, episode reward: 218.259, mean reward: 0.817 [-9.080, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: 0.141 [-0.564, 1.397], loss: 14.375172, mae: 60.488968, mean_q: 80.474121
  396742/1100000: episode: 973, duration: 7.417s, episode steps: 1000, steps per second: 135, episode reward: 116.256, mean reward: 0.116 [-19.020, 23.155], mean action: 1.406 [0.000, 3.000], mean observation: 0.181 [-0.770, 1.436], loss: 12.824461, mae: 60.449150, mean_q: 80.426376
  397073/1100000: episode: 974, duration: 2.250s, episode steps: 331, steps per second: 147, episode reward: 266.274, mean reward: 0.804 [-17.652, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.253 [-0.760, 1.430], loss: 12.796999, mae: 60.264748, mean_q: 80.246796
  397232/1100000: episode: 975, duration: 1.067s, episode steps: 159, steps per second: 149, episode reward: 51.805, mean reward: 0.326 [-100.000, 19.040], mean action: 1.648 [0.000, 3.000], mean observation: 0.084 [-0.749, 1.457], loss: 9.861046, mae: 60.449402, mean_q: 80.318962
  397681/1100000: episode: 976, duration: 3.112s, episode steps: 449, steps per second: 144, episode reward: 265.631, mean reward: 0.592 [-17.488, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.081 [-0.843, 1.490], loss: 12.963597, mae: 60.233879, mean_q: 79.784157
  398209/1100000: episode: 977, duration: 3.835s, episode steps: 528, steps per second: 138, episode reward: 217.136, mean reward: 0.411 [-7.459, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.208 [-0.619, 1.416], loss: 18.496311, mae: 60.770256, mean_q: 80.671913
  398552/1100000: episode: 978, duration: 2.386s, episode steps: 343, steps per second: 144, episode reward: -12.776, mean reward: -0.037 [-100.000, 18.741], mean action: 1.790 [0.000, 3.000], mean observation: 0.130 [-0.520, 1.402], loss: 12.163946, mae: 60.851849, mean_q: 80.878448
  398749/1100000: episode: 979, duration: 1.336s, episode steps: 197, steps per second: 147, episode reward: -134.552, mean reward: -0.683 [-100.000, 49.040], mean action: 1.761 [0.000, 3.000], mean observation: 0.033 [-0.906, 1.784], loss: 14.537387, mae: 60.631294, mean_q: 80.779236
  399043/1100000: episode: 980, duration: 2.011s, episode steps: 294, steps per second: 146, episode reward: 244.215, mean reward: 0.831 [-7.319, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.047 [-0.870, 1.416], loss: 14.239549, mae: 61.363041, mean_q: 81.101921
  399315/1100000: episode: 981, duration: 1.853s, episode steps: 272, steps per second: 147, episode reward: 258.014, mean reward: 0.949 [-18.248, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.089 [-0.836, 1.407], loss: 11.873207, mae: 61.464355, mean_q: 81.799957
  399578/1100000: episode: 982, duration: 1.819s, episode steps: 263, steps per second: 145, episode reward: 273.572, mean reward: 1.040 [-4.077, 100.000], mean action: 1.852 [0.000, 3.000], mean observation: 0.133 [-0.835, 1.388], loss: 12.376826, mae: 61.636173, mean_q: 82.156303
  400092/1100000: episode: 983, duration: 3.505s, episode steps: 514, steps per second: 147, episode reward: -91.661, mean reward: -0.178 [-100.000, 20.458], mean action: 0.947 [0.000, 3.000], mean observation: 0.097 [-1.068, 1.517], loss: 13.609987, mae: 61.806187, mean_q: 82.290009
  400322/1100000: episode: 984, duration: 1.576s, episode steps: 230, steps per second: 146, episode reward: 268.981, mean reward: 1.169 [-19.151, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.084 [-0.767, 1.388], loss: 11.638206, mae: 62.424526, mean_q: 83.244240
  400667/1100000: episode: 985, duration: 2.405s, episode steps: 345, steps per second: 143, episode reward: 201.733, mean reward: 0.585 [-17.892, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.021 [-0.891, 1.435], loss: 10.388199, mae: 62.359566, mean_q: 83.236145
  401321/1100000: episode: 986, duration: 4.703s, episode steps: 654, steps per second: 139, episode reward: 185.247, mean reward: 0.283 [-23.670, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.239 [-0.890, 1.430], loss: 14.265780, mae: 62.439404, mean_q: 83.149574
  401613/1100000: episode: 987, duration: 1.979s, episode steps: 292, steps per second: 148, episode reward: 9.034, mean reward: 0.031 [-100.000, 13.297], mean action: 1.740 [0.000, 3.000], mean observation: -0.027 [-0.663, 1.408], loss: 13.075774, mae: 61.917522, mean_q: 82.640984
  401895/1100000: episode: 988, duration: 1.936s, episode steps: 282, steps per second: 146, episode reward: 213.398, mean reward: 0.757 [-18.986, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: -0.054 [-0.934, 1.410], loss: 13.287485, mae: 61.844414, mean_q: 81.931984
  402251/1100000: episode: 989, duration: 2.537s, episode steps: 356, steps per second: 140, episode reward: 244.259, mean reward: 0.686 [-20.540, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.159 [-0.894, 1.387], loss: 14.460139, mae: 62.285446, mean_q: 82.603065
  402398/1100000: episode: 990, duration: 1.043s, episode steps: 147, steps per second: 141, episode reward: -213.269, mean reward: -1.451 [-100.000, 5.750], mean action: 1.864 [0.000, 3.000], mean observation: -0.003 [-1.295, 3.177], loss: 10.859393, mae: 62.559299, mean_q: 83.327805
  402731/1100000: episode: 991, duration: 2.280s, episode steps: 333, steps per second: 146, episode reward: 252.673, mean reward: 0.759 [-19.713, 100.000], mean action: 1.691 [0.000, 3.000], mean observation: 0.079 [-0.667, 1.426], loss: 14.986481, mae: 62.623943, mean_q: 83.291298
  402914/1100000: episode: 992, duration: 1.232s, episode steps: 183, steps per second: 149, episode reward: -153.821, mean reward: -0.841 [-100.000, 3.768], mean action: 1.852 [0.000, 3.000], mean observation: -0.092 [-1.410, 1.389], loss: 6.879381, mae: 62.444790, mean_q: 83.027985
  403183/1100000: episode: 993, duration: 1.837s, episode steps: 269, steps per second: 146, episode reward: 236.563, mean reward: 0.879 [-18.399, 100.000], mean action: 2.041 [0.000, 3.000], mean observation: -0.017 [-0.910, 1.521], loss: 10.249523, mae: 62.315693, mean_q: 83.217453
  403790/1100000: episode: 994, duration: 4.377s, episode steps: 607, steps per second: 139, episode reward: -100.006, mean reward: -0.165 [-100.000, 21.314], mean action: 1.573 [0.000, 3.000], mean observation: 0.153 [-0.787, 1.440], loss: 12.145327, mae: 62.292526, mean_q: 83.174980
  404142/1100000: episode: 995, duration: 2.431s, episode steps: 352, steps per second: 145, episode reward: 288.228, mean reward: 0.819 [-18.851, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.081 [-0.695, 1.398], loss: 11.330013, mae: 62.261814, mean_q: 82.632172
  405142/1100000: episode: 996, duration: 7.854s, episode steps: 1000, steps per second: 127, episode reward: -109.202, mean reward: -0.109 [-5.193, 5.237], mean action: 1.742 [0.000, 3.000], mean observation: 0.148 [-0.790, 1.416], loss: 13.469586, mae: 61.687088, mean_q: 82.257233
  405868/1100000: episode: 997, duration: 5.249s, episode steps: 726, steps per second: 138, episode reward: -262.089, mean reward: -0.361 [-100.000, 26.403], mean action: 1.421 [0.000, 3.000], mean observation: 0.078 [-4.248, 1.536], loss: 10.451269, mae: 61.944420, mean_q: 82.709511
  406868/1100000: episode: 998, duration: 7.622s, episode steps: 1000, steps per second: 131, episode reward: 80.616, mean reward: 0.081 [-19.851, 20.878], mean action: 1.637 [0.000, 3.000], mean observation: 0.219 [-0.760, 1.398], loss: 13.333367, mae: 61.537666, mean_q: 81.687363
  406965/1100000: episode: 999, duration: 0.647s, episode steps: 97, steps per second: 150, episode reward: -47.299, mean reward: -0.488 [-100.000, 13.249], mean action: 1.206 [0.000, 3.000], mean observation: 0.061 [-1.600, 1.410], loss: 6.238829, mae: 61.715919, mean_q: 81.881264
  407616/1100000: episode: 1000, duration: 4.861s, episode steps: 651, steps per second: 134, episode reward: 172.731, mean reward: 0.265 [-19.321, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.169 [-0.731, 1.425], loss: 10.595305, mae: 61.748116, mean_q: 82.093735
  408616/1100000: episode: 1001, duration: 8.308s, episode steps: 1000, steps per second: 120, episode reward: -56.048, mean reward: -0.056 [-4.542, 5.637], mean action: 1.708 [0.000, 3.000], mean observation: 0.184 [-0.612, 1.394], loss: 12.616470, mae: 61.331062, mean_q: 81.410957
  409417/1100000: episode: 1002, duration: 5.886s, episode steps: 801, steps per second: 136, episode reward: 160.380, mean reward: 0.200 [-24.644, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.143 [-0.550, 1.471], loss: 9.097383, mae: 60.598423, mean_q: 80.601151
  409608/1100000: episode: 1003, duration: 1.297s, episode steps: 191, steps per second: 147, episode reward: 279.858, mean reward: 1.465 [-10.523, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.061 [-0.681, 1.388], loss: 13.389711, mae: 60.524326, mean_q: 79.581436
  409988/1100000: episode: 1004, duration: 2.604s, episode steps: 380, steps per second: 146, episode reward: 182.391, mean reward: 0.480 [-18.570, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.006 [-0.772, 1.410], loss: 12.649567, mae: 60.412521, mean_q: 80.176468
  410988/1100000: episode: 1005, duration: 8.027s, episode steps: 1000, steps per second: 125, episode reward: -89.519, mean reward: -0.090 [-4.817, 4.903], mean action: 1.856 [0.000, 3.000], mean observation: 0.191 [-0.696, 1.434], loss: 14.333112, mae: 60.720463, mean_q: 80.595421
  411962/1100000: episode: 1006, duration: 7.834s, episode steps: 974, steps per second: 124, episode reward: 106.747, mean reward: 0.110 [-15.137, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.159 [-0.746, 1.404], loss: 11.942513, mae: 60.308651, mean_q: 79.886864
  412207/1100000: episode: 1007, duration: 1.659s, episode steps: 245, steps per second: 148, episode reward: 278.818, mean reward: 1.138 [-7.622, 100.000], mean action: 1.886 [0.000, 3.000], mean observation: -0.038 [-0.764, 1.495], loss: 9.335677, mae: 60.227478, mean_q: 79.974487
  413207/1100000: episode: 1008, duration: 8.626s, episode steps: 1000, steps per second: 116, episode reward: -32.121, mean reward: -0.032 [-5.130, 5.248], mean action: 1.859 [0.000, 3.000], mean observation: 0.187 [-0.742, 1.395], loss: 31.039589, mae: 60.429890, mean_q: 79.942551
  413589/1100000: episode: 1009, duration: 2.566s, episode steps: 382, steps per second: 149, episode reward: 240.455, mean reward: 0.629 [-17.999, 100.000], mean action: 0.916 [0.000, 3.000], mean observation: 0.220 [-0.698, 1.391], loss: 11.653428, mae: 60.205395, mean_q: 80.108467
  414004/1100000: episode: 1010, duration: 2.892s, episode steps: 415, steps per second: 144, episode reward: 244.541, mean reward: 0.589 [-17.443, 100.000], mean action: 0.836 [0.000, 3.000], mean observation: 0.108 [-0.705, 1.412], loss: 14.152539, mae: 60.024780, mean_q: 78.786896
  414536/1100000: episode: 1011, duration: 3.892s, episode steps: 532, steps per second: 137, episode reward: 228.108, mean reward: 0.429 [-11.522, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.175 [-0.712, 1.388], loss: 7.337540, mae: 59.210987, mean_q: 78.495872
  414831/1100000: episode: 1012, duration: 2.037s, episode steps: 295, steps per second: 145, episode reward: 308.486, mean reward: 1.046 [-17.661, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.076 [-0.722, 1.511], loss: 11.442677, mae: 59.411346, mean_q: 78.759087
  415662/1100000: episode: 1013, duration: 6.114s, episode steps: 831, steps per second: 136, episode reward: 232.061, mean reward: 0.279 [-19.516, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.233 [-0.701, 1.387], loss: 9.037976, mae: 58.565063, mean_q: 77.696274
  416171/1100000: episode: 1014, duration: 3.511s, episode steps: 509, steps per second: 145, episode reward: 231.658, mean reward: 0.455 [-19.174, 100.000], mean action: 1.754 [0.000, 3.000], mean observation: 0.031 [-0.600, 1.396], loss: 8.757141, mae: 57.872566, mean_q: 76.705849
  417171/1100000: episode: 1015, duration: 7.319s, episode steps: 1000, steps per second: 137, episode reward: 56.306, mean reward: 0.056 [-20.127, 21.955], mean action: 1.781 [0.000, 3.000], mean observation: 0.243 [-0.954, 1.522], loss: 13.158307, mae: 58.219860, mean_q: 76.907562
  418171/1100000: episode: 1016, duration: 7.496s, episode steps: 1000, steps per second: 133, episode reward: 25.862, mean reward: 0.026 [-18.297, 14.249], mean action: 1.519 [0.000, 3.000], mean observation: 0.183 [-0.635, 1.403], loss: 9.416988, mae: 58.027641, mean_q: 76.955421
  418647/1100000: episode: 1017, duration: 3.259s, episode steps: 476, steps per second: 146, episode reward: 209.340, mean reward: 0.440 [-20.064, 100.000], mean action: 1.769 [0.000, 3.000], mean observation: 0.067 [-0.965, 1.498], loss: 11.263474, mae: 57.834648, mean_q: 76.531433
  418960/1100000: episode: 1018, duration: 2.137s, episode steps: 313, steps per second: 146, episode reward: 210.103, mean reward: 0.671 [-13.142, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: 0.182 [-1.267, 1.412], loss: 13.612320, mae: 58.139095, mean_q: 77.362129
  419387/1100000: episode: 1019, duration: 3.009s, episode steps: 427, steps per second: 142, episode reward: 275.054, mean reward: 0.644 [-17.370, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.170 [-1.042, 1.391], loss: 8.545461, mae: 57.715065, mean_q: 76.490448
  420387/1100000: episode: 1020, duration: 7.163s, episode steps: 1000, steps per second: 140, episode reward: 78.402, mean reward: 0.078 [-18.342, 21.154], mean action: 0.815 [0.000, 3.000], mean observation: 0.221 [-0.681, 1.399], loss: 9.753364, mae: 57.757370, mean_q: 76.812469
  421066/1100000: episode: 1021, duration: 4.753s, episode steps: 679, steps per second: 143, episode reward: 212.152, mean reward: 0.312 [-19.455, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.210 [-0.874, 1.489], loss: 7.496805, mae: 57.317219, mean_q: 75.993523
  421551/1100000: episode: 1022, duration: 3.353s, episode steps: 485, steps per second: 145, episode reward: -169.007, mean reward: -0.348 [-100.000, 11.171], mean action: 1.291 [0.000, 3.000], mean observation: 0.031 [-0.754, 1.401], loss: 9.896781, mae: 57.507553, mean_q: 76.433067
  422551/1100000: episode: 1023, duration: 7.722s, episode steps: 1000, steps per second: 130, episode reward: -29.185, mean reward: -0.029 [-17.961, 15.230], mean action: 1.519 [0.000, 3.000], mean observation: 0.158 [-0.661, 1.394], loss: 8.545463, mae: 57.115688, mean_q: 75.770241
  422837/1100000: episode: 1024, duration: 1.925s, episode steps: 286, steps per second: 149, episode reward: 238.946, mean reward: 0.835 [-17.699, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: -0.017 [-0.783, 1.442], loss: 8.519337, mae: 56.884209, mean_q: 75.614403
  423263/1100000: episode: 1025, duration: 2.964s, episode steps: 426, steps per second: 144, episode reward: 246.207, mean reward: 0.578 [-18.115, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.151 [-0.748, 1.439], loss: 10.102835, mae: 57.107372, mean_q: 75.902512
  423821/1100000: episode: 1026, duration: 3.945s, episode steps: 558, steps per second: 141, episode reward: 215.337, mean reward: 0.386 [-11.178, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: 0.169 [-1.054, 1.532], loss: 9.432049, mae: 56.992146, mean_q: 75.811646
  424108/1100000: episode: 1027, duration: 1.955s, episode steps: 287, steps per second: 147, episode reward: 219.191, mean reward: 0.764 [-12.687, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.192 [-0.914, 1.433], loss: 5.831338, mae: 56.715618, mean_q: 75.436966
  424301/1100000: episode: 1028, duration: 1.303s, episode steps: 193, steps per second: 148, episode reward: -2.794, mean reward: -0.014 [-100.000, 17.925], mean action: 1.808 [0.000, 3.000], mean observation: -0.081 [-0.964, 1.947], loss: 7.979434, mae: 56.442875, mean_q: 75.209984
  424621/1100000: episode: 1029, duration: 2.168s, episode steps: 320, steps per second: 148, episode reward: 250.395, mean reward: 0.782 [-19.145, 100.000], mean action: 1.581 [0.000, 3.000], mean observation: 0.086 [-0.707, 1.408], loss: 8.092852, mae: 56.667145, mean_q: 75.622330
  424876/1100000: episode: 1030, duration: 1.751s, episode steps: 255, steps per second: 146, episode reward: 230.635, mean reward: 0.904 [-14.980, 100.000], mean action: 2.235 [0.000, 3.000], mean observation: 0.031 [-0.944, 1.612], loss: 9.337830, mae: 56.628433, mean_q: 75.508972
  425102/1100000: episode: 1031, duration: 1.514s, episode steps: 226, steps per second: 149, episode reward: 256.936, mean reward: 1.137 [-12.353, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.081 [-0.631, 1.491], loss: 7.610840, mae: 56.459335, mean_q: 75.170097
  425524/1100000: episode: 1032, duration: 2.933s, episode steps: 422, steps per second: 144, episode reward: 191.277, mean reward: 0.453 [-22.352, 100.000], mean action: 1.955 [0.000, 3.000], mean observation: 0.184 [-0.683, 1.409], loss: 11.241928, mae: 56.804798, mean_q: 75.516464
  426524/1100000: episode: 1033, duration: 8.215s, episode steps: 1000, steps per second: 122, episode reward: 30.907, mean reward: 0.031 [-24.441, 25.700], mean action: 1.496 [0.000, 3.000], mean observation: 0.105 [-0.757, 1.392], loss: 9.640966, mae: 57.158916, mean_q: 76.160164
  426761/1100000: episode: 1034, duration: 1.601s, episode steps: 237, steps per second: 148, episode reward: -117.889, mean reward: -0.497 [-100.000, 5.713], mean action: 1.747 [0.000, 3.000], mean observation: 0.074 [-1.035, 3.809], loss: 7.743134, mae: 57.164120, mean_q: 76.181961
  427301/1100000: episode: 1035, duration: 3.818s, episode steps: 540, steps per second: 141, episode reward: 212.275, mean reward: 0.393 [-20.008, 100.000], mean action: 1.611 [0.000, 3.000], mean observation: 0.219 [-0.787, 1.402], loss: 14.019658, mae: 57.307472, mean_q: 76.468903
  427911/1100000: episode: 1036, duration: 4.324s, episode steps: 610, steps per second: 141, episode reward: 249.300, mean reward: 0.409 [-19.816, 100.000], mean action: 0.831 [0.000, 3.000], mean observation: 0.021 [-0.755, 1.503], loss: 11.658001, mae: 57.136871, mean_q: 76.454628
  428911/1100000: episode: 1037, duration: 7.890s, episode steps: 1000, steps per second: 127, episode reward: 56.289, mean reward: 0.056 [-23.920, 28.380], mean action: 1.476 [0.000, 3.000], mean observation: 0.135 [-0.596, 1.395], loss: 7.770164, mae: 56.886974, mean_q: 75.849678
  429407/1100000: episode: 1038, duration: 3.755s, episode steps: 496, steps per second: 132, episode reward: 178.229, mean reward: 0.359 [-19.186, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.167 [-0.791, 1.405], loss: 9.410806, mae: 56.678310, mean_q: 75.837143
  429985/1100000: episode: 1039, duration: 4.077s, episode steps: 578, steps per second: 142, episode reward: 236.235, mean reward: 0.409 [-23.740, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.053 [-0.883, 1.457], loss: 9.297619, mae: 56.431480, mean_q: 75.372887
  430148/1100000: episode: 1040, duration: 1.080s, episode steps: 163, steps per second: 151, episode reward: -0.873, mean reward: -0.005 [-100.000, 17.076], mean action: 1.460 [0.000, 3.000], mean observation: -0.082 [-0.686, 1.504], loss: 10.805411, mae: 56.947243, mean_q: 75.699257
  431148/1100000: episode: 1041, duration: 7.645s, episode steps: 1000, steps per second: 131, episode reward: 154.815, mean reward: 0.155 [-19.716, 14.190], mean action: 1.454 [0.000, 3.000], mean observation: 0.223 [-1.093, 1.664], loss: 7.804109, mae: 55.819202, mean_q: 74.551414
  431391/1100000: episode: 1042, duration: 1.643s, episode steps: 243, steps per second: 148, episode reward: -171.590, mean reward: -0.706 [-100.000, 7.666], mean action: 1.646 [0.000, 3.000], mean observation: 0.021 [-0.898, 2.391], loss: 10.444531, mae: 55.143059, mean_q: 73.876831
  431749/1100000: episode: 1043, duration: 2.481s, episode steps: 358, steps per second: 144, episode reward: 235.595, mean reward: 0.658 [-17.602, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.184 [-0.728, 1.464], loss: 9.654678, mae: 54.359562, mean_q: 72.304611
  431856/1100000: episode: 1044, duration: 0.719s, episode steps: 107, steps per second: 149, episode reward: 6.438, mean reward: 0.060 [-100.000, 11.619], mean action: 1.701 [0.000, 3.000], mean observation: -0.036 [-0.824, 1.399], loss: 6.834768, mae: 55.222858, mean_q: 73.422462
  432324/1100000: episode: 1045, duration: 3.413s, episode steps: 468, steps per second: 137, episode reward: 229.079, mean reward: 0.489 [-17.844, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.179 [-0.628, 1.388], loss: 11.706731, mae: 54.804394, mean_q: 73.251312
  432731/1100000: episode: 1046, duration: 2.902s, episode steps: 407, steps per second: 140, episode reward: 243.071, mean reward: 0.597 [-17.230, 100.000], mean action: 2.383 [0.000, 3.000], mean observation: 0.194 [-0.947, 1.445], loss: 9.746595, mae: 54.821301, mean_q: 73.362633
  432926/1100000: episode: 1047, duration: 1.310s, episode steps: 195, steps per second: 149, episode reward: -52.595, mean reward: -0.270 [-100.000, 12.508], mean action: 1.672 [0.000, 3.000], mean observation: -0.046 [-0.949, 1.396], loss: 9.070333, mae: 54.928795, mean_q: 73.528275
  433192/1100000: episode: 1048, duration: 1.815s, episode steps: 266, steps per second: 147, episode reward: -181.116, mean reward: -0.681 [-100.000, 17.972], mean action: 1.568 [0.000, 3.000], mean observation: 0.073 [-1.619, 1.400], loss: 9.938417, mae: 55.012184, mean_q: 73.727837
  433638/1100000: episode: 1049, duration: 3.160s, episode steps: 446, steps per second: 141, episode reward: 199.639, mean reward: 0.448 [-12.605, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: 0.143 [-0.565, 1.395], loss: 10.830668, mae: 55.139076, mean_q: 74.102325
  433749/1100000: episode: 1050, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: 25.383, mean reward: 0.229 [-100.000, 17.826], mean action: 1.622 [0.000, 3.000], mean observation: 0.124 [-1.016, 1.397], loss: 20.172762, mae: 55.252586, mean_q: 74.076942
  433861/1100000: episode: 1051, duration: 0.741s, episode steps: 112, steps per second: 151, episode reward: -48.369, mean reward: -0.432 [-100.000, 17.957], mean action: 1.375 [0.000, 3.000], mean observation: 0.006 [-2.016, 1.462], loss: 7.673071, mae: 54.822323, mean_q: 73.545898
  433985/1100000: episode: 1052, duration: 0.839s, episode steps: 124, steps per second: 148, episode reward: -178.792, mean reward: -1.442 [-100.000, 27.084], mean action: 1.363 [0.000, 3.000], mean observation: -0.012 [-1.005, 1.797], loss: 5.914371, mae: 54.809010, mean_q: 73.439041
  434200/1100000: episode: 1053, duration: 1.443s, episode steps: 215, steps per second: 149, episode reward: -501.782, mean reward: -2.334 [-100.000, 2.581], mean action: 1.805 [0.000, 3.000], mean observation: 0.418 [-0.350, 3.780], loss: 9.615788, mae: 55.118622, mean_q: 73.973488
  434438/1100000: episode: 1054, duration: 1.620s, episode steps: 238, steps per second: 147, episode reward: 255.368, mean reward: 1.073 [-5.949, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.171 [-0.703, 1.387], loss: 17.252371, mae: 55.145672, mean_q: 73.925758
  434784/1100000: episode: 1055, duration: 2.433s, episode steps: 346, steps per second: 142, episode reward: 271.559, mean reward: 0.785 [-17.451, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.096 [-0.765, 1.515], loss: 13.795018, mae: 54.501762, mean_q: 72.949944
  435784/1100000: episode: 1056, duration: 7.408s, episode steps: 1000, steps per second: 135, episode reward: -46.707, mean reward: -0.047 [-4.912, 7.098], mean action: 1.795 [0.000, 3.000], mean observation: 0.190 [-0.622, 1.455], loss: 9.059546, mae: 54.010666, mean_q: 72.236855
  436253/1100000: episode: 1057, duration: 3.368s, episode steps: 469, steps per second: 139, episode reward: 258.212, mean reward: 0.551 [-17.985, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: -0.007 [-1.321, 1.430], loss: 8.764886, mae: 54.172016, mean_q: 72.290398
  436578/1100000: episode: 1058, duration: 2.244s, episode steps: 325, steps per second: 145, episode reward: -240.746, mean reward: -0.741 [-100.000, 16.475], mean action: 1.569 [0.000, 3.000], mean observation: -0.054 [-1.001, 2.001], loss: 8.436460, mae: 53.613113, mean_q: 71.405533
  437578/1100000: episode: 1059, duration: 8.155s, episode steps: 1000, steps per second: 123, episode reward: -38.080, mean reward: -0.038 [-14.746, 15.599], mean action: 1.766 [0.000, 3.000], mean observation: 0.173 [-0.647, 1.463], loss: 10.060702, mae: 52.877773, mean_q: 70.500610
  437966/1100000: episode: 1060, duration: 2.650s, episode steps: 388, steps per second: 146, episode reward: 227.815, mean reward: 0.587 [-18.714, 100.000], mean action: 0.910 [0.000, 3.000], mean observation: 0.216 [-0.763, 1.410], loss: 6.365798, mae: 52.638958, mean_q: 70.375221
  438536/1100000: episode: 1061, duration: 4.094s, episode steps: 570, steps per second: 139, episode reward: 136.672, mean reward: 0.240 [-19.228, 100.000], mean action: 0.896 [0.000, 3.000], mean observation: 0.151 [-0.703, 1.403], loss: 8.490940, mae: 52.805672, mean_q: 70.433357
  439036/1100000: episode: 1062, duration: 3.608s, episode steps: 500, steps per second: 139, episode reward: 148.025, mean reward: 0.296 [-18.599, 100.000], mean action: 1.828 [0.000, 3.000], mean observation: 0.097 [-0.863, 1.405], loss: 10.535492, mae: 52.534710, mean_q: 70.535118
  439201/1100000: episode: 1063, duration: 1.116s, episode steps: 165, steps per second: 148, episode reward: 5.973, mean reward: 0.036 [-100.000, 7.478], mean action: 1.800 [0.000, 3.000], mean observation: 0.090 [-0.652, 1.523], loss: 11.468405, mae: 53.893311, mean_q: 72.304237
  439582/1100000: episode: 1064, duration: 2.644s, episode steps: 381, steps per second: 144, episode reward: 270.211, mean reward: 0.709 [-18.654, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.022 [-0.600, 1.500], loss: 8.529798, mae: 53.058495, mean_q: 70.869987
  439951/1100000: episode: 1065, duration: 2.527s, episode steps: 369, steps per second: 146, episode reward: 249.231, mean reward: 0.675 [-11.185, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: -0.003 [-1.009, 1.393], loss: 6.698040, mae: 52.865826, mean_q: 70.758118
  440306/1100000: episode: 1066, duration: 2.460s, episode steps: 355, steps per second: 144, episode reward: 174.223, mean reward: 0.491 [-17.382, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: -0.004 [-0.898, 1.392], loss: 9.359233, mae: 52.708599, mean_q: 70.505173
  440456/1100000: episode: 1067, duration: 1.013s, episode steps: 150, steps per second: 148, episode reward: -14.081, mean reward: -0.094 [-100.000, 15.782], mean action: 1.707 [0.000, 3.000], mean observation: -0.050 [-0.883, 1.403], loss: 10.281701, mae: 52.948715, mean_q: 70.650299
  440785/1100000: episode: 1068, duration: 2.331s, episode steps: 329, steps per second: 141, episode reward: -274.692, mean reward: -0.835 [-100.000, 13.178], mean action: 2.094 [0.000, 3.000], mean observation: 0.071 [-1.799, 1.395], loss: 7.177729, mae: 52.969025, mean_q: 70.778595
  441519/1100000: episode: 1069, duration: 5.084s, episode steps: 734, steps per second: 144, episode reward: -255.773, mean reward: -0.348 [-100.000, 35.187], mean action: 1.101 [0.000, 3.000], mean observation: 0.129 [-0.820, 1.922], loss: 8.783432, mae: 53.660400, mean_q: 71.647858
  442248/1100000: episode: 1070, duration: 5.558s, episode steps: 729, steps per second: 131, episode reward: -344.443, mean reward: -0.472 [-100.000, 15.719], mean action: 1.492 [0.000, 3.000], mean observation: 0.113 [-1.236, 1.855], loss: 7.726634, mae: 53.635868, mean_q: 71.858345
  442518/1100000: episode: 1071, duration: 1.873s, episode steps: 270, steps per second: 144, episode reward: 293.805, mean reward: 1.088 [-20.708, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.106 [-0.727, 1.387], loss: 20.619864, mae: 54.449638, mean_q: 72.632156
  442654/1100000: episode: 1072, duration: 0.900s, episode steps: 136, steps per second: 151, episode reward: -11.164, mean reward: -0.082 [-100.000, 20.456], mean action: 1.449 [0.000, 3.000], mean observation: -0.025 [-0.849, 1.726], loss: 7.412919, mae: 54.998852, mean_q: 73.326302
  443654/1100000: episode: 1073, duration: 8.010s, episode steps: 1000, steps per second: 125, episode reward: -76.695, mean reward: -0.077 [-19.247, 14.697], mean action: 1.672 [0.000, 3.000], mean observation: 0.073 [-0.910, 1.473], loss: 8.335088, mae: 54.530060, mean_q: 72.815247
  443814/1100000: episode: 1074, duration: 1.084s, episode steps: 160, steps per second: 148, episode reward: 277.085, mean reward: 1.732 [-8.415, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.037 [-0.797, 1.388], loss: 11.108988, mae: 54.967125, mean_q: 73.267723
  444151/1100000: episode: 1075, duration: 2.305s, episode steps: 337, steps per second: 146, episode reward: 220.988, mean reward: 0.656 [-18.311, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.246 [-0.615, 1.456], loss: 8.095136, mae: 54.715405, mean_q: 73.111382
  444424/1100000: episode: 1076, duration: 1.826s, episode steps: 273, steps per second: 150, episode reward: 249.415, mean reward: 0.914 [-2.363, 100.000], mean action: 0.784 [0.000, 3.000], mean observation: 0.009 [-0.629, 1.421], loss: 6.248744, mae: 54.964329, mean_q: 73.541992
  444625/1100000: episode: 1077, duration: 1.363s, episode steps: 201, steps per second: 147, episode reward: 279.629, mean reward: 1.391 [-9.520, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.184 [-0.812, 1.395], loss: 17.883574, mae: 54.768307, mean_q: 73.437546
  444811/1100000: episode: 1078, duration: 1.242s, episode steps: 186, steps per second: 150, episode reward: 246.125, mean reward: 1.323 [-3.536, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.079 [-0.762, 1.418], loss: 9.576241, mae: 55.688541, mean_q: 74.016319
  445131/1100000: episode: 1079, duration: 2.183s, episode steps: 320, steps per second: 147, episode reward: 255.979, mean reward: 0.800 [-18.552, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.113 [-0.780, 1.393], loss: 12.446968, mae: 55.730003, mean_q: 74.208237
  445374/1100000: episode: 1080, duration: 1.653s, episode steps: 243, steps per second: 147, episode reward: 249.629, mean reward: 1.027 [-13.205, 100.000], mean action: 1.551 [0.000, 3.000], mean observation: 0.170 [-0.729, 1.420], loss: 9.556508, mae: 55.623585, mean_q: 74.286026
  445497/1100000: episode: 1081, duration: 0.817s, episode steps: 123, steps per second: 151, episode reward: -9.668, mean reward: -0.079 [-100.000, 19.435], mean action: 1.333 [0.000, 3.000], mean observation: 0.007 [-0.820, 1.581], loss: 21.001648, mae: 56.654133, mean_q: 75.724823
  445976/1100000: episode: 1082, duration: 3.430s, episode steps: 479, steps per second: 140, episode reward: 222.538, mean reward: 0.465 [-24.543, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.161 [-1.014, 1.386], loss: 6.956319, mae: 55.186127, mean_q: 73.564323
  446764/1100000: episode: 1083, duration: 6.035s, episode steps: 788, steps per second: 131, episode reward: 204.607, mean reward: 0.260 [-20.527, 100.000], mean action: 2.069 [0.000, 3.000], mean observation: 0.039 [-0.786, 1.407], loss: 9.137362, mae: 55.485943, mean_q: 74.092606
  447032/1100000: episode: 1084, duration: 1.825s, episode steps: 268, steps per second: 147, episode reward: 223.083, mean reward: 0.832 [-15.383, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: -0.003 [-0.888, 1.415], loss: 11.411847, mae: 55.588799, mean_q: 74.483429
  447188/1100000: episode: 1085, duration: 1.052s, episode steps: 156, steps per second: 148, episode reward: -6.519, mean reward: -0.042 [-100.000, 12.798], mean action: 1.654 [0.000, 3.000], mean observation: -0.101 [-0.661, 1.434], loss: 14.826760, mae: 55.626301, mean_q: 74.205833
  447344/1100000: episode: 1086, duration: 1.047s, episode steps: 156, steps per second: 149, episode reward: -79.003, mean reward: -0.506 [-100.000, 10.370], mean action: 1.590 [0.000, 3.000], mean observation: -0.048 [-0.801, 1.418], loss: 11.636224, mae: 56.199051, mean_q: 75.336067
  447715/1100000: episode: 1087, duration: 2.539s, episode steps: 371, steps per second: 146, episode reward: 290.609, mean reward: 0.783 [-17.882, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.130 [-0.679, 1.411], loss: 7.034993, mae: 55.895699, mean_q: 74.540894
  447965/1100000: episode: 1088, duration: 1.692s, episode steps: 250, steps per second: 148, episode reward: 238.000, mean reward: 0.952 [-20.914, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.102 [-0.758, 1.405], loss: 7.608552, mae: 56.141109, mean_q: 74.964500
  448046/1100000: episode: 1089, duration: 0.539s, episode steps: 81, steps per second: 150, episode reward: -203.253, mean reward: -2.509 [-100.000, 5.541], mean action: 1.210 [0.000, 3.000], mean observation: -0.009 [-2.440, 1.409], loss: 13.496401, mae: 55.114204, mean_q: 74.230598
  449046/1100000: episode: 1090, duration: 7.020s, episode steps: 1000, steps per second: 142, episode reward: 110.774, mean reward: 0.111 [-19.673, 21.209], mean action: 1.485 [0.000, 3.000], mean observation: 0.270 [-0.912, 1.484], loss: 11.313580, mae: 55.885361, mean_q: 74.523781
  449227/1100000: episode: 1091, duration: 1.208s, episode steps: 181, steps per second: 150, episode reward: 52.383, mean reward: 0.289 [-100.000, 19.154], mean action: 1.298 [0.000, 3.000], mean observation: 0.019 [-1.014, 1.459], loss: 4.550324, mae: 55.859959, mean_q: 74.158089
  449546/1100000: episode: 1092, duration: 2.224s, episode steps: 319, steps per second: 143, episode reward: 230.790, mean reward: 0.723 [-19.274, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.176 [-0.962, 1.401], loss: 9.882116, mae: 55.812531, mean_q: 74.398865
  449871/1100000: episode: 1093, duration: 2.258s, episode steps: 325, steps per second: 144, episode reward: 281.455, mean reward: 0.866 [-10.670, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.017 [-0.972, 1.389], loss: 6.876483, mae: 56.713329, mean_q: 75.950722
  450185/1100000: episode: 1094, duration: 2.163s, episode steps: 314, steps per second: 145, episode reward: 284.456, mean reward: 0.906 [-2.738, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.084 [-0.840, 1.434], loss: 5.444892, mae: 56.393219, mean_q: 75.583588
  451004/1100000: episode: 1095, duration: 6.008s, episode steps: 819, steps per second: 136, episode reward: 196.800, mean reward: 0.240 [-20.090, 100.000], mean action: 1.802 [0.000, 3.000], mean observation: 0.186 [-0.976, 1.490], loss: 8.330836, mae: 56.141312, mean_q: 74.791611
  452004/1100000: episode: 1096, duration: 8.055s, episode steps: 1000, steps per second: 124, episode reward: -13.653, mean reward: -0.014 [-24.116, 27.685], mean action: 1.416 [0.000, 3.000], mean observation: 0.047 [-1.001, 1.472], loss: 8.649240, mae: 55.747658, mean_q: 74.145653
  452525/1100000: episode: 1097, duration: 3.674s, episode steps: 521, steps per second: 142, episode reward: -171.989, mean reward: -0.330 [-100.000, 14.108], mean action: 1.729 [0.000, 3.000], mean observation: 0.053 [-0.939, 1.475], loss: 10.606785, mae: 55.203259, mean_q: 73.630959
  452846/1100000: episode: 1098, duration: 2.224s, episode steps: 321, steps per second: 144, episode reward: 229.324, mean reward: 0.714 [-21.557, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.225 [-0.944, 1.387], loss: 9.693425, mae: 54.993801, mean_q: 73.414330
  453096/1100000: episode: 1099, duration: 1.697s, episode steps: 250, steps per second: 147, episode reward: 239.797, mean reward: 0.959 [-18.561, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.088 [-0.692, 1.402], loss: 8.073450, mae: 54.661903, mean_q: 73.077721
  453611/1100000: episode: 1100, duration: 3.714s, episode steps: 515, steps per second: 139, episode reward: -69.176, mean reward: -0.134 [-100.000, 18.811], mean action: 1.617 [0.000, 3.000], mean observation: -0.015 [-0.967, 1.400], loss: 7.231497, mae: 55.057411, mean_q: 73.614044
  454611/1100000: episode: 1101, duration: 7.684s, episode steps: 1000, steps per second: 130, episode reward: 102.641, mean reward: 0.103 [-20.788, 21.051], mean action: 1.346 [0.000, 3.000], mean observation: 0.192 [-0.760, 1.564], loss: 8.911934, mae: 54.875156, mean_q: 73.291000
  455405/1100000: episode: 1102, duration: 5.792s, episode steps: 794, steps per second: 137, episode reward: 164.073, mean reward: 0.207 [-23.310, 100.000], mean action: 1.782 [0.000, 3.000], mean observation: 0.182 [-0.862, 1.401], loss: 9.287193, mae: 54.917030, mean_q: 73.272858
  455624/1100000: episode: 1103, duration: 1.483s, episode steps: 219, steps per second: 148, episode reward: 39.116, mean reward: 0.179 [-100.000, 15.303], mean action: 1.900 [0.000, 3.000], mean observation: 0.099 [-1.134, 1.451], loss: 12.479219, mae: 55.079319, mean_q: 73.574356
  455904/1100000: episode: 1104, duration: 1.911s, episode steps: 280, steps per second: 147, episode reward: 277.890, mean reward: 0.992 [-7.378, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: 0.152 [-0.791, 1.414], loss: 7.259102, mae: 54.850159, mean_q: 73.438911
  456859/1100000: episode: 1105, duration: 7.250s, episode steps: 955, steps per second: 132, episode reward: 184.425, mean reward: 0.193 [-22.827, 100.000], mean action: 2.299 [0.000, 3.000], mean observation: 0.121 [-0.704, 1.413], loss: 8.827966, mae: 54.802006, mean_q: 73.081955
  457166/1100000: episode: 1106, duration: 2.128s, episode steps: 307, steps per second: 144, episode reward: -292.542, mean reward: -0.953 [-100.000, 12.940], mean action: 1.554 [0.000, 3.000], mean observation: -0.113 [-2.367, 1.386], loss: 9.071980, mae: 54.608383, mean_q: 72.900688
  458166/1100000: episode: 1107, duration: 7.948s, episode steps: 1000, steps per second: 126, episode reward: -57.713, mean reward: -0.058 [-5.618, 5.920], mean action: 1.690 [0.000, 3.000], mean observation: -0.091 [-0.677, 1.410], loss: 8.407223, mae: 54.328228, mean_q: 72.406036
  458382/1100000: episode: 1108, duration: 1.482s, episode steps: 216, steps per second: 146, episode reward: -208.797, mean reward: -0.967 [-100.000, 51.191], mean action: 2.171 [0.000, 3.000], mean observation: 0.049 [-0.832, 2.113], loss: 7.974970, mae: 54.530209, mean_q: 72.606262
  458822/1100000: episode: 1109, duration: 3.101s, episode steps: 440, steps per second: 142, episode reward: -172.244, mean reward: -0.391 [-100.000, 10.017], mean action: 1.493 [0.000, 3.000], mean observation: -0.129 [-1.002, 1.518], loss: 15.480005, mae: 53.856907, mean_q: 71.613983
  458937/1100000: episode: 1110, duration: 0.762s, episode steps: 115, steps per second: 151, episode reward: 12.370, mean reward: 0.108 [-100.000, 20.088], mean action: 1.391 [0.000, 3.000], mean observation: -0.063 [-0.912, 1.880], loss: 4.017380, mae: 53.869881, mean_q: 71.618416
  459294/1100000: episode: 1111, duration: 2.495s, episode steps: 357, steps per second: 143, episode reward: 213.110, mean reward: 0.597 [-11.543, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.159 [-0.718, 1.410], loss: 8.571487, mae: 53.939152, mean_q: 71.945358
  459515/1100000: episode: 1112, duration: 1.539s, episode steps: 221, steps per second: 144, episode reward: -131.439, mean reward: -0.595 [-100.000, 13.778], mean action: 1.792 [0.000, 3.000], mean observation: -0.008 [-1.291, 1.496], loss: 6.820648, mae: 53.192753, mean_q: 70.868408
  459817/1100000: episode: 1113, duration: 2.100s, episode steps: 302, steps per second: 144, episode reward: -202.111, mean reward: -0.669 [-100.000, 5.689], mean action: 1.705 [0.000, 3.000], mean observation: 0.021 [-0.946, 2.907], loss: 7.818254, mae: 53.738838, mean_q: 71.519905
  460251/1100000: episode: 1114, duration: 3.037s, episode steps: 434, steps per second: 143, episode reward: -129.239, mean reward: -0.298 [-100.000, 11.825], mean action: 1.698 [0.000, 3.000], mean observation: 0.002 [-2.574, 1.406], loss: 9.159388, mae: 53.477371, mean_q: 71.163094
  460766/1100000: episode: 1115, duration: 3.647s, episode steps: 515, steps per second: 141, episode reward: 228.305, mean reward: 0.443 [-19.475, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.244 [-0.681, 1.395], loss: 9.296517, mae: 53.619453, mean_q: 71.260201
  461359/1100000: episode: 1116, duration: 4.280s, episode steps: 593, steps per second: 139, episode reward: 167.152, mean reward: 0.282 [-22.132, 100.000], mean action: 1.604 [0.000, 3.000], mean observation: 0.143 [-1.199, 1.441], loss: 8.611548, mae: 53.672653, mean_q: 71.452042
  461492/1100000: episode: 1117, duration: 0.892s, episode steps: 133, steps per second: 149, episode reward: -8.983, mean reward: -0.068 [-100.000, 16.311], mean action: 1.617 [0.000, 3.000], mean observation: 0.065 [-1.403, 1.387], loss: 6.267367, mae: 54.068821, mean_q: 71.785362
  461771/1100000: episode: 1118, duration: 1.939s, episode steps: 279, steps per second: 144, episode reward: -141.371, mean reward: -0.507 [-100.000, 52.284], mean action: 1.659 [0.000, 3.000], mean observation: -0.056 [-1.611, 1.386], loss: 6.166176, mae: 54.080200, mean_q: 72.074020
  462037/1100000: episode: 1119, duration: 1.799s, episode steps: 266, steps per second: 148, episode reward: 267.403, mean reward: 1.005 [-17.462, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.073 [-1.365, 1.477], loss: 6.528898, mae: 53.891331, mean_q: 71.553040
  462287/1100000: episode: 1120, duration: 1.751s, episode steps: 250, steps per second: 143, episode reward: 209.324, mean reward: 0.837 [-18.045, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.058 [-0.845, 1.458], loss: 6.484497, mae: 53.931393, mean_q: 71.840248
  462564/1100000: episode: 1121, duration: 1.866s, episode steps: 277, steps per second: 148, episode reward: -124.711, mean reward: -0.450 [-100.000, 10.630], mean action: 1.408 [0.000, 3.000], mean observation: 0.045 [-0.998, 2.832], loss: 9.102023, mae: 54.393257, mean_q: 72.165108
  463564/1100000: episode: 1122, duration: 7.201s, episode steps: 1000, steps per second: 139, episode reward: 60.945, mean reward: 0.061 [-20.677, 22.895], mean action: 1.409 [0.000, 3.000], mean observation: 0.266 [-0.978, 1.395], loss: 9.190432, mae: 54.100517, mean_q: 71.755112
  464564/1100000: episode: 1123, duration: 8.485s, episode steps: 1000, steps per second: 118, episode reward: -90.405, mean reward: -0.090 [-24.306, 22.738], mean action: 1.683 [0.000, 3.000], mean observation: -0.039 [-0.735, 1.430], loss: 8.271744, mae: 53.223091, mean_q: 70.642761
  465395/1100000: episode: 1124, duration: 6.019s, episode steps: 831, steps per second: 138, episode reward: 229.588, mean reward: 0.276 [-20.008, 100.000], mean action: 2.521 [0.000, 3.000], mean observation: 0.043 [-0.760, 1.388], loss: 8.023677, mae: 53.020214, mean_q: 70.225403
  465810/1100000: episode: 1125, duration: 2.928s, episode steps: 415, steps per second: 142, episode reward: 169.918, mean reward: 0.409 [-18.255, 100.000], mean action: 2.354 [0.000, 3.000], mean observation: 0.103 [-0.758, 1.401], loss: 9.235231, mae: 52.480186, mean_q: 69.334503
  466180/1100000: episode: 1126, duration: 2.596s, episode steps: 370, steps per second: 143, episode reward: 243.340, mean reward: 0.658 [-18.754, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.215 [-0.735, 1.397], loss: 6.577951, mae: 53.015129, mean_q: 70.389877
  466449/1100000: episode: 1127, duration: 1.876s, episode steps: 269, steps per second: 143, episode reward: 265.383, mean reward: 0.987 [-9.631, 100.000], mean action: 1.680 [0.000, 3.000], mean observation: 0.136 [-0.665, 1.390], loss: 8.746065, mae: 52.973595, mean_q: 70.160507
  467238/1100000: episode: 1128, duration: 5.723s, episode steps: 789, steps per second: 138, episode reward: 139.960, mean reward: 0.177 [-18.391, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: -0.030 [-0.760, 1.416], loss: 9.682309, mae: 52.385822, mean_q: 69.419472
  468115/1100000: episode: 1129, duration: 6.812s, episode steps: 877, steps per second: 129, episode reward: 143.220, mean reward: 0.163 [-22.243, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: -0.061 [-0.691, 1.403], loss: 7.528962, mae: 52.100262, mean_q: 69.285591
  468247/1100000: episode: 1130, duration: 0.890s, episode steps: 132, steps per second: 148, episode reward: -32.915, mean reward: -0.249 [-100.000, 25.646], mean action: 1.727 [0.000, 3.000], mean observation: 0.136 [-1.574, 1.387], loss: 8.618164, mae: 51.332069, mean_q: 68.520470
  468361/1100000: episode: 1131, duration: 0.758s, episode steps: 114, steps per second: 150, episode reward: -15.264, mean reward: -0.134 [-100.000, 11.144], mean action: 1.675 [0.000, 3.000], mean observation: 0.009 [-1.513, 1.401], loss: 6.633546, mae: 51.501793, mean_q: 68.758385
  469361/1100000: episode: 1132, duration: 7.755s, episode steps: 1000, steps per second: 129, episode reward: 110.878, mean reward: 0.111 [-18.937, 13.295], mean action: 2.385 [0.000, 3.000], mean observation: 0.047 [-0.777, 1.387], loss: 8.948283, mae: 52.103951, mean_q: 69.327972
  470033/1100000: episode: 1133, duration: 4.834s, episode steps: 672, steps per second: 139, episode reward: 231.749, mean reward: 0.345 [-19.973, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.145 [-0.774, 1.557], loss: 10.136686, mae: 52.109787, mean_q: 68.902847
  470543/1100000: episode: 1134, duration: 3.656s, episode steps: 510, steps per second: 139, episode reward: 184.927, mean reward: 0.363 [-13.343, 100.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.026 [-0.786, 1.678], loss: 10.619438, mae: 52.585102, mean_q: 69.576180
  470740/1100000: episode: 1135, duration: 1.325s, episode steps: 197, steps per second: 149, episode reward: -7.771, mean reward: -0.039 [-100.000, 11.381], mean action: 1.492 [0.000, 3.000], mean observation: 0.102 [-1.042, 1.402], loss: 4.666059, mae: 51.669624, mean_q: 68.704674
  471159/1100000: episode: 1136, duration: 2.988s, episode steps: 419, steps per second: 140, episode reward: 218.175, mean reward: 0.521 [-17.887, 100.000], mean action: 2.110 [0.000, 3.000], mean observation: -0.024 [-0.771, 1.396], loss: 7.955432, mae: 51.838860, mean_q: 68.693604
  471517/1100000: episode: 1137, duration: 2.468s, episode steps: 358, steps per second: 145, episode reward: 221.701, mean reward: 0.619 [-11.831, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: -0.033 [-0.652, 1.483], loss: 6.166883, mae: 52.054688, mean_q: 69.257553
  472133/1100000: episode: 1138, duration: 4.332s, episode steps: 616, steps per second: 142, episode reward: 167.008, mean reward: 0.271 [-25.187, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: -0.000 [-1.083, 1.409], loss: 9.738384, mae: 52.113213, mean_q: 69.096695
  472569/1100000: episode: 1139, duration: 3.088s, episode steps: 436, steps per second: 141, episode reward: 208.243, mean reward: 0.478 [-25.410, 100.000], mean action: 1.635 [0.000, 3.000], mean observation: 0.145 [-0.742, 1.390], loss: 10.652617, mae: 52.267700, mean_q: 69.218124
  473013/1100000: episode: 1140, duration: 3.112s, episode steps: 444, steps per second: 143, episode reward: -164.396, mean reward: -0.370 [-100.000, 5.252], mean action: 1.597 [0.000, 3.000], mean observation: 0.038 [-0.600, 1.407], loss: 7.269974, mae: 52.680321, mean_q: 69.610710
  473249/1100000: episode: 1141, duration: 1.587s, episode steps: 236, steps per second: 149, episode reward: -128.345, mean reward: -0.544 [-100.000, 7.395], mean action: 1.508 [0.000, 3.000], mean observation: 0.143 [-1.706, 1.492], loss: 8.912189, mae: 52.617046, mean_q: 69.880257
  474042/1100000: episode: 1142, duration: 5.781s, episode steps: 793, steps per second: 137, episode reward: -179.791, mean reward: -0.227 [-100.000, 19.003], mean action: 1.832 [0.000, 3.000], mean observation: -0.019 [-1.240, 1.390], loss: 7.656308, mae: 52.771938, mean_q: 70.146111
  474313/1100000: episode: 1143, duration: 1.860s, episode steps: 271, steps per second: 146, episode reward: 270.471, mean reward: 0.998 [-12.076, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.098 [-0.871, 1.389], loss: 7.997364, mae: 52.556229, mean_q: 69.508476
  474474/1100000: episode: 1144, duration: 1.077s, episode steps: 161, steps per second: 150, episode reward: 270.192, mean reward: 1.678 [-2.212, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.071 [-0.813, 1.395], loss: 12.162797, mae: 52.643608, mean_q: 69.669464
  475474/1100000: episode: 1145, duration: 8.032s, episode steps: 1000, steps per second: 124, episode reward: 42.829, mean reward: 0.043 [-18.730, 13.143], mean action: 1.547 [0.000, 3.000], mean observation: -0.074 [-0.809, 1.393], loss: 7.744844, mae: 52.334156, mean_q: 68.988266
  475607/1100000: episode: 1146, duration: 0.890s, episode steps: 133, steps per second: 149, episode reward: 20.484, mean reward: 0.154 [-100.000, 14.034], mean action: 1.414 [0.000, 3.000], mean observation: 0.056 [-1.056, 1.401], loss: 6.360626, mae: 52.561569, mean_q: 69.018227
  476218/1100000: episode: 1147, duration: 4.528s, episode steps: 611, steps per second: 135, episode reward: 192.338, mean reward: 0.315 [-8.943, 100.000], mean action: 1.791 [0.000, 3.000], mean observation: -0.021 [-0.783, 1.433], loss: 9.897444, mae: 52.508030, mean_q: 69.253281
  476525/1100000: episode: 1148, duration: 2.096s, episode steps: 307, steps per second: 146, episode reward: 220.282, mean reward: 0.718 [-11.466, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.191 [-0.945, 1.404], loss: 10.513720, mae: 52.159920, mean_q: 69.010513
  477377/1100000: episode: 1149, duration: 6.428s, episode steps: 852, steps per second: 133, episode reward: 192.884, mean reward: 0.226 [-10.928, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.040 [-0.726, 1.404], loss: 7.886011, mae: 52.199284, mean_q: 68.981956
  477523/1100000: episode: 1150, duration: 0.986s, episode steps: 146, steps per second: 148, episode reward: -4.194, mean reward: -0.029 [-100.000, 19.002], mean action: 1.897 [0.000, 3.000], mean observation: 0.094 [-0.711, 1.397], loss: 6.946230, mae: 52.018799, mean_q: 68.704140
  477728/1100000: episode: 1151, duration: 1.368s, episode steps: 205, steps per second: 150, episode reward: -62.930, mean reward: -0.307 [-100.000, 15.692], mean action: 1.527 [0.000, 3.000], mean observation: 0.120 [-0.843, 1.489], loss: 13.527712, mae: 52.630169, mean_q: 68.989632
  477834/1100000: episode: 1152, duration: 0.699s, episode steps: 106, steps per second: 152, episode reward: -358.601, mean reward: -3.383 [-100.000, 3.483], mean action: 1.104 [0.000, 3.000], mean observation: -0.241 [-2.916, 1.438], loss: 9.107125, mae: 52.376530, mean_q: 69.418869
  478328/1100000: episode: 1153, duration: 3.402s, episode steps: 494, steps per second: 145, episode reward: 171.383, mean reward: 0.347 [-22.692, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: -0.033 [-0.845, 1.410], loss: 8.170445, mae: 53.357380, mean_q: 70.307503
  478577/1100000: episode: 1154, duration: 1.696s, episode steps: 249, steps per second: 147, episode reward: -161.377, mean reward: -0.648 [-100.000, 16.493], mean action: 1.823 [0.000, 3.000], mean observation: 0.106 [-1.716, 1.400], loss: 6.154658, mae: 53.018116, mean_q: 70.217667
  478785/1100000: episode: 1155, duration: 1.413s, episode steps: 208, steps per second: 147, episode reward: -244.951, mean reward: -1.178 [-100.000, 9.309], mean action: 1.750 [0.000, 3.000], mean observation: -0.018 [-1.676, 1.477], loss: 13.417281, mae: 53.135620, mean_q: 70.424530
  479201/1100000: episode: 1156, duration: 2.920s, episode steps: 416, steps per second: 142, episode reward: 253.003, mean reward: 0.608 [-21.079, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.122 [-0.641, 1.396], loss: 9.845696, mae: 52.898258, mean_q: 69.819687
  479422/1100000: episode: 1157, duration: 1.494s, episode steps: 221, steps per second: 148, episode reward: 250.400, mean reward: 1.133 [-8.085, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.056 [-0.779, 1.458], loss: 10.215909, mae: 52.688755, mean_q: 69.949745
  479698/1100000: episode: 1158, duration: 1.858s, episode steps: 276, steps per second: 149, episode reward: 258.272, mean reward: 0.936 [-9.052, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: -0.039 [-1.045, 1.407], loss: 9.121772, mae: 52.660248, mean_q: 68.904602
  479973/1100000: episode: 1159, duration: 1.860s, episode steps: 275, steps per second: 148, episode reward: 224.274, mean reward: 0.816 [-17.333, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.072 [-0.787, 1.412], loss: 12.832129, mae: 52.861942, mean_q: 69.591423
  480215/1100000: episode: 1160, duration: 1.649s, episode steps: 242, steps per second: 147, episode reward: 270.286, mean reward: 1.117 [-10.177, 100.000], mean action: 1.591 [0.000, 3.000], mean observation: 0.018 [-0.723, 1.390], loss: 7.617145, mae: 52.780617, mean_q: 69.484467
  481026/1100000: episode: 1161, duration: 6.021s, episode steps: 811, steps per second: 135, episode reward: -242.131, mean reward: -0.299 [-100.000, 4.965], mean action: 1.734 [0.000, 3.000], mean observation: 0.071 [-0.701, 1.447], loss: 10.868175, mae: 52.231178, mean_q: 68.808403
  481460/1100000: episode: 1162, duration: 2.981s, episode steps: 434, steps per second: 146, episode reward: 211.244, mean reward: 0.487 [-12.560, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: -0.066 [-0.704, 1.487], loss: 7.180273, mae: 52.566246, mean_q: 69.191147
  481531/1100000: episode: 1163, duration: 0.475s, episode steps: 71, steps per second: 149, episode reward: -27.199, mean reward: -0.383 [-100.000, 14.964], mean action: 1.592 [0.000, 3.000], mean observation: 0.135 [-1.143, 2.343], loss: 11.218848, mae: 51.456291, mean_q: 67.296524
  481986/1100000: episode: 1164, duration: 3.156s, episode steps: 455, steps per second: 144, episode reward: 210.792, mean reward: 0.463 [-18.084, 100.000], mean action: 0.826 [0.000, 3.000], mean observation: 0.231 [-0.861, 1.405], loss: 11.359090, mae: 52.889980, mean_q: 69.636360
  482556/1100000: episode: 1165, duration: 4.135s, episode steps: 570, steps per second: 138, episode reward: -235.999, mean reward: -0.414 [-100.000, 4.373], mean action: 1.679 [0.000, 3.000], mean observation: 0.062 [-0.600, 1.390], loss: 10.614599, mae: 52.987156, mean_q: 69.796204
  483110/1100000: episode: 1166, duration: 3.852s, episode steps: 554, steps per second: 144, episode reward: 222.686, mean reward: 0.402 [-19.097, 100.000], mean action: 2.197 [0.000, 3.000], mean observation: 0.070 [-0.720, 1.407], loss: 10.034252, mae: 52.868679, mean_q: 69.782410
  483850/1100000: episode: 1167, duration: 5.269s, episode steps: 740, steps per second: 140, episode reward: 251.359, mean reward: 0.340 [-20.561, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: 0.210 [-1.017, 1.386], loss: 9.021592, mae: 52.836475, mean_q: 69.542976
  484369/1100000: episode: 1168, duration: 3.799s, episode steps: 519, steps per second: 137, episode reward: 213.385, mean reward: 0.411 [-18.074, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.083 [-0.821, 1.404], loss: 9.526861, mae: 53.135208, mean_q: 70.094849
  484845/1100000: episode: 1169, duration: 3.415s, episode steps: 476, steps per second: 139, episode reward: 175.948, mean reward: 0.370 [-11.716, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: -0.056 [-0.919, 1.413], loss: 11.337390, mae: 53.087841, mean_q: 70.111473
  485546/1100000: episode: 1170, duration: 5.112s, episode steps: 701, steps per second: 137, episode reward: 121.521, mean reward: 0.173 [-20.054, 100.000], mean action: 1.970 [0.000, 3.000], mean observation: 0.037 [-0.792, 1.405], loss: 12.170245, mae: 53.120342, mean_q: 70.218781
  486420/1100000: episode: 1171, duration: 6.648s, episode steps: 874, steps per second: 131, episode reward: -277.496, mean reward: -0.318 [-100.000, 21.413], mean action: 1.558 [0.000, 3.000], mean observation: 0.018 [-1.608, 1.400], loss: 9.558414, mae: 53.317978, mean_q: 70.683899
  486713/1100000: episode: 1172, duration: 2.002s, episode steps: 293, steps per second: 146, episode reward: 241.264, mean reward: 0.823 [-24.428, 100.000], mean action: 0.840 [0.000, 3.000], mean observation: 0.151 [-0.861, 1.388], loss: 10.614944, mae: 53.582199, mean_q: 70.980186
  487102/1100000: episode: 1173, duration: 2.708s, episode steps: 389, steps per second: 144, episode reward: 252.478, mean reward: 0.649 [-19.986, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.067 [-0.827, 1.522], loss: 7.561220, mae: 53.255268, mean_q: 70.022789
  487712/1100000: episode: 1174, duration: 4.307s, episode steps: 610, steps per second: 142, episode reward: 135.853, mean reward: 0.223 [-18.297, 100.000], mean action: 2.005 [0.000, 3.000], mean observation: 0.183 [-0.869, 1.415], loss: 8.438524, mae: 53.208557, mean_q: 69.999916
  487820/1100000: episode: 1175, duration: 0.714s, episode steps: 108, steps per second: 151, episode reward: -17.252, mean reward: -0.160 [-100.000, 12.156], mean action: 1.287 [0.000, 3.000], mean observation: 0.087 [-0.974, 1.418], loss: 3.905163, mae: 53.267292, mean_q: 70.062218
  488693/1100000: episode: 1176, duration: 6.533s, episode steps: 873, steps per second: 134, episode reward: 96.871, mean reward: 0.111 [-21.618, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.122 [-1.090, 1.434], loss: 12.094504, mae: 53.065323, mean_q: 70.003021
  489102/1100000: episode: 1177, duration: 2.904s, episode steps: 409, steps per second: 141, episode reward: -185.195, mean reward: -0.453 [-100.000, 23.934], mean action: 1.645 [0.000, 3.000], mean observation: 0.003 [-2.051, 1.392], loss: 7.508653, mae: 53.122814, mean_q: 70.292046
  489347/1100000: episode: 1178, duration: 1.673s, episode steps: 245, steps per second: 146, episode reward: -0.991, mean reward: -0.004 [-100.000, 22.016], mean action: 1.551 [0.000, 3.000], mean observation: 0.197 [-1.066, 1.399], loss: 12.136980, mae: 53.261848, mean_q: 70.005424
  489737/1100000: episode: 1179, duration: 2.794s, episode steps: 390, steps per second: 140, episode reward: 233.967, mean reward: 0.600 [-7.218, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.025 [-0.778, 1.403], loss: 8.411390, mae: 52.777996, mean_q: 69.619789
  490216/1100000: episode: 1180, duration: 3.363s, episode steps: 479, steps per second: 142, episode reward: -147.276, mean reward: -0.307 [-100.000, 20.933], mean action: 1.236 [0.000, 3.000], mean observation: 0.090 [-0.892, 1.403], loss: 9.839474, mae: 53.128807, mean_q: 70.102638
  491039/1100000: episode: 1181, duration: 6.622s, episode steps: 823, steps per second: 124, episode reward: 208.548, mean reward: 0.253 [-21.408, 100.000], mean action: 1.697 [0.000, 3.000], mean observation: -0.047 [-0.618, 1.404], loss: 11.449339, mae: 52.938778, mean_q: 69.867073
  491130/1100000: episode: 1182, duration: 0.609s, episode steps: 91, steps per second: 149, episode reward: 33.892, mean reward: 0.372 [-100.000, 11.603], mean action: 1.802 [0.000, 3.000], mean observation: 0.076 [-1.075, 1.390], loss: 7.600489, mae: 52.359348, mean_q: 69.501595
  491375/1100000: episode: 1183, duration: 1.653s, episode steps: 245, steps per second: 148, episode reward: 5.453, mean reward: 0.022 [-100.000, 14.719], mean action: 1.763 [0.000, 3.000], mean observation: 0.182 [-1.000, 1.697], loss: 11.072180, mae: 52.666916, mean_q: 69.499031
  491770/1100000: episode: 1184, duration: 2.721s, episode steps: 395, steps per second: 145, episode reward: 280.377, mean reward: 0.710 [-17.501, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.009 [-0.701, 1.404], loss: 8.237195, mae: 52.674377, mean_q: 69.599686
  492185/1100000: episode: 1185, duration: 2.900s, episode steps: 415, steps per second: 143, episode reward: 190.910, mean reward: 0.460 [-12.115, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.161 [-0.820, 1.412], loss: 8.434757, mae: 52.813080, mean_q: 69.980240
  492352/1100000: episode: 1186, duration: 1.102s, episode steps: 167, steps per second: 151, episode reward: -112.600, mean reward: -0.674 [-100.000, 4.286], mean action: 1.341 [0.000, 3.000], mean observation: 0.038 [-0.841, 1.421], loss: 11.408665, mae: 52.762688, mean_q: 69.639534
  492839/1100000: episode: 1187, duration: 3.520s, episode steps: 487, steps per second: 138, episode reward: 206.280, mean reward: 0.424 [-20.031, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.032 [-1.126, 1.443], loss: 9.724804, mae: 52.994751, mean_q: 70.140503
  493240/1100000: episode: 1188, duration: 2.848s, episode steps: 401, steps per second: 141, episode reward: 224.301, mean reward: 0.559 [-17.514, 100.000], mean action: 1.793 [0.000, 3.000], mean observation: 0.151 [-0.649, 1.404], loss: 10.860186, mae: 52.875118, mean_q: 69.749329
  493342/1100000: episode: 1189, duration: 0.684s, episode steps: 102, steps per second: 149, episode reward: 28.209, mean reward: 0.277 [-100.000, 18.912], mean action: 1.794 [0.000, 3.000], mean observation: 0.047 [-0.861, 1.387], loss: 6.751236, mae: 53.182343, mean_q: 70.183289
  493894/1100000: episode: 1190, duration: 3.763s, episode steps: 552, steps per second: 147, episode reward: 229.673, mean reward: 0.416 [-19.600, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.227 [-0.856, 1.447], loss: 12.216249, mae: 52.815262, mean_q: 69.736771
  494067/1100000: episode: 1191, duration: 1.153s, episode steps: 173, steps per second: 150, episode reward: -12.010, mean reward: -0.069 [-100.000, 8.334], mean action: 1.457 [0.000, 3.000], mean observation: 0.086 [-1.725, 1.410], loss: 7.632802, mae: 53.268063, mean_q: 70.215202
  494382/1100000: episode: 1192, duration: 2.173s, episode steps: 315, steps per second: 145, episode reward: -218.402, mean reward: -0.693 [-100.000, 8.121], mean action: 1.581 [0.000, 3.000], mean observation: -0.057 [-3.951, 1.515], loss: 10.433927, mae: 53.187798, mean_q: 70.449181
  494786/1100000: episode: 1193, duration: 2.849s, episode steps: 404, steps per second: 142, episode reward: -89.176, mean reward: -0.221 [-100.000, 26.242], mean action: 1.478 [0.000, 3.000], mean observation: -0.030 [-1.000, 1.872], loss: 11.190372, mae: 52.895180, mean_q: 69.829758
  495207/1100000: episode: 1194, duration: 2.925s, episode steps: 421, steps per second: 144, episode reward: 176.067, mean reward: 0.418 [-17.783, 100.000], mean action: 2.083 [0.000, 3.000], mean observation: 0.084 [-0.763, 1.429], loss: 9.025971, mae: 53.401073, mean_q: 70.452850
  495305/1100000: episode: 1195, duration: 0.660s, episode steps: 98, steps per second: 148, episode reward: 10.407, mean reward: 0.106 [-100.000, 11.893], mean action: 1.796 [0.000, 3.000], mean observation: 0.204 [-0.942, 1.401], loss: 14.779764, mae: 52.623665, mean_q: 68.803261
  495543/1100000: episode: 1196, duration: 1.673s, episode steps: 238, steps per second: 142, episode reward: -55.073, mean reward: -0.231 [-100.000, 12.301], mean action: 1.622 [0.000, 3.000], mean observation: 0.031 [-1.432, 1.411], loss: 13.689190, mae: 52.414921, mean_q: 69.167068
  496543/1100000: episode: 1197, duration: 7.381s, episode steps: 1000, steps per second: 135, episode reward: 14.666, mean reward: 0.015 [-19.504, 18.632], mean action: 2.075 [0.000, 3.000], mean observation: 0.163 [-0.607, 1.423], loss: 11.655735, mae: 53.222790, mean_q: 70.445892
  496802/1100000: episode: 1198, duration: 1.771s, episode steps: 259, steps per second: 146, episode reward: 260.516, mean reward: 1.006 [-17.448, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.145 [-1.173, 1.393], loss: 14.380892, mae: 52.901680, mean_q: 70.366508
  497265/1100000: episode: 1199, duration: 3.218s, episode steps: 463, steps per second: 144, episode reward: 177.176, mean reward: 0.383 [-24.662, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: 0.120 [-0.797, 1.400], loss: 12.148664, mae: 53.204144, mean_q: 70.583466
  497405/1100000: episode: 1200, duration: 0.934s, episode steps: 140, steps per second: 150, episode reward: -3.718, mean reward: -0.027 [-100.000, 15.253], mean action: 1.764 [0.000, 3.000], mean observation: 0.056 [-0.825, 1.443], loss: 10.902339, mae: 53.200516, mean_q: 70.583763
  497833/1100000: episode: 1201, duration: 3.007s, episode steps: 428, steps per second: 142, episode reward: 244.494, mean reward: 0.571 [-19.898, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.222 [-0.827, 1.405], loss: 12.573463, mae: 53.297726, mean_q: 70.490234
  498031/1100000: episode: 1202, duration: 1.346s, episode steps: 198, steps per second: 147, episode reward: -35.798, mean reward: -0.181 [-100.000, 18.901], mean action: 1.520 [0.000, 3.000], mean observation: -0.022 [-0.868, 1.641], loss: 9.647091, mae: 53.539703, mean_q: 70.788582
  498493/1100000: episode: 1203, duration: 3.223s, episode steps: 462, steps per second: 143, episode reward: 264.549, mean reward: 0.573 [-9.956, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.034 [-0.770, 1.434], loss: 16.300138, mae: 53.065872, mean_q: 70.079453
  499139/1100000: episode: 1204, duration: 4.829s, episode steps: 646, steps per second: 134, episode reward: 233.166, mean reward: 0.361 [-11.906, 100.000], mean action: 1.831 [0.000, 3.000], mean observation: -0.035 [-1.191, 1.387], loss: 9.566793, mae: 53.213066, mean_q: 70.615440
  499643/1100000: episode: 1205, duration: 3.584s, episode steps: 504, steps per second: 141, episode reward: 187.769, mean reward: 0.373 [-24.055, 100.000], mean action: 1.857 [0.000, 3.000], mean observation: 0.120 [-0.964, 1.406], loss: 10.905850, mae: 52.822079, mean_q: 70.004730
  499892/1100000: episode: 1206, duration: 1.697s, episode steps: 249, steps per second: 147, episode reward: 233.113, mean reward: 0.936 [-13.255, 100.000], mean action: 2.028 [0.000, 3.000], mean observation: 0.044 [-0.765, 1.393], loss: 10.585479, mae: 52.706947, mean_q: 69.684471
  500155/1100000: episode: 1207, duration: 1.816s, episode steps: 263, steps per second: 145, episode reward: -172.738, mean reward: -0.657 [-100.000, 33.512], mean action: 1.540 [0.000, 3.000], mean observation: 0.001 [-2.072, 1.435], loss: 17.542440, mae: 53.183098, mean_q: 70.106285
  500610/1100000: episode: 1208, duration: 3.188s, episode steps: 455, steps per second: 143, episode reward: 252.952, mean reward: 0.556 [-17.800, 100.000], mean action: 0.782 [0.000, 3.000], mean observation: 0.107 [-0.708, 1.403], loss: 8.450021, mae: 52.852154, mean_q: 70.060333
  501610/1100000: episode: 1209, duration: 7.284s, episode steps: 1000, steps per second: 137, episode reward: 33.926, mean reward: 0.034 [-24.860, 23.836], mean action: 1.730 [0.000, 3.000], mean observation: 0.029 [-0.627, 1.404], loss: 9.314834, mae: 53.421520, mean_q: 70.589378
  501901/1100000: episode: 1210, duration: 2.017s, episode steps: 291, steps per second: 144, episode reward: 255.775, mean reward: 0.879 [-17.903, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.077 [-0.871, 1.400], loss: 10.652306, mae: 53.053703, mean_q: 70.162209
  502575/1100000: episode: 1211, duration: 5.031s, episode steps: 674, steps per second: 134, episode reward: 150.819, mean reward: 0.224 [-14.175, 100.000], mean action: 1.902 [0.000, 3.000], mean observation: -0.059 [-0.780, 1.394], loss: 9.520881, mae: 53.785336, mean_q: 71.343513
  502754/1100000: episode: 1212, duration: 1.198s, episode steps: 179, steps per second: 149, episode reward: 282.813, mean reward: 1.580 [-10.704, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: 0.043 [-0.869, 1.386], loss: 9.769261, mae: 53.888496, mean_q: 71.464561
  503158/1100000: episode: 1213, duration: 2.798s, episode steps: 404, steps per second: 144, episode reward: 237.169, mean reward: 0.587 [-24.635, 100.000], mean action: 1.671 [0.000, 3.000], mean observation: 0.212 [-0.850, 1.392], loss: 13.084524, mae: 54.688892, mean_q: 72.246895
  504001/1100000: episode: 1214, duration: 5.923s, episode steps: 843, steps per second: 142, episode reward: 203.861, mean reward: 0.242 [-19.732, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.098 [-0.697, 1.413], loss: 10.568390, mae: 54.393711, mean_q: 71.822083
  504482/1100000: episode: 1215, duration: 3.541s, episode steps: 481, steps per second: 136, episode reward: 226.562, mean reward: 0.471 [-11.222, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: -0.051 [-0.600, 1.395], loss: 7.821351, mae: 54.514305, mean_q: 72.079880
  504962/1100000: episode: 1216, duration: 3.412s, episode steps: 480, steps per second: 141, episode reward: 190.851, mean reward: 0.398 [-11.877, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: -0.061 [-0.731, 1.412], loss: 9.758279, mae: 54.256187, mean_q: 72.005455
  505459/1100000: episode: 1217, duration: 3.606s, episode steps: 497, steps per second: 138, episode reward: 233.616, mean reward: 0.470 [-20.583, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: -0.053 [-0.750, 1.389], loss: 7.454055, mae: 54.620548, mean_q: 72.368851
  506459/1100000: episode: 1218, duration: 8.568s, episode steps: 1000, steps per second: 117, episode reward: 58.858, mean reward: 0.059 [-20.619, 24.858], mean action: 1.336 [0.000, 3.000], mean observation: 0.242 [-1.020, 1.410], loss: 11.239971, mae: 54.654877, mean_q: 72.575157
  506742/1100000: episode: 1219, duration: 1.928s, episode steps: 283, steps per second: 147, episode reward: -402.082, mean reward: -1.421 [-100.000, 12.764], mean action: 1.717 [0.000, 3.000], mean observation: 0.205 [-1.006, 3.826], loss: 5.645173, mae: 54.179111, mean_q: 72.119499
  507079/1100000: episode: 1220, duration: 2.336s, episode steps: 337, steps per second: 144, episode reward: 271.183, mean reward: 0.805 [-10.998, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.030 [-0.653, 1.393], loss: 12.478496, mae: 54.021755, mean_q: 71.677811
  507423/1100000: episode: 1221, duration: 2.387s, episode steps: 344, steps per second: 144, episode reward: 228.297, mean reward: 0.664 [-13.573, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: -0.029 [-0.850, 1.490], loss: 8.844549, mae: 54.011208, mean_q: 71.683052
  507544/1100000: episode: 1222, duration: 0.810s, episode steps: 121, steps per second: 149, episode reward: 28.249, mean reward: 0.233 [-100.000, 11.564], mean action: 1.760 [0.000, 3.000], mean observation: 0.071 [-0.803, 1.406], loss: 14.562754, mae: 54.497688, mean_q: 72.634941
  507756/1100000: episode: 1223, duration: 1.430s, episode steps: 212, steps per second: 148, episode reward: -143.887, mean reward: -0.679 [-100.000, 12.544], mean action: 1.821 [0.000, 3.000], mean observation: -0.055 [-1.523, 1.502], loss: 7.274528, mae: 53.678288, mean_q: 71.326706
  508290/1100000: episode: 1224, duration: 3.804s, episode steps: 534, steps per second: 140, episode reward: 271.225, mean reward: 0.508 [-18.172, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.074 [-0.709, 1.402], loss: 8.408081, mae: 54.101635, mean_q: 71.613152
  508478/1100000: episode: 1225, duration: 1.265s, episode steps: 188, steps per second: 149, episode reward: -181.299, mean reward: -0.964 [-100.000, 12.818], mean action: 1.601 [0.000, 3.000], mean observation: 0.025 [-1.764, 1.406], loss: 12.519817, mae: 53.745201, mean_q: 70.834824
  508884/1100000: episode: 1226, duration: 2.818s, episode steps: 406, steps per second: 144, episode reward: 249.350, mean reward: 0.614 [-11.855, 100.000], mean action: 2.195 [0.000, 3.000], mean observation: -0.030 [-0.758, 1.522], loss: 7.600245, mae: 53.269794, mean_q: 70.204117
  509100/1100000: episode: 1227, duration: 1.467s, episode steps: 216, steps per second: 147, episode reward: -21.162, mean reward: -0.098 [-100.000, 12.465], mean action: 1.824 [0.000, 3.000], mean observation: 0.168 [-1.504, 1.593], loss: 8.840872, mae: 53.466286, mean_q: 70.966164
  509219/1100000: episode: 1228, duration: 0.796s, episode steps: 119, steps per second: 149, episode reward: -6.814, mean reward: -0.057 [-100.000, 20.884], mean action: 1.328 [0.000, 3.000], mean observation: 0.063 [-1.565, 1.431], loss: 10.740271, mae: 53.462563, mean_q: 70.301292
  509450/1100000: episode: 1229, duration: 1.601s, episode steps: 231, steps per second: 144, episode reward: -122.743, mean reward: -0.531 [-100.000, 5.211], mean action: 1.710 [0.000, 3.000], mean observation: 0.033 [-0.791, 1.411], loss: 11.351810, mae: 53.741280, mean_q: 71.022171
  510126/1100000: episode: 1230, duration: 5.003s, episode steps: 676, steps per second: 135, episode reward: 174.983, mean reward: 0.259 [-18.330, 100.000], mean action: 1.716 [0.000, 3.000], mean observation: 0.161 [-0.660, 1.521], loss: 10.492479, mae: 53.908081, mean_q: 71.231888
  510601/1100000: episode: 1231, duration: 3.406s, episode steps: 475, steps per second: 139, episode reward: 235.322, mean reward: 0.495 [-19.204, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.207 [-0.846, 1.395], loss: 13.640707, mae: 53.595329, mean_q: 70.534218
  511601/1100000: episode: 1232, duration: 7.228s, episode steps: 1000, steps per second: 138, episode reward: 83.031, mean reward: 0.083 [-19.645, 14.407], mean action: 1.239 [0.000, 3.000], mean observation: 0.028 [-0.751, 1.385], loss: 11.850243, mae: 53.568993, mean_q: 70.722076
  512107/1100000: episode: 1233, duration: 3.737s, episode steps: 506, steps per second: 135, episode reward: 196.129, mean reward: 0.388 [-15.112, 100.000], mean action: 1.706 [0.000, 3.000], mean observation: -0.053 [-0.749, 1.400], loss: 12.443008, mae: 53.029434, mean_q: 70.029419
  512980/1100000: episode: 1234, duration: 6.397s, episode steps: 873, steps per second: 136, episode reward: 189.153, mean reward: 0.217 [-19.992, 100.000], mean action: 1.597 [0.000, 3.000], mean observation: 0.241 [-0.812, 1.400], loss: 8.696871, mae: 52.980366, mean_q: 69.815239
  513130/1100000: episode: 1235, duration: 1.004s, episode steps: 150, steps per second: 149, episode reward: -167.812, mean reward: -1.119 [-100.000, 47.400], mean action: 1.480 [0.000, 3.000], mean observation: -0.112 [-2.151, 1.461], loss: 9.612177, mae: 53.281727, mean_q: 70.362083
  514010/1100000: episode: 1236, duration: 6.722s, episode steps: 880, steps per second: 131, episode reward: 218.272, mean reward: 0.248 [-20.886, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.150 [-0.826, 1.422], loss: 10.611868, mae: 53.062801, mean_q: 69.826973
  514226/1100000: episode: 1237, duration: 1.469s, episode steps: 216, steps per second: 147, episode reward: -128.349, mean reward: -0.594 [-100.000, 8.053], mean action: 1.755 [0.000, 3.000], mean observation: -0.107 [-1.607, 1.414], loss: 9.782274, mae: 52.479355, mean_q: 69.006310
  515165/1100000: episode: 1238, duration: 7.067s, episode steps: 939, steps per second: 133, episode reward: 118.466, mean reward: 0.126 [-11.880, 100.000], mean action: 2.035 [0.000, 3.000], mean observation: -0.053 [-0.873, 1.413], loss: 11.994373, mae: 52.702732, mean_q: 69.207176
  515558/1100000: episode: 1239, duration: 2.797s, episode steps: 393, steps per second: 140, episode reward: 241.545, mean reward: 0.615 [-10.323, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: 0.036 [-0.585, 1.410], loss: 7.479413, mae: 52.335072, mean_q: 69.121658
  516011/1100000: episode: 1240, duration: 3.336s, episode steps: 453, steps per second: 136, episode reward: 262.642, mean reward: 0.580 [-17.496, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.064 [-0.721, 1.465], loss: 9.097518, mae: 52.068321, mean_q: 68.859169
  516223/1100000: episode: 1241, duration: 1.432s, episode steps: 212, steps per second: 148, episode reward: 248.026, mean reward: 1.170 [-12.771, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.078 [-0.871, 1.402], loss: 8.401153, mae: 52.302734, mean_q: 68.786583
  516499/1100000: episode: 1242, duration: 1.906s, episode steps: 276, steps per second: 145, episode reward: 258.763, mean reward: 0.938 [-8.815, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.031 [-0.938, 1.393], loss: 7.360773, mae: 52.110424, mean_q: 68.751762
  517065/1100000: episode: 1243, duration: 4.057s, episode steps: 566, steps per second: 140, episode reward: 227.628, mean reward: 0.402 [-10.951, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: -0.023 [-0.998, 1.467], loss: 8.332633, mae: 51.947708, mean_q: 68.423561
  517588/1100000: episode: 1244, duration: 3.822s, episode steps: 523, steps per second: 137, episode reward: 234.277, mean reward: 0.448 [-21.668, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.186 [-0.843, 1.474], loss: 10.817904, mae: 52.188972, mean_q: 68.960770
  518112/1100000: episode: 1245, duration: 3.695s, episode steps: 524, steps per second: 142, episode reward: 156.358, mean reward: 0.298 [-19.205, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.001 [-0.894, 1.390], loss: 7.046293, mae: 52.115601, mean_q: 68.992920
  518253/1100000: episode: 1246, duration: 0.951s, episode steps: 141, steps per second: 148, episode reward: -295.555, mean reward: -2.096 [-100.000, 8.428], mean action: 1.475 [0.000, 3.000], mean observation: 0.103 [-1.534, 4.400], loss: 10.066740, mae: 52.019100, mean_q: 68.953087
  518435/1100000: episode: 1247, duration: 1.259s, episode steps: 182, steps per second: 145, episode reward: 299.515, mean reward: 1.646 [-8.858, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.067 [-1.666, 1.390], loss: 11.385224, mae: 52.175755, mean_q: 68.800201
  518700/1100000: episode: 1248, duration: 1.823s, episode steps: 265, steps per second: 145, episode reward: 257.652, mean reward: 0.972 [-13.429, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.046 [-0.869, 1.425], loss: 8.660577, mae: 52.422489, mean_q: 69.231087
  518965/1100000: episode: 1249, duration: 1.816s, episode steps: 265, steps per second: 146, episode reward: 259.788, mean reward: 0.980 [-11.611, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: -0.077 [-0.816, 1.385], loss: 8.924150, mae: 52.063110, mean_q: 69.027016
  519086/1100000: episode: 1250, duration: 0.804s, episode steps: 121, steps per second: 151, episode reward: -135.053, mean reward: -1.116 [-100.000, 6.504], mean action: 1.405 [0.000, 3.000], mean observation: 0.104 [-1.981, 1.427], loss: 5.557004, mae: 52.595573, mean_q: 70.078484
  520061/1100000: episode: 1251, duration: 7.293s, episode steps: 975, steps per second: 134, episode reward: 215.716, mean reward: 0.221 [-19.525, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.046 [-0.666, 1.424], loss: 7.478297, mae: 52.240032, mean_q: 69.103653
  520195/1100000: episode: 1252, duration: 0.903s, episode steps: 134, steps per second: 148, episode reward: 8.836, mean reward: 0.066 [-100.000, 17.836], mean action: 1.896 [0.000, 3.000], mean observation: 0.065 [-1.503, 1.391], loss: 8.835067, mae: 52.095608, mean_q: 69.151642
  520596/1100000: episode: 1253, duration: 2.875s, episode steps: 401, steps per second: 139, episode reward: 215.000, mean reward: 0.536 [-18.852, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: -0.029 [-0.611, 1.394], loss: 7.126711, mae: 52.446598, mean_q: 69.798500
  520945/1100000: episode: 1254, duration: 2.391s, episode steps: 349, steps per second: 146, episode reward: 250.808, mean reward: 0.719 [-19.359, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.042 [-0.990, 1.423], loss: 18.378048, mae: 52.452415, mean_q: 69.524719
  521075/1100000: episode: 1255, duration: 0.870s, episode steps: 130, steps per second: 150, episode reward: -30.522, mean reward: -0.235 [-100.000, 20.395], mean action: 1.715 [0.000, 3.000], mean observation: 0.110 [-1.892, 1.437], loss: 8.080447, mae: 51.495544, mean_q: 68.032753
  521953/1100000: episode: 1256, duration: 6.653s, episode steps: 878, steps per second: 132, episode reward: 172.364, mean reward: 0.196 [-18.489, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.030 [-1.110, 2.502], loss: 9.353314, mae: 52.378872, mean_q: 69.448837
  522380/1100000: episode: 1257, duration: 2.977s, episode steps: 427, steps per second: 143, episode reward: 243.091, mean reward: 0.569 [-17.926, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.113 [-0.657, 1.422], loss: 8.386362, mae: 52.347740, mean_q: 69.370415
  522523/1100000: episode: 1258, duration: 0.955s, episode steps: 143, steps per second: 150, episode reward: -146.358, mean reward: -1.023 [-100.000, 38.886], mean action: 1.308 [0.000, 3.000], mean observation: 0.019 [-0.955, 1.626], loss: 7.286414, mae: 52.363716, mean_q: 69.270218
  522774/1100000: episode: 1259, duration: 1.688s, episode steps: 251, steps per second: 149, episode reward: 272.447, mean reward: 1.085 [-10.373, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.083 [-0.650, 1.414], loss: 7.752855, mae: 52.503525, mean_q: 69.662376
  523774/1100000: episode: 1260, duration: 7.613s, episode steps: 1000, steps per second: 131, episode reward: 81.597, mean reward: 0.082 [-19.056, 14.014], mean action: 1.660 [0.000, 3.000], mean observation: 0.157 [-0.668, 1.537], loss: 8.447365, mae: 52.250561, mean_q: 69.441856
  524161/1100000: episode: 1261, duration: 2.719s, episode steps: 387, steps per second: 142, episode reward: 226.742, mean reward: 0.586 [-21.162, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.028 [-0.756, 1.412], loss: 10.112756, mae: 52.162502, mean_q: 69.061661
  525035/1100000: episode: 1262, duration: 6.473s, episode steps: 874, steps per second: 135, episode reward: 192.410, mean reward: 0.220 [-19.990, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.145 [-0.634, 1.433], loss: 9.481030, mae: 51.938755, mean_q: 68.852135
  525507/1100000: episode: 1263, duration: 3.338s, episode steps: 472, steps per second: 141, episode reward: 170.006, mean reward: 0.360 [-17.779, 100.000], mean action: 2.216 [0.000, 3.000], mean observation: 0.030 [-0.934, 1.408], loss: 7.485654, mae: 51.493565, mean_q: 68.306190
  526384/1100000: episode: 1264, duration: 6.983s, episode steps: 877, steps per second: 126, episode reward: 172.488, mean reward: 0.197 [-25.872, 100.000], mean action: 1.826 [0.000, 3.000], mean observation: 0.160 [-0.651, 1.525], loss: 7.579693, mae: 51.678120, mean_q: 68.482208
  526615/1100000: episode: 1265, duration: 1.554s, episode steps: 231, steps per second: 149, episode reward: 242.881, mean reward: 1.051 [-19.623, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.083 [-0.840, 1.424], loss: 7.070971, mae: 51.509098, mean_q: 68.504944
  526998/1100000: episode: 1266, duration: 2.623s, episode steps: 383, steps per second: 146, episode reward: 229.178, mean reward: 0.598 [-18.503, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: -0.005 [-0.640, 1.415], loss: 8.423960, mae: 51.312820, mean_q: 67.856171
  527653/1100000: episode: 1267, duration: 4.770s, episode steps: 655, steps per second: 137, episode reward: -157.616, mean reward: -0.241 [-100.000, 15.726], mean action: 1.748 [0.000, 3.000], mean observation: -0.009 [-0.845, 1.457], loss: 7.121341, mae: 51.051014, mean_q: 67.635864
  528200/1100000: episode: 1268, duration: 3.905s, episode steps: 547, steps per second: 140, episode reward: 172.882, mean reward: 0.316 [-21.415, 100.000], mean action: 1.724 [0.000, 3.000], mean observation: 0.121 [-0.792, 1.432], loss: 7.963469, mae: 51.754295, mean_q: 68.519012
  528322/1100000: episode: 1269, duration: 0.817s, episode steps: 122, steps per second: 149, episode reward: -11.088, mean reward: -0.091 [-100.000, 6.590], mean action: 1.541 [0.000, 3.000], mean observation: 0.050 [-2.579, 1.418], loss: 5.615433, mae: 50.929012, mean_q: 67.033432
  528610/1100000: episode: 1270, duration: 1.987s, episode steps: 288, steps per second: 145, episode reward: 268.829, mean reward: 0.933 [-22.848, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.126 [-0.729, 1.466], loss: 6.344027, mae: 51.402531, mean_q: 67.722450
  528938/1100000: episode: 1271, duration: 2.255s, episode steps: 328, steps per second: 145, episode reward: 279.880, mean reward: 0.853 [-9.978, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.097 [-0.566, 1.464], loss: 11.740753, mae: 50.957085, mean_q: 67.491020
  529391/1100000: episode: 1272, duration: 3.209s, episode steps: 453, steps per second: 141, episode reward: 239.620, mean reward: 0.529 [-9.238, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.172 [-0.772, 1.457], loss: 12.635458, mae: 51.951977, mean_q: 68.889267
  529957/1100000: episode: 1273, duration: 4.057s, episode steps: 566, steps per second: 140, episode reward: -123.750, mean reward: -0.219 [-100.000, 10.767], mean action: 1.968 [0.000, 3.000], mean observation: 0.040 [-0.698, 1.404], loss: 9.319240, mae: 51.430717, mean_q: 68.349060
  530271/1100000: episode: 1274, duration: 2.155s, episode steps: 314, steps per second: 146, episode reward: -313.610, mean reward: -0.999 [-100.000, 4.917], mean action: 1.669 [0.000, 3.000], mean observation: 0.026 [-1.230, 2.258], loss: 9.321474, mae: 52.102852, mean_q: 69.300369
  530487/1100000: episode: 1275, duration: 1.445s, episode steps: 216, steps per second: 150, episode reward: 276.996, mean reward: 1.282 [-7.632, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.083 [-0.888, 1.496], loss: 7.710175, mae: 52.225349, mean_q: 69.555206
  531135/1100000: episode: 1276, duration: 4.822s, episode steps: 648, steps per second: 134, episode reward: 238.431, mean reward: 0.368 [-18.189, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.099 [-0.925, 1.412], loss: 13.048721, mae: 52.218956, mean_q: 69.339256
  531466/1100000: episode: 1277, duration: 2.256s, episode steps: 331, steps per second: 147, episode reward: 266.333, mean reward: 0.805 [-7.625, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.076 [-0.799, 1.411], loss: 6.816776, mae: 52.327938, mean_q: 69.680954
  531778/1100000: episode: 1278, duration: 2.135s, episode steps: 312, steps per second: 146, episode reward: 250.596, mean reward: 0.803 [-8.296, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.021 [-1.089, 1.474], loss: 7.621786, mae: 52.471539, mean_q: 69.521255
  532769/1100000: episode: 1279, duration: 7.075s, episode steps: 991, steps per second: 140, episode reward: 225.610, mean reward: 0.228 [-18.846, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.041 [-0.805, 1.398], loss: 9.876812, mae: 52.474575, mean_q: 69.794762
  532939/1100000: episode: 1280, duration: 1.151s, episode steps: 170, steps per second: 148, episode reward: 26.297, mean reward: 0.155 [-100.000, 12.593], mean action: 1.882 [0.000, 3.000], mean observation: -0.036 [-0.715, 1.400], loss: 10.039868, mae: 53.106228, mean_q: 70.746780
  533129/1100000: episode: 1281, duration: 1.278s, episode steps: 190, steps per second: 149, episode reward: -143.654, mean reward: -0.756 [-100.000, 9.511], mean action: 1.516 [0.000, 3.000], mean observation: 0.204 [-0.978, 1.462], loss: 11.282143, mae: 52.564556, mean_q: 70.092056
  533577/1100000: episode: 1282, duration: 3.082s, episode steps: 448, steps per second: 145, episode reward: 200.075, mean reward: 0.447 [-17.412, 100.000], mean action: 1.752 [0.000, 3.000], mean observation: 0.021 [-0.742, 1.407], loss: 9.456546, mae: 52.707981, mean_q: 70.149452
  533763/1100000: episode: 1283, duration: 1.253s, episode steps: 186, steps per second: 149, episode reward: 15.003, mean reward: 0.081 [-100.000, 10.850], mean action: 1.710 [0.000, 3.000], mean observation: -0.009 [-1.349, 1.508], loss: 9.149556, mae: 52.266674, mean_q: 69.329681
  534034/1100000: episode: 1284, duration: 1.833s, episode steps: 271, steps per second: 148, episode reward: 207.963, mean reward: 0.767 [-9.897, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: -0.048 [-1.381, 1.433], loss: 7.270918, mae: 52.993370, mean_q: 70.792603
  534145/1100000: episode: 1285, duration: 0.737s, episode steps: 111, steps per second: 151, episode reward: -29.589, mean reward: -0.267 [-100.000, 14.732], mean action: 1.495 [0.000, 3.000], mean observation: 0.098 [-1.488, 1.396], loss: 11.064071, mae: 52.931976, mean_q: 70.319695
  534574/1100000: episode: 1286, duration: 3.067s, episode steps: 429, steps per second: 140, episode reward: 209.759, mean reward: 0.489 [-12.374, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: -0.053 [-0.600, 1.406], loss: 7.757748, mae: 53.268646, mean_q: 70.856178
  534690/1100000: episode: 1287, duration: 0.788s, episode steps: 116, steps per second: 147, episode reward: -140.174, mean reward: -1.208 [-100.000, 14.369], mean action: 1.810 [0.000, 3.000], mean observation: -0.185 [-1.395, 1.416], loss: 8.528106, mae: 53.295609, mean_q: 71.471664
  535030/1100000: episode: 1288, duration: 2.360s, episode steps: 340, steps per second: 144, episode reward: 233.041, mean reward: 0.685 [-6.964, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.064 [-1.358, 1.479], loss: 9.581652, mae: 53.795670, mean_q: 71.145309
  536030/1100000: episode: 1289, duration: 7.351s, episode steps: 1000, steps per second: 136, episode reward: 102.637, mean reward: 0.103 [-20.177, 22.286], mean action: 1.158 [0.000, 3.000], mean observation: 0.232 [-0.842, 1.416], loss: 11.096648, mae: 53.550884, mean_q: 71.078148
  536244/1100000: episode: 1290, duration: 1.436s, episode steps: 214, steps per second: 149, episode reward: 255.936, mean reward: 1.196 [-3.656, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.074 [-0.829, 1.406], loss: 14.521799, mae: 53.260807, mean_q: 70.600616
  536766/1100000: episode: 1291, duration: 3.775s, episode steps: 522, steps per second: 138, episode reward: 221.093, mean reward: 0.424 [-17.794, 100.000], mean action: 1.504 [0.000, 3.000], mean observation: 0.157 [-0.780, 1.398], loss: 11.286885, mae: 53.410160, mean_q: 70.643776
  537123/1100000: episode: 1292, duration: 2.504s, episode steps: 357, steps per second: 143, episode reward: 246.646, mean reward: 0.691 [-17.531, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.118 [-0.873, 1.402], loss: 9.866930, mae: 53.785084, mean_q: 71.308243
  537575/1100000: episode: 1293, duration: 3.172s, episode steps: 452, steps per second: 143, episode reward: 9.542, mean reward: 0.021 [-100.000, 21.740], mean action: 2.042 [0.000, 3.000], mean observation: 0.176 [-0.939, 1.407], loss: 7.563054, mae: 53.508640, mean_q: 71.035721
  537959/1100000: episode: 1294, duration: 2.687s, episode steps: 384, steps per second: 143, episode reward: -232.284, mean reward: -0.605 [-100.000, 33.351], mean action: 1.638 [0.000, 3.000], mean observation: 0.175 [-2.063, 1.511], loss: 60.124207, mae: 53.703796, mean_q: 71.211548
  538225/1100000: episode: 1295, duration: 1.822s, episode steps: 266, steps per second: 146, episode reward: 196.971, mean reward: 0.740 [-11.606, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.222 [-0.827, 1.393], loss: 8.727338, mae: 53.577782, mean_q: 71.093369
  538311/1100000: episode: 1296, duration: 0.572s, episode steps: 86, steps per second: 150, episode reward: -38.181, mean reward: -0.444 [-100.000, 16.227], mean action: 1.442 [0.000, 3.000], mean observation: 0.016 [-2.287, 1.405], loss: 20.233017, mae: 53.903713, mean_q: 71.761177
  538378/1100000: episode: 1297, duration: 0.451s, episode steps: 67, steps per second: 149, episode reward: -278.396, mean reward: -4.155 [-100.000, 4.769], mean action: 0.940 [0.000, 3.000], mean observation: 0.008 [-2.845, 1.516], loss: 7.739314, mae: 53.534752, mean_q: 71.357216
  538532/1100000: episode: 1298, duration: 1.034s, episode steps: 154, steps per second: 149, episode reward: -10.963, mean reward: -0.071 [-100.000, 13.584], mean action: 1.721 [0.000, 3.000], mean observation: -0.027 [-1.461, 1.499], loss: 6.220134, mae: 53.712486, mean_q: 71.088661
  538731/1100000: episode: 1299, duration: 1.342s, episode steps: 199, steps per second: 148, episode reward: -243.919, mean reward: -1.226 [-100.000, 5.040], mean action: 1.653 [0.000, 3.000], mean observation: 0.008 [-1.005, 1.649], loss: 8.963684, mae: 53.753654, mean_q: 71.336937
  538899/1100000: episode: 1300, duration: 1.128s, episode steps: 168, steps per second: 149, episode reward: -312.854, mean reward: -1.862 [-100.000, 39.178], mean action: 1.607 [0.000, 3.000], mean observation: 0.107 [-1.115, 3.595], loss: 40.741676, mae: 53.788086, mean_q: 71.718575
  539330/1100000: episode: 1301, duration: 2.950s, episode steps: 431, steps per second: 146, episode reward: 239.036, mean reward: 0.555 [-20.558, 100.000], mean action: 1.763 [0.000, 3.000], mean observation: -0.007 [-0.924, 1.518], loss: 17.716396, mae: 54.424889, mean_q: 72.421829
  539684/1100000: episode: 1302, duration: 2.487s, episode steps: 354, steps per second: 142, episode reward: 213.232, mean reward: 0.602 [-11.810, 100.000], mean action: 1.918 [0.000, 3.000], mean observation: 0.093 [-0.689, 1.424], loss: 115.941696, mae: 54.357521, mean_q: 72.412079
  540138/1100000: episode: 1303, duration: 3.244s, episode steps: 454, steps per second: 140, episode reward: 277.092, mean reward: 0.610 [-18.713, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.073 [-0.768, 1.402], loss: 11.203817, mae: 54.819733, mean_q: 73.294235
  540396/1100000: episode: 1304, duration: 1.787s, episode steps: 258, steps per second: 144, episode reward: -24.894, mean reward: -0.096 [-100.000, 12.834], mean action: 1.903 [0.000, 3.000], mean observation: 0.141 [-0.895, 1.387], loss: 109.611267, mae: 55.083485, mean_q: 73.566673
  540710/1100000: episode: 1305, duration: 2.177s, episode steps: 314, steps per second: 144, episode reward: 236.421, mean reward: 0.753 [-17.426, 100.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.076 [-1.154, 1.390], loss: 9.636723, mae: 55.501144, mean_q: 74.153404
  540795/1100000: episode: 1306, duration: 0.565s, episode steps: 85, steps per second: 151, episode reward: -285.865, mean reward: -3.363 [-100.000, 79.780], mean action: 1.224 [0.000, 3.000], mean observation: 0.068 [-1.096, 3.078], loss: 235.592072, mae: 56.580441, mean_q: 75.104607
  541001/1100000: episode: 1307, duration: 1.389s, episode steps: 206, steps per second: 148, episode reward: -318.350, mean reward: -1.545 [-100.000, 8.517], mean action: 1.665 [0.000, 3.000], mean observation: 0.239 [-1.507, 2.322], loss: 12.751131, mae: 57.066738, mean_q: 76.058357
  541176/1100000: episode: 1308, duration: 1.186s, episode steps: 175, steps per second: 148, episode reward: -451.502, mean reward: -2.580 [-100.000, 71.844], mean action: 1.531 [0.000, 3.000], mean observation: 0.249 [-2.836, 4.634], loss: 16.161335, mae: 56.440624, mean_q: 75.401962
  541287/1100000: episode: 1309, duration: 0.754s, episode steps: 111, steps per second: 147, episode reward: -16.476, mean reward: -0.148 [-100.000, 50.845], mean action: 1.505 [0.000, 3.000], mean observation: -0.106 [-1.602, 1.386], loss: 220.132202, mae: 58.156906, mean_q: 77.788780
  541341/1100000: episode: 1310, duration: 0.368s, episode steps: 54, steps per second: 147, episode reward: -311.814, mean reward: -5.774 [-100.000, -0.929], mean action: 0.815 [0.000, 3.000], mean observation: 0.107 [-1.823, 1.737], loss: 5.533637, mae: 56.838215, mean_q: 75.589020
  541468/1100000: episode: 1311, duration: 0.850s, episode steps: 127, steps per second: 149, episode reward: -69.823, mean reward: -0.550 [-100.000, 13.403], mean action: 1.772 [0.000, 3.000], mean observation: 0.073 [-2.802, 1.538], loss: 11.873576, mae: 57.165436, mean_q: 75.890060
  541590/1100000: episode: 1312, duration: 0.819s, episode steps: 122, steps per second: 149, episode reward: -115.336, mean reward: -0.945 [-100.000, 11.674], mean action: 1.574 [0.000, 3.000], mean observation: -0.032 [-2.616, 1.417], loss: 87.942772, mae: 58.307049, mean_q: 77.849770
  541693/1100000: episode: 1313, duration: 0.689s, episode steps: 103, steps per second: 149, episode reward: -25.078, mean reward: -0.243 [-100.000, 47.794], mean action: 1.515 [0.000, 3.000], mean observation: 0.055 [-1.470, 1.407], loss: 137.636703, mae: 58.198242, mean_q: 77.216087
  541817/1100000: episode: 1314, duration: 0.835s, episode steps: 124, steps per second: 148, episode reward: -336.317, mean reward: -2.712 [-100.000, 7.358], mean action: 1.613 [0.000, 3.000], mean observation: 0.252 [-1.732, 7.136], loss: 10.939635, mae: 58.372971, mean_q: 78.260986
  541976/1100000: episode: 1315, duration: 1.069s, episode steps: 159, steps per second: 149, episode reward: 25.363, mean reward: 0.160 [-100.000, 15.391], mean action: 1.887 [0.000, 3.000], mean observation: -0.073 [-0.835, 1.404], loss: 142.270569, mae: 58.683052, mean_q: 78.159538
  542263/1100000: episode: 1316, duration: 1.980s, episode steps: 287, steps per second: 145, episode reward: -393.490, mean reward: -1.371 [-100.000, 20.756], mean action: 1.662 [0.000, 3.000], mean observation: 0.237 [-1.192, 3.199], loss: 26.585022, mae: 58.726971, mean_q: 78.214203
  542356/1100000: episode: 1317, duration: 0.628s, episode steps: 93, steps per second: 148, episode reward: -278.964, mean reward: -3.000 [-100.000, 2.886], mean action: 1.581 [0.000, 3.000], mean observation: 0.143 [-1.402, 2.173], loss: 15.211545, mae: 60.046917, mean_q: 80.484886
  542706/1100000: episode: 1318, duration: 2.462s, episode steps: 350, steps per second: 142, episode reward: 188.665, mean reward: 0.539 [-20.397, 100.000], mean action: 2.289 [0.000, 3.000], mean observation: 0.070 [-0.863, 1.388], loss: 74.192001, mae: 58.794582, mean_q: 78.227791
  543406/1100000: episode: 1319, duration: 5.325s, episode steps: 700, steps per second: 131, episode reward: 219.670, mean reward: 0.314 [-18.862, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.204 [-0.440, 1.394], loss: 40.201904, mae: 59.266808, mean_q: 79.077835
  543763/1100000: episode: 1320, duration: 2.458s, episode steps: 357, steps per second: 145, episode reward: 220.859, mean reward: 0.619 [-17.705, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.108 [-0.918, 1.408], loss: 20.275732, mae: 59.880520, mean_q: 79.622276
  543855/1100000: episode: 1321, duration: 0.619s, episode steps: 92, steps per second: 149, episode reward: -512.036, mean reward: -5.566 [-100.000, 3.164], mean action: 1.130 [0.000, 3.000], mean observation: 0.369 [-1.783, 4.334], loss: 10.977019, mae: 59.248573, mean_q: 79.573929
  543964/1100000: episode: 1322, duration: 0.728s, episode steps: 109, steps per second: 150, episode reward: -239.042, mean reward: -2.193 [-100.000, 5.873], mean action: 1.220 [0.000, 3.000], mean observation: 0.110 [-1.111, 3.177], loss: 68.736458, mae: 60.336048, mean_q: 80.385162
  544087/1100000: episode: 1323, duration: 0.825s, episode steps: 123, steps per second: 149, episode reward: -101.025, mean reward: -0.821 [-100.000, 10.350], mean action: 1.967 [0.000, 3.000], mean observation: -0.050 [-1.499, 1.410], loss: 13.473016, mae: 60.459015, mean_q: 80.889755
  544317/1100000: episode: 1324, duration: 1.543s, episode steps: 230, steps per second: 149, episode reward: 22.027, mean reward: 0.096 [-100.000, 16.743], mean action: 1.739 [0.000, 3.000], mean observation: -0.044 [-0.689, 1.452], loss: 69.290916, mae: 60.807209, mean_q: 81.151939
  544375/1100000: episode: 1325, duration: 0.392s, episode steps: 58, steps per second: 148, episode reward: -265.963, mean reward: -4.586 [-100.000, 4.424], mean action: 1.328 [0.000, 3.000], mean observation: 0.158 [-2.486, 1.703], loss: 51.594051, mae: 60.465588, mean_q: 80.589096
  544531/1100000: episode: 1326, duration: 1.037s, episode steps: 156, steps per second: 150, episode reward: -270.755, mean reward: -1.736 [-100.000, 5.694], mean action: 1.282 [0.000, 3.000], mean observation: 0.123 [-1.021, 2.088], loss: 52.482189, mae: 60.875740, mean_q: 81.170822
  544621/1100000: episode: 1327, duration: 0.601s, episode steps: 90, steps per second: 150, episode reward: -50.683, mean reward: -0.563 [-100.000, 16.608], mean action: 1.789 [0.000, 3.000], mean observation: -0.063 [-2.635, 1.413], loss: 26.553240, mae: 61.364941, mean_q: 80.983086
  544768/1100000: episode: 1328, duration: 0.986s, episode steps: 147, steps per second: 149, episode reward: -375.999, mean reward: -2.558 [-100.000, 4.577], mean action: 1.442 [0.000, 3.000], mean observation: 0.149 [-3.467, 3.175], loss: 32.976959, mae: 61.391235, mean_q: 81.890434
  544859/1100000: episode: 1329, duration: 0.611s, episode steps: 91, steps per second: 149, episode reward: -273.881, mean reward: -3.010 [-100.000, 4.389], mean action: 1.451 [0.000, 3.000], mean observation: 0.136 [-1.220, 2.141], loss: 22.836283, mae: 60.423706, mean_q: 80.655640
  544933/1100000: episode: 1330, duration: 0.502s, episode steps: 74, steps per second: 148, episode reward: -535.952, mean reward: -7.243 [-100.000, 1.962], mean action: 1.419 [0.000, 3.000], mean observation: 0.216 [-2.483, 3.910], loss: 7.775625, mae: 60.840992, mean_q: 80.724823
  545005/1100000: episode: 1331, duration: 0.481s, episode steps: 72, steps per second: 150, episode reward: -587.049, mean reward: -8.153 [-100.000, 2.587], mean action: 1.056 [0.000, 2.000], mean observation: 0.258 [-2.240, 4.107], loss: 8.316310, mae: 61.649441, mean_q: 82.427643
  545097/1100000: episode: 1332, duration: 0.614s, episode steps: 92, steps per second: 150, episode reward: -32.481, mean reward: -0.353 [-100.000, 9.415], mean action: 1.957 [1.000, 3.000], mean observation: -0.046 [-1.204, 1.403], loss: 10.344745, mae: 61.369843, mean_q: 82.020515
  545221/1100000: episode: 1333, duration: 0.832s, episode steps: 124, steps per second: 149, episode reward: -45.462, mean reward: -0.367 [-100.000, 18.417], mean action: 1.847 [0.000, 3.000], mean observation: -0.039 [-1.637, 1.478], loss: 8.774483, mae: 61.100994, mean_q: 80.787437
  545294/1100000: episode: 1334, duration: 0.490s, episode steps: 73, steps per second: 149, episode reward: -416.751, mean reward: -5.709 [-100.000, 1.667], mean action: 1.082 [0.000, 3.000], mean observation: 0.134 [-6.529, 2.991], loss: 29.760693, mae: 61.598377, mean_q: 82.147003
  545391/1100000: episode: 1335, duration: 0.647s, episode steps: 97, steps per second: 150, episode reward: -64.339, mean reward: -0.663 [-100.000, 8.224], mean action: 1.485 [0.000, 3.000], mean observation: -0.090 [-2.319, 1.424], loss: 16.564747, mae: 62.140182, mean_q: 83.120689
  545474/1100000: episode: 1336, duration: 0.558s, episode steps: 83, steps per second: 149, episode reward: -774.130, mean reward: -9.327 [-100.000, 1.780], mean action: 0.843 [0.000, 2.000], mean observation: 0.341 [-4.589, 6.806], loss: 18.071337, mae: 61.450008, mean_q: 82.239037
  545744/1100000: episode: 1337, duration: 1.837s, episode steps: 270, steps per second: 147, episode reward: 251.732, mean reward: 0.932 [-10.208, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.116 [-0.924, 1.418], loss: 11.282765, mae: 61.750816, mean_q: 82.209274
  545825/1100000: episode: 1338, duration: 0.567s, episode steps: 81, steps per second: 143, episode reward: -535.916, mean reward: -6.616 [-100.000, 1.788], mean action: 0.963 [0.000, 2.000], mean observation: 0.327 [-1.821, 4.609], loss: 9.183739, mae: 60.833134, mean_q: 81.036049
  545931/1100000: episode: 1339, duration: 0.706s, episode steps: 106, steps per second: 150, episode reward: -84.955, mean reward: -0.801 [-100.000, 13.487], mean action: 1.528 [0.000, 3.000], mean observation: -0.046 [-1.087, 1.432], loss: 7.280691, mae: 60.313248, mean_q: 80.412285
  546014/1100000: episode: 1340, duration: 0.554s, episode steps: 83, steps per second: 150, episode reward: -348.077, mean reward: -4.194 [-100.000, 4.309], mean action: 1.361 [0.000, 3.000], mean observation: 0.272 [-1.564, 2.703], loss: 9.612153, mae: 62.027184, mean_q: 82.494896
  546095/1100000: episode: 1341, duration: 0.542s, episode steps: 81, steps per second: 149, episode reward: -642.266, mean reward: -7.929 [-100.000, 2.001], mean action: 0.988 [0.000, 3.000], mean observation: 0.376 [-1.883, 5.085], loss: 7.718101, mae: 61.847534, mean_q: 82.001915
  546756/1100000: episode: 1342, duration: 4.645s, episode steps: 661, steps per second: 142, episode reward: 259.378, mean reward: 0.392 [-19.255, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.134 [-0.998, 1.394], loss: 15.977924, mae: 61.880135, mean_q: 81.934196
  546923/1100000: episode: 1343, duration: 1.114s, episode steps: 167, steps per second: 150, episode reward: 270.506, mean reward: 1.620 [-2.395, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.086 [-0.914, 1.400], loss: 11.115160, mae: 61.353573, mean_q: 80.438660
  547371/1100000: episode: 1344, duration: 3.108s, episode steps: 448, steps per second: 144, episode reward: 223.118, mean reward: 0.498 [-17.506, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.001 [-0.600, 1.402], loss: 16.426563, mae: 61.889885, mean_q: 81.084785
  547641/1100000: episode: 1345, duration: 1.839s, episode steps: 270, steps per second: 147, episode reward: 272.510, mean reward: 1.009 [-17.566, 100.000], mean action: 0.778 [0.000, 3.000], mean observation: 0.119 [-0.943, 1.394], loss: 15.064044, mae: 61.615639, mean_q: 80.783554
  548207/1100000: episode: 1346, duration: 4.060s, episode steps: 566, steps per second: 139, episode reward: 236.228, mean reward: 0.417 [-18.751, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.003 [-1.292, 1.395], loss: 19.016792, mae: 61.380543, mean_q: 80.110275
  548429/1100000: episode: 1347, duration: 1.507s, episode steps: 222, steps per second: 147, episode reward: 277.118, mean reward: 1.248 [-10.737, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.038 [-0.882, 1.391], loss: 15.699547, mae: 61.333202, mean_q: 79.539543
  548758/1100000: episode: 1348, duration: 2.266s, episode steps: 329, steps per second: 145, episode reward: -255.020, mean reward: -0.775 [-100.000, 8.194], mean action: 1.553 [0.000, 3.000], mean observation: 0.228 [-1.468, 2.117], loss: 11.980373, mae: 61.754856, mean_q: 80.475960
  548811/1100000: episode: 1349, duration: 0.357s, episode steps: 53, steps per second: 148, episode reward: -219.209, mean reward: -4.136 [-100.000, 3.132], mean action: 1.566 [0.000, 3.000], mean observation: -0.153 [-1.624, 3.922], loss: 11.179600, mae: 61.497505, mean_q: 80.878181
  548998/1100000: episode: 1350, duration: 1.261s, episode steps: 187, steps per second: 148, episode reward: -146.866, mean reward: -0.785 [-100.000, 10.488], mean action: 1.674 [0.000, 3.000], mean observation: 0.127 [-0.894, 1.593], loss: 23.932570, mae: 61.953636, mean_q: 79.873596
  549482/1100000: episode: 1351, duration: 3.421s, episode steps: 484, steps per second: 141, episode reward: 206.735, mean reward: 0.427 [-9.868, 100.000], mean action: 2.165 [0.000, 3.000], mean observation: 0.078 [-0.658, 1.691], loss: 15.748913, mae: 61.758919, mean_q: 79.578224
  549687/1100000: episode: 1352, duration: 1.381s, episode steps: 205, steps per second: 148, episode reward: 291.504, mean reward: 1.422 [-3.919, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.079 [-0.643, 1.400], loss: 18.199652, mae: 62.075600, mean_q: 80.297752
  549950/1100000: episode: 1353, duration: 1.774s, episode steps: 263, steps per second: 148, episode reward: 244.748, mean reward: 0.931 [-10.752, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.193 [-0.812, 1.481], loss: 6.720063, mae: 62.326668, mean_q: 81.499771
  550950/1100000: episode: 1354, duration: 7.381s, episode steps: 1000, steps per second: 135, episode reward: 3.179, mean reward: 0.003 [-25.395, 29.966], mean action: 2.037 [0.000, 3.000], mean observation: 0.006 [-0.635, 1.388], loss: 11.681084, mae: 61.591782, mean_q: 79.804405
  551950/1100000: episode: 1355, duration: 8.379s, episode steps: 1000, steps per second: 119, episode reward: -54.943, mean reward: -0.055 [-4.782, 5.062], mean action: 1.792 [0.000, 3.000], mean observation: 0.172 [-0.665, 1.421], loss: 11.633459, mae: 61.190582, mean_q: 79.829651
  552950/1100000: episode: 1356, duration: 7.673s, episode steps: 1000, steps per second: 130, episode reward: -120.923, mean reward: -0.121 [-4.857, 3.984], mean action: 1.889 [0.000, 3.000], mean observation: 0.087 [-0.600, 1.514], loss: 9.950072, mae: 61.237534, mean_q: 79.976547
  553271/1100000: episode: 1357, duration: 2.202s, episode steps: 321, steps per second: 146, episode reward: 251.265, mean reward: 0.783 [-18.873, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.014 [-1.171, 1.425], loss: 27.399363, mae: 61.954639, mean_q: 80.585403
  553413/1100000: episode: 1358, duration: 0.935s, episode steps: 142, steps per second: 152, episode reward: -160.887, mean reward: -1.133 [-100.000, 1.843], mean action: 1.092 [0.000, 3.000], mean observation: 0.131 [-0.664, 1.418], loss: 8.117244, mae: 61.550774, mean_q: 80.906281
  553569/1100000: episode: 1359, duration: 1.048s, episode steps: 156, steps per second: 149, episode reward: -217.718, mean reward: -1.396 [-100.000, 23.041], mean action: 1.571 [0.000, 3.000], mean observation: 0.053 [-1.803, 1.407], loss: 14.993733, mae: 62.093864, mean_q: 81.138756
  554569/1100000: episode: 1360, duration: 7.549s, episode steps: 1000, steps per second: 132, episode reward: -58.955, mean reward: -0.059 [-4.980, 4.769], mean action: 1.904 [0.000, 3.000], mean observation: 0.024 [-0.649, 1.391], loss: 10.801125, mae: 61.739933, mean_q: 80.583176
  554872/1100000: episode: 1361, duration: 2.040s, episode steps: 303, steps per second: 149, episode reward: 247.224, mean reward: 0.816 [-17.910, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: 0.115 [-0.813, 1.402], loss: 11.495180, mae: 61.499241, mean_q: 80.307388
  555872/1100000: episode: 1362, duration: 7.844s, episode steps: 1000, steps per second: 127, episode reward: -12.467, mean reward: -0.012 [-4.693, 5.067], mean action: 1.855 [0.000, 3.000], mean observation: -0.023 [-0.766, 1.388], loss: 9.183361, mae: 61.677757, mean_q: 80.167267
  556052/1100000: episode: 1363, duration: 1.206s, episode steps: 180, steps per second: 149, episode reward: -209.738, mean reward: -1.165 [-100.000, 5.455], mean action: 1.500 [0.000, 3.000], mean observation: 0.152 [-0.758, 1.389], loss: 15.225502, mae: 61.658909, mean_q: 80.046021
  556737/1100000: episode: 1364, duration: 4.980s, episode steps: 685, steps per second: 138, episode reward: 257.380, mean reward: 0.376 [-19.151, 100.000], mean action: 0.896 [0.000, 3.000], mean observation: 0.161 [-0.800, 1.481], loss: 11.247826, mae: 61.914524, mean_q: 79.677612
  556834/1100000: episode: 1365, duration: 0.646s, episode steps: 97, steps per second: 150, episode reward: 18.444, mean reward: 0.190 [-100.000, 11.275], mean action: 1.351 [0.000, 3.000], mean observation: -0.051 [-0.779, 1.392], loss: 8.819064, mae: 61.078533, mean_q: 78.564636
  557506/1100000: episode: 1366, duration: 5.145s, episode steps: 672, steps per second: 131, episode reward: 166.167, mean reward: 0.247 [-14.094, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.147 [-0.797, 1.439], loss: 12.683350, mae: 61.375641, mean_q: 79.280502
  557951/1100000: episode: 1367, duration: 3.049s, episode steps: 445, steps per second: 146, episode reward: 280.411, mean reward: 0.630 [-19.484, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.057 [-0.570, 1.426], loss: 8.831333, mae: 61.545147, mean_q: 80.309814
  558297/1100000: episode: 1368, duration: 2.416s, episode steps: 346, steps per second: 143, episode reward: 259.840, mean reward: 0.751 [-10.826, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.009 [-0.681, 1.389], loss: 14.392300, mae: 61.196869, mean_q: 79.073875
  558407/1100000: episode: 1369, duration: 0.731s, episode steps: 110, steps per second: 150, episode reward: 31.505, mean reward: 0.286 [-100.000, 21.273], mean action: 1.345 [0.000, 3.000], mean observation: 0.007 [-0.787, 1.415], loss: 12.157147, mae: 61.045990, mean_q: 78.336853
  559299/1100000: episode: 1370, duration: 6.478s, episode steps: 892, steps per second: 138, episode reward: 171.831, mean reward: 0.193 [-22.472, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.215 [-0.636, 1.436], loss: 14.537887, mae: 61.288631, mean_q: 79.996887
  559467/1100000: episode: 1371, duration: 1.124s, episode steps: 168, steps per second: 150, episode reward: -12.016, mean reward: -0.072 [-100.000, 22.391], mean action: 1.482 [0.000, 3.000], mean observation: 0.092 [-1.102, 1.440], loss: 15.393862, mae: 60.397705, mean_q: 79.189560
  560013/1100000: episode: 1372, duration: 3.960s, episode steps: 546, steps per second: 138, episode reward: 272.439, mean reward: 0.499 [-17.332, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.119 [-0.591, 1.389], loss: 11.519133, mae: 60.419872, mean_q: 78.927269
  560129/1100000: episode: 1373, duration: 0.767s, episode steps: 116, steps per second: 151, episode reward: -33.983, mean reward: -0.293 [-100.000, 11.961], mean action: 1.164 [0.000, 3.000], mean observation: -0.019 [-1.500, 1.435], loss: 10.609030, mae: 60.789474, mean_q: 79.061737
  560398/1100000: episode: 1374, duration: 1.855s, episode steps: 269, steps per second: 145, episode reward: 253.273, mean reward: 0.942 [-12.640, 100.000], mean action: 2.030 [0.000, 3.000], mean observation: 0.158 [-0.665, 1.394], loss: 17.077595, mae: 60.431995, mean_q: 79.477264
  560683/1100000: episode: 1375, duration: 1.980s, episode steps: 285, steps per second: 144, episode reward: 246.778, mean reward: 0.866 [-3.318, 100.000], mean action: 2.154 [0.000, 3.000], mean observation: -0.018 [-0.874, 1.433], loss: 10.421645, mae: 60.437878, mean_q: 78.755966
  561032/1100000: episode: 1376, duration: 2.429s, episode steps: 349, steps per second: 144, episode reward: 284.389, mean reward: 0.815 [-12.336, 100.000], mean action: 1.610 [0.000, 3.000], mean observation: 0.093 [-0.639, 1.406], loss: 13.613929, mae: 60.621559, mean_q: 79.008034
  561500/1100000: episode: 1377, duration: 3.242s, episode steps: 468, steps per second: 144, episode reward: 205.269, mean reward: 0.439 [-17.867, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: -0.020 [-0.851, 1.405], loss: 7.521416, mae: 60.185211, mean_q: 78.611794
  561718/1100000: episode: 1378, duration: 1.477s, episode steps: 218, steps per second: 148, episode reward: 274.681, mean reward: 1.260 [-10.187, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.069 [-0.798, 1.390], loss: 12.085758, mae: 60.890274, mean_q: 79.673004
  562404/1100000: episode: 1379, duration: 4.915s, episode steps: 686, steps per second: 140, episode reward: 273.477, mean reward: 0.399 [-19.057, 100.000], mean action: 0.819 [0.000, 3.000], mean observation: 0.151 [-0.743, 1.703], loss: 11.894871, mae: 60.179489, mean_q: 77.823242
  563258/1100000: episode: 1380, duration: 6.285s, episode steps: 854, steps per second: 136, episode reward: 187.439, mean reward: 0.219 [-23.379, 100.000], mean action: 1.693 [0.000, 3.000], mean observation: 0.062 [-0.932, 1.469], loss: 9.443654, mae: 60.219803, mean_q: 78.231171
  563446/1100000: episode: 1381, duration: 1.261s, episode steps: 188, steps per second: 149, episode reward: 259.001, mean reward: 1.378 [-9.148, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.096 [-0.729, 1.401], loss: 8.800928, mae: 60.143593, mean_q: 77.772957
  564243/1100000: episode: 1382, duration: 5.735s, episode steps: 797, steps per second: 139, episode reward: -152.134, mean reward: -0.191 [-100.000, 14.388], mean action: 1.814 [0.000, 3.000], mean observation: -0.015 [-0.755, 1.394], loss: 10.186878, mae: 60.119328, mean_q: 77.646088
  564527/1100000: episode: 1383, duration: 1.946s, episode steps: 284, steps per second: 146, episode reward: 263.763, mean reward: 0.929 [-9.915, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.038 [-1.029, 1.405], loss: 8.046609, mae: 60.151093, mean_q: 77.972366
  564952/1100000: episode: 1384, duration: 2.973s, episode steps: 425, steps per second: 143, episode reward: 256.740, mean reward: 0.604 [-9.789, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.043 [-0.670, 1.539], loss: 6.222002, mae: 60.129089, mean_q: 77.642426
  565560/1100000: episode: 1385, duration: 4.376s, episode steps: 608, steps per second: 139, episode reward: 219.660, mean reward: 0.361 [-8.657, 100.000], mean action: 1.867 [0.000, 3.000], mean observation: 0.169 [-0.825, 1.660], loss: 11.616831, mae: 59.861084, mean_q: 77.208397
  565695/1100000: episode: 1386, duration: 0.914s, episode steps: 135, steps per second: 148, episode reward: -13.057, mean reward: -0.097 [-100.000, 24.687], mean action: 1.822 [0.000, 3.000], mean observation: -0.062 [-0.721, 1.393], loss: 8.817199, mae: 60.085266, mean_q: 77.084435
  566039/1100000: episode: 1387, duration: 2.392s, episode steps: 344, steps per second: 144, episode reward: 258.902, mean reward: 0.753 [-9.301, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.169 [-0.534, 1.430], loss: 7.506573, mae: 59.805607, mean_q: 77.489532
  566269/1100000: episode: 1388, duration: 1.564s, episode steps: 230, steps per second: 147, episode reward: 255.703, mean reward: 1.112 [-5.042, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.104 [-1.019, 1.406], loss: 9.032909, mae: 59.827003, mean_q: 76.780006
  566508/1100000: episode: 1389, duration: 1.603s, episode steps: 239, steps per second: 149, episode reward: 262.732, mean reward: 1.099 [-4.435, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.104 [-0.664, 1.442], loss: 8.706216, mae: 59.858215, mean_q: 77.722687
  566754/1100000: episode: 1390, duration: 1.673s, episode steps: 246, steps per second: 147, episode reward: -7.557, mean reward: -0.031 [-100.000, 21.423], mean action: 1.846 [0.000, 3.000], mean observation: -0.002 [-1.005, 1.513], loss: 7.463926, mae: 60.297832, mean_q: 77.972939
  567033/1100000: episode: 1391, duration: 1.904s, episode steps: 279, steps per second: 147, episode reward: -245.548, mean reward: -0.880 [-100.000, 4.416], mean action: 1.452 [0.000, 3.000], mean observation: 0.052 [-1.129, 1.439], loss: 8.319196, mae: 60.417862, mean_q: 77.917282
  567337/1100000: episode: 1392, duration: 2.065s, episode steps: 304, steps per second: 147, episode reward: 225.590, mean reward: 0.742 [-14.722, 100.000], mean action: 0.911 [0.000, 3.000], mean observation: -0.004 [-0.878, 1.464], loss: 15.838360, mae: 59.811859, mean_q: 76.636703
  568000/1100000: episode: 1393, duration: 4.871s, episode steps: 663, steps per second: 136, episode reward: 237.445, mean reward: 0.358 [-23.720, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.197 [-0.776, 1.420], loss: 9.719004, mae: 59.594868, mean_q: 76.821930
  568348/1100000: episode: 1394, duration: 2.455s, episode steps: 348, steps per second: 142, episode reward: 287.450, mean reward: 0.826 [-17.407, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.125 [-0.954, 1.395], loss: 9.121826, mae: 59.177399, mean_q: 76.322998
  569348/1100000: episode: 1395, duration: 7.288s, episode steps: 1000, steps per second: 137, episode reward: 21.580, mean reward: 0.022 [-23.364, 26.754], mean action: 1.161 [0.000, 3.000], mean observation: 0.038 [-0.758, 1.409], loss: 8.809406, mae: 59.270493, mean_q: 76.376221
  569523/1100000: episode: 1396, duration: 1.180s, episode steps: 175, steps per second: 148, episode reward: 10.371, mean reward: 0.059 [-100.000, 8.721], mean action: 1.811 [0.000, 3.000], mean observation: -0.056 [-0.754, 1.476], loss: 5.373951, mae: 59.061222, mean_q: 76.021477
  569778/1100000: episode: 1397, duration: 1.730s, episode steps: 255, steps per second: 147, episode reward: 255.334, mean reward: 1.001 [-17.373, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.098 [-0.766, 1.443], loss: 8.097754, mae: 59.401947, mean_q: 76.134506
  570291/1100000: episode: 1398, duration: 3.692s, episode steps: 513, steps per second: 139, episode reward: 257.722, mean reward: 0.502 [-24.142, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.185 [-0.996, 1.400], loss: 7.113801, mae: 59.455933, mean_q: 76.602493
  570781/1100000: episode: 1399, duration: 3.561s, episode steps: 490, steps per second: 138, episode reward: 179.158, mean reward: 0.366 [-17.271, 100.000], mean action: 1.869 [0.000, 3.000], mean observation: 0.038 [-0.659, 1.482], loss: 6.589285, mae: 59.293560, mean_q: 76.570419
  570885/1100000: episode: 1400, duration: 0.726s, episode steps: 104, steps per second: 143, episode reward: -18.511, mean reward: -0.178 [-100.000, 12.976], mean action: 1.654 [0.000, 3.000], mean observation: -0.098 [-1.159, 3.934], loss: 11.240555, mae: 59.634052, mean_q: 76.221375
  571293/1100000: episode: 1401, duration: 2.911s, episode steps: 408, steps per second: 140, episode reward: 230.810, mean reward: 0.566 [-18.457, 100.000], mean action: 1.515 [0.000, 3.000], mean observation: 0.123 [-0.712, 1.386], loss: 8.907956, mae: 59.267941, mean_q: 75.496216
  571455/1100000: episode: 1402, duration: 1.093s, episode steps: 162, steps per second: 148, episode reward: -28.915, mean reward: -0.178 [-100.000, 11.155], mean action: 1.802 [0.000, 3.000], mean observation: -0.008 [-0.822, 1.458], loss: 15.685861, mae: 59.144012, mean_q: 76.493881
  571960/1100000: episode: 1403, duration: 3.632s, episode steps: 505, steps per second: 139, episode reward: -177.917, mean reward: -0.352 [-100.000, 36.459], mean action: 1.701 [0.000, 3.000], mean observation: 0.061 [-1.007, 1.740], loss: 7.949247, mae: 59.047253, mean_q: 75.972725
  572239/1100000: episode: 1404, duration: 1.901s, episode steps: 279, steps per second: 147, episode reward: -216.103, mean reward: -0.775 [-100.000, 26.252], mean action: 1.703 [0.000, 3.000], mean observation: 0.090 [-0.600, 1.755], loss: 7.809233, mae: 58.222073, mean_q: 75.045769
  572596/1100000: episode: 1405, duration: 2.486s, episode steps: 357, steps per second: 144, episode reward: -117.725, mean reward: -0.330 [-100.000, 10.867], mean action: 1.560 [0.000, 3.000], mean observation: 0.107 [-0.937, 1.385], loss: 11.169989, mae: 58.190159, mean_q: 74.836632
  572800/1100000: episode: 1406, duration: 1.372s, episode steps: 204, steps per second: 149, episode reward: 36.870, mean reward: 0.181 [-100.000, 13.025], mean action: 1.809 [0.000, 3.000], mean observation: 0.123 [-0.701, 1.485], loss: 14.454894, mae: 57.813526, mean_q: 73.837769
  573800/1100000: episode: 1407, duration: 7.378s, episode steps: 1000, steps per second: 136, episode reward: 58.843, mean reward: 0.059 [-18.741, 21.081], mean action: 1.491 [0.000, 3.000], mean observation: -0.002 [-0.605, 1.441], loss: 10.876637, mae: 57.893898, mean_q: 74.267097
  574800/1100000: episode: 1408, duration: 7.515s, episode steps: 1000, steps per second: 133, episode reward: 141.364, mean reward: 0.141 [-19.323, 15.883], mean action: 0.843 [0.000, 3.000], mean observation: 0.173 [-0.778, 1.389], loss: 9.231922, mae: 57.649281, mean_q: 74.014847
  575023/1100000: episode: 1409, duration: 1.515s, episode steps: 223, steps per second: 147, episode reward: -117.245, mean reward: -0.526 [-100.000, 11.935], mean action: 1.623 [0.000, 3.000], mean observation: 0.032 [-1.234, 1.399], loss: 19.147802, mae: 57.516609, mean_q: 74.018990
  575325/1100000: episode: 1410, duration: 2.100s, episode steps: 302, steps per second: 144, episode reward: 240.692, mean reward: 0.797 [-10.529, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.047 [-0.797, 1.418], loss: 9.418622, mae: 57.600067, mean_q: 73.945290
  576325/1100000: episode: 1411, duration: 7.671s, episode steps: 1000, steps per second: 130, episode reward: -121.652, mean reward: -0.122 [-5.661, 4.991], mean action: 1.872 [0.000, 3.000], mean observation: 0.015 [-0.745, 1.401], loss: 12.240637, mae: 57.738728, mean_q: 74.504921
  576653/1100000: episode: 1412, duration: 2.343s, episode steps: 328, steps per second: 140, episode reward: 225.388, mean reward: 0.687 [-20.002, 100.000], mean action: 1.774 [0.000, 3.000], mean observation: 0.154 [-1.048, 1.412], loss: 10.305877, mae: 57.725407, mean_q: 74.800522
  577458/1100000: episode: 1413, duration: 5.880s, episode steps: 805, steps per second: 137, episode reward: 185.061, mean reward: 0.230 [-18.908, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: -0.019 [-0.670, 1.409], loss: 10.302672, mae: 57.936199, mean_q: 74.445175
  577788/1100000: episode: 1414, duration: 2.272s, episode steps: 330, steps per second: 145, episode reward: 293.741, mean reward: 0.890 [-21.224, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.205 [-1.278, 1.977], loss: 9.929008, mae: 58.418526, mean_q: 74.467484
  578315/1100000: episode: 1415, duration: 3.695s, episode steps: 527, steps per second: 143, episode reward: 247.116, mean reward: 0.469 [-21.506, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.024 [-0.634, 1.413], loss: 11.752166, mae: 58.290066, mean_q: 74.839806
  578541/1100000: episode: 1416, duration: 1.533s, episode steps: 226, steps per second: 147, episode reward: 240.989, mean reward: 1.066 [-17.745, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.055 [-0.802, 1.410], loss: 8.543211, mae: 58.189377, mean_q: 74.961121
  578909/1100000: episode: 1417, duration: 2.555s, episode steps: 368, steps per second: 144, episode reward: 228.604, mean reward: 0.621 [-9.536, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: -0.053 [-0.756, 1.392], loss: 13.017377, mae: 58.154469, mean_q: 75.147789
  579223/1100000: episode: 1418, duration: 2.185s, episode steps: 314, steps per second: 144, episode reward: -25.781, mean reward: -0.082 [-100.000, 17.496], mean action: 1.876 [0.000, 3.000], mean observation: -0.013 [-0.750, 1.681], loss: 11.200402, mae: 57.674358, mean_q: 74.879433
  580066/1100000: episode: 1419, duration: 6.095s, episode steps: 843, steps per second: 138, episode reward: 201.141, mean reward: 0.239 [-18.616, 100.000], mean action: 2.052 [0.000, 3.000], mean observation: 0.246 [-0.737, 1.403], loss: 17.806210, mae: 57.948307, mean_q: 75.087852
  580727/1100000: episode: 1420, duration: 5.010s, episode steps: 661, steps per second: 132, episode reward: 195.072, mean reward: 0.295 [-19.558, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: -0.016 [-1.004, 1.400], loss: 10.091838, mae: 57.965416, mean_q: 75.385368
  581254/1100000: episode: 1421, duration: 3.677s, episode steps: 527, steps per second: 143, episode reward: 270.311, mean reward: 0.513 [-22.739, 100.000], mean action: 0.763 [0.000, 3.000], mean observation: 0.108 [-0.731, 1.442], loss: 9.246632, mae: 57.719090, mean_q: 74.998650
  581467/1100000: episode: 1422, duration: 1.446s, episode steps: 213, steps per second: 147, episode reward: 282.814, mean reward: 1.328 [-8.556, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.076 [-0.872, 1.513], loss: 7.243455, mae: 57.905910, mean_q: 75.467804
  581683/1100000: episode: 1423, duration: 1.471s, episode steps: 216, steps per second: 147, episode reward: -79.391, mean reward: -0.368 [-100.000, 6.419], mean action: 1.894 [0.000, 3.000], mean observation: -0.117 [-2.789, 1.402], loss: 10.937860, mae: 58.163315, mean_q: 75.634094
  582642/1100000: episode: 1424, duration: 7.660s, episode steps: 959, steps per second: 125, episode reward: 246.450, mean reward: 0.257 [-18.744, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: -0.010 [-1.544, 2.034], loss: 7.862496, mae: 58.523373, mean_q: 76.163124
  582767/1100000: episode: 1425, duration: 0.831s, episode steps: 125, steps per second: 150, episode reward: -38.318, mean reward: -0.307 [-100.000, 10.350], mean action: 1.576 [0.000, 3.000], mean observation: 0.160 [-0.951, 2.864], loss: 14.536783, mae: 59.429256, mean_q: 77.261375
  583083/1100000: episode: 1426, duration: 2.186s, episode steps: 316, steps per second: 145, episode reward: 22.716, mean reward: 0.072 [-100.000, 16.956], mean action: 1.509 [0.000, 3.000], mean observation: -0.057 [-0.688, 1.484], loss: 10.232838, mae: 59.234356, mean_q: 77.186737
  583230/1100000: episode: 1427, duration: 0.989s, episode steps: 147, steps per second: 149, episode reward: -183.042, mean reward: -1.245 [-100.000, 14.703], mean action: 1.646 [0.000, 3.000], mean observation: 0.197 [-1.182, 1.526], loss: 11.174811, mae: 58.958973, mean_q: 77.402367
  583354/1100000: episode: 1428, duration: 0.826s, episode steps: 124, steps per second: 150, episode reward: 11.844, mean reward: 0.096 [-100.000, 13.092], mean action: 1.315 [0.000, 3.000], mean observation: 0.152 [-1.234, 1.479], loss: 6.886825, mae: 60.767941, mean_q: 78.946686
  583514/1100000: episode: 1429, duration: 1.078s, episode steps: 160, steps per second: 148, episode reward: 55.442, mean reward: 0.347 [-100.000, 16.700], mean action: 1.594 [0.000, 3.000], mean observation: 0.071 [-0.788, 1.440], loss: 12.861467, mae: 59.853722, mean_q: 77.512833
  583732/1100000: episode: 1430, duration: 1.492s, episode steps: 218, steps per second: 146, episode reward: 194.314, mean reward: 0.891 [-8.295, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.112 [-1.132, 1.404], loss: 9.070840, mae: 60.053814, mean_q: 78.330193
  584069/1100000: episode: 1431, duration: 2.369s, episode steps: 337, steps per second: 142, episode reward: -132.650, mean reward: -0.394 [-100.000, 59.963], mean action: 1.588 [0.000, 3.000], mean observation: -0.039 [-1.318, 1.413], loss: 12.644198, mae: 60.458206, mean_q: 78.981781
  584252/1100000: episode: 1432, duration: 1.240s, episode steps: 183, steps per second: 148, episode reward: 34.107, mean reward: 0.186 [-100.000, 19.801], mean action: 1.820 [0.000, 3.000], mean observation: 0.092 [-0.788, 1.612], loss: 11.009255, mae: 61.217384, mean_q: 80.204948
  584339/1100000: episode: 1433, duration: 0.587s, episode steps: 87, steps per second: 148, episode reward: -2.257, mean reward: -0.026 [-100.000, 18.657], mean action: 1.402 [0.000, 3.000], mean observation: 0.089 [-1.001, 1.689], loss: 44.497971, mae: 61.663288, mean_q: 80.532173
  584486/1100000: episode: 1434, duration: 0.985s, episode steps: 147, steps per second: 149, episode reward: 53.814, mean reward: 0.366 [-100.000, 27.162], mean action: 1.544 [0.000, 3.000], mean observation: 0.079 [-0.791, 1.410], loss: 12.300463, mae: 61.750500, mean_q: 80.952431
  584602/1100000: episode: 1435, duration: 0.775s, episode steps: 116, steps per second: 150, episode reward: -3.525, mean reward: -0.030 [-100.000, 11.572], mean action: 1.457 [0.000, 3.000], mean observation: -0.024 [-0.852, 1.495], loss: 10.748951, mae: 62.317368, mean_q: 81.191635
  585602/1100000: episode: 1436, duration: 7.408s, episode steps: 1000, steps per second: 135, episode reward: 63.042, mean reward: 0.063 [-18.801, 21.545], mean action: 1.910 [0.000, 3.000], mean observation: 0.265 [-0.904, 1.399], loss: 13.267850, mae: 62.453358, mean_q: 81.716972
  585766/1100000: episode: 1437, duration: 1.102s, episode steps: 164, steps per second: 149, episode reward: -207.533, mean reward: -1.265 [-100.000, 8.739], mean action: 1.482 [0.000, 3.000], mean observation: 0.171 [-1.169, 1.394], loss: 13.369785, mae: 63.433479, mean_q: 83.630219
  585940/1100000: episode: 1438, duration: 1.168s, episode steps: 174, steps per second: 149, episode reward: 34.245, mean reward: 0.197 [-100.000, 17.233], mean action: 1.603 [0.000, 3.000], mean observation: 0.195 [-1.403, 1.488], loss: 14.195922, mae: 63.683426, mean_q: 83.630867
  586142/1100000: episode: 1439, duration: 1.363s, episode steps: 202, steps per second: 148, episode reward: 59.236, mean reward: 0.293 [-100.000, 16.369], mean action: 1.564 [0.000, 3.000], mean observation: 0.021 [-0.681, 1.442], loss: 11.425428, mae: 63.233929, mean_q: 83.104561
  586422/1100000: episode: 1440, duration: 1.900s, episode steps: 280, steps per second: 147, episode reward: 277.548, mean reward: 0.991 [-6.376, 100.000], mean action: 0.768 [0.000, 3.000], mean observation: 0.137 [-0.846, 1.485], loss: 15.560271, mae: 64.205261, mean_q: 84.507072
  586665/1100000: episode: 1441, duration: 1.651s, episode steps: 243, steps per second: 147, episode reward: -1.441, mean reward: -0.006 [-100.000, 15.452], mean action: 1.782 [0.000, 3.000], mean observation: -0.029 [-0.815, 1.403], loss: 18.161589, mae: 64.213326, mean_q: 84.794449
  587445/1100000: episode: 1442, duration: 5.846s, episode steps: 780, steps per second: 133, episode reward: 229.492, mean reward: 0.294 [-18.597, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.003 [-0.600, 1.403], loss: 15.792585, mae: 65.293877, mean_q: 85.950569
  587872/1100000: episode: 1443, duration: 3.044s, episode steps: 427, steps per second: 140, episode reward: 263.946, mean reward: 0.618 [-2.927, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.052 [-0.541, 1.508], loss: 14.612531, mae: 65.115562, mean_q: 85.822601
  588035/1100000: episode: 1444, duration: 1.093s, episode steps: 163, steps per second: 149, episode reward: -2.173, mean reward: -0.013 [-100.000, 11.783], mean action: 1.693 [0.000, 3.000], mean observation: 0.088 [-0.955, 2.266], loss: 11.690543, mae: 65.457848, mean_q: 86.675095
  588124/1100000: episode: 1445, duration: 0.597s, episode steps: 89, steps per second: 149, episode reward: 30.308, mean reward: 0.341 [-100.000, 17.996], mean action: 1.449 [0.000, 3.000], mean observation: -0.007 [-1.137, 1.494], loss: 10.829124, mae: 65.993713, mean_q: 87.633102
  588541/1100000: episode: 1446, duration: 2.902s, episode steps: 417, steps per second: 144, episode reward: 273.218, mean reward: 0.655 [-10.271, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.046 [-0.686, 1.414], loss: 18.200903, mae: 65.431030, mean_q: 86.472900
  588821/1100000: episode: 1447, duration: 1.922s, episode steps: 280, steps per second: 146, episode reward: 258.087, mean reward: 0.922 [-18.251, 100.000], mean action: 1.907 [0.000, 3.000], mean observation: 0.157 [-0.924, 1.400], loss: 17.443972, mae: 66.088593, mean_q: 87.371414
  589101/1100000: episode: 1448, duration: 1.892s, episode steps: 280, steps per second: 148, episode reward: 263.257, mean reward: 0.940 [-18.744, 100.000], mean action: 0.739 [0.000, 3.000], mean observation: 0.098 [-0.856, 1.393], loss: 16.869947, mae: 65.724983, mean_q: 87.164925
  589410/1100000: episode: 1449, duration: 2.110s, episode steps: 309, steps per second: 146, episode reward: 229.003, mean reward: 0.741 [-16.866, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.053 [-0.642, 1.397], loss: 15.525806, mae: 65.818443, mean_q: 86.618385
  590266/1100000: episode: 1450, duration: 6.307s, episode steps: 856, steps per second: 136, episode reward: -292.530, mean reward: -0.342 [-100.000, 18.903], mean action: 1.436 [0.000, 3.000], mean observation: 0.056 [-1.829, 1.388], loss: 22.416061, mae: 65.950188, mean_q: 87.256058
  591132/1100000: episode: 1451, duration: 6.433s, episode steps: 866, steps per second: 135, episode reward: -176.431, mean reward: -0.204 [-100.000, 21.989], mean action: 1.490 [0.000, 3.000], mean observation: 0.160 [-0.533, 1.517], loss: 12.027797, mae: 66.308464, mean_q: 87.474281
  591670/1100000: episode: 1452, duration: 3.832s, episode steps: 538, steps per second: 140, episode reward: 199.737, mean reward: 0.371 [-8.490, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.034 [-0.772, 1.398], loss: 14.003003, mae: 66.164528, mean_q: 87.615967
  591758/1100000: episode: 1453, duration: 0.581s, episode steps: 88, steps per second: 151, episode reward: -74.723, mean reward: -0.849 [-100.000, 10.113], mean action: 1.125 [0.000, 3.000], mean observation: 0.004 [-1.335, 1.438], loss: 11.184114, mae: 66.319832, mean_q: 88.020271
  591820/1100000: episode: 1454, duration: 0.416s, episode steps: 62, steps per second: 149, episode reward: -61.095, mean reward: -0.985 [-100.000, 13.744], mean action: 0.952 [0.000, 3.000], mean observation: -0.011 [-1.263, 3.924], loss: 47.244129, mae: 65.521240, mean_q: 86.877541
  591929/1100000: episode: 1455, duration: 0.724s, episode steps: 109, steps per second: 150, episode reward: 35.142, mean reward: 0.322 [-100.000, 17.870], mean action: 1.578 [0.000, 3.000], mean observation: -0.003 [-0.802, 1.696], loss: 13.298502, mae: 65.888504, mean_q: 86.637863
  592427/1100000: episode: 1456, duration: 3.620s, episode steps: 498, steps per second: 138, episode reward: -171.351, mean reward: -0.344 [-100.000, 26.158], mean action: 1.651 [0.000, 3.000], mean observation: 0.086 [-1.388, 1.404], loss: 15.351541, mae: 66.541420, mean_q: 88.244995
  593108/1100000: episode: 1457, duration: 4.846s, episode steps: 681, steps per second: 141, episode reward: 271.306, mean reward: 0.398 [-18.666, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.114 [-0.882, 1.404], loss: 13.366422, mae: 66.387291, mean_q: 88.082138
  593208/1100000: episode: 1458, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: -7.382, mean reward: -0.074 [-100.000, 17.525], mean action: 1.500 [0.000, 3.000], mean observation: 0.056 [-1.268, 2.261], loss: 16.406374, mae: 66.170486, mean_q: 87.990067
  593763/1100000: episode: 1459, duration: 4.014s, episode steps: 555, steps per second: 138, episode reward: 224.207, mean reward: 0.404 [-9.605, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.141 [-1.152, 1.442], loss: 20.078243, mae: 66.621109, mean_q: 88.496216
  593882/1100000: episode: 1460, duration: 0.792s, episode steps: 119, steps per second: 150, episode reward: -9.719, mean reward: -0.082 [-100.000, 13.528], mean action: 1.529 [0.000, 3.000], mean observation: -0.037 [-1.019, 1.561], loss: 14.368808, mae: 66.324562, mean_q: 87.925789
  593977/1100000: episode: 1461, duration: 0.636s, episode steps: 95, steps per second: 149, episode reward: 22.854, mean reward: 0.241 [-100.000, 16.167], mean action: 1.568 [0.000, 3.000], mean observation: -0.059 [-1.155, 1.400], loss: 13.996456, mae: 66.320198, mean_q: 88.522644
  594501/1100000: episode: 1462, duration: 3.703s, episode steps: 524, steps per second: 141, episode reward: -53.659, mean reward: -0.102 [-100.000, 11.469], mean action: 1.710 [0.000, 3.000], mean observation: 0.082 [-1.066, 1.396], loss: 20.207968, mae: 65.910393, mean_q: 87.619675
  595137/1100000: episode: 1463, duration: 4.766s, episode steps: 636, steps per second: 133, episode reward: -125.356, mean reward: -0.197 [-100.000, 17.515], mean action: 1.627 [0.000, 3.000], mean observation: 0.117 [-1.587, 1.555], loss: 17.264975, mae: 65.388603, mean_q: 86.561409
  595436/1100000: episode: 1464, duration: 2.060s, episode steps: 299, steps per second: 145, episode reward: -222.510, mean reward: -0.744 [-100.000, 30.211], mean action: 1.465 [0.000, 3.000], mean observation: 0.055 [-2.083, 1.389], loss: 15.457080, mae: 65.603668, mean_q: 86.801529
  595864/1100000: episode: 1465, duration: 3.000s, episode steps: 428, steps per second: 143, episode reward: 221.164, mean reward: 0.517 [-19.688, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: -0.006 [-0.648, 1.416], loss: 14.675134, mae: 65.426437, mean_q: 86.115578
  596263/1100000: episode: 1466, duration: 2.889s, episode steps: 399, steps per second: 138, episode reward: 236.743, mean reward: 0.593 [-11.832, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.048 [-0.864, 1.407], loss: 12.408147, mae: 65.443298, mean_q: 86.799164
  596936/1100000: episode: 1467, duration: 4.789s, episode steps: 673, steps per second: 141, episode reward: -231.908, mean reward: -0.345 [-100.000, 16.021], mean action: 1.550 [0.000, 3.000], mean observation: 0.161 [-0.862, 1.395], loss: 13.521184, mae: 64.862350, mean_q: 85.791748
  597547/1100000: episode: 1468, duration: 4.444s, episode steps: 611, steps per second: 137, episode reward: -355.338, mean reward: -0.582 [-100.000, 12.011], mean action: 1.715 [0.000, 3.000], mean observation: 0.110 [-0.667, 2.599], loss: 13.787731, mae: 65.140381, mean_q: 86.290230
  597643/1100000: episode: 1469, duration: 0.644s, episode steps: 96, steps per second: 149, episode reward: -51.348, mean reward: -0.535 [-100.000, 17.657], mean action: 1.740 [0.000, 3.000], mean observation: -0.148 [-3.654, 1.461], loss: 9.828136, mae: 63.927380, mean_q: 84.541237
  597771/1100000: episode: 1470, duration: 0.861s, episode steps: 128, steps per second: 149, episode reward: 13.295, mean reward: 0.104 [-100.000, 14.561], mean action: 1.883 [0.000, 3.000], mean observation: -0.067 [-0.784, 1.475], loss: 20.952168, mae: 64.694366, mean_q: 85.716629
  598174/1100000: episode: 1471, duration: 2.849s, episode steps: 403, steps per second: 141, episode reward: 251.037, mean reward: 0.623 [-10.867, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.050 [-0.659, 1.389], loss: 13.911779, mae: 64.006973, mean_q: 84.253296
  599174/1100000: episode: 1472, duration: 7.526s, episode steps: 1000, steps per second: 133, episode reward: 35.648, mean reward: 0.036 [-22.350, 21.038], mean action: 1.421 [0.000, 3.000], mean observation: 0.170 [-0.687, 1.400], loss: 13.171106, mae: 63.611923, mean_q: 83.318512
  599850/1100000: episode: 1473, duration: 4.755s, episode steps: 676, steps per second: 142, episode reward: -170.107, mean reward: -0.252 [-100.000, 16.298], mean action: 1.709 [0.000, 3.000], mean observation: 0.092 [-0.599, 1.413], loss: 14.705452, mae: 63.350185, mean_q: 82.848572
  600377/1100000: episode: 1474, duration: 3.769s, episode steps: 527, steps per second: 140, episode reward: 233.566, mean reward: 0.443 [-17.764, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.074 [-0.857, 1.422], loss: 14.709375, mae: 62.906052, mean_q: 81.694199
  601110/1100000: episode: 1475, duration: 5.473s, episode steps: 733, steps per second: 134, episode reward: 175.017, mean reward: 0.239 [-18.648, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: -0.015 [-0.695, 1.488], loss: 13.582511, mae: 62.857121, mean_q: 81.204300
  602110/1100000: episode: 1476, duration: 8.085s, episode steps: 1000, steps per second: 124, episode reward: -3.261, mean reward: -0.003 [-4.193, 4.979], mean action: 1.824 [0.000, 3.000], mean observation: 0.115 [-0.484, 1.443], loss: 17.928610, mae: 62.386726, mean_q: 80.473732
  602721/1100000: episode: 1477, duration: 4.274s, episode steps: 611, steps per second: 143, episode reward: -461.444, mean reward: -0.755 [-100.000, 4.614], mean action: 1.691 [0.000, 3.000], mean observation: 0.005 [-1.546, 1.686], loss: 10.495559, mae: 61.535942, mean_q: 79.706154
  602878/1100000: episode: 1478, duration: 1.059s, episode steps: 157, steps per second: 148, episode reward: -92.732, mean reward: -0.591 [-100.000, 10.906], mean action: 1.949 [0.000, 3.000], mean observation: -0.069 [-0.969, 1.403], loss: 10.580366, mae: 61.427181, mean_q: 79.203445
  603292/1100000: episode: 1479, duration: 2.930s, episode steps: 414, steps per second: 141, episode reward: 287.388, mean reward: 0.694 [-18.053, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.113 [-0.444, 1.466], loss: 13.166117, mae: 61.483952, mean_q: 79.116165
  603689/1100000: episode: 1480, duration: 2.747s, episode steps: 397, steps per second: 144, episode reward: 220.490, mean reward: 0.555 [-18.231, 100.000], mean action: 1.524 [0.000, 3.000], mean observation: 0.130 [-0.572, 1.398], loss: 17.338301, mae: 61.143734, mean_q: 78.478561
  604689/1100000: episode: 1481, duration: 7.415s, episode steps: 1000, steps per second: 135, episode reward: -2.395, mean reward: -0.002 [-4.716, 4.928], mean action: 1.685 [0.000, 3.000], mean observation: 0.110 [-0.622, 1.429], loss: 20.201126, mae: 60.780453, mean_q: 78.202408
  604908/1100000: episode: 1482, duration: 1.477s, episode steps: 219, steps per second: 148, episode reward: -38.645, mean reward: -0.176 [-100.000, 16.827], mean action: 1.863 [0.000, 3.000], mean observation: 0.010 [-1.916, 1.537], loss: 14.080693, mae: 60.875134, mean_q: 78.693001
  605283/1100000: episode: 1483, duration: 2.711s, episode steps: 375, steps per second: 138, episode reward: 257.832, mean reward: 0.688 [-11.131, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.038 [-0.853, 1.462], loss: 15.988223, mae: 60.392845, mean_q: 77.996613
  605523/1100000: episode: 1484, duration: 1.609s, episode steps: 240, steps per second: 149, episode reward: -98.992, mean reward: -0.412 [-100.000, 28.759], mean action: 1.600 [0.000, 3.000], mean observation: -0.054 [-1.550, 1.464], loss: 12.438137, mae: 60.449684, mean_q: 77.090889
  605728/1100000: episode: 1485, duration: 1.376s, episode steps: 205, steps per second: 149, episode reward: 27.108, mean reward: 0.132 [-100.000, 15.289], mean action: 1.590 [0.000, 3.000], mean observation: 0.034 [-0.819, 1.418], loss: 15.274021, mae: 60.659779, mean_q: 76.972534
  606141/1100000: episode: 1486, duration: 2.875s, episode steps: 413, steps per second: 144, episode reward: 261.396, mean reward: 0.633 [-20.870, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.148 [-1.070, 1.396], loss: 13.725543, mae: 59.800465, mean_q: 76.369873
  606479/1100000: episode: 1487, duration: 2.321s, episode steps: 338, steps per second: 146, episode reward: 241.038, mean reward: 0.713 [-17.789, 100.000], mean action: 0.908 [0.000, 3.000], mean observation: 0.053 [-0.923, 1.436], loss: 16.271692, mae: 59.765369, mean_q: 76.822800
  607479/1100000: episode: 1488, duration: 8.199s, episode steps: 1000, steps per second: 122, episode reward: 18.921, mean reward: 0.019 [-20.311, 19.756], mean action: 1.318 [0.000, 3.000], mean observation: 0.132 [-0.433, 1.408], loss: 12.060607, mae: 59.174339, mean_q: 76.150452
  607850/1100000: episode: 1489, duration: 2.527s, episode steps: 371, steps per second: 147, episode reward: -166.895, mean reward: -0.450 [-100.000, 15.398], mean action: 1.340 [0.000, 3.000], mean observation: 0.002 [-1.100, 1.394], loss: 12.492333, mae: 58.879044, mean_q: 76.249329
  608276/1100000: episode: 1490, duration: 2.932s, episode steps: 426, steps per second: 145, episode reward: -132.992, mean reward: -0.312 [-100.000, 13.994], mean action: 1.458 [0.000, 3.000], mean observation: 0.001 [-0.791, 1.416], loss: 8.210166, mae: 58.483669, mean_q: 76.093384
  608561/1100000: episode: 1491, duration: 1.950s, episode steps: 285, steps per second: 146, episode reward: 253.197, mean reward: 0.888 [-8.671, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.031 [-0.746, 1.425], loss: 13.183463, mae: 58.773594, mean_q: 76.146950
  608662/1100000: episode: 1492, duration: 0.680s, episode steps: 101, steps per second: 149, episode reward: 8.249, mean reward: 0.082 [-100.000, 11.343], mean action: 1.485 [0.000, 3.000], mean observation: 0.077 [-1.499, 1.387], loss: 17.312666, mae: 59.002602, mean_q: 75.993111
  608780/1100000: episode: 1493, duration: 0.792s, episode steps: 118, steps per second: 149, episode reward: 34.751, mean reward: 0.294 [-100.000, 9.646], mean action: 1.890 [0.000, 3.000], mean observation: 0.114 [-1.451, 1.456], loss: 12.629624, mae: 58.696758, mean_q: 76.276497
  608872/1100000: episode: 1494, duration: 0.622s, episode steps: 92, steps per second: 148, episode reward: 20.555, mean reward: 0.223 [-100.000, 9.882], mean action: 1.913 [0.000, 3.000], mean observation: 0.044 [-1.179, 1.390], loss: 11.978672, mae: 58.900635, mean_q: 75.867661
  609072/1100000: episode: 1495, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -27.130, mean reward: -0.136 [-100.000, 15.527], mean action: 1.660 [0.000, 3.000], mean observation: -0.085 [-1.886, 1.423], loss: 15.560722, mae: 58.185387, mean_q: 75.450066
  609502/1100000: episode: 1496, duration: 2.989s, episode steps: 430, steps per second: 144, episode reward: 265.611, mean reward: 0.618 [-10.247, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.047 [-0.713, 1.388], loss: 12.444079, mae: 57.958775, mean_q: 76.237915
  609718/1100000: episode: 1497, duration: 1.491s, episode steps: 216, steps per second: 145, episode reward: 268.859, mean reward: 1.245 [-8.426, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.049 [-0.781, 1.393], loss: 10.951884, mae: 58.709492, mean_q: 76.482239
  610120/1100000: episode: 1498, duration: 2.874s, episode steps: 402, steps per second: 140, episode reward: 272.507, mean reward: 0.678 [-11.028, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.084 [-0.758, 1.395], loss: 10.577627, mae: 57.490520, mean_q: 75.750801
  610355/1100000: episode: 1499, duration: 1.575s, episode steps: 235, steps per second: 149, episode reward: 270.391, mean reward: 1.151 [-17.409, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.086 [-1.121, 1.477], loss: 13.414916, mae: 58.131134, mean_q: 76.420013
  611153/1100000: episode: 1500, duration: 5.765s, episode steps: 798, steps per second: 138, episode reward: 98.929, mean reward: 0.124 [-17.017, 100.000], mean action: 1.822 [0.000, 3.000], mean observation: 0.095 [-0.600, 1.433], loss: 12.944138, mae: 57.586277, mean_q: 75.979507
  611252/1100000: episode: 1501, duration: 0.658s, episode steps: 99, steps per second: 151, episode reward: -32.965, mean reward: -0.333 [-100.000, 14.853], mean action: 1.970 [1.000, 3.000], mean observation: 0.138 [-1.169, 2.935], loss: 8.583934, mae: 56.584694, mean_q: 75.210655
  612252/1100000: episode: 1502, duration: 7.717s, episode steps: 1000, steps per second: 130, episode reward: 56.633, mean reward: 0.057 [-13.513, 22.538], mean action: 1.416 [0.000, 3.000], mean observation: 0.025 [-1.005, 1.408], loss: 12.000539, mae: 57.582344, mean_q: 76.400658
  613252/1100000: episode: 1503, duration: 7.735s, episode steps: 1000, steps per second: 129, episode reward: -46.871, mean reward: -0.047 [-5.961, 5.282], mean action: 1.860 [0.000, 3.000], mean observation: 0.069 [-0.571, 1.391], loss: 11.987129, mae: 57.371811, mean_q: 76.280632
  613309/1100000: episode: 1504, duration: 0.386s, episode steps: 57, steps per second: 148, episode reward: -194.741, mean reward: -3.417 [-100.000, 3.840], mean action: 1.702 [0.000, 3.000], mean observation: -0.137 [-1.828, 1.627], loss: 14.240967, mae: 56.266918, mean_q: 75.045280
  614187/1100000: episode: 1505, duration: 6.414s, episode steps: 878, steps per second: 137, episode reward: -228.984, mean reward: -0.261 [-100.000, 19.300], mean action: 1.216 [0.000, 3.000], mean observation: 0.070 [-0.883, 1.491], loss: 14.604891, mae: 56.928810, mean_q: 75.624535
  615187/1100000: episode: 1506, duration: 7.720s, episode steps: 1000, steps per second: 130, episode reward: -13.209, mean reward: -0.013 [-5.395, 4.951], mean action: 1.874 [0.000, 3.000], mean observation: 0.079 [-0.607, 1.394], loss: 13.225704, mae: 56.341656, mean_q: 74.996826
  615765/1100000: episode: 1507, duration: 4.273s, episode steps: 578, steps per second: 135, episode reward: 235.893, mean reward: 0.408 [-18.052, 100.000], mean action: 2.277 [0.000, 3.000], mean observation: 0.194 [-1.119, 1.387], loss: 16.724464, mae: 56.069557, mean_q: 74.613541
  616021/1100000: episode: 1508, duration: 1.742s, episode steps: 256, steps per second: 147, episode reward: -398.774, mean reward: -1.558 [-100.000, 52.054], mean action: 1.988 [0.000, 3.000], mean observation: 0.141 [-3.104, 1.390], loss: 32.023670, mae: 55.146130, mean_q: 73.450119
  617021/1100000: episode: 1509, duration: 7.590s, episode steps: 1000, steps per second: 132, episode reward: 78.664, mean reward: 0.079 [-19.461, 22.175], mean action: 1.833 [0.000, 3.000], mean observation: 0.134 [-0.784, 1.405], loss: 13.839604, mae: 55.385834, mean_q: 73.572609
  617700/1100000: episode: 1510, duration: 4.871s, episode steps: 679, steps per second: 139, episode reward: -542.517, mean reward: -0.799 [-100.000, 13.304], mean action: 1.730 [0.000, 3.000], mean observation: -0.027 [-3.860, 1.843], loss: 19.366934, mae: 55.415318, mean_q: 73.654793
  618318/1100000: episode: 1511, duration: 4.544s, episode steps: 618, steps per second: 136, episode reward: 135.082, mean reward: 0.219 [-19.054, 100.000], mean action: 1.642 [0.000, 3.000], mean observation: -0.005 [-0.832, 1.410], loss: 19.309349, mae: 54.995102, mean_q: 72.998764
  619318/1100000: episode: 1512, duration: 7.604s, episode steps: 1000, steps per second: 132, episode reward: -332.054, mean reward: -0.332 [-14.116, 12.345], mean action: 1.746 [0.000, 3.000], mean observation: 0.130 [-1.242, 1.738], loss: 36.516243, mae: 55.119045, mean_q: 73.192596
  619842/1100000: episode: 1513, duration: 3.665s, episode steps: 524, steps per second: 143, episode reward: -84.321, mean reward: -0.161 [-100.000, 19.712], mean action: 1.792 [0.000, 3.000], mean observation: 0.065 [-1.973, 1.520], loss: 11.185679, mae: 55.825878, mean_q: 74.031960
  620238/1100000: episode: 1514, duration: 2.756s, episode steps: 396, steps per second: 144, episode reward: 245.728, mean reward: 0.621 [-15.802, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.094 [-0.608, 1.410], loss: 17.171589, mae: 55.666973, mean_q: 73.688065
  620501/1100000: episode: 1515, duration: 1.790s, episode steps: 263, steps per second: 147, episode reward: 257.712, mean reward: 0.980 [-11.040, 100.000], mean action: 1.913 [0.000, 3.000], mean observation: 0.088 [-0.704, 1.517], loss: 22.711056, mae: 55.929161, mean_q: 74.250999
  621269/1100000: episode: 1516, duration: 5.676s, episode steps: 768, steps per second: 135, episode reward: 149.340, mean reward: 0.194 [-22.909, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: -0.004 [-0.650, 1.418], loss: 13.198715, mae: 55.825439, mean_q: 74.086700
  621528/1100000: episode: 1517, duration: 1.766s, episode steps: 259, steps per second: 147, episode reward: 280.105, mean reward: 1.081 [-9.248, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.061 [-0.776, 1.385], loss: 12.149247, mae: 56.455952, mean_q: 75.025696
  622064/1100000: episode: 1518, duration: 3.872s, episode steps: 536, steps per second: 138, episode reward: 259.783, mean reward: 0.485 [-17.467, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.082 [-0.705, 1.409], loss: 11.155245, mae: 56.908035, mean_q: 75.650490
  622505/1100000: episode: 1519, duration: 3.135s, episode steps: 441, steps per second: 141, episode reward: 230.885, mean reward: 0.524 [-18.192, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.101 [-0.802, 1.393], loss: 36.564529, mae: 56.822155, mean_q: 75.772491
  622831/1100000: episode: 1520, duration: 2.261s, episode steps: 326, steps per second: 144, episode reward: -273.325, mean reward: -0.838 [-100.000, 15.115], mean action: 1.460 [0.000, 3.000], mean observation: 0.009 [-0.792, 1.492], loss: 16.739088, mae: 56.770897, mean_q: 75.756065
  623187/1100000: episode: 1521, duration: 2.422s, episode steps: 356, steps per second: 147, episode reward: 294.116, mean reward: 0.826 [-18.121, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.058 [-1.286, 1.415], loss: 43.326771, mae: 57.234661, mean_q: 76.265175
  623267/1100000: episode: 1522, duration: 0.533s, episode steps: 80, steps per second: 150, episode reward: -454.033, mean reward: -5.675 [-100.000, 0.310], mean action: 1.975 [0.000, 3.000], mean observation: 0.126 [-1.580, 2.398], loss: 17.213835, mae: 57.642578, mean_q: 76.820206
  623530/1100000: episode: 1523, duration: 1.787s, episode steps: 263, steps per second: 147, episode reward: -19.879, mean reward: -0.076 [-100.000, 12.084], mean action: 1.692 [0.000, 3.000], mean observation: 0.031 [-0.647, 1.417], loss: 15.586767, mae: 57.571804, mean_q: 76.898842
  623766/1100000: episode: 1524, duration: 1.581s, episode steps: 236, steps per second: 149, episode reward: 268.776, mean reward: 1.139 [-17.679, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.085 [-0.772, 1.445], loss: 13.677780, mae: 58.726646, mean_q: 78.372765
  623862/1100000: episode: 1525, duration: 0.635s, episode steps: 96, steps per second: 151, episode reward: -104.028, mean reward: -1.084 [-100.000, 9.298], mean action: 0.979 [0.000, 3.000], mean observation: -0.164 [-1.413, 4.941], loss: 29.498678, mae: 57.840984, mean_q: 76.984268
  623954/1100000: episode: 1526, duration: 0.615s, episode steps: 92, steps per second: 149, episode reward: -378.223, mean reward: -4.111 [-100.000, 2.157], mean action: 1.739 [0.000, 3.000], mean observation: 0.074 [-1.261, 2.212], loss: 13.618960, mae: 57.987377, mean_q: 76.992615
  624175/1100000: episode: 1527, duration: 1.483s, episode steps: 221, steps per second: 149, episode reward: 257.662, mean reward: 1.166 [-6.490, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.058 [-0.857, 1.462], loss: 46.622261, mae: 58.338902, mean_q: 77.714401
  624524/1100000: episode: 1528, duration: 2.376s, episode steps: 349, steps per second: 147, episode reward: -49.094, mean reward: -0.141 [-100.000, 13.396], mean action: 1.854 [0.000, 3.000], mean observation: 0.107 [-0.688, 1.389], loss: 39.459419, mae: 58.627010, mean_q: 78.173477
  624773/1100000: episode: 1529, duration: 1.678s, episode steps: 249, steps per second: 148, episode reward: 280.597, mean reward: 1.127 [-17.326, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.079 [-0.623, 1.406], loss: 32.256680, mae: 58.730125, mean_q: 78.494270
  624862/1100000: episode: 1530, duration: 0.588s, episode steps: 89, steps per second: 151, episode reward: -530.155, mean reward: -5.957 [-100.000, -0.219], mean action: 2.022 [0.000, 3.000], mean observation: 0.003 [-2.937, 1.767], loss: 19.208147, mae: 59.299759, mean_q: 78.819481
  625028/1100000: episode: 1531, duration: 1.104s, episode steps: 166, steps per second: 150, episode reward: -84.529, mean reward: -0.509 [-100.000, 19.479], mean action: 1.554 [0.000, 3.000], mean observation: -0.008 [-1.494, 1.654], loss: 8.843066, mae: 59.601067, mean_q: 79.573349
  625253/1100000: episode: 1532, duration: 1.528s, episode steps: 225, steps per second: 147, episode reward: -364.630, mean reward: -1.621 [-100.000, 2.166], mean action: 1.671 [0.000, 3.000], mean observation: 0.047 [-1.667, 1.569], loss: 159.926331, mae: 59.294262, mean_q: 78.836861
  625660/1100000: episode: 1533, duration: 2.844s, episode steps: 407, steps per second: 143, episode reward: -212.564, mean reward: -0.522 [-100.000, 12.167], mean action: 1.582 [0.000, 3.000], mean observation: 0.168 [-0.801, 1.404], loss: 59.899204, mae: 60.021236, mean_q: 79.877892
  625898/1100000: episode: 1534, duration: 1.609s, episode steps: 238, steps per second: 148, episode reward: -25.199, mean reward: -0.106 [-100.000, 15.026], mean action: 1.437 [0.000, 3.000], mean observation: 0.022 [-1.474, 1.785], loss: 27.965057, mae: 60.082455, mean_q: 80.026001
  626552/1100000: episode: 1535, duration: 4.793s, episode steps: 654, steps per second: 136, episode reward: 161.635, mean reward: 0.247 [-14.592, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.141 [-0.619, 1.402], loss: 25.208424, mae: 60.061958, mean_q: 79.970329
  626910/1100000: episode: 1536, duration: 2.526s, episode steps: 358, steps per second: 142, episode reward: 257.444, mean reward: 0.719 [-2.462, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.067 [-0.550, 1.413], loss: 30.983330, mae: 59.842941, mean_q: 79.714317
  626998/1100000: episode: 1537, duration: 0.585s, episode steps: 88, steps per second: 150, episode reward: -107.608, mean reward: -1.223 [-100.000, 6.791], mean action: 1.591 [0.000, 3.000], mean observation: 0.052 [-3.612, 1.419], loss: 9.056366, mae: 60.864025, mean_q: 80.990990
  627075/1100000: episode: 1538, duration: 0.506s, episode steps: 77, steps per second: 152, episode reward: -59.564, mean reward: -0.774 [-100.000, 42.111], mean action: 1.156 [0.000, 3.000], mean observation: -0.184 [-1.238, 1.410], loss: 28.694098, mae: 60.494389, mean_q: 80.498856
  627819/1100000: episode: 1539, duration: 5.488s, episode steps: 744, steps per second: 136, episode reward: -366.978, mean reward: -0.493 [-100.000, 14.418], mean action: 1.675 [0.000, 3.000], mean observation: 0.215 [-0.597, 1.778], loss: 26.714960, mae: 60.261341, mean_q: 79.814651
  628132/1100000: episode: 1540, duration: 2.127s, episode steps: 313, steps per second: 147, episode reward: 242.276, mean reward: 0.774 [-17.947, 100.000], mean action: 0.831 [0.000, 3.000], mean observation: 0.112 [-0.863, 1.407], loss: 20.327389, mae: 60.130291, mean_q: 80.065155
  628515/1100000: episode: 1541, duration: 2.691s, episode steps: 383, steps per second: 142, episode reward: 229.831, mean reward: 0.600 [-17.879, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.178 [-0.600, 1.393], loss: 19.421055, mae: 60.525314, mean_q: 80.557251
  628909/1100000: episode: 1542, duration: 2.725s, episode steps: 394, steps per second: 145, episode reward: 233.038, mean reward: 0.591 [-10.365, 100.000], mean action: 0.787 [0.000, 3.000], mean observation: 0.053 [-0.874, 1.418], loss: 14.634870, mae: 60.276802, mean_q: 80.157143
  629024/1100000: episode: 1543, duration: 0.773s, episode steps: 115, steps per second: 149, episode reward: 12.076, mean reward: 0.105 [-100.000, 14.577], mean action: 1.809 [0.000, 3.000], mean observation: 0.099 [-0.984, 1.488], loss: 11.058447, mae: 61.206734, mean_q: 81.437683
  629301/1100000: episode: 1544, duration: 1.899s, episode steps: 277, steps per second: 146, episode reward: 281.732, mean reward: 1.017 [-10.071, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.091 [-1.246, 1.389], loss: 12.054641, mae: 60.860504, mean_q: 80.987602
  630079/1100000: episode: 1545, duration: 5.779s, episode steps: 778, steps per second: 135, episode reward: 214.790, mean reward: 0.276 [-23.386, 100.000], mean action: 1.973 [0.000, 3.000], mean observation: 0.112 [-0.714, 1.889], loss: 16.014164, mae: 60.755764, mean_q: 80.435234
  630412/1100000: episode: 1546, duration: 2.286s, episode steps: 333, steps per second: 146, episode reward: 215.099, mean reward: 0.646 [-10.374, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: -0.040 [-0.826, 1.402], loss: 19.463129, mae: 60.439964, mean_q: 79.938095
  630561/1100000: episode: 1547, duration: 1.009s, episode steps: 149, steps per second: 148, episode reward: -22.814, mean reward: -0.153 [-100.000, 10.274], mean action: 1.805 [0.000, 3.000], mean observation: 0.184 [-1.533, 1.388], loss: 20.097383, mae: 60.645306, mean_q: 80.808739
  630853/1100000: episode: 1548, duration: 1.989s, episode steps: 292, steps per second: 147, episode reward: -24.322, mean reward: -0.083 [-100.000, 26.727], mean action: 1.812 [0.000, 3.000], mean observation: 0.149 [-2.159, 1.408], loss: 13.297872, mae: 60.008045, mean_q: 79.387192
  630946/1100000: episode: 1549, duration: 0.627s, episode steps: 93, steps per second: 148, episode reward: 46.011, mean reward: 0.495 [-100.000, 16.429], mean action: 1.753 [0.000, 3.000], mean observation: -0.011 [-0.972, 1.394], loss: 12.412519, mae: 60.552704, mean_q: 80.465034
  631149/1100000: episode: 1550, duration: 1.364s, episode steps: 203, steps per second: 149, episode reward: -17.890, mean reward: -0.088 [-100.000, 29.441], mean action: 1.640 [0.000, 3.000], mean observation: -0.077 [-1.141, 1.400], loss: 13.669188, mae: 60.858681, mean_q: 80.479698
  631595/1100000: episode: 1551, duration: 3.190s, episode steps: 446, steps per second: 140, episode reward: 252.294, mean reward: 0.566 [-11.815, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: 0.078 [-0.787, 1.400], loss: 24.396944, mae: 60.198063, mean_q: 79.675606
  631801/1100000: episode: 1552, duration: 1.397s, episode steps: 206, steps per second: 147, episode reward: -73.011, mean reward: -0.354 [-100.000, 12.580], mean action: 1.738 [0.000, 3.000], mean observation: -0.108 [-1.927, 1.407], loss: 7.440818, mae: 60.397499, mean_q: 80.069443
  632078/1100000: episode: 1553, duration: 1.921s, episode steps: 277, steps per second: 144, episode reward: 290.135, mean reward: 1.047 [-2.737, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.094 [-0.719, 1.386], loss: 13.373716, mae: 60.071682, mean_q: 79.512161
  632245/1100000: episode: 1554, duration: 1.131s, episode steps: 167, steps per second: 148, episode reward: -25.254, mean reward: -0.151 [-100.000, 14.774], mean action: 1.874 [0.000, 3.000], mean observation: 0.035 [-1.051, 1.579], loss: 11.497111, mae: 60.626144, mean_q: 80.423035
  632298/1100000: episode: 1555, duration: 0.354s, episode steps: 53, steps per second: 150, episode reward: -104.092, mean reward: -1.964 [-100.000, 11.087], mean action: 0.962 [0.000, 3.000], mean observation: -0.057 [-3.315, 1.385], loss: 13.077243, mae: 61.048080, mean_q: 80.233612
  632380/1100000: episode: 1556, duration: 0.551s, episode steps: 82, steps per second: 149, episode reward: -59.627, mean reward: -0.727 [-100.000, 13.840], mean action: 1.634 [0.000, 3.000], mean observation: -0.034 [-3.070, 1.406], loss: 10.927603, mae: 60.317501, mean_q: 79.130035
  632671/1100000: episode: 1557, duration: 2.025s, episode steps: 291, steps per second: 144, episode reward: 257.448, mean reward: 0.885 [-10.942, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.040 [-0.753, 1.402], loss: 14.009150, mae: 60.692078, mean_q: 80.221985
  633110/1100000: episode: 1558, duration: 3.092s, episode steps: 439, steps per second: 142, episode reward: 243.219, mean reward: 0.554 [-17.390, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.119 [-0.687, 1.396], loss: 13.260904, mae: 60.312069, mean_q: 79.463127
  633639/1100000: episode: 1559, duration: 3.763s, episode steps: 529, steps per second: 141, episode reward: 265.041, mean reward: 0.501 [-19.757, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.079 [-1.062, 1.418], loss: 13.690009, mae: 60.226135, mean_q: 79.494209
  633756/1100000: episode: 1560, duration: 0.789s, episode steps: 117, steps per second: 148, episode reward: -21.036, mean reward: -0.180 [-100.000, 24.997], mean action: 1.838 [0.000, 3.000], mean observation: -0.063 [-1.748, 1.395], loss: 14.837232, mae: 60.074474, mean_q: 79.022438
  634001/1100000: episode: 1561, duration: 1.693s, episode steps: 245, steps per second: 145, episode reward: -246.422, mean reward: -1.006 [-100.000, 8.171], mean action: 1.784 [0.000, 3.000], mean observation: -0.131 [-2.096, 1.391], loss: 9.042068, mae: 59.136574, mean_q: 78.328064
  634346/1100000: episode: 1562, duration: 2.415s, episode steps: 345, steps per second: 143, episode reward: 278.432, mean reward: 0.807 [-11.730, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.121 [-0.680, 1.434], loss: 17.186724, mae: 59.646194, mean_q: 78.777992
  634900/1100000: episode: 1563, duration: 3.917s, episode steps: 554, steps per second: 141, episode reward: 258.978, mean reward: 0.467 [-17.958, 100.000], mean action: 0.650 [0.000, 3.000], mean observation: 0.157 [-0.876, 1.467], loss: 13.555121, mae: 58.938282, mean_q: 77.961586
  635136/1100000: episode: 1564, duration: 1.614s, episode steps: 236, steps per second: 146, episode reward: 243.021, mean reward: 1.030 [-9.640, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: -0.041 [-0.817, 1.394], loss: 12.243462, mae: 58.061092, mean_q: 76.374084
  635586/1100000: episode: 1565, duration: 3.147s, episode steps: 450, steps per second: 143, episode reward: -238.971, mean reward: -0.531 [-100.000, 12.114], mean action: 1.713 [0.000, 3.000], mean observation: 0.180 [-0.576, 1.502], loss: 15.704402, mae: 58.084614, mean_q: 76.453766
  635872/1100000: episode: 1566, duration: 1.953s, episode steps: 286, steps per second: 146, episode reward: -307.525, mean reward: -1.075 [-100.000, 15.132], mean action: 1.661 [0.000, 3.000], mean observation: -0.054 [-2.602, 1.482], loss: 17.805120, mae: 58.183014, mean_q: 76.354263
  636617/1100000: episode: 1567, duration: 5.783s, episode steps: 745, steps per second: 129, episode reward: -211.886, mean reward: -0.284 [-100.000, 13.862], mean action: 1.774 [0.000, 3.000], mean observation: -0.047 [-1.367, 1.432], loss: 17.274954, mae: 58.489513, mean_q: 76.758987
  636967/1100000: episode: 1568, duration: 2.461s, episode steps: 350, steps per second: 142, episode reward: 235.118, mean reward: 0.672 [-12.380, 100.000], mean action: 2.123 [0.000, 3.000], mean observation: 0.053 [-1.389, 1.510], loss: 11.151719, mae: 58.761276, mean_q: 77.263245
  637167/1100000: episode: 1569, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -6.267, mean reward: -0.031 [-100.000, 14.323], mean action: 1.660 [0.000, 3.000], mean observation: -0.135 [-1.012, 1.456], loss: 13.373843, mae: 59.208241, mean_q: 77.657776
  637584/1100000: episode: 1570, duration: 2.883s, episode steps: 417, steps per second: 145, episode reward: -7.281, mean reward: -0.017 [-100.000, 19.865], mean action: 1.499 [0.000, 3.000], mean observation: 0.135 [-1.020, 1.389], loss: 9.889698, mae: 58.550083, mean_q: 76.983063
  637690/1100000: episode: 1571, duration: 0.732s, episode steps: 106, steps per second: 145, episode reward: -12.683, mean reward: -0.120 [-100.000, 8.845], mean action: 1.981 [0.000, 3.000], mean observation: -0.085 [-0.924, 1.400], loss: 20.494139, mae: 58.291256, mean_q: 76.956078
  638070/1100000: episode: 1572, duration: 2.698s, episode steps: 380, steps per second: 141, episode reward: 217.845, mean reward: 0.573 [-18.452, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: -0.021 [-0.745, 1.395], loss: 11.065507, mae: 58.990822, mean_q: 77.495255
  638475/1100000: episode: 1573, duration: 2.819s, episode steps: 405, steps per second: 144, episode reward: -31.044, mean reward: -0.077 [-100.000, 23.068], mean action: 1.807 [0.000, 3.000], mean observation: 0.099 [-0.892, 1.441], loss: 14.562526, mae: 57.810181, mean_q: 75.961914
  638887/1100000: episode: 1574, duration: 2.896s, episode steps: 412, steps per second: 142, episode reward: 240.506, mean reward: 0.584 [-9.301, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.019 [-0.666, 1.394], loss: 10.567260, mae: 58.275993, mean_q: 76.431007
  639587/1100000: episode: 1575, duration: 5.203s, episode steps: 700, steps per second: 135, episode reward: 227.762, mean reward: 0.325 [-17.938, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: -0.016 [-1.039, 1.391], loss: 16.451670, mae: 58.011620, mean_q: 76.208557
  640001/1100000: episode: 1576, duration: 2.951s, episode steps: 414, steps per second: 140, episode reward: 274.414, mean reward: 0.663 [-12.292, 100.000], mean action: 1.483 [0.000, 3.000], mean observation: 0.097 [-0.444, 1.437], loss: 11.937055, mae: 58.307537, mean_q: 76.468895
  640088/1100000: episode: 1577, duration: 0.589s, episode steps: 87, steps per second: 148, episode reward: -4.979, mean reward: -0.057 [-100.000, 12.771], mean action: 1.920 [0.000, 3.000], mean observation: -0.044 [-0.989, 1.393], loss: 7.112011, mae: 57.785057, mean_q: 76.544594
  640483/1100000: episode: 1578, duration: 2.714s, episode steps: 395, steps per second: 146, episode reward: -41.120, mean reward: -0.104 [-100.000, 14.649], mean action: 1.443 [0.000, 3.000], mean observation: 0.101 [-0.824, 1.387], loss: 14.589446, mae: 57.679054, mean_q: 75.475372
  640689/1100000: episode: 1579, duration: 1.399s, episode steps: 206, steps per second: 147, episode reward: 259.379, mean reward: 1.259 [-2.985, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.093 [-0.709, 1.406], loss: 7.662082, mae: 57.824120, mean_q: 76.010010
  641156/1100000: episode: 1580, duration: 3.296s, episode steps: 467, steps per second: 142, episode reward: 201.723, mean reward: 0.432 [-9.775, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: -0.019 [-0.611, 1.435], loss: 12.736895, mae: 58.078068, mean_q: 76.011581
  641293/1100000: episode: 1581, duration: 0.916s, episode steps: 137, steps per second: 150, episode reward: -17.547, mean reward: -0.128 [-100.000, 18.563], mean action: 1.562 [0.000, 3.000], mean observation: -0.019 [-0.951, 1.882], loss: 11.984560, mae: 57.415379, mean_q: 74.638290
  642293/1100000: episode: 1582, duration: 7.709s, episode steps: 1000, steps per second: 130, episode reward: -89.687, mean reward: -0.090 [-21.243, 19.811], mean action: 1.343 [0.000, 3.000], mean observation: 0.008 [-1.124, 1.410], loss: 12.294176, mae: 57.534000, mean_q: 75.401138
  642917/1100000: episode: 1583, duration: 4.443s, episode steps: 624, steps per second: 140, episode reward: 219.811, mean reward: 0.352 [-20.216, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: -0.001 [-1.532, 1.405], loss: 14.547720, mae: 57.516045, mean_q: 75.329430
  643312/1100000: episode: 1584, duration: 2.784s, episode steps: 395, steps per second: 142, episode reward: 250.910, mean reward: 0.635 [-9.613, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: -0.016 [-0.610, 1.513], loss: 10.237218, mae: 57.304035, mean_q: 74.950432
  643727/1100000: episode: 1585, duration: 2.919s, episode steps: 415, steps per second: 142, episode reward: 212.302, mean reward: 0.512 [-21.578, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.149 [-0.883, 1.386], loss: 11.422442, mae: 57.392220, mean_q: 75.088768
  644086/1100000: episode: 1586, duration: 2.560s, episode steps: 359, steps per second: 140, episode reward: 261.675, mean reward: 0.729 [-17.379, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.101 [-1.004, 1.387], loss: 16.791084, mae: 57.187504, mean_q: 74.611839
  645086/1100000: episode: 1587, duration: 7.546s, episode steps: 1000, steps per second: 133, episode reward: 4.175, mean reward: 0.004 [-18.881, 18.572], mean action: 1.579 [0.000, 3.000], mean observation: 0.129 [-0.746, 1.385], loss: 12.756143, mae: 57.348568, mean_q: 74.987999
  645486/1100000: episode: 1588, duration: 2.869s, episode steps: 400, steps per second: 139, episode reward: 204.757, mean reward: 0.512 [-19.921, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: 0.120 [-0.781, 1.406], loss: 14.612202, mae: 57.229107, mean_q: 75.106682
  646084/1100000: episode: 1589, duration: 4.301s, episode steps: 598, steps per second: 139, episode reward: 211.495, mean reward: 0.354 [-23.003, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.002 [-0.600, 1.517], loss: 10.858690, mae: 57.196709, mean_q: 75.155907
  646708/1100000: episode: 1590, duration: 4.401s, episode steps: 624, steps per second: 142, episode reward: 216.606, mean reward: 0.347 [-22.699, 100.000], mean action: 1.694 [0.000, 3.000], mean observation: -0.000 [-0.600, 1.406], loss: 13.865166, mae: 56.624474, mean_q: 74.149010
  646980/1100000: episode: 1591, duration: 1.873s, episode steps: 272, steps per second: 145, episode reward: 281.009, mean reward: 1.033 [-17.821, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.124 [-0.811, 1.409], loss: 16.140478, mae: 55.998047, mean_q: 73.057922
  647443/1100000: episode: 1592, duration: 3.315s, episode steps: 463, steps per second: 140, episode reward: 239.187, mean reward: 0.517 [-17.784, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.186 [-0.641, 1.393], loss: 11.535393, mae: 56.048309, mean_q: 73.714470
  647794/1100000: episode: 1593, duration: 2.425s, episode steps: 351, steps per second: 145, episode reward: 268.688, mean reward: 0.765 [-19.698, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.095 [-0.795, 1.410], loss: 8.310599, mae: 56.039040, mean_q: 73.357964
  648214/1100000: episode: 1594, duration: 2.908s, episode steps: 420, steps per second: 144, episode reward: -252.901, mean reward: -0.602 [-100.000, 71.999], mean action: 1.717 [0.000, 3.000], mean observation: 0.176 [-1.433, 3.488], loss: 11.451981, mae: 55.841614, mean_q: 72.952477
  648693/1100000: episode: 1595, duration: 3.399s, episode steps: 479, steps per second: 141, episode reward: 223.694, mean reward: 0.467 [-17.996, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.009 [-0.908, 1.412], loss: 10.863363, mae: 55.564468, mean_q: 73.104156
  649693/1100000: episode: 1596, duration: 8.374s, episode steps: 1000, steps per second: 119, episode reward: -62.063, mean reward: -0.062 [-5.497, 5.461], mean action: 1.917 [0.000, 3.000], mean observation: 0.157 [-0.555, 1.417], loss: 10.492922, mae: 55.480537, mean_q: 72.992813
  650011/1100000: episode: 1597, duration: 2.198s, episode steps: 318, steps per second: 145, episode reward: 241.838, mean reward: 0.760 [-17.556, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.103 [-0.957, 1.411], loss: 13.327546, mae: 55.616917, mean_q: 73.444664
  650168/1100000: episode: 1598, duration: 1.051s, episode steps: 157, steps per second: 149, episode reward: -293.370, mean reward: -1.869 [-100.000, 3.975], mean action: 1.363 [0.000, 3.000], mean observation: -0.083 [-1.004, 1.406], loss: 9.399498, mae: 56.032284, mean_q: 73.809380
  650390/1100000: episode: 1599, duration: 1.505s, episode steps: 222, steps per second: 147, episode reward: 281.520, mean reward: 1.268 [-7.170, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.077 [-0.830, 1.447], loss: 12.078927, mae: 55.587040, mean_q: 73.209274
  651363/1100000: episode: 1600, duration: 7.189s, episode steps: 973, steps per second: 135, episode reward: 144.142, mean reward: 0.148 [-19.212, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.173 [-0.871, 1.414], loss: 12.935011, mae: 55.351315, mean_q: 72.926659
  651643/1100000: episode: 1601, duration: 1.989s, episode steps: 280, steps per second: 141, episode reward: -388.409, mean reward: -1.387 [-100.000, 5.161], mean action: 1.918 [0.000, 3.000], mean observation: 0.348 [-0.981, 3.955], loss: 14.530065, mae: 55.552631, mean_q: 73.090858
  651914/1100000: episode: 1602, duration: 1.904s, episode steps: 271, steps per second: 142, episode reward: 251.946, mean reward: 0.930 [-3.112, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.046 [-0.806, 1.389], loss: 9.097459, mae: 55.758995, mean_q: 73.491730
  652072/1100000: episode: 1603, duration: 1.076s, episode steps: 158, steps per second: 147, episode reward: -13.713, mean reward: -0.087 [-100.000, 13.545], mean action: 1.930 [0.000, 3.000], mean observation: 0.019 [-1.359, 1.430], loss: 18.767296, mae: 55.439922, mean_q: 73.171951
  652555/1100000: episode: 1604, duration: 3.462s, episode steps: 483, steps per second: 140, episode reward: 256.884, mean reward: 0.532 [-19.427, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.097 [-0.742, 1.398], loss: 9.851193, mae: 55.538982, mean_q: 73.115570
  653508/1100000: episode: 1605, duration: 7.195s, episode steps: 953, steps per second: 132, episode reward: 192.073, mean reward: 0.202 [-19.770, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.034 [-0.794, 1.397], loss: 12.543102, mae: 55.599274, mean_q: 73.181290
  654291/1100000: episode: 1606, duration: 5.697s, episode steps: 783, steps per second: 137, episode reward: 144.556, mean reward: 0.185 [-10.794, 100.000], mean action: 1.808 [0.000, 3.000], mean observation: 0.090 [-0.846, 1.451], loss: 11.959863, mae: 54.819538, mean_q: 72.076286
  654406/1100000: episode: 1607, duration: 0.763s, episode steps: 115, steps per second: 151, episode reward: -25.562, mean reward: -0.222 [-100.000, 13.756], mean action: 1.087 [0.000, 3.000], mean observation: 0.023 [-1.104, 1.477], loss: 10.242513, mae: 54.273460, mean_q: 71.705482
  654742/1100000: episode: 1608, duration: 2.305s, episode steps: 336, steps per second: 146, episode reward: -49.359, mean reward: -0.147 [-100.000, 14.764], mean action: 1.679 [0.000, 3.000], mean observation: 0.074 [-0.899, 1.404], loss: 9.224005, mae: 54.851460, mean_q: 72.194283
  655664/1100000: episode: 1609, duration: 6.975s, episode steps: 922, steps per second: 132, episode reward: 129.563, mean reward: 0.141 [-17.566, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: -0.059 [-1.079, 1.408], loss: 10.311072, mae: 54.669724, mean_q: 71.583000
  655730/1100000: episode: 1610, duration: 0.443s, episode steps: 66, steps per second: 149, episode reward: -46.875, mean reward: -0.710 [-100.000, 9.935], mean action: 1.182 [0.000, 3.000], mean observation: 0.106 [-2.569, 1.386], loss: 10.235422, mae: 54.953175, mean_q: 72.318398
  655994/1100000: episode: 1611, duration: 1.826s, episode steps: 264, steps per second: 145, episode reward: 250.528, mean reward: 0.949 [-10.093, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.091 [-0.892, 1.405], loss: 10.538084, mae: 54.729385, mean_q: 71.608223
  656239/1100000: episode: 1612, duration: 1.661s, episode steps: 245, steps per second: 147, episode reward: 259.748, mean reward: 1.060 [-11.188, 100.000], mean action: 1.686 [0.000, 3.000], mean observation: 0.075 [-1.020, 1.398], loss: 10.974681, mae: 54.873901, mean_q: 72.072472
  656345/1100000: episode: 1613, duration: 0.705s, episode steps: 106, steps per second: 150, episode reward: -149.016, mean reward: -1.406 [-100.000, 3.760], mean action: 1.340 [0.000, 3.000], mean observation: 0.083 [-1.494, 4.831], loss: 13.181117, mae: 54.425545, mean_q: 71.306877
  657345/1100000: episode: 1614, duration: 7.803s, episode steps: 1000, steps per second: 128, episode reward: -80.929, mean reward: -0.081 [-5.473, 4.476], mean action: 1.941 [0.000, 3.000], mean observation: 0.258 [-0.290, 1.516], loss: 9.152363, mae: 54.840454, mean_q: 72.146614
  658345/1100000: episode: 1615, duration: 7.672s, episode steps: 1000, steps per second: 130, episode reward: -66.439, mean reward: -0.066 [-5.381, 4.033], mean action: 1.911 [0.000, 3.000], mean observation: 0.222 [-0.784, 1.399], loss: 8.621688, mae: 55.443054, mean_q: 73.146614
  659345/1100000: episode: 1616, duration: 8.160s, episode steps: 1000, steps per second: 123, episode reward: 23.214, mean reward: 0.023 [-17.480, 14.484], mean action: 1.448 [0.000, 3.000], mean observation: -0.058 [-0.924, 1.458], loss: 10.054169, mae: 55.855179, mean_q: 73.550903
  659481/1100000: episode: 1617, duration: 0.936s, episode steps: 136, steps per second: 145, episode reward: -225.358, mean reward: -1.657 [-100.000, 2.405], mean action: 1.669 [0.000, 3.000], mean observation: 0.067 [-1.432, 1.499], loss: 7.589713, mae: 56.513672, mean_q: 74.514732
  659610/1100000: episode: 1618, duration: 0.916s, episode steps: 129, steps per second: 141, episode reward: -299.381, mean reward: -2.321 [-100.000, 4.954], mean action: 1.767 [0.000, 3.000], mean observation: 0.136 [-2.172, 3.076], loss: 6.919229, mae: 56.189323, mean_q: 73.912209
  659889/1100000: episode: 1619, duration: 1.934s, episode steps: 279, steps per second: 144, episode reward: 259.269, mean reward: 0.929 [-8.105, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.033 [-0.858, 1.402], loss: 7.927888, mae: 55.850487, mean_q: 72.853981
  660171/1100000: episode: 1620, duration: 1.915s, episode steps: 282, steps per second: 147, episode reward: 242.834, mean reward: 0.861 [-17.883, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.102 [-1.037, 1.439], loss: 15.265213, mae: 56.085938, mean_q: 73.870415
  660852/1100000: episode: 1621, duration: 5.131s, episode steps: 681, steps per second: 133, episode reward: 229.032, mean reward: 0.336 [-18.042, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.003 [-0.853, 1.394], loss: 9.801197, mae: 56.126446, mean_q: 73.829681
  661852/1100000: episode: 1622, duration: 7.725s, episode steps: 1000, steps per second: 129, episode reward: 86.070, mean reward: 0.086 [-22.945, 23.254], mean action: 1.727 [0.000, 3.000], mean observation: 0.155 [-0.996, 1.393], loss: 10.219802, mae: 55.971920, mean_q: 73.405884
  662852/1100000: episode: 1623, duration: 7.520s, episode steps: 1000, steps per second: 133, episode reward: -45.688, mean reward: -0.046 [-4.923, 5.867], mean action: 1.813 [0.000, 3.000], mean observation: -0.019 [-0.600, 1.445], loss: 8.709962, mae: 56.347233, mean_q: 74.380531
  663852/1100000: episode: 1624, duration: 7.889s, episode steps: 1000, steps per second: 127, episode reward: -49.886, mean reward: -0.050 [-5.944, 5.627], mean action: 1.828 [0.000, 3.000], mean observation: 0.202 [-0.517, 1.395], loss: 7.919157, mae: 56.817566, mean_q: 75.060600
  664017/1100000: episode: 1625, duration: 1.105s, episode steps: 165, steps per second: 149, episode reward: 37.720, mean reward: 0.229 [-100.000, 20.916], mean action: 1.539 [0.000, 3.000], mean observation: 0.051 [-1.284, 1.492], loss: 7.233150, mae: 57.182198, mean_q: 75.222000
  664085/1100000: episode: 1626, duration: 0.456s, episode steps: 68, steps per second: 149, episode reward: -8.955, mean reward: -0.132 [-100.000, 13.331], mean action: 1.368 [0.000, 3.000], mean observation: 0.106 [-2.802, 1.392], loss: 6.348192, mae: 57.547894, mean_q: 75.905190
  664207/1100000: episode: 1627, duration: 0.835s, episode steps: 122, steps per second: 146, episode reward: -45.693, mean reward: -0.375 [-100.000, 9.171], mean action: 1.689 [0.000, 3.000], mean observation: -0.009 [-0.862, 1.387], loss: 8.666247, mae: 57.005199, mean_q: 75.769096
  665207/1100000: episode: 1628, duration: 7.965s, episode steps: 1000, steps per second: 126, episode reward: -51.869, mean reward: -0.052 [-4.451, 4.746], mean action: 1.942 [0.000, 3.000], mean observation: 0.234 [-0.281, 1.469], loss: 8.668822, mae: 58.325111, mean_q: 76.961365
  665399/1100000: episode: 1629, duration: 1.295s, episode steps: 192, steps per second: 148, episode reward: -53.993, mean reward: -0.281 [-100.000, 23.960], mean action: 1.505 [0.000, 3.000], mean observation: -0.035 [-1.242, 3.219], loss: 12.788879, mae: 58.852375, mean_q: 77.444740
  666193/1100000: episode: 1630, duration: 5.855s, episode steps: 794, steps per second: 136, episode reward: 153.934, mean reward: 0.194 [-19.431, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: -0.025 [-0.600, 1.472], loss: 7.984589, mae: 58.855167, mean_q: 77.796394
  666568/1100000: episode: 1631, duration: 2.638s, episode steps: 375, steps per second: 142, episode reward: 259.275, mean reward: 0.691 [-8.207, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.068 [-1.081, 1.407], loss: 8.510552, mae: 59.565952, mean_q: 78.685654
  666727/1100000: episode: 1632, duration: 1.073s, episode steps: 159, steps per second: 148, episode reward: -125.806, mean reward: -0.791 [-100.000, 47.967], mean action: 1.774 [0.000, 3.000], mean observation: -0.031 [-0.724, 1.537], loss: 8.468731, mae: 59.250282, mean_q: 78.518463
  667124/1100000: episode: 1633, duration: 2.802s, episode steps: 397, steps per second: 142, episode reward: 192.270, mean reward: 0.484 [-8.654, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.093 [-0.933, 1.396], loss: 11.147278, mae: 58.970562, mean_q: 77.779526
  668062/1100000: episode: 1634, duration: 7.151s, episode steps: 938, steps per second: 131, episode reward: 181.798, mean reward: 0.194 [-21.094, 100.000], mean action: 1.961 [0.000, 3.000], mean observation: 0.063 [-0.716, 1.396], loss: 8.866207, mae: 58.928127, mean_q: 77.956657
  668446/1100000: episode: 1635, duration: 2.658s, episode steps: 384, steps per second: 144, episode reward: 257.209, mean reward: 0.670 [-12.255, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.079 [-0.837, 1.480], loss: 7.403692, mae: 58.609085, mean_q: 77.753174
  668758/1100000: episode: 1636, duration: 2.129s, episode steps: 312, steps per second: 147, episode reward: -294.444, mean reward: -0.944 [-100.000, 10.809], mean action: 1.612 [0.000, 3.000], mean observation: -0.120 [-2.211, 1.412], loss: 8.269501, mae: 59.386154, mean_q: 78.477905
  669475/1100000: episode: 1637, duration: 4.992s, episode steps: 717, steps per second: 144, episode reward: 191.134, mean reward: 0.267 [-21.154, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.017 [-0.752, 1.401], loss: 7.846182, mae: 59.368382, mean_q: 78.687279
  670475/1100000: episode: 1638, duration: 7.915s, episode steps: 1000, steps per second: 126, episode reward: -158.862, mean reward: -0.159 [-8.570, 60.904], mean action: 1.771 [0.000, 3.000], mean observation: 0.072 [-1.395, 1.410], loss: 7.684145, mae: 59.389816, mean_q: 78.317001
  670558/1100000: episode: 1639, duration: 0.558s, episode steps: 83, steps per second: 149, episode reward: -92.844, mean reward: -1.119 [-100.000, 10.921], mean action: 2.000 [0.000, 3.000], mean observation: -0.012 [-1.126, 2.835], loss: 7.565938, mae: 60.066799, mean_q: 78.950729
  670701/1100000: episode: 1640, duration: 0.958s, episode steps: 143, steps per second: 149, episode reward: 19.798, mean reward: 0.138 [-100.000, 10.468], mean action: 1.517 [0.000, 3.000], mean observation: -0.048 [-0.619, 1.412], loss: 7.611954, mae: 59.726994, mean_q: 78.844444
  670796/1100000: episode: 1641, duration: 0.635s, episode steps: 95, steps per second: 150, episode reward: -148.639, mean reward: -1.565 [-100.000, 44.184], mean action: 1.379 [0.000, 3.000], mean observation: -0.023 [-3.557, 1.468], loss: 12.596631, mae: 59.357063, mean_q: 78.037689
  671152/1100000: episode: 1642, duration: 2.442s, episode steps: 356, steps per second: 146, episode reward: 232.599, mean reward: 0.653 [-12.632, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.007 [-0.693, 1.422], loss: 9.709508, mae: 59.536644, mean_q: 78.258492
  671299/1100000: episode: 1643, duration: 0.990s, episode steps: 147, steps per second: 149, episode reward: -202.276, mean reward: -1.376 [-100.000, 2.352], mean action: 1.769 [0.000, 3.000], mean observation: 0.042 [-1.004, 1.977], loss: 7.298145, mae: 59.661911, mean_q: 78.477592
  672134/1100000: episode: 1644, duration: 6.476s, episode steps: 835, steps per second: 129, episode reward: 152.528, mean reward: 0.183 [-17.834, 100.000], mean action: 2.018 [0.000, 3.000], mean observation: 0.144 [-1.124, 1.394], loss: 12.948681, mae: 59.008911, mean_q: 77.314453
  672787/1100000: episode: 1645, duration: 4.806s, episode steps: 653, steps per second: 136, episode reward: 242.078, mean reward: 0.371 [-20.010, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.225 [-0.822, 1.385], loss: 10.855225, mae: 59.240723, mean_q: 77.813171
  672924/1100000: episode: 1646, duration: 0.910s, episode steps: 137, steps per second: 151, episode reward: -247.209, mean reward: -1.804 [-100.000, 1.785], mean action: 1.613 [0.000, 3.000], mean observation: 0.067 [-1.005, 2.242], loss: 6.069685, mae: 59.410496, mean_q: 78.108818
  673214/1100000: episode: 1647, duration: 1.998s, episode steps: 290, steps per second: 145, episode reward: 243.212, mean reward: 0.839 [-8.874, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: -0.002 [-0.617, 1.397], loss: 9.314758, mae: 59.369846, mean_q: 78.347694
  673372/1100000: episode: 1648, duration: 1.049s, episode steps: 158, steps per second: 151, episode reward: -198.934, mean reward: -1.259 [-100.000, 2.079], mean action: 1.456 [0.000, 3.000], mean observation: 0.069 [-1.004, 1.964], loss: 6.062387, mae: 58.697018, mean_q: 77.743767
  673937/1100000: episode: 1649, duration: 3.821s, episode steps: 565, steps per second: 148, episode reward: 214.803, mean reward: 0.380 [-20.685, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.037 [-0.912, 1.466], loss: 8.597481, mae: 58.872662, mean_q: 77.780449
  674937/1100000: episode: 1650, duration: 8.286s, episode steps: 1000, steps per second: 121, episode reward: -141.570, mean reward: -0.142 [-21.609, 13.413], mean action: 1.816 [0.000, 3.000], mean observation: 0.153 [-1.209, 1.401], loss: 7.535121, mae: 58.003731, mean_q: 76.390755
  675767/1100000: episode: 1651, duration: 6.154s, episode steps: 830, steps per second: 135, episode reward: 247.718, mean reward: 0.298 [-19.758, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.032 [-0.855, 1.495], loss: 12.221616, mae: 57.420712, mean_q: 75.390381
  676767/1100000: episode: 1652, duration: 8.105s, episode steps: 1000, steps per second: 123, episode reward: -28.669, mean reward: -0.029 [-24.983, 14.613], mean action: 1.865 [0.000, 3.000], mean observation: 0.139 [-0.644, 1.477], loss: 12.193795, mae: 56.804913, mean_q: 74.540359
  677459/1100000: episode: 1653, duration: 5.100s, episode steps: 692, steps per second: 136, episode reward: 178.434, mean reward: 0.258 [-19.949, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: 0.155 [-0.753, 1.397], loss: 17.006552, mae: 56.360245, mean_q: 74.131485
  677627/1100000: episode: 1654, duration: 1.127s, episode steps: 168, steps per second: 149, episode reward: 66.321, mean reward: 0.395 [-100.000, 10.832], mean action: 1.952 [0.000, 3.000], mean observation: -0.081 [-0.773, 1.490], loss: 20.229866, mae: 56.703438, mean_q: 74.374619
  678627/1100000: episode: 1655, duration: 7.708s, episode steps: 1000, steps per second: 130, episode reward: -17.239, mean reward: -0.017 [-4.612, 5.332], mean action: 1.856 [0.000, 3.000], mean observation: 0.114 [-0.637, 1.394], loss: 8.425362, mae: 56.655689, mean_q: 74.611877
  679197/1100000: episode: 1656, duration: 4.202s, episode steps: 570, steps per second: 136, episode reward: 242.171, mean reward: 0.425 [-19.786, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.030 [-0.761, 1.396], loss: 12.621906, mae: 56.373444, mean_q: 74.223305
  679526/1100000: episode: 1657, duration: 2.220s, episode steps: 329, steps per second: 148, episode reward: 202.170, mean reward: 0.614 [-13.356, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: -0.032 [-0.600, 1.437], loss: 7.161913, mae: 56.111820, mean_q: 73.986229
  680136/1100000: episode: 1658, duration: 4.367s, episode steps: 610, steps per second: 140, episode reward: 166.731, mean reward: 0.273 [-23.827, 100.000], mean action: 1.718 [0.000, 3.000], mean observation: 0.110 [-0.788, 1.401], loss: 16.014793, mae: 55.765991, mean_q: 73.383804
  680398/1100000: episode: 1659, duration: 1.758s, episode steps: 262, steps per second: 149, episode reward: -168.670, mean reward: -0.644 [-100.000, 8.234], mean action: 1.332 [0.000, 3.000], mean observation: 0.118 [-0.934, 3.644], loss: 15.422325, mae: 56.127041, mean_q: 73.579735
  681172/1100000: episode: 1660, duration: 5.610s, episode steps: 774, steps per second: 138, episode reward: 158.546, mean reward: 0.205 [-17.872, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.157 [-0.611, 1.385], loss: 10.693767, mae: 55.159424, mean_q: 72.557190
  681417/1100000: episode: 1661, duration: 1.662s, episode steps: 245, steps per second: 147, episode reward: 199.067, mean reward: 0.813 [-14.189, 100.000], mean action: 1.551 [0.000, 3.000], mean observation: -0.034 [-0.776, 1.414], loss: 11.244119, mae: 55.479942, mean_q: 73.287514
  681727/1100000: episode: 1662, duration: 2.162s, episode steps: 310, steps per second: 143, episode reward: 250.284, mean reward: 0.807 [-17.867, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.010 [-1.061, 1.387], loss: 6.759911, mae: 54.945053, mean_q: 72.520645
  682345/1100000: episode: 1663, duration: 4.712s, episode steps: 618, steps per second: 131, episode reward: 283.073, mean reward: 0.458 [-20.345, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.131 [-0.746, 1.404], loss: 13.071301, mae: 55.332764, mean_q: 73.096504
  683345/1100000: episode: 1664, duration: 7.875s, episode steps: 1000, steps per second: 127, episode reward: 78.766, mean reward: 0.079 [-21.964, 22.518], mean action: 1.640 [0.000, 3.000], mean observation: 0.199 [-0.704, 1.386], loss: 11.245516, mae: 54.897205, mean_q: 72.200226
  683598/1100000: episode: 1665, duration: 1.724s, episode steps: 253, steps per second: 147, episode reward: 218.018, mean reward: 0.862 [-17.399, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: -0.060 [-0.600, 1.401], loss: 6.838050, mae: 54.749596, mean_q: 72.321617
  683776/1100000: episode: 1666, duration: 1.244s, episode steps: 178, steps per second: 143, episode reward: 18.214, mean reward: 0.102 [-100.000, 18.306], mean action: 1.629 [0.000, 3.000], mean observation: 0.003 [-0.763, 1.404], loss: 9.509227, mae: 54.576862, mean_q: 72.455574
  684776/1100000: episode: 1667, duration: 8.225s, episode steps: 1000, steps per second: 122, episode reward: -57.525, mean reward: -0.058 [-4.353, 4.932], mean action: 1.802 [0.000, 3.000], mean observation: 0.170 [-0.752, 1.389], loss: 8.747586, mae: 54.553524, mean_q: 71.892693
  685249/1100000: episode: 1668, duration: 3.299s, episode steps: 473, steps per second: 143, episode reward: -120.436, mean reward: -0.255 [-100.000, 19.530], mean action: 1.526 [0.000, 3.000], mean observation: 0.020 [-1.161, 2.676], loss: 7.898736, mae: 54.380272, mean_q: 71.703606
  685809/1100000: episode: 1669, duration: 4.246s, episode steps: 560, steps per second: 132, episode reward: -62.644, mean reward: -0.112 [-100.000, 15.710], mean action: 1.795 [0.000, 3.000], mean observation: 0.112 [-0.939, 1.398], loss: 6.987100, mae: 54.540752, mean_q: 71.986290
  686124/1100000: episode: 1670, duration: 2.170s, episode steps: 315, steps per second: 145, episode reward: 229.465, mean reward: 0.728 [-8.836, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: -0.025 [-0.715, 1.439], loss: 10.034623, mae: 54.525448, mean_q: 71.948296
  687124/1100000: episode: 1671, duration: 7.378s, episode steps: 1000, steps per second: 136, episode reward: -42.916, mean reward: -0.043 [-4.716, 4.675], mean action: 1.829 [0.000, 3.000], mean observation: 0.116 [-0.454, 1.412], loss: 9.093662, mae: 54.554554, mean_q: 71.942703
  687307/1100000: episode: 1672, duration: 1.218s, episode steps: 183, steps per second: 150, episode reward: 20.135, mean reward: 0.110 [-100.000, 13.974], mean action: 1.295 [0.000, 3.000], mean observation: 0.060 [-1.310, 1.504], loss: 10.186236, mae: 54.716084, mean_q: 71.897156
  687544/1100000: episode: 1673, duration: 1.604s, episode steps: 237, steps per second: 148, episode reward: 35.353, mean reward: 0.149 [-100.000, 11.840], mean action: 1.443 [0.000, 3.000], mean observation: 0.024 [-0.561, 1.532], loss: 8.945752, mae: 54.073463, mean_q: 71.142433
  688001/1100000: episode: 1674, duration: 3.192s, episode steps: 457, steps per second: 143, episode reward: 244.933, mean reward: 0.536 [-19.240, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.143 [-0.774, 1.400], loss: 11.026479, mae: 53.792149, mean_q: 70.766777
  688498/1100000: episode: 1675, duration: 3.463s, episode steps: 497, steps per second: 144, episode reward: 194.903, mean reward: 0.392 [-20.658, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.172 [-0.457, 1.408], loss: 11.041497, mae: 53.601669, mean_q: 70.547981
  688875/1100000: episode: 1676, duration: 2.580s, episode steps: 377, steps per second: 146, episode reward: -45.145, mean reward: -0.120 [-100.000, 15.146], mean action: 1.485 [0.000, 3.000], mean observation: 0.129 [-0.535, 1.402], loss: 11.890615, mae: 53.177025, mean_q: 70.273544
  689875/1100000: episode: 1677, duration: 7.515s, episode steps: 1000, steps per second: 133, episode reward: -44.011, mean reward: -0.044 [-4.484, 5.651], mean action: 1.904 [0.000, 3.000], mean observation: -0.023 [-0.772, 1.416], loss: 9.666739, mae: 52.644905, mean_q: 69.537918
  690273/1100000: episode: 1678, duration: 2.835s, episode steps: 398, steps per second: 140, episode reward: -109.790, mean reward: -0.276 [-100.000, 5.022], mean action: 1.673 [0.000, 3.000], mean observation: 0.024 [-0.876, 1.396], loss: 8.872224, mae: 52.283016, mean_q: 69.128700
  691273/1100000: episode: 1679, duration: 7.993s, episode steps: 1000, steps per second: 125, episode reward: 70.637, mean reward: 0.071 [-17.317, 12.486], mean action: 1.440 [0.000, 3.000], mean observation: -0.020 [-0.779, 1.425], loss: 7.618275, mae: 51.834923, mean_q: 68.689590
  691468/1100000: episode: 1680, duration: 1.315s, episode steps: 195, steps per second: 148, episode reward: -72.274, mean reward: -0.371 [-100.000, 7.278], mean action: 1.610 [0.000, 3.000], mean observation: 0.121 [-0.810, 2.865], loss: 8.601698, mae: 52.017605, mean_q: 69.140686
  692066/1100000: episode: 1681, duration: 4.271s, episode steps: 598, steps per second: 140, episode reward: 224.540, mean reward: 0.375 [-17.853, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: 0.003 [-0.826, 1.492], loss: 8.463831, mae: 51.791050, mean_q: 68.690056
  692263/1100000: episode: 1682, duration: 1.313s, episode steps: 197, steps per second: 150, episode reward: -9.062, mean reward: -0.046 [-100.000, 16.652], mean action: 1.543 [0.000, 3.000], mean observation: -0.061 [-0.691, 1.405], loss: 11.113983, mae: 51.767242, mean_q: 68.512146
  692923/1100000: episode: 1683, duration: 4.828s, episode steps: 660, steps per second: 137, episode reward: 266.869, mean reward: 0.404 [-21.352, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.109 [-1.118, 1.415], loss: 9.388455, mae: 51.973984, mean_q: 68.893517
  693340/1100000: episode: 1684, duration: 2.875s, episode steps: 417, steps per second: 145, episode reward: 289.625, mean reward: 0.695 [-18.408, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.107 [-0.875, 1.502], loss: 11.003412, mae: 51.347187, mean_q: 68.169258
  693525/1100000: episode: 1685, duration: 1.248s, episode steps: 185, steps per second: 148, episode reward: -73.316, mean reward: -0.396 [-100.000, 15.169], mean action: 1.968 [0.000, 3.000], mean observation: 0.101 [-0.848, 2.252], loss: 8.782278, mae: 51.343967, mean_q: 68.188309
  694525/1100000: episode: 1686, duration: 8.168s, episode steps: 1000, steps per second: 122, episode reward: 86.084, mean reward: 0.086 [-19.188, 15.376], mean action: 1.301 [0.000, 3.000], mean observation: -0.027 [-0.786, 1.414], loss: 7.223149, mae: 51.321781, mean_q: 68.351547
  694860/1100000: episode: 1687, duration: 2.302s, episode steps: 335, steps per second: 146, episode reward: 244.566, mean reward: 0.730 [-17.380, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: -0.042 [-0.720, 1.410], loss: 7.555954, mae: 51.046165, mean_q: 68.225006
  695225/1100000: episode: 1688, duration: 2.510s, episode steps: 365, steps per second: 145, episode reward: 194.242, mean reward: 0.532 [-20.001, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.122 [-1.190, 1.404], loss: 7.240985, mae: 51.175697, mean_q: 68.119507
  695496/1100000: episode: 1689, duration: 1.848s, episode steps: 271, steps per second: 147, episode reward: -90.472, mean reward: -0.334 [-100.000, 16.588], mean action: 1.513 [0.000, 3.000], mean observation: 0.048 [-0.844, 1.428], loss: 8.110660, mae: 51.349384, mean_q: 68.464035
  696409/1100000: episode: 1690, duration: 6.749s, episode steps: 913, steps per second: 135, episode reward: 207.811, mean reward: 0.228 [-17.406, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: -0.008 [-0.719, 1.389], loss: 8.778183, mae: 50.819790, mean_q: 67.677208
  697040/1100000: episode: 1691, duration: 4.463s, episode steps: 631, steps per second: 141, episode reward: 237.038, mean reward: 0.376 [-12.937, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.170 [-0.778, 1.463], loss: 4.822446, mae: 50.577087, mean_q: 67.472504
  698040/1100000: episode: 1692, duration: 8.008s, episode steps: 1000, steps per second: 125, episode reward: -87.917, mean reward: -0.088 [-6.677, 5.655], mean action: 1.812 [0.000, 3.000], mean observation: -0.037 [-0.726, 1.426], loss: 7.873011, mae: 50.128231, mean_q: 66.916595
  698683/1100000: episode: 1693, duration: 4.704s, episode steps: 643, steps per second: 137, episode reward: -93.545, mean reward: -0.145 [-100.000, 48.932], mean action: 1.493 [0.000, 3.000], mean observation: 0.061 [-1.197, 1.395], loss: 9.508060, mae: 49.574120, mean_q: 66.096298
  699464/1100000: episode: 1694, duration: 5.684s, episode steps: 781, steps per second: 137, episode reward: 183.539, mean reward: 0.235 [-20.557, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.226 [-0.889, 1.412], loss: 6.506432, mae: 49.373684, mean_q: 65.847839
  699870/1100000: episode: 1695, duration: 2.806s, episode steps: 406, steps per second: 145, episode reward: 287.932, mean reward: 0.709 [-17.369, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.101 [-1.354, 1.522], loss: 7.520475, mae: 49.038418, mean_q: 65.316795
  700285/1100000: episode: 1696, duration: 2.905s, episode steps: 415, steps per second: 143, episode reward: -73.120, mean reward: -0.176 [-100.000, 18.379], mean action: 1.463 [0.000, 3.000], mean observation: 0.130 [-1.001, 1.872], loss: 8.498764, mae: 49.336800, mean_q: 65.703751
  701156/1100000: episode: 1697, duration: 7.233s, episode steps: 871, steps per second: 120, episode reward: 182.392, mean reward: 0.209 [-14.155, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.128 [-0.879, 1.388], loss: 7.065554, mae: 49.493469, mean_q: 66.029213
  701622/1100000: episode: 1698, duration: 3.286s, episode steps: 466, steps per second: 142, episode reward: -63.500, mean reward: -0.136 [-100.000, 11.168], mean action: 1.597 [0.000, 3.000], mean observation: 0.115 [-0.730, 1.730], loss: 8.007024, mae: 49.357540, mean_q: 65.901695
  702030/1100000: episode: 1699, duration: 2.815s, episode steps: 408, steps per second: 145, episode reward: 228.396, mean reward: 0.560 [-11.250, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: -0.015 [-1.098, 1.410], loss: 6.807784, mae: 48.960247, mean_q: 65.467949
  702212/1100000: episode: 1700, duration: 1.228s, episode steps: 182, steps per second: 148, episode reward: 28.459, mean reward: 0.156 [-100.000, 14.506], mean action: 1.621 [0.000, 3.000], mean observation: 0.144 [-0.656, 1.414], loss: 8.523668, mae: 49.043118, mean_q: 65.158379
  702654/1100000: episode: 1701, duration: 3.178s, episode steps: 442, steps per second: 139, episode reward: 212.952, mean reward: 0.482 [-17.352, 100.000], mean action: 0.910 [0.000, 3.000], mean observation: 0.146 [-0.872, 1.405], loss: 9.095611, mae: 49.759064, mean_q: 66.396133
  703654/1100000: episode: 1702, duration: 7.498s, episode steps: 1000, steps per second: 133, episode reward: -81.573, mean reward: -0.082 [-4.812, 4.021], mean action: 1.823 [0.000, 3.000], mean observation: 0.095 [-0.613, 1.522], loss: 6.274252, mae: 49.476437, mean_q: 66.118332
  703973/1100000: episode: 1703, duration: 2.199s, episode steps: 319, steps per second: 145, episode reward: 244.906, mean reward: 0.768 [-6.775, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: -0.023 [-0.606, 1.394], loss: 9.075868, mae: 48.936855, mean_q: 65.429909
  704973/1100000: episode: 1704, duration: 7.263s, episode steps: 1000, steps per second: 138, episode reward: -77.832, mean reward: -0.078 [-4.754, 4.671], mean action: 1.638 [0.000, 3.000], mean observation: 0.081 [-0.707, 1.421], loss: 8.573840, mae: 48.483234, mean_q: 64.770012
  705361/1100000: episode: 1705, duration: 2.649s, episode steps: 388, steps per second: 146, episode reward: 216.809, mean reward: 0.559 [-7.782, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: -0.016 [-1.000, 1.442], loss: 4.076478, mae: 48.288101, mean_q: 64.790031
  705885/1100000: episode: 1706, duration: 3.877s, episode steps: 524, steps per second: 135, episode reward: 166.584, mean reward: 0.318 [-10.435, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: -0.070 [-0.686, 1.399], loss: 6.956213, mae: 47.928928, mean_q: 64.395874
  706175/1100000: episode: 1707, duration: 2.013s, episode steps: 290, steps per second: 144, episode reward: 226.866, mean reward: 0.782 [-18.274, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.068 [-0.893, 1.405], loss: 9.399579, mae: 47.841911, mean_q: 64.181038
  706442/1100000: episode: 1708, duration: 1.844s, episode steps: 267, steps per second: 145, episode reward: 225.339, mean reward: 0.844 [-10.327, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: -0.037 [-0.710, 1.401], loss: 5.733670, mae: 47.843494, mean_q: 64.089745
  706821/1100000: episode: 1709, duration: 2.626s, episode steps: 379, steps per second: 144, episode reward: 200.974, mean reward: 0.530 [-19.862, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.085 [-0.739, 1.404], loss: 5.882835, mae: 48.022530, mean_q: 64.504288
  707133/1100000: episode: 1710, duration: 2.125s, episode steps: 312, steps per second: 147, episode reward: -205.889, mean reward: -0.660 [-100.000, 5.569], mean action: 1.708 [0.000, 3.000], mean observation: 0.267 [-0.922, 2.237], loss: 7.190375, mae: 48.364788, mean_q: 64.855804
  707813/1100000: episode: 1711, duration: 4.989s, episode steps: 680, steps per second: 136, episode reward: 223.143, mean reward: 0.328 [-19.430, 100.000], mean action: 1.501 [0.000, 3.000], mean observation: 0.018 [-0.855, 1.392], loss: 7.623357, mae: 48.146873, mean_q: 64.567047
  708786/1100000: episode: 1712, duration: 7.560s, episode steps: 973, steps per second: 129, episode reward: 114.501, mean reward: 0.118 [-17.444, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.124 [-0.687, 1.409], loss: 7.976346, mae: 47.936279, mean_q: 64.228737
  709190/1100000: episode: 1713, duration: 2.800s, episode steps: 404, steps per second: 144, episode reward: 240.881, mean reward: 0.596 [-13.543, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.002 [-0.827, 1.525], loss: 6.651278, mae: 47.636013, mean_q: 63.817478
  710190/1100000: episode: 1714, duration: 7.205s, episode steps: 1000, steps per second: 139, episode reward: -67.861, mean reward: -0.068 [-4.845, 4.393], mean action: 1.841 [0.000, 3.000], mean observation: 0.084 [-0.670, 1.400], loss: 7.030434, mae: 47.986256, mean_q: 64.195953
  711190/1100000: episode: 1715, duration: 7.771s, episode steps: 1000, steps per second: 129, episode reward: 2.734, mean reward: 0.003 [-24.466, 14.499], mean action: 1.577 [0.000, 3.000], mean observation: 0.141 [-0.703, 1.392], loss: 5.320279, mae: 47.779484, mean_q: 63.818813
  711511/1100000: episode: 1716, duration: 2.187s, episode steps: 321, steps per second: 147, episode reward: 232.313, mean reward: 0.724 [-17.519, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: -0.014 [-0.742, 1.485], loss: 6.357387, mae: 47.761742, mean_q: 63.650421
  711911/1100000: episode: 1717, duration: 2.836s, episode steps: 400, steps per second: 141, episode reward: 246.539, mean reward: 0.616 [-17.359, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.185 [-0.835, 1.481], loss: 8.551039, mae: 47.707130, mean_q: 63.667530
  712058/1100000: episode: 1718, duration: 0.984s, episode steps: 147, steps per second: 149, episode reward: 27.950, mean reward: 0.190 [-100.000, 4.508], mean action: 1.816 [0.000, 3.000], mean observation: -0.022 [-0.796, 1.385], loss: 5.478015, mae: 47.377377, mean_q: 63.258530
  712272/1100000: episode: 1719, duration: 1.459s, episode steps: 214, steps per second: 147, episode reward: -95.546, mean reward: -0.446 [-100.000, 33.013], mean action: 1.921 [0.000, 3.000], mean observation: 0.085 [-1.118, 1.410], loss: 5.327457, mae: 47.940830, mean_q: 64.152725
  712490/1100000: episode: 1720, duration: 1.488s, episode steps: 218, steps per second: 147, episode reward: -209.800, mean reward: -0.962 [-100.000, 29.528], mean action: 1.748 [0.000, 3.000], mean observation: 0.053 [-2.055, 1.495], loss: 8.715643, mae: 47.533119, mean_q: 63.510258
  712958/1100000: episode: 1721, duration: 3.397s, episode steps: 468, steps per second: 138, episode reward: 138.339, mean reward: 0.296 [-20.948, 100.000], mean action: 2.280 [0.000, 3.000], mean observation: 0.071 [-0.937, 1.387], loss: 6.039549, mae: 47.655422, mean_q: 63.838848
  713042/1100000: episode: 1722, duration: 0.563s, episode steps: 84, steps per second: 149, episode reward: -135.937, mean reward: -1.618 [-100.000, 7.388], mean action: 1.988 [0.000, 3.000], mean observation: -0.111 [-1.823, 1.457], loss: 22.010113, mae: 47.269543, mean_q: 63.238060
  713148/1100000: episode: 1723, duration: 0.709s, episode steps: 106, steps per second: 150, episode reward: 0.425, mean reward: 0.004 [-100.000, 17.395], mean action: 1.358 [0.000, 3.000], mean observation: 0.038 [-1.227, 1.406], loss: 3.721233, mae: 48.410973, mean_q: 64.688927
  713321/1100000: episode: 1724, duration: 1.153s, episode steps: 173, steps per second: 150, episode reward: 21.456, mean reward: 0.124 [-100.000, 18.852], mean action: 1.543 [0.000, 3.000], mean observation: -0.007 [-1.086, 1.481], loss: 3.825773, mae: 47.706734, mean_q: 63.988400
  714321/1100000: episode: 1725, duration: 7.405s, episode steps: 1000, steps per second: 135, episode reward: 54.337, mean reward: 0.054 [-19.620, 21.817], mean action: 1.370 [0.000, 3.000], mean observation: 0.144 [-0.667, 1.512], loss: 7.911735, mae: 47.814095, mean_q: 64.076218
  714706/1100000: episode: 1726, duration: 2.631s, episode steps: 385, steps per second: 146, episode reward: -159.243, mean reward: -0.414 [-100.000, 17.046], mean action: 1.587 [0.000, 3.000], mean observation: -0.042 [-1.670, 1.388], loss: 5.776771, mae: 47.596672, mean_q: 63.946743
  715190/1100000: episode: 1727, duration: 3.483s, episode steps: 484, steps per second: 139, episode reward: 233.940, mean reward: 0.483 [-17.766, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.126 [-0.895, 1.404], loss: 10.442323, mae: 47.729259, mean_q: 63.872547
  715606/1100000: episode: 1728, duration: 2.888s, episode steps: 416, steps per second: 144, episode reward: 257.181, mean reward: 0.618 [-19.964, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.006 [-1.008, 1.449], loss: 8.339870, mae: 47.664875, mean_q: 63.857132
  716272/1100000: episode: 1729, duration: 5.011s, episode steps: 666, steps per second: 133, episode reward: 169.350, mean reward: 0.254 [-23.210, 100.000], mean action: 1.698 [0.000, 3.000], mean observation: 0.135 [-0.749, 1.395], loss: 8.233851, mae: 47.748234, mean_q: 63.939976
  716586/1100000: episode: 1730, duration: 2.255s, episode steps: 314, steps per second: 139, episode reward: 256.857, mean reward: 0.818 [-11.044, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.151 [-0.929, 1.385], loss: 6.684034, mae: 47.761208, mean_q: 63.935558
  716968/1100000: episode: 1731, duration: 2.622s, episode steps: 382, steps per second: 146, episode reward: 216.207, mean reward: 0.566 [-11.378, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: 0.128 [-0.735, 1.472], loss: 7.886621, mae: 47.630299, mean_q: 63.656494
  717759/1100000: episode: 1732, duration: 5.661s, episode steps: 791, steps per second: 140, episode reward: 264.406, mean reward: 0.334 [-19.906, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.115 [-1.235, 1.393], loss: 6.800943, mae: 47.682785, mean_q: 63.892673
  718378/1100000: episode: 1733, duration: 4.693s, episode steps: 619, steps per second: 132, episode reward: 197.434, mean reward: 0.319 [-10.789, 100.000], mean action: 1.596 [0.000, 3.000], mean observation: 0.125 [-0.573, 1.404], loss: 8.838468, mae: 48.413494, mean_q: 64.872719
  718943/1100000: episode: 1734, duration: 4.298s, episode steps: 565, steps per second: 131, episode reward: 177.340, mean reward: 0.314 [-10.532, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.051 [-0.753, 1.411], loss: 8.416589, mae: 49.079258, mean_q: 65.687775
  719620/1100000: episode: 1735, duration: 5.144s, episode steps: 677, steps per second: 132, episode reward: 177.020, mean reward: 0.261 [-13.742, 100.000], mean action: 1.888 [0.000, 3.000], mean observation: 0.113 [-0.884, 1.399], loss: 8.903867, mae: 48.735611, mean_q: 65.328461
  720196/1100000: episode: 1736, duration: 4.331s, episode steps: 576, steps per second: 133, episode reward: 190.676, mean reward: 0.331 [-19.922, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.141 [-1.056, 1.408], loss: 9.261977, mae: 49.209263, mean_q: 65.916290
  720782/1100000: episode: 1737, duration: 4.230s, episode steps: 586, steps per second: 139, episode reward: 211.908, mean reward: 0.362 [-17.856, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.041 [-0.822, 1.390], loss: 14.329987, mae: 49.536781, mean_q: 66.559509
  721213/1100000: episode: 1738, duration: 3.005s, episode steps: 431, steps per second: 143, episode reward: 257.393, mean reward: 0.597 [-19.824, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.092 [-1.080, 1.419], loss: 9.499218, mae: 51.010323, mean_q: 68.658859
  721708/1100000: episode: 1739, duration: 3.533s, episode steps: 495, steps per second: 140, episode reward: 181.924, mean reward: 0.368 [-12.162, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: 0.109 [-1.084, 1.397], loss: 8.381723, mae: 51.388573, mean_q: 69.256332
  721807/1100000: episode: 1740, duration: 0.668s, episode steps: 99, steps per second: 148, episode reward: -154.050, mean reward: -1.556 [-100.000, 47.908], mean action: 1.505 [0.000, 3.000], mean observation: -0.013 [-5.175, 1.395], loss: 5.332621, mae: 52.012779, mean_q: 70.288467
  722034/1100000: episode: 1741, duration: 1.542s, episode steps: 227, steps per second: 147, episode reward: 49.026, mean reward: 0.216 [-100.000, 13.914], mean action: 1.824 [0.000, 3.000], mean observation: -0.023 [-1.055, 1.389], loss: 13.642242, mae: 52.448410, mean_q: 70.453995
  722121/1100000: episode: 1742, duration: 0.582s, episode steps: 87, steps per second: 149, episode reward: -33.587, mean reward: -0.386 [-100.000, 14.554], mean action: 1.701 [0.000, 3.000], mean observation: -0.050 [-0.926, 1.388], loss: 7.834524, mae: 52.701374, mean_q: 70.781914
  722183/1100000: episode: 1743, duration: 0.420s, episode steps: 62, steps per second: 148, episode reward: 29.626, mean reward: 0.478 [-100.000, 20.803], mean action: 1.419 [0.000, 3.000], mean observation: -0.003 [-1.324, 1.385], loss: 13.489475, mae: 53.965534, mean_q: 72.803413
  722482/1100000: episode: 1744, duration: 2.043s, episode steps: 299, steps per second: 146, episode reward: 276.027, mean reward: 0.923 [-3.466, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.227 [-0.895, 1.456], loss: 12.034926, mae: 53.255558, mean_q: 71.686844
  722837/1100000: episode: 1745, duration: 2.538s, episode steps: 355, steps per second: 140, episode reward: -467.347, mean reward: -1.316 [-100.000, 4.063], mean action: 1.772 [0.000, 3.000], mean observation: 0.058 [-3.274, 1.387], loss: 15.402381, mae: 54.943573, mean_q: 74.049370
  723837/1100000: episode: 1746, duration: 7.930s, episode steps: 1000, steps per second: 126, episode reward: 42.151, mean reward: 0.042 [-22.802, 24.485], mean action: 1.820 [0.000, 3.000], mean observation: -0.073 [-0.961, 1.443], loss: 17.604727, mae: 57.089172, mean_q: 77.190285
  723987/1100000: episode: 1747, duration: 0.999s, episode steps: 150, steps per second: 150, episode reward: -369.274, mean reward: -2.462 [-100.000, 61.318], mean action: 1.487 [0.000, 3.000], mean observation: -0.062 [-2.548, 1.459], loss: 11.666698, mae: 59.295345, mean_q: 80.225220
  724348/1100000: episode: 1748, duration: 2.527s, episode steps: 361, steps per second: 143, episode reward: 278.676, mean reward: 0.772 [-10.927, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.147 [-0.929, 1.399], loss: 44.029938, mae: 59.826443, mean_q: 80.633644
  724448/1100000: episode: 1749, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: -267.826, mean reward: -2.678 [-100.000, 1.791], mean action: 1.840 [0.000, 3.000], mean observation: 0.108 [-0.902, 1.490], loss: 24.741535, mae: 62.505775, mean_q: 84.298920
  724590/1100000: episode: 1750, duration: 0.991s, episode steps: 142, steps per second: 143, episode reward: -331.464, mean reward: -2.334 [-100.000, 3.823], mean action: 1.620 [0.000, 3.000], mean observation: 0.121 [-0.600, 1.577], loss: 32.063530, mae: 61.770130, mean_q: 83.715973
  724827/1100000: episode: 1751, duration: 1.664s, episode steps: 237, steps per second: 142, episode reward: -161.565, mean reward: -0.682 [-100.000, 44.780], mean action: 1.624 [0.000, 3.000], mean observation: 0.012 [-1.654, 1.390], loss: 21.917234, mae: 62.358017, mean_q: 84.180893
  725240/1100000: episode: 1752, duration: 2.939s, episode steps: 413, steps per second: 141, episode reward: 223.997, mean reward: 0.542 [-18.167, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.154 [-1.000, 1.489], loss: 16.032017, mae: 64.197876, mean_q: 86.819725
  725747/1100000: episode: 1753, duration: 3.607s, episode steps: 507, steps per second: 141, episode reward: 301.433, mean reward: 0.595 [-19.924, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.126 [-1.371, 1.414], loss: 30.270531, mae: 66.090912, mean_q: 89.041298
  725891/1100000: episode: 1754, duration: 0.962s, episode steps: 144, steps per second: 150, episode reward: -242.470, mean reward: -1.684 [-100.000, 2.663], mean action: 1.354 [0.000, 3.000], mean observation: 0.163 [-0.781, 2.011], loss: 21.381863, mae: 68.449730, mean_q: 92.303925
  725970/1100000: episode: 1755, duration: 0.529s, episode steps: 79, steps per second: 149, episode reward: 33.243, mean reward: 0.421 [-100.000, 19.870], mean action: 2.000 [1.000, 3.000], mean observation: -0.025 [-1.119, 1.472], loss: 19.898073, mae: 65.861221, mean_q: 88.808304
  726127/1100000: episode: 1756, duration: 1.050s, episode steps: 157, steps per second: 150, episode reward: -304.911, mean reward: -1.942 [-100.000, 4.623], mean action: 1.707 [0.000, 3.000], mean observation: -0.000 [-0.875, 2.059], loss: 17.526958, mae: 67.412910, mean_q: 90.792175
  726274/1100000: episode: 1757, duration: 0.986s, episode steps: 147, steps per second: 149, episode reward: -3.939, mean reward: -0.027 [-100.000, 16.892], mean action: 1.748 [0.000, 3.000], mean observation: 0.048 [-1.420, 2.689], loss: 16.189245, mae: 68.528099, mean_q: 92.430634
  726452/1100000: episode: 1758, duration: 1.202s, episode steps: 178, steps per second: 148, episode reward: -296.143, mean reward: -1.664 [-100.000, 2.414], mean action: 1.663 [0.000, 3.000], mean observation: 0.158 [-0.968, 1.997], loss: 12.724213, mae: 71.853958, mean_q: 96.634567
  726679/1100000: episode: 1759, duration: 1.540s, episode steps: 227, steps per second: 147, episode reward: -144.688, mean reward: -0.637 [-100.000, 5.868], mean action: 1.744 [0.000, 3.000], mean observation: 0.122 [-0.823, 1.398], loss: 34.811710, mae: 70.483696, mean_q: 95.205376
  726767/1100000: episode: 1760, duration: 0.597s, episode steps: 88, steps per second: 147, episode reward: 39.959, mean reward: 0.454 [-100.000, 12.580], mean action: 1.977 [0.000, 3.000], mean observation: -0.035 [-1.131, 1.441], loss: 6.700574, mae: 75.257339, mean_q: 101.701973
  726996/1100000: episode: 1761, duration: 1.579s, episode steps: 229, steps per second: 145, episode reward: -156.078, mean reward: -0.682 [-100.000, 5.795], mean action: 1.734 [0.000, 3.000], mean observation: 0.151 [-0.743, 1.432], loss: 31.470259, mae: 73.173248, mean_q: 98.679642
  727315/1100000: episode: 1762, duration: 2.208s, episode steps: 319, steps per second: 144, episode reward: 262.339, mean reward: 0.822 [-9.571, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.061 [-1.066, 1.408], loss: 14.141440, mae: 73.511574, mean_q: 99.420280
  727439/1100000: episode: 1763, duration: 0.823s, episode steps: 124, steps per second: 151, episode reward: -216.078, mean reward: -1.743 [-100.000, 3.488], mean action: 1.637 [0.000, 3.000], mean observation: 0.050 [-0.723, 1.490], loss: 53.399063, mae: 72.995064, mean_q: 98.211426
  727707/1100000: episode: 1764, duration: 1.829s, episode steps: 268, steps per second: 147, episode reward: -243.020, mean reward: -0.907 [-100.000, 3.679], mean action: 1.840 [0.000, 3.000], mean observation: 0.146 [-0.769, 1.973], loss: 15.391307, mae: 72.558563, mean_q: 98.393951
  727997/1100000: episode: 1765, duration: 1.993s, episode steps: 290, steps per second: 146, episode reward: -111.562, mean reward: -0.385 [-100.000, 14.563], mean action: 1.903 [0.000, 3.000], mean observation: 0.082 [-1.172, 1.512], loss: 32.795578, mae: 74.889221, mean_q: 100.822250
  728429/1100000: episode: 1766, duration: 3.120s, episode steps: 432, steps per second: 138, episode reward: -237.736, mean reward: -0.550 [-100.000, 4.912], mean action: 1.500 [0.000, 3.000], mean observation: -0.036 [-0.750, 1.433], loss: 28.355032, mae: 76.964287, mean_q: 103.855652
  728630/1100000: episode: 1767, duration: 1.344s, episode steps: 201, steps per second: 150, episode reward: -199.422, mean reward: -0.992 [-100.000, 4.724], mean action: 1.478 [0.000, 3.000], mean observation: 0.091 [-0.711, 1.521], loss: 102.962997, mae: 77.911613, mean_q: 105.032982
  729630/1100000: episode: 1768, duration: 7.271s, episode steps: 1000, steps per second: 138, episode reward: -61.212, mean reward: -0.061 [-16.298, 17.736], mean action: 1.987 [0.000, 3.000], mean observation: 0.076 [-0.758, 1.427], loss: 51.676285, mae: 80.373001, mean_q: 108.549706
  729820/1100000: episode: 1769, duration: 1.269s, episode steps: 190, steps per second: 150, episode reward: -185.790, mean reward: -0.978 [-100.000, 2.796], mean action: 1.500 [0.000, 3.000], mean observation: 0.149 [-0.600, 1.517], loss: 13.970019, mae: 81.818115, mean_q: 110.534851
  730485/1100000: episode: 1770, duration: 4.982s, episode steps: 665, steps per second: 133, episode reward: 185.614, mean reward: 0.279 [-13.492, 100.000], mean action: 0.997 [0.000, 3.000], mean observation: 0.176 [-0.751, 1.402], loss: 19.913115, mae: 85.754333, mean_q: 116.160629
  731134/1100000: episode: 1771, duration: 4.619s, episode steps: 649, steps per second: 140, episode reward: -226.695, mean reward: -0.349 [-100.000, 11.797], mean action: 1.704 [0.000, 3.000], mean observation: 0.072 [-1.523, 1.442], loss: 49.214657, mae: 85.174850, mean_q: 115.230583
  732027/1100000: episode: 1772, duration: 6.687s, episode steps: 893, steps per second: 134, episode reward: -289.457, mean reward: -0.324 [-100.000, 9.349], mean action: 1.774 [0.000, 3.000], mean observation: 0.138 [-1.254, 2.894], loss: 37.119839, mae: 87.832359, mean_q: 118.879936
  732091/1100000: episode: 1773, duration: 0.426s, episode steps: 64, steps per second: 150, episode reward: -69.717, mean reward: -1.089 [-100.000, 13.175], mean action: 0.953 [0.000, 3.000], mean observation: -0.176 [-1.547, 4.668], loss: 15.759090, mae: 88.609764, mean_q: 119.788101
  732411/1100000: episode: 1774, duration: 2.188s, episode steps: 320, steps per second: 146, episode reward: -213.344, mean reward: -0.667 [-100.000, 4.279], mean action: 1.597 [0.000, 3.000], mean observation: 0.111 [-0.750, 1.558], loss: 15.781499, mae: 87.837097, mean_q: 118.794968
  733411/1100000: episode: 1775, duration: 7.633s, episode steps: 1000, steps per second: 131, episode reward: -63.490, mean reward: -0.063 [-21.278, 22.891], mean action: 1.679 [0.000, 3.000], mean observation: 0.142 [-0.780, 1.398], loss: 56.280746, mae: 89.206581, mean_q: 120.470200
  733600/1100000: episode: 1776, duration: 1.273s, episode steps: 189, steps per second: 148, episode reward: -239.689, mean reward: -1.268 [-100.000, 3.397], mean action: 1.566 [0.000, 3.000], mean observation: 0.087 [-0.600, 1.409], loss: 25.447887, mae: 89.843742, mean_q: 121.183128
  734166/1100000: episode: 1777, duration: 4.200s, episode steps: 566, steps per second: 135, episode reward: 228.416, mean reward: 0.404 [-3.342, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.158 [-0.728, 1.523], loss: 17.409115, mae: 90.034973, mean_q: 121.603561
  734558/1100000: episode: 1778, duration: 2.710s, episode steps: 392, steps per second: 145, episode reward: -209.604, mean reward: -0.535 [-100.000, 3.597], mean action: 1.472 [0.000, 3.000], mean observation: 0.129 [-0.600, 1.635], loss: 18.486765, mae: 91.610947, mean_q: 123.660019
  734836/1100000: episode: 1779, duration: 1.888s, episode steps: 278, steps per second: 147, episode reward: 290.747, mean reward: 1.046 [-7.328, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.060 [-1.230, 1.537], loss: 22.257669, mae: 91.055199, mean_q: 122.949516
  735063/1100000: episode: 1780, duration: 1.539s, episode steps: 227, steps per second: 147, episode reward: -197.766, mean reward: -0.871 [-100.000, 4.264], mean action: 1.577 [0.000, 3.000], mean observation: 0.062 [-0.690, 1.433], loss: 26.409042, mae: 93.193199, mean_q: 125.541908
  735358/1100000: episode: 1781, duration: 2.067s, episode steps: 295, steps per second: 143, episode reward: -111.439, mean reward: -0.378 [-100.000, 11.685], mean action: 1.837 [0.000, 3.000], mean observation: 0.110 [-1.626, 1.389], loss: 23.372427, mae: 91.812775, mean_q: 123.632286
  735651/1100000: episode: 1782, duration: 2.019s, episode steps: 293, steps per second: 145, episode reward: -53.064, mean reward: -0.181 [-100.000, 13.822], mean action: 1.628 [0.000, 3.000], mean observation: 0.195 [-0.715, 2.225], loss: 25.579206, mae: 92.457184, mean_q: 124.343353
  735736/1100000: episode: 1783, duration: 0.565s, episode steps: 85, steps per second: 151, episode reward: -16.409, mean reward: -0.193 [-100.000, 9.399], mean action: 1.365 [0.000, 3.000], mean observation: -0.050 [-1.258, 1.397], loss: 15.880274, mae: 95.062912, mean_q: 128.171600
  736736/1100000: episode: 1784, duration: 8.056s, episode steps: 1000, steps per second: 124, episode reward: 59.628, mean reward: 0.060 [-24.663, 27.068], mean action: 1.586 [0.000, 3.000], mean observation: 0.078 [-0.544, 1.509], loss: 41.001938, mae: 93.361549, mean_q: 125.649673
  737028/1100000: episode: 1785, duration: 2.055s, episode steps: 292, steps per second: 142, episode reward: 279.636, mean reward: 0.958 [-11.820, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.060 [-0.961, 1.391], loss: 20.415724, mae: 93.868172, mean_q: 126.180115
  738028/1100000: episode: 1786, duration: 7.148s, episode steps: 1000, steps per second: 140, episode reward: -133.743, mean reward: -0.134 [-5.379, 4.755], mean action: 1.488 [0.000, 3.000], mean observation: 0.055 [-0.600, 1.403], loss: 33.305710, mae: 95.939064, mean_q: 128.921371
  738272/1100000: episode: 1787, duration: 1.660s, episode steps: 244, steps per second: 147, episode reward: 239.720, mean reward: 0.982 [-9.723, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.080 [-0.780, 1.407], loss: 19.402397, mae: 97.413582, mean_q: 131.085678
  738398/1100000: episode: 1788, duration: 0.848s, episode steps: 126, steps per second: 149, episode reward: -24.854, mean reward: -0.197 [-100.000, 18.611], mean action: 1.984 [0.000, 3.000], mean observation: 0.118 [-1.063, 1.397], loss: 12.472571, mae: 97.407143, mean_q: 131.170471
  738738/1100000: episode: 1789, duration: 2.329s, episode steps: 340, steps per second: 146, episode reward: -121.910, mean reward: -0.359 [-100.000, 6.091], mean action: 1.759 [0.000, 3.000], mean observation: 0.039 [-0.602, 1.402], loss: 16.252502, mae: 98.207489, mean_q: 131.723114
  738824/1100000: episode: 1790, duration: 0.571s, episode steps: 86, steps per second: 151, episode reward: 0.682, mean reward: 0.008 [-100.000, 17.777], mean action: 1.407 [0.000, 3.000], mean observation: -0.011 [-1.303, 1.406], loss: 13.337426, mae: 99.693916, mean_q: 134.000885
  739623/1100000: episode: 1791, duration: 6.016s, episode steps: 799, steps per second: 133, episode reward: -243.801, mean reward: -0.305 [-100.000, 22.841], mean action: 1.712 [0.000, 3.000], mean observation: 0.057 [-0.764, 1.542], loss: 33.396877, mae: 103.145218, mean_q: 138.522110
  739938/1100000: episode: 1792, duration: 2.163s, episode steps: 315, steps per second: 146, episode reward: -53.292, mean reward: -0.169 [-100.000, 11.563], mean action: 1.632 [0.000, 3.000], mean observation: 0.245 [-1.347, 2.450], loss: 73.393158, mae: 105.736313, mean_q: 141.777954
  740063/1100000: episode: 1793, duration: 0.839s, episode steps: 125, steps per second: 149, episode reward: -508.725, mean reward: -4.070 [-100.000, 3.520], mean action: 2.032 [0.000, 3.000], mean observation: 0.229 [-1.835, 1.957], loss: 76.934822, mae: 101.989929, mean_q: 136.496262
  740239/1100000: episode: 1794, duration: 1.182s, episode steps: 176, steps per second: 149, episode reward: -456.912, mean reward: -2.596 [-100.000, 3.854], mean action: 1.545 [0.000, 3.000], mean observation: 0.355 [-0.479, 2.790], loss: 61.000217, mae: 106.458031, mean_q: 142.958191
  740332/1100000: episode: 1795, duration: 0.615s, episode steps: 93, steps per second: 151, episode reward: -326.498, mean reward: -3.511 [-100.000, 0.743], mean action: 1.441 [0.000, 3.000], mean observation: 0.364 [-0.254, 2.419], loss: 39.265297, mae: 103.779663, mean_q: 139.356430
  740434/1100000: episode: 1796, duration: 0.679s, episode steps: 102, steps per second: 150, episode reward: -493.086, mean reward: -4.834 [-100.000, 0.844], mean action: 1.608 [0.000, 3.000], mean observation: 0.368 [-0.547, 2.927], loss: 24.310963, mae: 105.034340, mean_q: 140.963394
  740668/1100000: episode: 1797, duration: 1.581s, episode steps: 234, steps per second: 148, episode reward: -91.273, mean reward: -0.390 [-100.000, 5.457], mean action: 1.726 [0.000, 3.000], mean observation: 0.030 [-0.649, 1.402], loss: 20.695251, mae: 103.271141, mean_q: 138.726700
  741374/1100000: episode: 1798, duration: 5.284s, episode steps: 706, steps per second: 134, episode reward: -171.202, mean reward: -0.242 [-100.000, 4.988], mean action: 1.550 [0.000, 3.000], mean observation: 0.056 [-0.607, 1.394], loss: 56.423069, mae: 107.216667, mean_q: 143.818939
  742374/1100000: episode: 1799, duration: 7.388s, episode steps: 1000, steps per second: 135, episode reward: -87.912, mean reward: -0.088 [-5.162, 6.428], mean action: 1.693 [0.000, 3.000], mean observation: 0.062 [-0.646, 1.516], loss: 48.499016, mae: 113.639748, mean_q: 152.361343
  742503/1100000: episode: 1800, duration: 0.866s, episode steps: 129, steps per second: 149, episode reward: -250.071, mean reward: -1.939 [-100.000, 2.109], mean action: 1.667 [0.000, 3.000], mean observation: 0.330 [-0.377, 1.584], loss: 15.850692, mae: 120.849197, mean_q: 162.055389
  742932/1100000: episode: 1801, duration: 2.991s, episode steps: 429, steps per second: 143, episode reward: 250.518, mean reward: 0.584 [-5.251, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: 0.122 [-0.560, 1.547], loss: 21.354622, mae: 115.993935, mean_q: 155.353699
  743932/1100000: episode: 1802, duration: 8.005s, episode steps: 1000, steps per second: 125, episode reward: -74.811, mean reward: -0.075 [-4.860, 4.299], mean action: 1.686 [0.000, 3.000], mean observation: 0.048 [-0.600, 1.393], loss: 32.660034, mae: 117.246513, mean_q: 157.353516
  744264/1100000: episode: 1803, duration: 2.334s, episode steps: 332, steps per second: 142, episode reward: -55.030, mean reward: -0.166 [-100.000, 47.600], mean action: 1.491 [0.000, 3.000], mean observation: 0.284 [-1.234, 2.435], loss: 15.090714, mae: 116.328476, mean_q: 156.089279
  744481/1100000: episode: 1804, duration: 1.464s, episode steps: 217, steps per second: 148, episode reward: -327.190, mean reward: -1.508 [-100.000, 88.401], mean action: 1.442 [0.000, 3.000], mean observation: 0.098 [-2.815, 1.406], loss: 13.358341, mae: 115.333878, mean_q: 154.405975
  744819/1100000: episode: 1805, duration: 2.353s, episode steps: 338, steps per second: 144, episode reward: 215.125, mean reward: 0.636 [-25.442, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.097 [-0.809, 1.409], loss: 47.143734, mae: 116.586815, mean_q: 155.621765
  745819/1100000: episode: 1806, duration: 7.823s, episode steps: 1000, steps per second: 128, episode reward: -90.544, mean reward: -0.091 [-5.030, 5.199], mean action: 1.699 [0.000, 3.000], mean observation: 0.033 [-0.600, 1.404], loss: 33.741405, mae: 118.702751, mean_q: 158.732376
  745958/1100000: episode: 1807, duration: 0.928s, episode steps: 139, steps per second: 150, episode reward: -380.596, mean reward: -2.738 [-100.000, 2.340], mean action: 2.043 [0.000, 3.000], mean observation: 0.117 [-0.876, 2.325], loss: 15.657024, mae: 121.558998, mean_q: 162.194901
  746156/1100000: episode: 1808, duration: 1.351s, episode steps: 198, steps per second: 147, episode reward: -207.307, mean reward: -1.047 [-100.000, 24.845], mean action: 1.646 [0.000, 3.000], mean observation: 0.079 [-2.042, 1.404], loss: 25.507793, mae: 119.585129, mean_q: 159.482559
  747156/1100000: episode: 1809, duration: 7.784s, episode steps: 1000, steps per second: 128, episode reward: -66.368, mean reward: -0.066 [-4.770, 5.135], mean action: 1.852 [0.000, 3.000], mean observation: 0.040 [-0.600, 1.415], loss: 23.640518, mae: 120.895073, mean_q: 161.491745
  747743/1100000: episode: 1810, duration: 4.090s, episode steps: 587, steps per second: 144, episode reward: 218.479, mean reward: 0.372 [-17.344, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.168 [-1.196, 1.626], loss: 43.896755, mae: 121.400154, mean_q: 161.575012
  748743/1100000: episode: 1811, duration: 7.434s, episode steps: 1000, steps per second: 135, episode reward: -88.317, mean reward: -0.088 [-5.200, 4.680], mean action: 1.778 [0.000, 3.000], mean observation: 0.029 [-0.600, 1.494], loss: 21.443289, mae: 121.642632, mean_q: 161.824188
  749143/1100000: episode: 1812, duration: 2.819s, episode steps: 400, steps per second: 142, episode reward: 151.871, mean reward: 0.380 [-13.766, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.044 [-0.922, 1.424], loss: 31.901052, mae: 123.341843, mean_q: 164.556870
  750143/1100000: episode: 1813, duration: 8.461s, episode steps: 1000, steps per second: 118, episode reward: 26.337, mean reward: 0.026 [-24.766, 24.900], mean action: 2.036 [0.000, 3.000], mean observation: 0.127 [-0.728, 1.410], loss: 25.600767, mae: 122.067017, mean_q: 162.685898
  751143/1100000: episode: 1814, duration: 7.363s, episode steps: 1000, steps per second: 136, episode reward: -88.879, mean reward: -0.089 [-24.304, 17.731], mean action: 1.797 [0.000, 3.000], mean observation: 0.084 [-0.749, 1.446], loss: 22.013817, mae: 121.296669, mean_q: 161.319046
  752143/1100000: episode: 1815, duration: 8.594s, episode steps: 1000, steps per second: 116, episode reward: -116.806, mean reward: -0.117 [-5.822, 4.696], mean action: 1.918 [0.000, 3.000], mean observation: 0.088 [-0.431, 1.422], loss: 22.923145, mae: 119.652779, mean_q: 159.374695
  753143/1100000: episode: 1816, duration: 7.555s, episode steps: 1000, steps per second: 132, episode reward: -86.785, mean reward: -0.087 [-6.006, 4.638], mean action: 1.825 [0.000, 3.000], mean observation: 0.007 [-0.621, 1.398], loss: 23.168344, mae: 120.857231, mean_q: 161.124817
  753315/1100000: episode: 1817, duration: 1.161s, episode steps: 172, steps per second: 148, episode reward: -270.531, mean reward: -1.573 [-100.000, 7.542], mean action: 1.953 [0.000, 3.000], mean observation: 0.107 [-2.117, 2.469], loss: 14.750800, mae: 122.392715, mean_q: 163.060608
  754014/1100000: episode: 1818, duration: 5.037s, episode steps: 699, steps per second: 139, episode reward: -216.394, mean reward: -0.310 [-100.000, 5.458], mean action: 1.791 [0.000, 3.000], mean observation: 0.121 [-0.848, 1.392], loss: 23.949118, mae: 120.694527, mean_q: 160.400085
  755014/1100000: episode: 1819, duration: 8.621s, episode steps: 1000, steps per second: 116, episode reward: -121.743, mean reward: -0.122 [-22.743, 12.682], mean action: 1.755 [0.000, 3.000], mean observation: 0.107 [-0.875, 1.395], loss: 29.342270, mae: 117.133888, mean_q: 155.875351
  755495/1100000: episode: 1820, duration: 3.480s, episode steps: 481, steps per second: 138, episode reward: -182.099, mean reward: -0.379 [-100.000, 5.579], mean action: 1.850 [0.000, 3.000], mean observation: 0.097 [-0.772, 1.402], loss: 32.285107, mae: 115.772926, mean_q: 154.605103
  755829/1100000: episode: 1821, duration: 2.293s, episode steps: 334, steps per second: 146, episode reward: -135.644, mean reward: -0.406 [-100.000, 8.392], mean action: 1.775 [0.000, 3.000], mean observation: 0.144 [-0.802, 1.756], loss: 35.685207, mae: 113.864334, mean_q: 151.731308
  756192/1100000: episode: 1822, duration: 2.538s, episode steps: 363, steps per second: 143, episode reward: 281.009, mean reward: 0.774 [-18.540, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.100 [-0.531, 1.444], loss: 16.566492, mae: 113.601456, mean_q: 151.590881
  757112/1100000: episode: 1823, duration: 7.895s, episode steps: 920, steps per second: 117, episode reward: -340.529, mean reward: -0.370 [-100.000, 8.529], mean action: 1.784 [0.000, 3.000], mean observation: 0.060 [-0.754, 2.216], loss: 22.715200, mae: 111.804146, mean_q: 149.232391
  757414/1100000: episode: 1824, duration: 2.055s, episode steps: 302, steps per second: 147, episode reward: 227.329, mean reward: 0.753 [-5.671, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.016 [-0.681, 1.537], loss: 14.842504, mae: 112.289658, mean_q: 149.898880
  758414/1100000: episode: 1825, duration: 7.349s, episode steps: 1000, steps per second: 136, episode reward: 4.000, mean reward: 0.004 [-23.265, 18.323], mean action: 1.595 [0.000, 3.000], mean observation: 0.164 [-0.792, 1.512], loss: 14.525864, mae: 112.752472, mean_q: 150.475235
  758751/1100000: episode: 1826, duration: 2.357s, episode steps: 337, steps per second: 143, episode reward: -306.446, mean reward: -0.909 [-100.000, 9.990], mean action: 1.840 [0.000, 3.000], mean observation: 0.212 [-1.423, 1.981], loss: 19.209387, mae: 111.369110, mean_q: 148.540878
  759120/1100000: episode: 1827, duration: 2.629s, episode steps: 369, steps per second: 140, episode reward: 219.662, mean reward: 0.595 [-12.316, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.051 [-1.314, 1.387], loss: 24.100389, mae: 111.319771, mean_q: 148.515121
  759242/1100000: episode: 1828, duration: 0.817s, episode steps: 122, steps per second: 149, episode reward: -232.296, mean reward: -1.904 [-100.000, 2.783], mean action: 1.598 [0.000, 3.000], mean observation: 0.071 [-1.271, 1.412], loss: 24.183931, mae: 112.284477, mean_q: 149.906021
  760068/1100000: episode: 1829, duration: 6.330s, episode steps: 826, steps per second: 130, episode reward: -465.794, mean reward: -0.564 [-100.000, 5.334], mean action: 1.740 [0.000, 3.000], mean observation: 0.168 [-1.885, 5.847], loss: 23.736324, mae: 111.644066, mean_q: 148.735703
  760312/1100000: episode: 1830, duration: 1.646s, episode steps: 244, steps per second: 148, episode reward: -2.833, mean reward: -0.012 [-100.000, 19.103], mean action: 1.656 [0.000, 3.000], mean observation: -0.023 [-1.750, 1.493], loss: 18.937984, mae: 111.069771, mean_q: 148.255341
  760432/1100000: episode: 1831, duration: 0.807s, episode steps: 120, steps per second: 149, episode reward: -203.765, mean reward: -1.698 [-100.000, 3.324], mean action: 1.550 [0.000, 3.000], mean observation: 0.056 [-1.215, 1.398], loss: 23.033583, mae: 110.727135, mean_q: 146.952240
  761432/1100000: episode: 1832, duration: 8.262s, episode steps: 1000, steps per second: 121, episode reward: -35.122, mean reward: -0.035 [-4.833, 5.581], mean action: 1.755 [0.000, 3.000], mean observation: -0.016 [-0.681, 1.412], loss: 15.939251, mae: 111.280457, mean_q: 148.084717
  762432/1100000: episode: 1833, duration: 7.681s, episode steps: 1000, steps per second: 130, episode reward: -70.506, mean reward: -0.071 [-5.104, 5.375], mean action: 1.797 [0.000, 3.000], mean observation: -0.013 [-0.600, 1.398], loss: 23.160824, mae: 112.524239, mean_q: 149.190903
  763432/1100000: episode: 1834, duration: 7.649s, episode steps: 1000, steps per second: 131, episode reward: -50.084, mean reward: -0.050 [-5.585, 5.147], mean action: 1.872 [0.000, 3.000], mean observation: -0.025 [-0.600, 1.405], loss: 19.871332, mae: 111.355232, mean_q: 147.640366
  764432/1100000: episode: 1835, duration: 8.635s, episode steps: 1000, steps per second: 116, episode reward: -53.731, mean reward: -0.054 [-4.884, 5.130], mean action: 1.763 [0.000, 3.000], mean observation: -0.014 [-0.600, 1.391], loss: 14.822165, mae: 113.113403, mean_q: 149.705780
  765432/1100000: episode: 1836, duration: 7.426s, episode steps: 1000, steps per second: 135, episode reward: -50.446, mean reward: -0.050 [-5.278, 5.306], mean action: 1.672 [0.000, 3.000], mean observation: 0.009 [-0.671, 1.386], loss: 20.500074, mae: 112.835602, mean_q: 149.763367
  766432/1100000: episode: 1837, duration: 7.220s, episode steps: 1000, steps per second: 138, episode reward: -98.650, mean reward: -0.099 [-5.095, 4.906], mean action: 1.789 [0.000, 3.000], mean observation: 0.015 [-0.600, 1.399], loss: 26.188116, mae: 112.388138, mean_q: 149.313019
  766770/1100000: episode: 1838, duration: 2.388s, episode steps: 338, steps per second: 142, episode reward: -305.629, mean reward: -0.904 [-100.000, 8.280], mean action: 1.678 [0.000, 3.000], mean observation: 0.064 [-4.459, 1.388], loss: 20.247223, mae: 111.285851, mean_q: 147.453659
  767407/1100000: episode: 1839, duration: 4.655s, episode steps: 637, steps per second: 137, episode reward: 210.230, mean reward: 0.330 [-18.089, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.100 [-0.731, 1.481], loss: 20.711523, mae: 109.939453, mean_q: 145.849442
  768217/1100000: episode: 1840, duration: 5.737s, episode steps: 810, steps per second: 141, episode reward: 203.657, mean reward: 0.251 [-20.961, 100.000], mean action: 0.816 [0.000, 3.000], mean observation: 0.250 [-1.047, 1.432], loss: 22.829273, mae: 108.013771, mean_q: 143.166336
  768355/1100000: episode: 1841, duration: 0.921s, episode steps: 138, steps per second: 150, episode reward: -17.867, mean reward: -0.129 [-100.000, 20.987], mean action: 1.522 [0.000, 3.000], mean observation: 0.027 [-0.896, 1.625], loss: 27.505066, mae: 106.991196, mean_q: 141.758957
  769355/1100000: episode: 1842, duration: 7.477s, episode steps: 1000, steps per second: 134, episode reward: -64.724, mean reward: -0.065 [-5.146, 5.397], mean action: 1.675 [0.000, 3.000], mean observation: 0.013 [-0.609, 1.404], loss: 19.784941, mae: 106.478661, mean_q: 141.567474
  769878/1100000: episode: 1843, duration: 3.621s, episode steps: 523, steps per second: 144, episode reward: -179.592, mean reward: -0.343 [-100.000, 15.459], mean action: 1.478 [0.000, 3.000], mean observation: 0.091 [-1.337, 1.973], loss: 19.772228, mae: 103.949081, mean_q: 138.534210
  770338/1100000: episode: 1844, duration: 3.233s, episode steps: 460, steps per second: 142, episode reward: 215.506, mean reward: 0.468 [-17.036, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.077 [-1.222, 1.480], loss: 11.952660, mae: 103.440109, mean_q: 137.918869
  771093/1100000: episode: 1845, duration: 5.626s, episode steps: 755, steps per second: 134, episode reward: 192.386, mean reward: 0.255 [-20.058, 100.000], mean action: 1.669 [0.000, 3.000], mean observation: 0.212 [-1.154, 1.437], loss: 17.858309, mae: 101.199348, mean_q: 134.694809
  771889/1100000: episode: 1846, duration: 5.754s, episode steps: 796, steps per second: 138, episode reward: 173.203, mean reward: 0.218 [-21.547, 100.000], mean action: 1.633 [0.000, 3.000], mean observation: 0.089 [-1.125, 1.994], loss: 20.282473, mae: 98.959778, mean_q: 131.439865
  772076/1100000: episode: 1847, duration: 1.254s, episode steps: 187, steps per second: 149, episode reward: -67.658, mean reward: -0.362 [-100.000, 8.003], mean action: 1.198 [0.000, 3.000], mean observation: -0.011 [-0.808, 3.219], loss: 12.229546, mae: 96.948662, mean_q: 128.983749
  772484/1100000: episode: 1848, duration: 2.848s, episode steps: 408, steps per second: 143, episode reward: 263.608, mean reward: 0.646 [-21.056, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.110 [-0.805, 1.393], loss: 18.167559, mae: 97.021133, mean_q: 129.111282
  773484/1100000: episode: 1849, duration: 7.903s, episode steps: 1000, steps per second: 127, episode reward: 109.782, mean reward: 0.110 [-22.171, 22.278], mean action: 1.588 [0.000, 3.000], mean observation: 0.187 [-0.942, 1.495], loss: 18.255075, mae: 93.827705, mean_q: 124.463425
  773578/1100000: episode: 1850, duration: 0.625s, episode steps: 94, steps per second: 150, episode reward: -141.959, mean reward: -1.510 [-100.000, 10.024], mean action: 0.543 [0.000, 2.000], mean observation: 0.179 [-2.358, 1.489], loss: 17.430248, mae: 92.098785, mean_q: 122.342628
  773709/1100000: episode: 1851, duration: 0.879s, episode steps: 131, steps per second: 149, episode reward: -332.656, mean reward: -2.539 [-100.000, 81.429], mean action: 1.145 [0.000, 3.000], mean observation: 0.131 [-2.213, 1.485], loss: 6.837127, mae: 92.864746, mean_q: 123.258438
  774198/1100000: episode: 1852, duration: 3.484s, episode steps: 489, steps per second: 140, episode reward: 205.976, mean reward: 0.421 [-14.496, 100.000], mean action: 2.086 [0.000, 3.000], mean observation: 0.157 [-0.766, 1.464], loss: 16.960463, mae: 92.923172, mean_q: 123.266396
  774546/1100000: episode: 1853, duration: 2.497s, episode steps: 348, steps per second: 139, episode reward: 289.740, mean reward: 0.833 [-19.019, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.088 [-0.937, 1.388], loss: 15.030380, mae: 91.473915, mean_q: 121.294662
  775468/1100000: episode: 1854, duration: 7.063s, episode steps: 922, steps per second: 131, episode reward: 132.008, mean reward: 0.143 [-21.398, 100.000], mean action: 2.216 [0.000, 3.000], mean observation: 0.180 [-0.496, 1.442], loss: 15.866255, mae: 89.773064, mean_q: 118.886856
  776468/1100000: episode: 1855, duration: 7.437s, episode steps: 1000, steps per second: 134, episode reward: -40.704, mean reward: -0.041 [-6.404, 6.850], mean action: 1.862 [0.000, 3.000], mean observation: -0.012 [-0.653, 1.392], loss: 13.319313, mae: 89.448540, mean_q: 118.342316
  776770/1100000: episode: 1856, duration: 2.082s, episode steps: 302, steps per second: 145, episode reward: 226.230, mean reward: 0.749 [-7.596, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.057 [-1.198, 1.426], loss: 10.639721, mae: 88.336914, mean_q: 116.903145
  777433/1100000: episode: 1857, duration: 4.948s, episode steps: 663, steps per second: 134, episode reward: -401.656, mean reward: -0.606 [-100.000, 15.002], mean action: 1.786 [0.000, 3.000], mean observation: 0.066 [-2.130, 1.410], loss: 13.740667, mae: 88.025978, mean_q: 116.462173
  777828/1100000: episode: 1858, duration: 2.761s, episode steps: 395, steps per second: 143, episode reward: 273.376, mean reward: 0.692 [-9.859, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.169 [-1.169, 2.439], loss: 15.854522, mae: 87.850121, mean_q: 116.325417
  777927/1100000: episode: 1859, duration: 0.658s, episode steps: 99, steps per second: 150, episode reward: -202.207, mean reward: -2.042 [-100.000, 6.769], mean action: 1.081 [0.000, 3.000], mean observation: -0.125 [-1.335, 1.387], loss: 23.914978, mae: 86.992828, mean_q: 115.096825
  778512/1100000: episode: 1860, duration: 4.180s, episode steps: 585, steps per second: 140, episode reward: 253.456, mean reward: 0.433 [-15.473, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.207 [-0.889, 1.387], loss: 14.551768, mae: 87.068916, mean_q: 115.234482
  779512/1100000: episode: 1861, duration: 7.724s, episode steps: 1000, steps per second: 129, episode reward: -49.886, mean reward: -0.050 [-5.079, 5.133], mean action: 1.825 [0.000, 3.000], mean observation: -0.011 [-0.607, 1.439], loss: 13.877557, mae: 87.709282, mean_q: 115.799049
  779983/1100000: episode: 1862, duration: 3.444s, episode steps: 471, steps per second: 137, episode reward: 250.422, mean reward: 0.532 [-20.544, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.097 [-1.188, 1.395], loss: 14.027204, mae: 87.583961, mean_q: 115.607361
  780408/1100000: episode: 1863, duration: 2.939s, episode steps: 425, steps per second: 145, episode reward: -213.999, mean reward: -0.504 [-100.000, 24.003], mean action: 1.913 [0.000, 3.000], mean observation: 0.033 [-2.134, 1.420], loss: 11.615797, mae: 87.869598, mean_q: 116.271561
  780694/1100000: episode: 1864, duration: 1.936s, episode steps: 286, steps per second: 148, episode reward: 271.674, mean reward: 0.950 [-12.483, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.056 [-0.836, 1.414], loss: 14.269980, mae: 87.530266, mean_q: 115.175125
  781671/1100000: episode: 1865, duration: 7.499s, episode steps: 977, steps per second: 130, episode reward: 174.712, mean reward: 0.179 [-19.488, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.139 [-1.031, 1.392], loss: 12.047190, mae: 86.833801, mean_q: 114.824898
  782551/1100000: episode: 1866, duration: 6.273s, episode steps: 880, steps per second: 140, episode reward: 294.321, mean reward: 0.334 [-19.727, 100.000], mean action: 0.858 [0.000, 3.000], mean observation: 0.125 [-0.996, 1.824], loss: 15.978560, mae: 85.603554, mean_q: 112.707428
  783367/1100000: episode: 1867, duration: 5.955s, episode steps: 816, steps per second: 137, episode reward: 173.713, mean reward: 0.213 [-13.862, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.139 [-1.015, 1.662], loss: 13.213369, mae: 83.768074, mean_q: 110.793602
  783642/1100000: episode: 1868, duration: 1.883s, episode steps: 275, steps per second: 146, episode reward: -160.285, mean reward: -0.583 [-100.000, 9.864], mean action: 1.796 [0.000, 3.000], mean observation: 0.080 [-3.495, 1.402], loss: 12.044424, mae: 83.033211, mean_q: 109.724258
  783881/1100000: episode: 1869, duration: 1.615s, episode steps: 239, steps per second: 148, episode reward: -260.161, mean reward: -1.089 [-100.000, 2.893], mean action: 1.669 [0.000, 3.000], mean observation: 0.019 [-1.815, 1.708], loss: 11.841260, mae: 82.689880, mean_q: 109.330170
  784437/1100000: episode: 1870, duration: 4.017s, episode steps: 556, steps per second: 138, episode reward: 248.678, mean reward: 0.447 [-19.436, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.102 [-0.897, 1.392], loss: 11.578331, mae: 82.257423, mean_q: 108.452904
  784812/1100000: episode: 1871, duration: 2.641s, episode steps: 375, steps per second: 142, episode reward: 209.528, mean reward: 0.559 [-19.774, 100.000], mean action: 2.059 [0.000, 3.000], mean observation: 0.093 [-1.462, 1.395], loss: 11.418140, mae: 81.667763, mean_q: 108.221321
  785260/1100000: episode: 1872, duration: 3.103s, episode steps: 448, steps per second: 144, episode reward: 266.722, mean reward: 0.595 [-18.261, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: 0.133 [-0.663, 1.441], loss: 10.387250, mae: 81.017685, mean_q: 106.861427
  785558/1100000: episode: 1873, duration: 2.035s, episode steps: 298, steps per second: 146, episode reward: 308.155, mean reward: 1.034 [-10.614, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.109 [-0.605, 1.523], loss: 10.801447, mae: 80.635223, mean_q: 105.813469
  786014/1100000: episode: 1874, duration: 3.274s, episode steps: 456, steps per second: 139, episode reward: -121.103, mean reward: -0.266 [-100.000, 15.570], mean action: 1.579 [0.000, 3.000], mean observation: -0.013 [-0.726, 1.410], loss: 10.575204, mae: 80.102913, mean_q: 104.954735
  787014/1100000: episode: 1875, duration: 8.166s, episode steps: 1000, steps per second: 122, episode reward: 22.260, mean reward: 0.022 [-10.971, 12.935], mean action: 1.642 [0.000, 3.000], mean observation: 0.088 [-0.767, 1.439], loss: 10.972010, mae: 79.149376, mean_q: 104.244598
  787884/1100000: episode: 1876, duration: 6.548s, episode steps: 870, steps per second: 133, episode reward: 136.936, mean reward: 0.157 [-18.626, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: 0.187 [-0.881, 1.652], loss: 8.396522, mae: 78.077766, mean_q: 102.923500
  788884/1100000: episode: 1877, duration: 8.070s, episode steps: 1000, steps per second: 124, episode reward: -17.540, mean reward: -0.018 [-4.888, 5.167], mean action: 1.533 [0.000, 3.000], mean observation: 0.124 [-0.535, 1.411], loss: 11.963252, mae: 75.617691, mean_q: 99.381279
  789884/1100000: episode: 1878, duration: 7.726s, episode steps: 1000, steps per second: 129, episode reward: -83.793, mean reward: -0.084 [-5.967, 8.893], mean action: 1.626 [0.000, 3.000], mean observation: -0.034 [-0.758, 1.388], loss: 7.783736, mae: 74.326271, mean_q: 98.115562
  790116/1100000: episode: 1879, duration: 1.575s, episode steps: 232, steps per second: 147, episode reward: -118.643, mean reward: -0.511 [-100.000, 15.275], mean action: 1.772 [0.000, 3.000], mean observation: -0.021 [-0.864, 1.699], loss: 6.293235, mae: 73.703377, mean_q: 97.330017
  790385/1100000: episode: 1880, duration: 1.840s, episode steps: 269, steps per second: 146, episode reward: -219.171, mean reward: -0.815 [-100.000, 4.267], mean action: 1.851 [0.000, 3.000], mean observation: 0.111 [-1.003, 1.503], loss: 18.596088, mae: 72.197464, mean_q: 95.230728
  791004/1100000: episode: 1881, duration: 4.506s, episode steps: 619, steps per second: 137, episode reward: 209.448, mean reward: 0.338 [-19.341, 100.000], mean action: 1.691 [0.000, 3.000], mean observation: 0.137 [-0.672, 1.478], loss: 11.812982, mae: 72.670914, mean_q: 95.536377
  792004/1100000: episode: 1882, duration: 7.863s, episode steps: 1000, steps per second: 127, episode reward: -24.222, mean reward: -0.024 [-4.335, 5.748], mean action: 1.795 [0.000, 3.000], mean observation: 0.099 [-0.794, 1.405], loss: 9.180775, mae: 70.491966, mean_q: 92.819092
  792586/1100000: episode: 1883, duration: 4.056s, episode steps: 582, steps per second: 144, episode reward: 233.723, mean reward: 0.402 [-17.405, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.220 [-0.612, 1.402], loss: 8.616106, mae: 68.775414, mean_q: 90.716614
  792801/1100000: episode: 1884, duration: 1.475s, episode steps: 215, steps per second: 146, episode reward: -53.022, mean reward: -0.247 [-100.000, 17.409], mean action: 1.837 [0.000, 3.000], mean observation: -0.029 [-1.141, 1.404], loss: 8.971824, mae: 68.107239, mean_q: 90.028831
  793043/1100000: episode: 1885, duration: 1.646s, episode steps: 242, steps per second: 147, episode reward: -228.602, mean reward: -0.945 [-100.000, 9.124], mean action: 2.087 [0.000, 3.000], mean observation: 0.179 [-0.985, 1.786], loss: 11.815933, mae: 67.706482, mean_q: 89.090523
  793505/1100000: episode: 1886, duration: 3.268s, episode steps: 462, steps per second: 141, episode reward: 254.636, mean reward: 0.551 [-13.325, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.088 [-1.355, 1.483], loss: 10.211066, mae: 66.795799, mean_q: 88.161545
  793740/1100000: episode: 1887, duration: 1.600s, episode steps: 235, steps per second: 147, episode reward: -853.339, mean reward: -3.631 [-100.000, 12.873], mean action: 1.438 [0.000, 3.000], mean observation: 0.511 [-5.530, 6.887], loss: 7.449584, mae: 65.913422, mean_q: 86.873344
  794424/1100000: episode: 1888, duration: 5.012s, episode steps: 684, steps per second: 136, episode reward: 223.666, mean reward: 0.327 [-18.164, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.134 [-0.948, 1.440], loss: 12.778432, mae: 66.297638, mean_q: 87.053802
  794651/1100000: episode: 1889, duration: 1.533s, episode steps: 227, steps per second: 148, episode reward: 45.773, mean reward: 0.202 [-100.000, 12.629], mean action: 1.537 [0.000, 3.000], mean observation: 0.025 [-1.259, 1.473], loss: 7.761680, mae: 65.980133, mean_q: 86.203369
  795651/1100000: episode: 1890, duration: 7.282s, episode steps: 1000, steps per second: 137, episode reward: -71.216, mean reward: -0.071 [-5.221, 4.822], mean action: 1.564 [0.000, 3.000], mean observation: 0.088 [-0.666, 1.477], loss: 9.491401, mae: 65.127548, mean_q: 85.395409
  795973/1100000: episode: 1891, duration: 2.235s, episode steps: 322, steps per second: 144, episode reward: 229.125, mean reward: 0.712 [-17.997, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.081 [-0.851, 1.409], loss: 7.554762, mae: 64.252441, mean_q: 83.813156
  796301/1100000: episode: 1892, duration: 2.276s, episode steps: 328, steps per second: 144, episode reward: 281.526, mean reward: 0.858 [-10.766, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.074 [-1.084, 1.397], loss: 9.074043, mae: 63.985577, mean_q: 84.092285
  796549/1100000: episode: 1893, duration: 1.671s, episode steps: 248, steps per second: 148, episode reward: -230.436, mean reward: -0.929 [-100.000, 5.265], mean action: 1.657 [0.000, 3.000], mean observation: -0.062 [-1.415, 1.547], loss: 6.212451, mae: 64.234825, mean_q: 84.596115
  796744/1100000: episode: 1894, duration: 1.289s, episode steps: 195, steps per second: 151, episode reward: -423.513, mean reward: -2.172 [-100.000, 2.240], mean action: 1.349 [0.000, 3.000], mean observation: 0.451 [-2.535, 2.721], loss: 8.417563, mae: 64.027718, mean_q: 84.143372
  796930/1100000: episode: 1895, duration: 1.253s, episode steps: 186, steps per second: 148, episode reward: -72.900, mean reward: -0.392 [-100.000, 16.617], mean action: 1.919 [0.000, 3.000], mean observation: -0.018 [-1.762, 1.415], loss: 8.333246, mae: 64.313301, mean_q: 84.235878
  797124/1100000: episode: 1896, duration: 1.308s, episode steps: 194, steps per second: 148, episode reward: -71.577, mean reward: -0.369 [-100.000, 13.481], mean action: 1.907 [0.000, 3.000], mean observation: -0.027 [-3.383, 1.477], loss: 7.057597, mae: 64.948174, mean_q: 85.178680
  797323/1100000: episode: 1897, duration: 1.336s, episode steps: 199, steps per second: 149, episode reward: -21.361, mean reward: -0.107 [-100.000, 18.009], mean action: 1.814 [0.000, 3.000], mean observation: -0.042 [-1.847, 1.488], loss: 10.427237, mae: 64.622490, mean_q: 84.972107
  797696/1100000: episode: 1898, duration: 2.686s, episode steps: 373, steps per second: 139, episode reward: -261.135, mean reward: -0.700 [-100.000, 6.863], mean action: 1.912 [0.000, 3.000], mean observation: 0.018 [-1.471, 1.412], loss: 9.276347, mae: 65.173134, mean_q: 85.475105
  797949/1100000: episode: 1899, duration: 1.740s, episode steps: 253, steps per second: 145, episode reward: -303.806, mean reward: -1.201 [-100.000, 12.033], mean action: 1.625 [0.000, 3.000], mean observation: -0.133 [-3.185, 1.396], loss: 7.083980, mae: 64.126602, mean_q: 84.384171
  798216/1100000: episode: 1900, duration: 1.810s, episode steps: 267, steps per second: 147, episode reward: 278.746, mean reward: 1.044 [-2.712, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.064 [-0.619, 1.488], loss: 9.074362, mae: 64.644882, mean_q: 84.623451
  798661/1100000: episode: 1901, duration: 3.186s, episode steps: 445, steps per second: 140, episode reward: -1070.284, mean reward: -2.405 [-100.000, 4.294], mean action: 1.654 [0.000, 3.000], mean observation: 0.360 [-2.363, 7.068], loss: 11.238395, mae: 64.515968, mean_q: 84.727425
  798956/1100000: episode: 1902, duration: 2.052s, episode steps: 295, steps per second: 144, episode reward: -208.152, mean reward: -0.706 [-100.000, 20.909], mean action: 1.844 [0.000, 3.000], mean observation: -0.100 [-2.066, 1.417], loss: 13.219085, mae: 65.457634, mean_q: 85.783432
  799388/1100000: episode: 1903, duration: 2.971s, episode steps: 432, steps per second: 145, episode reward: 28.964, mean reward: 0.067 [-100.000, 13.005], mean action: 1.993 [0.000, 3.000], mean observation: 0.116 [-0.751, 1.404], loss: 11.041164, mae: 65.770569, mean_q: 86.251480
  799803/1100000: episode: 1904, duration: 2.960s, episode steps: 415, steps per second: 140, episode reward: 270.634, mean reward: 0.652 [-21.132, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.098 [-0.923, 1.399], loss: 7.584175, mae: 65.292984, mean_q: 86.465538
  800077/1100000: episode: 1905, duration: 1.863s, episode steps: 274, steps per second: 147, episode reward: -238.647, mean reward: -0.871 [-100.000, 20.636], mean action: 1.668 [0.000, 3.000], mean observation: -0.097 [-2.034, 1.395], loss: 7.769649, mae: 65.945099, mean_q: 86.819283
  800261/1100000: episode: 1906, duration: 1.249s, episode steps: 184, steps per second: 147, episode reward: -93.679, mean reward: -0.509 [-100.000, 11.250], mean action: 2.043 [0.000, 3.000], mean observation: -0.064 [-1.616, 1.399], loss: 11.306332, mae: 65.099144, mean_q: 85.187469
  800575/1100000: episode: 1907, duration: 2.182s, episode steps: 314, steps per second: 144, episode reward: 290.369, mean reward: 0.925 [-13.236, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.069 [-1.072, 1.474], loss: 8.837220, mae: 65.895256, mean_q: 86.717316
  801305/1100000: episode: 1908, duration: 5.210s, episode steps: 730, steps per second: 140, episode reward: 184.511, mean reward: 0.253 [-18.463, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.120 [-0.558, 1.390], loss: 10.526740, mae: 65.576332, mean_q: 85.956810
  802305/1100000: episode: 1909, duration: 7.334s, episode steps: 1000, steps per second: 136, episode reward: 143.283, mean reward: 0.143 [-20.305, 24.018], mean action: 1.417 [0.000, 3.000], mean observation: 0.259 [-0.779, 1.386], loss: 11.510191, mae: 65.116684, mean_q: 85.401955
  802884/1100000: episode: 1910, duration: 4.135s, episode steps: 579, steps per second: 140, episode reward: -26.855, mean reward: -0.046 [-100.000, 16.640], mean action: 1.848 [0.000, 3.000], mean observation: 0.011 [-1.232, 1.416], loss: 7.426622, mae: 64.574516, mean_q: 84.823494
  803499/1100000: episode: 1911, duration: 4.465s, episode steps: 615, steps per second: 138, episode reward: 203.326, mean reward: 0.331 [-3.562, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.107 [-1.292, 1.420], loss: 12.451186, mae: 64.121346, mean_q: 84.453560
  804101/1100000: episode: 1912, duration: 4.328s, episode steps: 602, steps per second: 139, episode reward: 265.123, mean reward: 0.440 [-10.426, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.100 [-0.816, 1.529], loss: 8.250154, mae: 63.906887, mean_q: 84.399193
  804443/1100000: episode: 1913, duration: 2.354s, episode steps: 342, steps per second: 145, episode reward: -147.309, mean reward: -0.431 [-100.000, 14.974], mean action: 2.015 [0.000, 3.000], mean observation: 0.051 [-0.835, 1.424], loss: 11.578602, mae: 63.819580, mean_q: 83.657890
  804681/1100000: episode: 1914, duration: 1.611s, episode steps: 238, steps per second: 148, episode reward: -106.790, mean reward: -0.449 [-100.000, 30.360], mean action: 1.622 [0.000, 3.000], mean observation: 0.003 [-0.861, 1.481], loss: 13.173103, mae: 63.299419, mean_q: 83.336174
  805224/1100000: episode: 1915, duration: 3.812s, episode steps: 543, steps per second: 142, episode reward: 234.711, mean reward: 0.432 [-20.952, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.132 [-0.653, 1.409], loss: 8.401429, mae: 63.586872, mean_q: 83.409187
  805617/1100000: episode: 1916, duration: 2.712s, episode steps: 393, steps per second: 145, episode reward: 199.815, mean reward: 0.508 [-12.232, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.169 [-1.021, 1.427], loss: 7.435113, mae: 62.833424, mean_q: 82.470482
  806139/1100000: episode: 1917, duration: 3.664s, episode steps: 522, steps per second: 142, episode reward: -117.177, mean reward: -0.224 [-100.000, 6.036], mean action: 1.310 [0.000, 3.000], mean observation: 0.082 [-0.810, 1.507], loss: 8.488773, mae: 62.251369, mean_q: 81.867111
  807139/1100000: episode: 1918, duration: 8.529s, episode steps: 1000, steps per second: 117, episode reward: -33.666, mean reward: -0.034 [-5.115, 5.615], mean action: 1.633 [0.000, 3.000], mean observation: -0.076 [-0.776, 1.393], loss: 7.345014, mae: 61.875790, mean_q: 81.679237
  807321/1100000: episode: 1919, duration: 1.234s, episode steps: 182, steps per second: 148, episode reward: 26.609, mean reward: 0.146 [-100.000, 17.715], mean action: 1.857 [0.000, 3.000], mean observation: 0.199 [-0.733, 1.386], loss: 7.703368, mae: 61.593697, mean_q: 81.556526
  808321/1100000: episode: 1920, duration: 7.665s, episode steps: 1000, steps per second: 130, episode reward: -91.186, mean reward: -0.091 [-5.657, 5.297], mean action: 1.692 [0.000, 3.000], mean observation: 0.045 [-0.600, 1.414], loss: 8.548774, mae: 60.991322, mean_q: 80.508560
  808763/1100000: episode: 1921, duration: 3.096s, episode steps: 442, steps per second: 143, episode reward: 226.893, mean reward: 0.513 [-5.104, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.217 [-1.289, 1.397], loss: 8.070363, mae: 60.729912, mean_q: 79.696526
  809299/1100000: episode: 1922, duration: 3.887s, episode steps: 536, steps per second: 138, episode reward: 143.688, mean reward: 0.268 [-23.614, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.016 [-0.783, 1.402], loss: 6.178587, mae: 59.749447, mean_q: 78.850159
  809578/1100000: episode: 1923, duration: 1.894s, episode steps: 279, steps per second: 147, episode reward: -23.813, mean reward: -0.085 [-100.000, 16.622], mean action: 1.491 [0.000, 3.000], mean observation: 0.101 [-1.499, 1.412], loss: 8.833202, mae: 59.335987, mean_q: 77.925362
  810424/1100000: episode: 1924, duration: 6.640s, episode steps: 846, steps per second: 127, episode reward: -86.899, mean reward: -0.103 [-100.000, 11.523], mean action: 1.584 [0.000, 3.000], mean observation: 0.112 [-1.120, 1.439], loss: 7.297338, mae: 58.886322, mean_q: 77.416092
  810961/1100000: episode: 1925, duration: 3.751s, episode steps: 537, steps per second: 143, episode reward: -161.673, mean reward: -0.301 [-100.000, 4.736], mean action: 1.518 [0.000, 3.000], mean observation: -0.017 [-1.003, 1.387], loss: 8.364631, mae: 58.463089, mean_q: 77.021660
  811961/1100000: episode: 1926, duration: 7.843s, episode steps: 1000, steps per second: 127, episode reward: -33.891, mean reward: -0.034 [-5.599, 4.741], mean action: 1.626 [0.000, 3.000], mean observation: 0.103 [-0.586, 1.427], loss: 7.517081, mae: 58.790668, mean_q: 77.375565
  812325/1100000: episode: 1927, duration: 2.623s, episode steps: 364, steps per second: 139, episode reward: 233.544, mean reward: 0.642 [-10.087, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.173 [-0.640, 1.411], loss: 7.787824, mae: 59.195053, mean_q: 77.640617
  812650/1100000: episode: 1928, duration: 2.280s, episode steps: 325, steps per second: 143, episode reward: 271.285, mean reward: 0.835 [-3.602, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.075 [-0.775, 1.385], loss: 7.951874, mae: 59.558250, mean_q: 78.566765
  813116/1100000: episode: 1929, duration: 3.280s, episode steps: 466, steps per second: 142, episode reward: 240.453, mean reward: 0.516 [-9.140, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.052 [-1.042, 1.403], loss: 9.659904, mae: 60.376419, mean_q: 79.020004
  813650/1100000: episode: 1930, duration: 3.692s, episode steps: 534, steps per second: 145, episode reward: 241.896, mean reward: 0.453 [-14.774, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.229 [-0.810, 1.408], loss: 8.671156, mae: 61.000645, mean_q: 79.428925
  814041/1100000: episode: 1931, duration: 2.711s, episode steps: 391, steps per second: 144, episode reward: -93.033, mean reward: -0.238 [-100.000, 18.450], mean action: 1.217 [0.000, 3.000], mean observation: 0.088 [-1.300, 1.413], loss: 6.528489, mae: 61.378517, mean_q: 80.003273
  815041/1100000: episode: 1932, duration: 7.475s, episode steps: 1000, steps per second: 134, episode reward: 126.269, mean reward: 0.126 [-20.173, 20.672], mean action: 2.033 [0.000, 3.000], mean observation: 0.248 [-0.640, 1.448], loss: 9.587293, mae: 61.928543, mean_q: 81.018471
  815200/1100000: episode: 1933, duration: 1.077s, episode steps: 159, steps per second: 148, episode reward: -90.338, mean reward: -0.568 [-100.000, 15.158], mean action: 1.730 [0.000, 3.000], mean observation: 0.089 [-1.184, 1.398], loss: 14.074689, mae: 62.742752, mean_q: 81.780914
  815588/1100000: episode: 1934, duration: 2.641s, episode steps: 388, steps per second: 147, episode reward: 222.383, mean reward: 0.573 [-19.406, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.232 [-1.337, 1.395], loss: 9.644642, mae: 62.096470, mean_q: 81.410355
  816152/1100000: episode: 1935, duration: 3.987s, episode steps: 564, steps per second: 141, episode reward: 202.698, mean reward: 0.359 [-14.854, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.137 [-1.072, 1.434], loss: 9.183564, mae: 62.947258, mean_q: 82.628654
  816440/1100000: episode: 1936, duration: 1.971s, episode steps: 288, steps per second: 146, episode reward: 244.057, mean reward: 0.847 [-17.382, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.107 [-0.816, 1.417], loss: 10.481151, mae: 63.549072, mean_q: 83.140152
  816710/1100000: episode: 1937, duration: 1.841s, episode steps: 270, steps per second: 147, episode reward: 25.809, mean reward: 0.096 [-100.000, 12.616], mean action: 1.667 [0.000, 3.000], mean observation: 0.069 [-0.746, 1.410], loss: 11.078894, mae: 63.001503, mean_q: 82.532173
  817036/1100000: episode: 1938, duration: 2.261s, episode steps: 326, steps per second: 144, episode reward: -179.465, mean reward: -0.551 [-100.000, 15.245], mean action: 1.325 [0.000, 3.000], mean observation: -0.009 [-1.598, 1.481], loss: 9.254661, mae: 64.160309, mean_q: 84.270836
  818036/1100000: episode: 1939, duration: 7.914s, episode steps: 1000, steps per second: 126, episode reward: -70.236, mean reward: -0.070 [-5.033, 5.461], mean action: 1.732 [0.000, 3.000], mean observation: -0.051 [-0.611, 1.396], loss: 7.240703, mae: 63.194874, mean_q: 82.915970
  818742/1100000: episode: 1940, duration: 5.281s, episode steps: 706, steps per second: 134, episode reward: 210.508, mean reward: 0.298 [-20.117, 100.000], mean action: 1.516 [0.000, 3.000], mean observation: 0.197 [-1.108, 1.408], loss: 9.215669, mae: 63.694340, mean_q: 83.306587
  819143/1100000: episode: 1941, duration: 2.821s, episode steps: 401, steps per second: 142, episode reward: 245.615, mean reward: 0.613 [-19.104, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.215 [-1.211, 1.443], loss: 7.502947, mae: 63.176163, mean_q: 82.921143
  819504/1100000: episode: 1942, duration: 2.550s, episode steps: 361, steps per second: 142, episode reward: 252.047, mean reward: 0.698 [-20.236, 100.000], mean action: 1.626 [0.000, 3.000], mean observation: 0.114 [-0.712, 1.393], loss: 7.442338, mae: 63.064312, mean_q: 82.579346
  820504/1100000: episode: 1943, duration: 7.961s, episode steps: 1000, steps per second: 126, episode reward: -58.957, mean reward: -0.059 [-6.586, 5.423], mean action: 1.704 [0.000, 3.000], mean observation: -0.002 [-0.738, 2.212], loss: 9.169676, mae: 62.400673, mean_q: 81.905579
  821504/1100000: episode: 1944, duration: 7.739s, episode steps: 1000, steps per second: 129, episode reward: -96.353, mean reward: -0.096 [-5.129, 4.776], mean action: 1.850 [0.000, 3.000], mean observation: 0.027 [-0.600, 1.425], loss: 10.708125, mae: 61.971424, mean_q: 81.439796
  822504/1100000: episode: 1945, duration: 7.933s, episode steps: 1000, steps per second: 126, episode reward: -110.154, mean reward: -0.110 [-6.274, 4.898], mean action: 1.906 [0.000, 3.000], mean observation: 0.046 [-0.600, 1.480], loss: 8.394062, mae: 61.847973, mean_q: 81.466812
  822839/1100000: episode: 1946, duration: 2.385s, episode steps: 335, steps per second: 140, episode reward: 275.645, mean reward: 0.823 [-9.692, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.058 [-1.343, 1.392], loss: 10.714793, mae: 61.377949, mean_q: 81.041588
  823359/1100000: episode: 1947, duration: 3.686s, episode steps: 520, steps per second: 141, episode reward: -189.097, mean reward: -0.364 [-100.000, 7.782], mean action: 1.762 [0.000, 3.000], mean observation: -0.029 [-0.872, 1.488], loss: 7.127345, mae: 62.156284, mean_q: 81.849922
  823681/1100000: episode: 1948, duration: 2.215s, episode steps: 322, steps per second: 145, episode reward: 239.197, mean reward: 0.743 [-10.280, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.158 [-0.656, 1.391], loss: 10.178846, mae: 61.654114, mean_q: 81.063805
  824050/1100000: episode: 1949, duration: 2.562s, episode steps: 369, steps per second: 144, episode reward: -437.658, mean reward: -1.186 [-100.000, 4.486], mean action: 1.783 [0.000, 3.000], mean observation: 0.029 [-1.049, 2.374], loss: 12.629190, mae: 61.010555, mean_q: 80.514557
  824889/1100000: episode: 1950, duration: 5.950s, episode steps: 839, steps per second: 141, episode reward: -149.895, mean reward: -0.179 [-100.000, 18.877], mean action: 1.460 [0.000, 3.000], mean observation: 0.140 [-0.638, 1.432], loss: 6.869354, mae: 60.463554, mean_q: 79.812149
  825216/1100000: episode: 1951, duration: 2.278s, episode steps: 327, steps per second: 144, episode reward: 253.145, mean reward: 0.774 [-9.532, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.106 [-1.072, 1.465], loss: 10.035494, mae: 59.601742, mean_q: 78.477982
  825523/1100000: episode: 1952, duration: 2.118s, episode steps: 307, steps per second: 145, episode reward: 244.015, mean reward: 0.795 [-9.709, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.173 [-0.977, 1.386], loss: 18.317417, mae: 59.238804, mean_q: 77.947166
  826499/1100000: episode: 1953, duration: 7.580s, episode steps: 976, steps per second: 129, episode reward: 233.076, mean reward: 0.239 [-19.198, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.212 [-1.060, 1.436], loss: 6.593059, mae: 58.599346, mean_q: 77.054741
  826865/1100000: episode: 1954, duration: 2.555s, episode steps: 366, steps per second: 143, episode reward: 277.637, mean reward: 0.759 [-18.150, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.068 [-1.219, 1.523], loss: 5.487570, mae: 58.339298, mean_q: 77.022888
  827062/1100000: episode: 1955, duration: 1.347s, episode steps: 197, steps per second: 146, episode reward: -244.153, mean reward: -1.239 [-100.000, 6.534], mean action: 1.421 [0.000, 3.000], mean observation: 0.098 [-1.181, 2.397], loss: 19.212507, mae: 57.829899, mean_q: 75.974701
  827357/1100000: episode: 1956, duration: 2.026s, episode steps: 295, steps per second: 146, episode reward: -226.180, mean reward: -0.767 [-100.000, 26.099], mean action: 1.715 [0.000, 3.000], mean observation: 0.058 [-1.997, 1.390], loss: 6.660672, mae: 57.871517, mean_q: 76.366508
  827668/1100000: episode: 1957, duration: 2.167s, episode steps: 311, steps per second: 144, episode reward: -201.573, mean reward: -0.648 [-100.000, 10.043], mean action: 1.900 [0.000, 3.000], mean observation: 0.110 [-1.795, 1.501], loss: 6.923421, mae: 57.925838, mean_q: 75.857185
  828668/1100000: episode: 1958, duration: 7.437s, episode steps: 1000, steps per second: 134, episode reward: -32.512, mean reward: -0.033 [-22.080, 18.422], mean action: 1.776 [0.000, 3.000], mean observation: -0.048 [-1.000, 1.398], loss: 9.269080, mae: 57.153381, mean_q: 74.947342
  828877/1100000: episode: 1959, duration: 1.421s, episode steps: 209, steps per second: 147, episode reward: -178.857, mean reward: -0.856 [-100.000, 19.594], mean action: 1.861 [0.000, 3.000], mean observation: 0.173 [-0.887, 2.036], loss: 7.497854, mae: 56.511311, mean_q: 73.958336
  829138/1100000: episode: 1960, duration: 1.781s, episode steps: 261, steps per second: 147, episode reward: -219.766, mean reward: -0.842 [-100.000, 3.866], mean action: 1.598 [0.000, 3.000], mean observation: 0.076 [-0.665, 1.973], loss: 6.730685, mae: 56.329914, mean_q: 73.718140
  830026/1100000: episode: 1961, duration: 6.737s, episode steps: 888, steps per second: 132, episode reward: -192.798, mean reward: -0.217 [-100.000, 12.694], mean action: 1.642 [0.000, 3.000], mean observation: -0.002 [-0.680, 2.510], loss: 13.601962, mae: 56.200970, mean_q: 73.571289
  830194/1100000: episode: 1962, duration: 1.125s, episode steps: 168, steps per second: 149, episode reward: 61.364, mean reward: 0.365 [-100.000, 14.661], mean action: 1.774 [0.000, 3.000], mean observation: 0.016 [-1.517, 1.412], loss: 9.742067, mae: 55.867828, mean_q: 73.552887
  830490/1100000: episode: 1963, duration: 2.058s, episode steps: 296, steps per second: 144, episode reward: 15.219, mean reward: 0.051 [-100.000, 14.573], mean action: 1.885 [0.000, 3.000], mean observation: 0.100 [-0.547, 1.388], loss: 9.584726, mae: 56.078377, mean_q: 73.198463
  830925/1100000: episode: 1964, duration: 3.157s, episode steps: 435, steps per second: 138, episode reward: 230.472, mean reward: 0.530 [-20.670, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.213 [-0.546, 1.395], loss: 7.658292, mae: 55.997063, mean_q: 73.426537
  831385/1100000: episode: 1965, duration: 3.221s, episode steps: 460, steps per second: 143, episode reward: 160.953, mean reward: 0.350 [-19.081, 100.000], mean action: 2.137 [0.000, 3.000], mean observation: -0.002 [-0.696, 1.402], loss: 8.600291, mae: 55.705479, mean_q: 73.288475
  831625/1100000: episode: 1966, duration: 1.620s, episode steps: 240, steps per second: 148, episode reward: 47.887, mean reward: 0.200 [-100.000, 19.295], mean action: 1.679 [0.000, 3.000], mean observation: 0.051 [-1.665, 1.414], loss: 8.668512, mae: 55.510078, mean_q: 73.150108
  831952/1100000: episode: 1967, duration: 2.242s, episode steps: 327, steps per second: 146, episode reward: 290.693, mean reward: 0.889 [-18.337, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.044 [-0.643, 1.496], loss: 7.340647, mae: 55.164387, mean_q: 72.632965
  832952/1100000: episode: 1968, duration: 7.848s, episode steps: 1000, steps per second: 127, episode reward: -115.887, mean reward: -0.116 [-5.046, 5.441], mean action: 1.712 [0.000, 3.000], mean observation: 0.053 [-0.672, 1.462], loss: 7.942816, mae: 55.233540, mean_q: 72.306282
  833278/1100000: episode: 1969, duration: 2.428s, episode steps: 326, steps per second: 134, episode reward: 180.632, mean reward: 0.554 [-11.723, 100.000], mean action: 1.837 [0.000, 3.000], mean observation: 0.120 [-0.906, 1.400], loss: 6.441227, mae: 55.900860, mean_q: 73.370186
  833506/1100000: episode: 1970, duration: 1.547s, episode steps: 228, steps per second: 147, episode reward: -322.855, mean reward: -1.416 [-100.000, 3.645], mean action: 1.982 [0.000, 3.000], mean observation: 0.275 [-0.600, 3.183], loss: 7.750445, mae: 55.814934, mean_q: 72.957909
  833985/1100000: episode: 1971, duration: 3.402s, episode steps: 479, steps per second: 141, episode reward: 264.575, mean reward: 0.552 [-18.047, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.085 [-0.735, 1.500], loss: 8.708011, mae: 55.853111, mean_q: 72.591896
  834392/1100000: episode: 1972, duration: 2.891s, episode steps: 407, steps per second: 141, episode reward: 230.550, mean reward: 0.566 [-19.233, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.116 [-0.953, 1.419], loss: 6.832752, mae: 55.664207, mean_q: 72.724449
  835392/1100000: episode: 1973, duration: 7.910s, episode steps: 1000, steps per second: 126, episode reward: -70.531, mean reward: -0.071 [-4.421, 5.117], mean action: 1.533 [0.000, 3.000], mean observation: 0.039 [-0.862, 1.390], loss: 9.425846, mae: 55.307648, mean_q: 71.873245
  836392/1100000: episode: 1974, duration: 8.321s, episode steps: 1000, steps per second: 120, episode reward: -24.513, mean reward: -0.025 [-4.718, 5.289], mean action: 1.664 [0.000, 3.000], mean observation: -0.067 [-0.602, 1.402], loss: 7.708447, mae: 54.596046, mean_q: 71.320923
  836993/1100000: episode: 1975, duration: 4.318s, episode steps: 601, steps per second: 139, episode reward: 239.465, mean reward: 0.398 [-20.588, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.123 [-1.296, 1.424], loss: 6.905519, mae: 54.317810, mean_q: 70.839706
  837182/1100000: episode: 1976, duration: 1.272s, episode steps: 189, steps per second: 149, episode reward: -28.309, mean reward: -0.150 [-100.000, 9.531], mean action: 1.651 [0.000, 3.000], mean observation: 0.055 [-1.257, 1.466], loss: 9.286706, mae: 53.401554, mean_q: 70.079025
  838182/1100000: episode: 1977, duration: 7.186s, episode steps: 1000, steps per second: 139, episode reward: -64.906, mean reward: -0.065 [-5.095, 5.643], mean action: 1.599 [0.000, 3.000], mean observation: 0.059 [-0.744, 1.405], loss: 7.351768, mae: 53.453255, mean_q: 69.430679
  838467/1100000: episode: 1978, duration: 1.950s, episode steps: 285, steps per second: 146, episode reward: -23.375, mean reward: -0.082 [-100.000, 11.864], mean action: 1.477 [0.000, 3.000], mean observation: 0.160 [-0.515, 1.422], loss: 7.583189, mae: 53.248669, mean_q: 69.520233
  839065/1100000: episode: 1979, duration: 4.378s, episode steps: 598, steps per second: 137, episode reward: 255.776, mean reward: 0.428 [-20.501, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.101 [-1.132, 1.387], loss: 7.471513, mae: 52.581333, mean_q: 68.673950
  839543/1100000: episode: 1980, duration: 3.361s, episode steps: 478, steps per second: 142, episode reward: 165.610, mean reward: 0.346 [-19.095, 100.000], mean action: 1.885 [0.000, 3.000], mean observation: 0.059 [-0.997, 1.425], loss: 7.636889, mae: 52.690315, mean_q: 68.914589
  840543/1100000: episode: 1981, duration: 7.688s, episode steps: 1000, steps per second: 130, episode reward: -88.621, mean reward: -0.089 [-5.956, 5.028], mean action: 1.611 [0.000, 3.000], mean observation: -0.076 [-0.687, 1.458], loss: 8.306312, mae: 52.276913, mean_q: 68.051170
  840869/1100000: episode: 1982, duration: 2.242s, episode steps: 326, steps per second: 145, episode reward: 189.610, mean reward: 0.582 [-9.885, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.147 [-0.769, 1.417], loss: 11.860726, mae: 51.688992, mean_q: 67.205681
  841385/1100000: episode: 1983, duration: 3.870s, episode steps: 516, steps per second: 133, episode reward: 188.508, mean reward: 0.365 [-16.498, 100.000], mean action: 2.153 [0.000, 3.000], mean observation: 0.151 [-1.379, 1.422], loss: 9.165397, mae: 51.620369, mean_q: 67.455002
  842385/1100000: episode: 1984, duration: 7.223s, episode steps: 1000, steps per second: 138, episode reward: -172.341, mean reward: -0.172 [-5.563, 4.855], mean action: 1.729 [0.000, 3.000], mean observation: 0.039 [-0.741, 1.498], loss: 8.313106, mae: 51.272736, mean_q: 66.878815
  842813/1100000: episode: 1985, duration: 3.043s, episode steps: 428, steps per second: 141, episode reward: -42.269, mean reward: -0.099 [-100.000, 17.483], mean action: 1.614 [0.000, 3.000], mean observation: 0.107 [-0.763, 1.406], loss: 11.146039, mae: 50.340630, mean_q: 66.006439
  843813/1100000: episode: 1986, duration: 8.110s, episode steps: 1000, steps per second: 123, episode reward: -121.802, mean reward: -0.122 [-11.712, 10.413], mean action: 1.832 [0.000, 3.000], mean observation: -0.005 [-0.600, 1.401], loss: 8.477679, mae: 50.335201, mean_q: 65.744392
  844633/1100000: episode: 1987, duration: 6.208s, episode steps: 820, steps per second: 132, episode reward: 114.358, mean reward: 0.139 [-18.153, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: -0.056 [-0.618, 1.402], loss: 7.020689, mae: 50.383389, mean_q: 65.774025
  845241/1100000: episode: 1988, duration: 4.319s, episode steps: 608, steps per second: 141, episode reward: 178.094, mean reward: 0.293 [-10.070, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.142 [-0.449, 1.423], loss: 8.516855, mae: 49.932468, mean_q: 65.335205
  846241/1100000: episode: 1989, duration: 7.231s, episode steps: 1000, steps per second: 138, episode reward: -167.059, mean reward: -0.167 [-5.370, 4.626], mean action: 1.614 [0.000, 3.000], mean observation: 0.045 [-0.600, 1.409], loss: 7.716659, mae: 49.275661, mean_q: 64.484993
  847172/1100000: episode: 1990, duration: 6.732s, episode steps: 931, steps per second: 138, episode reward: 233.887, mean reward: 0.251 [-19.876, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.236 [-0.869, 1.522], loss: 6.876890, mae: 49.419212, mean_q: 64.199211
  847269/1100000: episode: 1991, duration: 0.656s, episode steps: 97, steps per second: 148, episode reward: -178.388, mean reward: -1.839 [-100.000, 2.790], mean action: 1.392 [0.000, 3.000], mean observation: 0.249 [-0.683, 1.403], loss: 4.967649, mae: 49.079742, mean_q: 63.894859
  847429/1100000: episode: 1992, duration: 1.072s, episode steps: 160, steps per second: 149, episode reward: 62.638, mean reward: 0.391 [-100.000, 16.095], mean action: 1.475 [0.000, 3.000], mean observation: 0.149 [-0.744, 1.453], loss: 6.238757, mae: 49.191376, mean_q: 64.154556
  848048/1100000: episode: 1993, duration: 4.688s, episode steps: 619, steps per second: 132, episode reward: -204.515, mean reward: -0.330 [-100.000, 7.231], mean action: 1.813 [0.000, 3.000], mean observation: -0.095 [-1.595, 1.533], loss: 7.538959, mae: 49.175911, mean_q: 63.611572
  848602/1100000: episode: 1994, duration: 3.943s, episode steps: 554, steps per second: 140, episode reward: -153.026, mean reward: -0.276 [-100.000, 4.076], mean action: 1.623 [0.000, 3.000], mean observation: 0.056 [-0.600, 1.403], loss: 7.750909, mae: 49.015560, mean_q: 63.374603
  848711/1100000: episode: 1995, duration: 0.741s, episode steps: 109, steps per second: 147, episode reward: -116.983, mean reward: -1.073 [-100.000, 52.039], mean action: 1.661 [0.000, 3.000], mean observation: -0.032 [-1.662, 1.389], loss: 3.181497, mae: 48.756870, mean_q: 62.932114
  849711/1100000: episode: 1996, duration: 8.291s, episode steps: 1000, steps per second: 121, episode reward: -71.118, mean reward: -0.071 [-4.585, 5.578], mean action: 1.661 [0.000, 3.000], mean observation: -0.104 [-0.643, 1.396], loss: 7.796010, mae: 48.960415, mean_q: 63.539490
  850105/1100000: episode: 1997, duration: 2.777s, episode steps: 394, steps per second: 142, episode reward: 233.127, mean reward: 0.592 [-18.648, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.147 [-0.802, 1.411], loss: 6.276518, mae: 48.901539, mean_q: 63.696560
  850503/1100000: episode: 1998, duration: 2.844s, episode steps: 398, steps per second: 140, episode reward: 240.540, mean reward: 0.604 [-13.529, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.070 [-0.560, 1.398], loss: 6.730629, mae: 49.401932, mean_q: 64.091347
  851313/1100000: episode: 1999, duration: 6.218s, episode steps: 810, steps per second: 130, episode reward: 135.674, mean reward: 0.167 [-18.599, 100.000], mean action: 1.656 [0.000, 3.000], mean observation: -0.029 [-0.650, 1.522], loss: 8.028693, mae: 49.095371, mean_q: 63.919022
  852313/1100000: episode: 2000, duration: 7.717s, episode steps: 1000, steps per second: 130, episode reward: -16.255, mean reward: -0.016 [-19.458, 21.443], mean action: 1.836 [0.000, 3.000], mean observation: -0.091 [-0.675, 1.455], loss: 9.348469, mae: 48.847679, mean_q: 63.475346
  852643/1100000: episode: 2001, duration: 2.262s, episode steps: 330, steps per second: 146, episode reward: 255.318, mean reward: 0.774 [-9.339, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.127 [-1.197, 1.451], loss: 6.784674, mae: 49.206135, mean_q: 63.956120
  853339/1100000: episode: 2002, duration: 4.988s, episode steps: 696, steps per second: 140, episode reward: -767.741, mean reward: -1.103 [-100.000, 3.642], mean action: 1.792 [0.000, 3.000], mean observation: 0.187 [-0.648, 5.284], loss: 7.053865, mae: 49.448956, mean_q: 64.537567
  853473/1100000: episode: 2003, duration: 0.903s, episode steps: 134, steps per second: 148, episode reward: -615.069, mean reward: -4.590 [-100.000, 2.484], mean action: 1.978 [0.000, 3.000], mean observation: 0.261 [-1.895, 3.343], loss: 4.082369, mae: 50.522598, mean_q: 66.488525
  853749/1100000: episode: 2004, duration: 1.879s, episode steps: 276, steps per second: 147, episode reward: 241.179, mean reward: 0.874 [-18.024, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.091 [-0.944, 1.406], loss: 9.547951, mae: 51.312374, mean_q: 66.903877
  853994/1100000: episode: 2005, duration: 1.672s, episode steps: 245, steps per second: 147, episode reward: 293.440, mean reward: 1.198 [-13.766, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.051 [-1.223, 1.406], loss: 40.028744, mae: 50.818806, mean_q: 66.654556
  854208/1100000: episode: 2006, duration: 1.454s, episode steps: 214, steps per second: 147, episode reward: -1063.414, mean reward: -4.969 [-100.000, 3.014], mean action: 1.949 [0.000, 3.000], mean observation: 0.269 [-1.195, 5.479], loss: 25.472759, mae: 51.022606, mean_q: 66.604424
  854582/1100000: episode: 2007, duration: 2.613s, episode steps: 374, steps per second: 143, episode reward: 299.592, mean reward: 0.801 [-18.023, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.094 [-0.761, 1.390], loss: 7.296371, mae: 51.592663, mean_q: 68.270355
  854891/1100000: episode: 2008, duration: 2.219s, episode steps: 309, steps per second: 139, episode reward: 300.450, mean reward: 0.972 [-9.634, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.106 [-1.516, 1.428], loss: 7.738039, mae: 51.777332, mean_q: 68.065010
  855891/1100000: episode: 2009, duration: 7.776s, episode steps: 1000, steps per second: 129, episode reward: 35.591, mean reward: 0.036 [-5.258, 12.856], mean action: 1.585 [0.000, 3.000], mean observation: 0.027 [-1.045, 1.387], loss: 9.360065, mae: 51.770313, mean_q: 68.047256
  856704/1100000: episode: 2010, duration: 5.982s, episode steps: 813, steps per second: 136, episode reward: 165.695, mean reward: 0.204 [-17.714, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: -0.021 [-0.600, 1.517], loss: 9.207952, mae: 51.406456, mean_q: 67.457611
  857495/1100000: episode: 2011, duration: 6.473s, episode steps: 791, steps per second: 122, episode reward: 170.395, mean reward: 0.215 [-11.970, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: -0.059 [-0.959, 1.388], loss: 7.554548, mae: 52.288025, mean_q: 68.976471
  858060/1100000: episode: 2012, duration: 3.924s, episode steps: 565, steps per second: 144, episode reward: 147.143, mean reward: 0.260 [-11.985, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: -0.039 [-0.600, 1.412], loss: 8.371428, mae: 52.177601, mean_q: 68.727005
  858301/1100000: episode: 2013, duration: 1.642s, episode steps: 241, steps per second: 147, episode reward: 282.841, mean reward: 1.174 [-11.657, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.050 [-0.781, 1.404], loss: 66.851570, mae: 53.337696, mean_q: 70.684105
  858679/1100000: episode: 2014, duration: 2.681s, episode steps: 378, steps per second: 141, episode reward: -276.389, mean reward: -0.731 [-100.000, 14.967], mean action: 1.899 [0.000, 3.000], mean observation: 0.110 [-0.875, 1.695], loss: 7.733556, mae: 52.811512, mean_q: 70.039856
  859577/1100000: episode: 2015, duration: 6.409s, episode steps: 898, steps per second: 140, episode reward: 175.834, mean reward: 0.196 [-9.974, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.034 [-1.016, 1.485], loss: 7.472660, mae: 54.052109, mean_q: 71.780449
  860063/1100000: episode: 2016, duration: 3.521s, episode steps: 486, steps per second: 138, episode reward: 185.924, mean reward: 0.383 [-9.961, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.083 [-0.590, 1.538], loss: 6.983110, mae: 54.579453, mean_q: 72.403511
  860367/1100000: episode: 2017, duration: 2.108s, episode steps: 304, steps per second: 144, episode reward: 265.433, mean reward: 0.873 [-14.931, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.042 [-0.768, 1.451], loss: 6.664740, mae: 54.450542, mean_q: 72.297096
  860811/1100000: episode: 2018, duration: 3.248s, episode steps: 444, steps per second: 137, episode reward: 186.936, mean reward: 0.421 [-14.030, 100.000], mean action: 2.014 [0.000, 3.000], mean observation: 0.113 [-0.684, 1.441], loss: 9.427010, mae: 53.794868, mean_q: 71.139725
  861516/1100000: episode: 2019, duration: 5.219s, episode steps: 705, steps per second: 135, episode reward: -283.050, mean reward: -0.401 [-100.000, 13.205], mean action: 1.887 [0.000, 3.000], mean observation: -0.099 [-1.009, 1.464], loss: 9.791955, mae: 54.767166, mean_q: 72.197166
  861748/1100000: episode: 2020, duration: 1.578s, episode steps: 232, steps per second: 147, episode reward: -30.034, mean reward: -0.129 [-100.000, 12.410], mean action: 1.595 [0.000, 3.000], mean observation: 0.193 [-2.950, 2.226], loss: 11.023468, mae: 55.465836, mean_q: 73.600555
  861876/1100000: episode: 2021, duration: 0.857s, episode steps: 128, steps per second: 149, episode reward: 41.542, mean reward: 0.325 [-100.000, 17.931], mean action: 1.609 [0.000, 3.000], mean observation: 0.107 [-1.027, 1.389], loss: 6.081795, mae: 55.518051, mean_q: 73.995697
  862333/1100000: episode: 2022, duration: 3.218s, episode steps: 457, steps per second: 142, episode reward: 284.286, mean reward: 0.622 [-18.122, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: 0.123 [-0.840, 1.411], loss: 10.537286, mae: 55.446030, mean_q: 73.654640
  862703/1100000: episode: 2023, duration: 2.571s, episode steps: 370, steps per second: 144, episode reward: 225.570, mean reward: 0.610 [-9.240, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.058 [-0.717, 1.442], loss: 45.698532, mae: 55.411053, mean_q: 73.278526
  863262/1100000: episode: 2024, duration: 4.328s, episode steps: 559, steps per second: 129, episode reward: 216.088, mean reward: 0.387 [-19.410, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: -0.024 [-0.731, 1.407], loss: 13.571680, mae: 55.903267, mean_q: 74.095985
  863646/1100000: episode: 2025, duration: 2.707s, episode steps: 384, steps per second: 142, episode reward: 240.890, mean reward: 0.627 [-18.559, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.104 [-0.780, 1.397], loss: 14.483265, mae: 55.650700, mean_q: 74.096596
  863858/1100000: episode: 2026, duration: 1.428s, episode steps: 212, steps per second: 148, episode reward: -254.740, mean reward: -1.202 [-100.000, 25.692], mean action: 1.382 [0.000, 3.000], mean observation: 0.095 [-0.975, 1.409], loss: 9.253160, mae: 55.467793, mean_q: 73.644051
  864858/1100000: episode: 2027, duration: 7.346s, episode steps: 1000, steps per second: 136, episode reward: -272.289, mean reward: -0.272 [-6.409, 6.148], mean action: 1.813 [0.000, 3.000], mean observation: 0.267 [-0.968, 3.643], loss: 12.532593, mae: 57.441235, mean_q: 76.329796
  865131/1100000: episode: 2028, duration: 1.850s, episode steps: 273, steps per second: 148, episode reward: 266.034, mean reward: 0.974 [-16.191, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.051 [-0.863, 1.423], loss: 20.332735, mae: 58.843033, mean_q: 78.169312
  866131/1100000: episode: 2029, duration: 8.191s, episode steps: 1000, steps per second: 122, episode reward: 3.749, mean reward: 0.004 [-19.225, 14.357], mean action: 1.757 [0.000, 3.000], mean observation: -0.043 [-0.894, 1.396], loss: 11.165083, mae: 59.054962, mean_q: 78.396133
  866662/1100000: episode: 2030, duration: 3.725s, episode steps: 531, steps per second: 143, episode reward: 167.582, mean reward: 0.316 [-18.935, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: -0.027 [-0.600, 1.416], loss: 9.571487, mae: 58.304672, mean_q: 77.514061
  867009/1100000: episode: 2031, duration: 2.401s, episode steps: 347, steps per second: 145, episode reward: -372.466, mean reward: -1.073 [-100.000, 3.811], mean action: 1.882 [0.000, 3.000], mean observation: 0.384 [-0.709, 3.490], loss: 45.630554, mae: 60.145939, mean_q: 80.086037
  868009/1100000: episode: 2032, duration: 7.314s, episode steps: 1000, steps per second: 137, episode reward: 89.096, mean reward: 0.089 [-19.882, 15.241], mean action: 1.509 [0.000, 3.000], mean observation: 0.035 [-1.493, 1.687], loss: 22.335125, mae: 59.332569, mean_q: 78.848633
  868315/1100000: episode: 2033, duration: 2.119s, episode steps: 306, steps per second: 144, episode reward: -282.920, mean reward: -0.925 [-100.000, 6.098], mean action: 1.739 [0.000, 3.000], mean observation: 0.177 [-1.003, 2.402], loss: 8.279319, mae: 58.860806, mean_q: 77.714180
  868730/1100000: episode: 2034, duration: 2.910s, episode steps: 415, steps per second: 143, episode reward: 166.829, mean reward: 0.402 [-12.935, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: -0.051 [-1.427, 1.411], loss: 27.402920, mae: 59.341339, mean_q: 78.835663
  869467/1100000: episode: 2035, duration: 5.417s, episode steps: 737, steps per second: 136, episode reward: 148.850, mean reward: 0.202 [-19.717, 100.000], mean action: 1.668 [0.000, 3.000], mean observation: 0.014 [-0.800, 1.399], loss: 8.995749, mae: 59.054352, mean_q: 78.451271
  869869/1100000: episode: 2036, duration: 2.759s, episode steps: 402, steps per second: 146, episode reward: 225.962, mean reward: 0.562 [-19.146, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: -0.034 [-0.682, 1.513], loss: 30.715759, mae: 58.780937, mean_q: 78.115059
  870048/1100000: episode: 2037, duration: 1.212s, episode steps: 179, steps per second: 148, episode reward: -242.123, mean reward: -1.353 [-100.000, 4.320], mean action: 2.006 [0.000, 3.000], mean observation: 0.096 [-1.512, 1.409], loss: 5.826844, mae: 59.635651, mean_q: 79.606766
  871048/1100000: episode: 2038, duration: 7.557s, episode steps: 1000, steps per second: 132, episode reward: -116.634, mean reward: -0.117 [-7.595, 5.621], mean action: 1.850 [0.000, 3.000], mean observation: 0.007 [-1.809, 1.393], loss: 9.150517, mae: 60.020538, mean_q: 79.784348
  871364/1100000: episode: 2039, duration: 2.281s, episode steps: 316, steps per second: 139, episode reward: 262.741, mean reward: 0.831 [-9.836, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: 0.066 [-0.768, 1.394], loss: 8.970332, mae: 60.038307, mean_q: 79.842415
  871816/1100000: episode: 2040, duration: 3.237s, episode steps: 452, steps per second: 140, episode reward: 177.814, mean reward: 0.393 [-18.422, 100.000], mean action: 2.119 [0.000, 3.000], mean observation: 0.109 [-0.689, 1.403], loss: 25.819513, mae: 60.362137, mean_q: 80.205727
  872297/1100000: episode: 2041, duration: 3.460s, episode steps: 481, steps per second: 139, episode reward: 239.248, mean reward: 0.497 [-18.870, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.105 [-1.111, 1.419], loss: 20.232183, mae: 60.668003, mean_q: 80.846237
  872589/1100000: episode: 2042, duration: 1.987s, episode steps: 292, steps per second: 147, episode reward: -261.763, mean reward: -0.896 [-100.000, 4.598], mean action: 1.747 [0.000, 3.000], mean observation: 0.186 [-1.567, 1.492], loss: 11.127337, mae: 60.399467, mean_q: 80.633186
  873278/1100000: episode: 2043, duration: 5.152s, episode steps: 689, steps per second: 134, episode reward: 231.145, mean reward: 0.335 [-19.043, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.105 [-0.755, 1.391], loss: 8.153703, mae: 61.294754, mean_q: 81.825424
  873625/1100000: episode: 2044, duration: 2.421s, episode steps: 347, steps per second: 143, episode reward: 168.007, mean reward: 0.484 [-15.638, 100.000], mean action: 1.643 [0.000, 3.000], mean observation: 0.025 [-1.152, 1.455], loss: 8.631593, mae: 61.183201, mean_q: 81.564590
  874160/1100000: episode: 2045, duration: 3.808s, episode steps: 535, steps per second: 140, episode reward: 262.385, mean reward: 0.490 [-17.748, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.119 [-1.169, 1.398], loss: 7.983407, mae: 61.947285, mean_q: 82.766869
  874394/1100000: episode: 2046, duration: 1.604s, episode steps: 234, steps per second: 146, episode reward: -59.240, mean reward: -0.253 [-100.000, 17.147], mean action: 1.833 [0.000, 3.000], mean observation: -0.065 [-1.544, 1.386], loss: 6.729795, mae: 62.347282, mean_q: 83.182693
  874835/1100000: episode: 2047, duration: 3.151s, episode steps: 441, steps per second: 140, episode reward: 238.373, mean reward: 0.541 [-10.810, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.075 [-0.865, 1.444], loss: 87.417511, mae: 61.954334, mean_q: 82.363129
  875337/1100000: episode: 2048, duration: 3.618s, episode steps: 502, steps per second: 139, episode reward: 167.752, mean reward: 0.334 [-10.731, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: -0.088 [-1.176, 1.398], loss: 20.911255, mae: 62.461613, mean_q: 83.350266
  875531/1100000: episode: 2049, duration: 1.310s, episode steps: 194, steps per second: 148, episode reward: -217.039, mean reward: -1.119 [-100.000, 4.134], mean action: 1.928 [0.000, 3.000], mean observation: 0.108 [-1.007, 1.428], loss: 11.187422, mae: 61.709026, mean_q: 82.165947
  875654/1100000: episode: 2050, duration: 0.823s, episode steps: 123, steps per second: 149, episode reward: -846.421, mean reward: -6.881 [-100.000, 1.530], mean action: 2.569 [0.000, 3.000], mean observation: -0.395 [-8.082, 1.406], loss: 6.405195, mae: 62.560375, mean_q: 83.610817
  876457/1100000: episode: 2051, duration: 6.053s, episode steps: 803, steps per second: 133, episode reward: 172.388, mean reward: 0.215 [-23.826, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.083 [-1.183, 1.452], loss: 13.198944, mae: 62.501888, mean_q: 83.404442
  876862/1100000: episode: 2052, duration: 2.874s, episode steps: 405, steps per second: 141, episode reward: 212.868, mean reward: 0.526 [-14.806, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.037 [-0.833, 1.396], loss: 8.159693, mae: 62.567608, mean_q: 83.611267
  877124/1100000: episode: 2053, duration: 1.779s, episode steps: 262, steps per second: 147, episode reward: -305.277, mean reward: -1.165 [-100.000, 4.454], mean action: 1.599 [0.000, 3.000], mean observation: 0.144 [-1.458, 1.418], loss: 36.711796, mae: 63.265923, mean_q: 84.560860
  877644/1100000: episode: 2054, duration: 3.787s, episode steps: 520, steps per second: 137, episode reward: 203.797, mean reward: 0.392 [-18.362, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.027 [-0.666, 1.453], loss: 14.863095, mae: 63.580574, mean_q: 84.866814
  878211/1100000: episode: 2055, duration: 4.216s, episode steps: 567, steps per second: 134, episode reward: 249.309, mean reward: 0.440 [-22.329, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.102 [-0.560, 1.410], loss: 8.894526, mae: 63.031029, mean_q: 84.399651
  878378/1100000: episode: 2056, duration: 1.117s, episode steps: 167, steps per second: 150, episode reward: -203.908, mean reward: -1.221 [-100.000, 4.762], mean action: 1.551 [0.000, 3.000], mean observation: 0.072 [-1.720, 1.403], loss: 6.111542, mae: 62.562420, mean_q: 83.922348
  878593/1100000: episode: 2057, duration: 1.466s, episode steps: 215, steps per second: 147, episode reward: -241.534, mean reward: -1.123 [-100.000, 5.594], mean action: 1.888 [0.000, 3.000], mean observation: 0.131 [-1.112, 1.535], loss: 26.058640, mae: 62.803471, mean_q: 83.993347
  879593/1100000: episode: 2058, duration: 7.733s, episode steps: 1000, steps per second: 129, episode reward: -44.587, mean reward: -0.045 [-24.465, 25.970], mean action: 1.835 [0.000, 3.000], mean observation: 0.036 [-0.626, 1.409], loss: 38.226040, mae: 63.358910, mean_q: 84.467957
  879941/1100000: episode: 2059, duration: 2.441s, episode steps: 348, steps per second: 143, episode reward: 210.641, mean reward: 0.605 [-17.379, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.039 [-1.051, 1.392], loss: 13.581484, mae: 63.258274, mean_q: 84.365089
  880941/1100000: episode: 2060, duration: 7.522s, episode steps: 1000, steps per second: 133, episode reward: -350.589, mean reward: -0.351 [-6.507, 5.858], mean action: 1.889 [0.000, 3.000], mean observation: 0.236 [-0.914, 2.912], loss: 13.622900, mae: 64.261353, mean_q: 85.840149
  881110/1100000: episode: 2061, duration: 1.132s, episode steps: 169, steps per second: 149, episode reward: -164.275, mean reward: -0.972 [-100.000, 8.910], mean action: 1.574 [0.000, 3.000], mean observation: -0.075 [-1.386, 2.839], loss: 5.901873, mae: 64.785240, mean_q: 86.472595
  881281/1100000: episode: 2062, duration: 1.144s, episode steps: 171, steps per second: 149, episode reward: -238.605, mean reward: -1.395 [-100.000, 5.506], mean action: 1.591 [0.000, 3.000], mean observation: 0.072 [-1.403, 1.410], loss: 8.164745, mae: 64.657509, mean_q: 86.549248
  881829/1100000: episode: 2063, duration: 3.912s, episode steps: 548, steps per second: 140, episode reward: 201.139, mean reward: 0.367 [-18.687, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.108 [-0.867, 1.473], loss: 16.845448, mae: 64.826813, mean_q: 86.299904
  882105/1100000: episode: 2064, duration: 1.868s, episode steps: 276, steps per second: 148, episode reward: 253.023, mean reward: 0.917 [-10.976, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: -0.027 [-1.371, 1.458], loss: 12.195308, mae: 64.616920, mean_q: 86.156349
  882559/1100000: episode: 2065, duration: 3.290s, episode steps: 454, steps per second: 138, episode reward: 241.915, mean reward: 0.533 [-19.403, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.089 [-1.014, 1.396], loss: 8.840950, mae: 65.047104, mean_q: 86.437775
  882668/1100000: episode: 2066, duration: 0.728s, episode steps: 109, steps per second: 150, episode reward: -385.045, mean reward: -3.533 [-100.000, 1.344], mean action: 2.028 [0.000, 3.000], mean observation: 0.257 [-1.137, 3.241], loss: 6.144591, mae: 66.080597, mean_q: 87.422935
  882832/1100000: episode: 2067, duration: 1.103s, episode steps: 164, steps per second: 149, episode reward: -333.550, mean reward: -2.034 [-100.000, 5.383], mean action: 1.909 [0.000, 3.000], mean observation: 0.203 [-1.507, 2.361], loss: 12.815337, mae: 65.070198, mean_q: 86.389366
  882990/1100000: episode: 2068, duration: 1.062s, episode steps: 158, steps per second: 149, episode reward: -408.579, mean reward: -2.586 [-100.000, 1.686], mean action: 1.994 [0.000, 3.000], mean observation: 0.271 [-1.005, 3.358], loss: 11.861192, mae: 66.017876, mean_q: 87.966568
  883174/1100000: episode: 2069, duration: 1.233s, episode steps: 184, steps per second: 149, episode reward: 28.340, mean reward: 0.154 [-100.000, 23.431], mean action: 1.598 [0.000, 3.000], mean observation: -0.059 [-1.678, 1.434], loss: 15.677934, mae: 67.061584, mean_q: 89.017921
  883350/1100000: episode: 2070, duration: 1.181s, episode steps: 176, steps per second: 149, episode reward: -329.433, mean reward: -1.872 [-100.000, 3.780], mean action: 1.898 [0.000, 3.000], mean observation: 0.191 [-1.502, 1.969], loss: 23.308001, mae: 65.625076, mean_q: 87.032661
  883465/1100000: episode: 2071, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: 33.797, mean reward: 0.294 [-100.000, 52.097], mean action: 1.939 [0.000, 3.000], mean observation: 0.122 [-1.319, 1.390], loss: 10.286942, mae: 65.632675, mean_q: 87.173401
  883777/1100000: episode: 2072, duration: 2.147s, episode steps: 312, steps per second: 145, episode reward: 258.261, mean reward: 0.828 [-8.582, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.073 [-0.819, 1.512], loss: 13.659099, mae: 66.868980, mean_q: 88.937019
  883894/1100000: episode: 2073, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -315.215, mean reward: -2.694 [-100.000, 1.733], mean action: 1.983 [0.000, 3.000], mean observation: 0.182 [-1.121, 2.419], loss: 10.341901, mae: 65.248886, mean_q: 86.777710
  884173/1100000: episode: 2074, duration: 1.891s, episode steps: 279, steps per second: 148, episode reward: -184.172, mean reward: -0.660 [-100.000, 21.960], mean action: 1.616 [0.000, 3.000], mean observation: -0.028 [-0.600, 1.747], loss: 10.062768, mae: 65.453674, mean_q: 86.680649
  884376/1100000: episode: 2075, duration: 1.370s, episode steps: 203, steps per second: 148, episode reward: -137.841, mean reward: -0.679 [-100.000, 7.608], mean action: 1.695 [0.000, 3.000], mean observation: -0.034 [-0.770, 1.934], loss: 7.763994, mae: 65.524277, mean_q: 87.520241
  885168/1100000: episode: 2076, duration: 5.733s, episode steps: 792, steps per second: 138, episode reward: 57.855, mean reward: 0.073 [-11.861, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.040 [-0.711, 1.408], loss: 12.893487, mae: 64.989265, mean_q: 85.821754
  885297/1100000: episode: 2077, duration: 0.872s, episode steps: 129, steps per second: 148, episode reward: -406.024, mean reward: -3.147 [-100.000, 1.404], mean action: 1.992 [0.000, 3.000], mean observation: 0.265 [-1.481, 3.005], loss: 10.759435, mae: 65.248413, mean_q: 86.075806
  885550/1100000: episode: 2078, duration: 1.722s, episode steps: 253, steps per second: 147, episode reward: 289.676, mean reward: 1.145 [-7.164, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.065 [-0.697, 1.399], loss: 11.896444, mae: 65.103989, mean_q: 85.796906
  886550/1100000: episode: 2079, duration: 7.338s, episode steps: 1000, steps per second: 136, episode reward: -172.451, mean reward: -0.172 [-7.049, 5.999], mean action: 1.725 [0.000, 3.000], mean observation: 0.210 [-0.989, 2.646], loss: 12.565689, mae: 65.083900, mean_q: 86.157494
  887061/1100000: episode: 2080, duration: 3.518s, episode steps: 511, steps per second: 145, episode reward: -226.839, mean reward: -0.444 [-100.000, 6.806], mean action: 1.659 [0.000, 3.000], mean observation: 0.167 [-1.003, 2.435], loss: 43.656677, mae: 64.957970, mean_q: 85.703972
  888061/1100000: episode: 2081, duration: 7.734s, episode steps: 1000, steps per second: 129, episode reward: 51.137, mean reward: 0.051 [-18.635, 13.003], mean action: 1.195 [0.000, 3.000], mean observation: -0.036 [-0.646, 1.394], loss: 14.657036, mae: 64.287300, mean_q: 84.548866
  888670/1100000: episode: 2082, duration: 4.442s, episode steps: 609, steps per second: 137, episode reward: 206.153, mean reward: 0.339 [-18.918, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.270 [-0.640, 1.410], loss: 7.777608, mae: 64.401642, mean_q: 84.752937
  889136/1100000: episode: 2083, duration: 3.214s, episode steps: 466, steps per second: 145, episode reward: 212.364, mean reward: 0.456 [-8.770, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.190 [-0.837, 1.773], loss: 8.311584, mae: 64.151215, mean_q: 84.329903
  889577/1100000: episode: 2084, duration: 3.110s, episode steps: 441, steps per second: 142, episode reward: 195.459, mean reward: 0.443 [-10.677, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.205 [-0.879, 1.402], loss: 11.028967, mae: 64.211952, mean_q: 84.958939
  889740/1100000: episode: 2085, duration: 1.095s, episode steps: 163, steps per second: 149, episode reward: -1354.418, mean reward: -8.309 [-100.000, 1.697], mean action: 2.129 [0.000, 3.000], mean observation: -0.337 [-11.120, 2.229], loss: 13.604868, mae: 64.676765, mean_q: 85.036064
  890221/1100000: episode: 2086, duration: 3.442s, episode steps: 481, steps per second: 140, episode reward: 153.812, mean reward: 0.320 [-18.349, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.242 [-0.955, 1.407], loss: 13.257331, mae: 64.237305, mean_q: 84.947197
  890404/1100000: episode: 2087, duration: 1.225s, episode steps: 183, steps per second: 149, episode reward: -205.841, mean reward: -1.125 [-100.000, 2.451], mean action: 1.601 [0.000, 3.000], mean observation: 0.115 [-1.002, 1.804], loss: 7.688316, mae: 63.262733, mean_q: 83.448120
  891185/1100000: episode: 2088, duration: 5.899s, episode steps: 781, steps per second: 132, episode reward: 165.973, mean reward: 0.213 [-19.469, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: -0.061 [-0.654, 1.415], loss: 10.085483, mae: 63.850033, mean_q: 84.055557
  891704/1100000: episode: 2089, duration: 3.675s, episode steps: 519, steps per second: 141, episode reward: 130.352, mean reward: 0.251 [-11.696, 100.000], mean action: 1.653 [0.000, 3.000], mean observation: -0.062 [-1.100, 1.418], loss: 9.652098, mae: 63.114986, mean_q: 82.910950
  892125/1100000: episode: 2090, duration: 3.012s, episode steps: 421, steps per second: 140, episode reward: 241.428, mean reward: 0.573 [-17.839, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.124 [-0.715, 1.401], loss: 6.302174, mae: 62.798294, mean_q: 82.249870
  892644/1100000: episode: 2091, duration: 3.624s, episode steps: 519, steps per second: 143, episode reward: 190.529, mean reward: 0.367 [-17.766, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.264 [-0.610, 1.413], loss: 12.546369, mae: 63.186024, mean_q: 83.018013
  893644/1100000: episode: 2092, duration: 7.583s, episode steps: 1000, steps per second: 132, episode reward: 4.506, mean reward: 0.005 [-12.947, 22.482], mean action: 1.633 [0.000, 3.000], mean observation: 0.191 [-0.702, 1.396], loss: 12.135605, mae: 62.020073, mean_q: 81.442627
  893802/1100000: episode: 2093, duration: 1.062s, episode steps: 158, steps per second: 149, episode reward: -1531.828, mean reward: -9.695 [-100.000, 2.126], mean action: 2.639 [0.000, 3.000], mean observation: -0.566 [-14.356, 2.141], loss: 28.035435, mae: 61.688614, mean_q: 80.902260
  894802/1100000: episode: 2094, duration: 7.731s, episode steps: 1000, steps per second: 129, episode reward: -7.121, mean reward: -0.007 [-21.044, 22.518], mean action: 1.905 [0.000, 3.000], mean observation: 0.193 [-0.862, 1.951], loss: 10.404072, mae: 62.009548, mean_q: 81.775513
  895226/1100000: episode: 2095, duration: 3.039s, episode steps: 424, steps per second: 140, episode reward: 257.097, mean reward: 0.606 [-18.125, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.113 [-0.606, 1.394], loss: 9.067344, mae: 62.049591, mean_q: 81.779488
  895923/1100000: episode: 2096, duration: 5.403s, episode steps: 697, steps per second: 129, episode reward: 117.220, mean reward: 0.168 [-16.952, 100.000], mean action: 1.623 [0.000, 3.000], mean observation: -0.067 [-0.600, 1.413], loss: 19.453531, mae: 61.422817, mean_q: 80.766983
  896129/1100000: episode: 2097, duration: 1.460s, episode steps: 206, steps per second: 141, episode reward: 249.624, mean reward: 1.212 [-5.481, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.057 [-0.797, 1.402], loss: 7.497683, mae: 61.695709, mean_q: 81.481529
  896962/1100000: episode: 2098, duration: 6.122s, episode steps: 833, steps per second: 136, episode reward: 245.914, mean reward: 0.295 [-18.651, 100.000], mean action: 2.187 [0.000, 3.000], mean observation: 0.249 [-0.965, 1.463], loss: 17.768845, mae: 61.072330, mean_q: 80.440781
  897234/1100000: episode: 2099, duration: 1.863s, episode steps: 272, steps per second: 146, episode reward: 257.250, mean reward: 0.946 [-9.779, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.056 [-0.755, 1.399], loss: 9.294788, mae: 60.787498, mean_q: 80.112679
  897907/1100000: episode: 2100, duration: 5.017s, episode steps: 673, steps per second: 134, episode reward: 195.249, mean reward: 0.290 [-21.163, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.231 [-0.867, 1.388], loss: 10.362965, mae: 61.040432, mean_q: 80.533676
  898153/1100000: episode: 2101, duration: 1.675s, episode steps: 246, steps per second: 147, episode reward: 6.363, mean reward: 0.026 [-100.000, 11.611], mean action: 1.776 [0.000, 3.000], mean observation: 0.164 [-1.289, 1.447], loss: 128.794693, mae: 61.141235, mean_q: 80.806404
  898320/1100000: episode: 2102, duration: 1.128s, episode steps: 167, steps per second: 148, episode reward: -163.860, mean reward: -0.981 [-100.000, 4.125], mean action: 1.653 [0.000, 3.000], mean observation: 0.201 [-0.822, 1.434], loss: 11.453574, mae: 60.568527, mean_q: 79.668579
  898761/1100000: episode: 2103, duration: 3.093s, episode steps: 441, steps per second: 143, episode reward: 190.198, mean reward: 0.431 [-10.079, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: 0.182 [-1.134, 1.409], loss: 36.143368, mae: 60.899315, mean_q: 80.161789
  899066/1100000: episode: 2104, duration: 2.114s, episode steps: 305, steps per second: 144, episode reward: -36.890, mean reward: -0.121 [-100.000, 10.392], mean action: 1.866 [0.000, 3.000], mean observation: 0.153 [-0.672, 1.631], loss: 12.760719, mae: 60.184105, mean_q: 79.209068
  899209/1100000: episode: 2105, duration: 0.954s, episode steps: 143, steps per second: 150, episode reward: -170.935, mean reward: -1.195 [-100.000, 1.891], mean action: 1.734 [0.000, 3.000], mean observation: 0.186 [-0.594, 1.541], loss: 6.962730, mae: 61.362251, mean_q: 81.241478
  899757/1100000: episode: 2106, duration: 3.989s, episode steps: 548, steps per second: 137, episode reward: 218.779, mean reward: 0.399 [-10.032, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.145 [-0.673, 1.476], loss: 114.345703, mae: 61.031235, mean_q: 80.247803
  900757/1100000: episode: 2107, duration: 7.776s, episode steps: 1000, steps per second: 129, episode reward: 167.290, mean reward: 0.167 [-20.934, 22.440], mean action: 1.189 [0.000, 3.000], mean observation: 0.153 [-1.457, 1.391], loss: 18.526512, mae: 61.343681, mean_q: 80.803398
  901006/1100000: episode: 2108, duration: 1.685s, episode steps: 249, steps per second: 148, episode reward: -136.039, mean reward: -0.546 [-100.000, 10.417], mean action: 1.514 [0.000, 3.000], mean observation: 0.041 [-0.796, 1.440], loss: 9.895243, mae: 61.578205, mean_q: 80.743095
  901271/1100000: episode: 2109, duration: 1.820s, episode steps: 265, steps per second: 146, episode reward: 281.457, mean reward: 1.062 [-10.189, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.073 [-0.653, 1.390], loss: 8.709664, mae: 60.768326, mean_q: 79.781181
  901522/1100000: episode: 2110, duration: 1.711s, episode steps: 251, steps per second: 147, episode reward: -66.354, mean reward: -0.264 [-100.000, 13.161], mean action: 1.721 [0.000, 3.000], mean observation: 0.149 [-0.818, 1.401], loss: 12.419006, mae: 60.968819, mean_q: 80.266632
  901874/1100000: episode: 2111, duration: 2.437s, episode steps: 352, steps per second: 144, episode reward: 263.297, mean reward: 0.748 [-10.392, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.099 [-0.728, 1.429], loss: 25.179283, mae: 60.880604, mean_q: 79.891907
  902363/1100000: episode: 2112, duration: 3.479s, episode steps: 489, steps per second: 141, episode reward: -602.543, mean reward: -1.232 [-100.000, 5.176], mean action: 1.695 [0.000, 3.000], mean observation: 0.138 [-4.496, 1.473], loss: 48.412628, mae: 60.797321, mean_q: 79.730858
  902485/1100000: episode: 2113, duration: 0.818s, episode steps: 122, steps per second: 149, episode reward: -167.196, mean reward: -1.370 [-100.000, 15.456], mean action: 1.295 [0.000, 3.000], mean observation: 0.004 [-0.722, 1.696], loss: 15.014167, mae: 61.267254, mean_q: 79.779938
  902676/1100000: episode: 2114, duration: 1.290s, episode steps: 191, steps per second: 148, episode reward: 5.076, mean reward: 0.027 [-100.000, 15.500], mean action: 1.550 [0.000, 3.000], mean observation: 0.165 [-1.224, 1.610], loss: 6.697873, mae: 60.814842, mean_q: 80.112762
  903098/1100000: episode: 2115, duration: 2.961s, episode steps: 422, steps per second: 143, episode reward: 245.383, mean reward: 0.581 [-17.087, 100.000], mean action: 2.062 [0.000, 3.000], mean observation: 0.016 [-0.768, 1.411], loss: 9.158411, mae: 60.738049, mean_q: 79.509804
  903803/1100000: episode: 2116, duration: 5.026s, episode steps: 705, steps per second: 140, episode reward: 217.167, mean reward: 0.308 [-23.861, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.209 [-1.302, 1.496], loss: 8.849309, mae: 60.538754, mean_q: 79.281464
  904186/1100000: episode: 2117, duration: 2.640s, episode steps: 383, steps per second: 145, episode reward: 201.402, mean reward: 0.526 [-9.411, 100.000], mean action: 1.410 [0.000, 3.000], mean observation: -0.032 [-0.613, 1.404], loss: 11.402033, mae: 61.034065, mean_q: 79.953606
  904437/1100000: episode: 2118, duration: 1.749s, episode steps: 251, steps per second: 143, episode reward: 260.057, mean reward: 1.036 [-7.638, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.090 [-1.076, 1.422], loss: 27.590334, mae: 61.055481, mean_q: 79.622154
  904532/1100000: episode: 2119, duration: 0.677s, episode steps: 95, steps per second: 140, episode reward: -181.435, mean reward: -1.910 [-100.000, 2.187], mean action: 1.916 [0.000, 3.000], mean observation: 0.076 [-0.793, 1.602], loss: 16.318714, mae: 60.794567, mean_q: 79.970352
  904822/1100000: episode: 2120, duration: 2.060s, episode steps: 290, steps per second: 141, episode reward: -164.953, mean reward: -0.569 [-100.000, 44.702], mean action: 1.634 [0.000, 3.000], mean observation: 0.008 [-1.882, 1.526], loss: 9.424497, mae: 60.934853, mean_q: 80.262352
  904955/1100000: episode: 2121, duration: 0.899s, episode steps: 133, steps per second: 148, episode reward: -102.023, mean reward: -0.767 [-100.000, 3.790], mean action: 1.639 [0.000, 3.000], mean observation: 0.079 [-0.663, 1.398], loss: 10.510947, mae: 61.678146, mean_q: 80.427589
  905103/1100000: episode: 2122, duration: 0.997s, episode steps: 148, steps per second: 148, episode reward: 20.726, mean reward: 0.140 [-100.000, 16.132], mean action: 1.655 [0.000, 3.000], mean observation: 0.156 [-1.075, 1.394], loss: 6.256083, mae: 61.078449, mean_q: 79.194351
  905546/1100000: episode: 2123, duration: 3.072s, episode steps: 443, steps per second: 144, episode reward: 22.714, mean reward: 0.051 [-100.000, 20.693], mean action: 1.655 [0.000, 3.000], mean observation: 0.163 [-1.428, 1.669], loss: 19.538118, mae: 61.889858, mean_q: 80.813576
  905601/1100000: episode: 2124, duration: 0.375s, episode steps: 55, steps per second: 147, episode reward: -143.168, mean reward: -2.603 [-100.000, 4.852], mean action: 1.764 [0.000, 3.000], mean observation: 0.007 [-1.448, 1.386], loss: 8.317290, mae: 60.834248, mean_q: 78.839012
  905784/1100000: episode: 2125, duration: 1.235s, episode steps: 183, steps per second: 148, episode reward: -77.432, mean reward: -0.423 [-100.000, 4.966], mean action: 1.907 [0.000, 3.000], mean observation: 0.067 [-0.707, 1.398], loss: 9.936997, mae: 61.636543, mean_q: 80.703896
  905950/1100000: episode: 2126, duration: 1.128s, episode steps: 166, steps per second: 147, episode reward: -217.830, mean reward: -1.312 [-100.000, 11.978], mean action: 1.898 [0.000, 3.000], mean observation: -0.009 [-1.594, 1.397], loss: 7.678856, mae: 61.442936, mean_q: 79.932320
  906193/1100000: episode: 2127, duration: 1.637s, episode steps: 243, steps per second: 148, episode reward: 273.748, mean reward: 1.127 [-3.356, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.076 [-0.709, 1.444], loss: 10.590890, mae: 61.768898, mean_q: 80.748619
  906915/1100000: episode: 2128, duration: 5.764s, episode steps: 722, steps per second: 125, episode reward: -377.415, mean reward: -0.523 [-100.000, 21.517], mean action: 1.824 [0.000, 3.000], mean observation: -0.022 [-1.908, 2.668], loss: 12.667268, mae: 61.014095, mean_q: 79.701591
  907258/1100000: episode: 2129, duration: 2.422s, episode steps: 343, steps per second: 142, episode reward: 205.392, mean reward: 0.599 [-11.723, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: -0.058 [-0.896, 1.409], loss: 20.306639, mae: 61.423157, mean_q: 79.917084
  908089/1100000: episode: 2130, duration: 6.038s, episode steps: 831, steps per second: 138, episode reward: 174.391, mean reward: 0.210 [-21.224, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: -0.051 [-0.848, 1.524], loss: 16.043331, mae: 60.996040, mean_q: 79.244171
  908275/1100000: episode: 2131, duration: 1.247s, episode steps: 186, steps per second: 149, episode reward: -71.790, mean reward: -0.386 [-100.000, 10.938], mean action: 1.656 [0.000, 3.000], mean observation: 0.176 [-2.438, 1.491], loss: 14.801117, mae: 61.838543, mean_q: 80.547272
  908469/1100000: episode: 2132, duration: 1.309s, episode steps: 194, steps per second: 148, episode reward: 249.448, mean reward: 1.286 [-7.948, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.184 [-1.392, 1.393], loss: 10.968254, mae: 60.882107, mean_q: 79.248100
  909148/1100000: episode: 2133, duration: 5.017s, episode steps: 679, steps per second: 135, episode reward: 157.812, mean reward: 0.232 [-8.179, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.174 [-0.654, 1.411], loss: 12.568863, mae: 61.117485, mean_q: 79.523476
  909609/1100000: episode: 2134, duration: 3.345s, episode steps: 461, steps per second: 138, episode reward: 269.244, mean reward: 0.584 [-13.029, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.069 [-0.712, 1.403], loss: 30.032221, mae: 61.422306, mean_q: 80.063934
  910166/1100000: episode: 2135, duration: 3.894s, episode steps: 557, steps per second: 143, episode reward: 227.129, mean reward: 0.408 [-18.323, 100.000], mean action: 1.795 [0.000, 3.000], mean observation: 0.210 [-1.050, 1.420], loss: 10.792848, mae: 61.531223, mean_q: 80.525826
  910367/1100000: episode: 2136, duration: 1.361s, episode steps: 201, steps per second: 148, episode reward: -190.427, mean reward: -0.947 [-100.000, 2.599], mean action: 1.697 [0.000, 3.000], mean observation: 0.154 [-0.718, 1.707], loss: 8.939443, mae: 61.725674, mean_q: 80.060867
  911024/1100000: episode: 2137, duration: 4.697s, episode steps: 657, steps per second: 140, episode reward: 215.787, mean reward: 0.328 [-19.471, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: -0.039 [-0.759, 1.431], loss: 9.088511, mae: 61.235481, mean_q: 79.678337
  911293/1100000: episode: 2138, duration: 1.844s, episode steps: 269, steps per second: 146, episode reward: -2.329, mean reward: -0.009 [-100.000, 19.671], mean action: 1.993 [0.000, 3.000], mean observation: 0.122 [-0.751, 1.421], loss: 12.708619, mae: 61.070576, mean_q: 79.255112
  912293/1100000: episode: 2139, duration: 8.059s, episode steps: 1000, steps per second: 124, episode reward: -32.503, mean reward: -0.033 [-5.200, 5.835], mean action: 1.772 [0.000, 3.000], mean observation: 0.147 [-0.723, 1.585], loss: 21.114563, mae: 61.656815, mean_q: 80.102280
  912603/1100000: episode: 2140, duration: 2.142s, episode steps: 310, steps per second: 145, episode reward: 282.869, mean reward: 0.912 [-16.822, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.096 [-0.649, 1.403], loss: 13.026094, mae: 62.068378, mean_q: 80.587059
  913213/1100000: episode: 2141, duration: 4.435s, episode steps: 610, steps per second: 138, episode reward: 269.164, mean reward: 0.441 [-22.436, 100.000], mean action: 0.780 [0.000, 3.000], mean observation: 0.096 [-0.635, 1.405], loss: 16.715454, mae: 62.594749, mean_q: 81.755226
  914161/1100000: episode: 2142, duration: 6.987s, episode steps: 948, steps per second: 136, episode reward: 184.644, mean reward: 0.195 [-19.032, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: -0.008 [-0.600, 1.391], loss: 19.322826, mae: 61.663891, mean_q: 80.448601
  914863/1100000: episode: 2143, duration: 4.958s, episode steps: 702, steps per second: 142, episode reward: 217.032, mean reward: 0.309 [-20.046, 100.000], mean action: 1.728 [0.000, 3.000], mean observation: 0.001 [-0.770, 1.439], loss: 9.144593, mae: 61.435902, mean_q: 80.233360
  915714/1100000: episode: 2144, duration: 6.195s, episode steps: 851, steps per second: 137, episode reward: 197.585, mean reward: 0.232 [-18.953, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.201 [-0.710, 1.408], loss: 8.955304, mae: 61.344448, mean_q: 80.263145
  915835/1100000: episode: 2145, duration: 0.807s, episode steps: 121, steps per second: 150, episode reward: -70.289, mean reward: -0.581 [-100.000, 15.078], mean action: 1.884 [0.000, 3.000], mean observation: -0.148 [-0.896, 1.606], loss: 8.016552, mae: 61.238079, mean_q: 80.638893
  916541/1100000: episode: 2146, duration: 5.164s, episode steps: 706, steps per second: 137, episode reward: 164.729, mean reward: 0.233 [-19.784, 100.000], mean action: 1.758 [0.000, 3.000], mean observation: 0.011 [-1.095, 1.409], loss: 12.907044, mae: 61.516014, mean_q: 80.362343
  917299/1100000: episode: 2147, duration: 5.384s, episode steps: 758, steps per second: 141, episode reward: 142.495, mean reward: 0.188 [-17.604, 100.000], mean action: 1.591 [0.000, 3.000], mean observation: 0.090 [-0.651, 1.388], loss: 25.110447, mae: 62.394806, mean_q: 81.696999
  917505/1100000: episode: 2148, duration: 1.390s, episode steps: 206, steps per second: 148, episode reward: 41.476, mean reward: 0.201 [-100.000, 14.083], mean action: 1.568 [0.000, 3.000], mean observation: 0.051 [-0.515, 1.483], loss: 10.766618, mae: 62.796257, mean_q: 82.335274
  917622/1100000: episode: 2149, duration: 0.779s, episode steps: 117, steps per second: 150, episode reward: -708.091, mean reward: -6.052 [-100.000, 6.508], mean action: 2.248 [0.000, 3.000], mean observation: -0.255 [-8.195, 1.772], loss: 12.090551, mae: 62.030090, mean_q: 81.694405
  918189/1100000: episode: 2150, duration: 4.189s, episode steps: 567, steps per second: 135, episode reward: 243.843, mean reward: 0.430 [-20.994, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: -0.006 [-0.772, 1.413], loss: 11.400174, mae: 61.925816, mean_q: 80.670334
  918556/1100000: episode: 2151, duration: 2.558s, episode steps: 367, steps per second: 143, episode reward: 212.708, mean reward: 0.580 [-8.803, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.175 [-0.546, 1.393], loss: 9.852138, mae: 61.735725, mean_q: 80.386696
  919302/1100000: episode: 2152, duration: 5.772s, episode steps: 746, steps per second: 129, episode reward: 169.472, mean reward: 0.227 [-18.094, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.012 [-0.600, 1.408], loss: 10.626364, mae: 61.362411, mean_q: 79.869049
  919583/1100000: episode: 2153, duration: 1.924s, episode steps: 281, steps per second: 146, episode reward: 284.331, mean reward: 1.012 [-18.028, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.103 [-1.198, 1.513], loss: 12.167685, mae: 61.214512, mean_q: 79.620674
  919757/1100000: episode: 2154, duration: 1.158s, episode steps: 174, steps per second: 150, episode reward: -17.454, mean reward: -0.100 [-100.000, 17.067], mean action: 1.391 [0.000, 3.000], mean observation: 0.114 [-1.356, 1.457], loss: 10.861045, mae: 60.844933, mean_q: 79.156944
  920399/1100000: episode: 2155, duration: 4.532s, episode steps: 642, steps per second: 142, episode reward: 186.652, mean reward: 0.291 [-24.003, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.197 [-0.998, 1.403], loss: 10.608962, mae: 60.931561, mean_q: 79.212669
  920794/1100000: episode: 2156, duration: 2.751s, episode steps: 395, steps per second: 144, episode reward: 212.157, mean reward: 0.537 [-17.324, 100.000], mean action: 2.154 [0.000, 3.000], mean observation: 0.084 [-0.819, 1.397], loss: 9.564990, mae: 60.367577, mean_q: 78.306053
  921067/1100000: episode: 2157, duration: 1.907s, episode steps: 273, steps per second: 143, episode reward: 245.534, mean reward: 0.899 [-17.696, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.088 [-0.779, 1.425], loss: 10.566105, mae: 60.542240, mean_q: 78.067360
  921504/1100000: episode: 2158, duration: 3.099s, episode steps: 437, steps per second: 141, episode reward: 290.842, mean reward: 0.666 [-19.449, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.061 [-1.168, 1.392], loss: 9.248512, mae: 59.675522, mean_q: 77.261246
  921799/1100000: episode: 2159, duration: 2.017s, episode steps: 295, steps per second: 146, episode reward: 282.675, mean reward: 0.958 [-17.405, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.057 [-0.617, 1.532], loss: 14.356596, mae: 60.608467, mean_q: 78.433090
  922147/1100000: episode: 2160, duration: 2.398s, episode steps: 348, steps per second: 145, episode reward: 230.755, mean reward: 0.663 [-7.935, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: -0.025 [-0.776, 1.388], loss: 10.236234, mae: 60.276188, mean_q: 77.744598
  922526/1100000: episode: 2161, duration: 2.670s, episode steps: 379, steps per second: 142, episode reward: 269.225, mean reward: 0.710 [-9.373, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.099 [-0.548, 1.418], loss: 13.462800, mae: 60.222015, mean_q: 77.921326
  922805/1100000: episode: 2162, duration: 1.912s, episode steps: 279, steps per second: 146, episode reward: 2.445, mean reward: 0.009 [-100.000, 15.757], mean action: 1.803 [0.000, 3.000], mean observation: 0.119 [-0.503, 1.407], loss: 6.982532, mae: 60.433426, mean_q: 78.538544
  923116/1100000: episode: 2163, duration: 2.165s, episode steps: 311, steps per second: 144, episode reward: 268.081, mean reward: 0.862 [-8.625, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.030 [-0.728, 1.402], loss: 6.411940, mae: 59.976574, mean_q: 77.975960
  923272/1100000: episode: 2164, duration: 1.051s, episode steps: 156, steps per second: 148, episode reward: 56.687, mean reward: 0.363 [-100.000, 21.334], mean action: 1.949 [0.000, 3.000], mean observation: -0.006 [-1.616, 1.392], loss: 15.452640, mae: 60.689915, mean_q: 78.249428
  923494/1100000: episode: 2165, duration: 1.526s, episode steps: 222, steps per second: 145, episode reward: 273.184, mean reward: 1.231 [-17.994, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.073 [-0.748, 1.389], loss: 12.302051, mae: 60.521538, mean_q: 77.797661
  924494/1100000: episode: 2166, duration: 7.613s, episode steps: 1000, steps per second: 131, episode reward: -60.050, mean reward: -0.060 [-23.472, 14.321], mean action: 1.968 [0.000, 3.000], mean observation: 0.134 [-0.602, 1.402], loss: 12.718003, mae: 60.416538, mean_q: 77.858185
  924873/1100000: episode: 2167, duration: 2.710s, episode steps: 379, steps per second: 140, episode reward: 190.097, mean reward: 0.502 [-8.714, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: -0.040 [-0.882, 1.398], loss: 10.593439, mae: 60.280437, mean_q: 77.872452
  925820/1100000: episode: 2168, duration: 7.104s, episode steps: 947, steps per second: 133, episode reward: 108.848, mean reward: 0.115 [-21.279, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.101 [-0.691, 1.411], loss: 13.013606, mae: 60.081921, mean_q: 77.727013
  926270/1100000: episode: 2169, duration: 3.148s, episode steps: 450, steps per second: 143, episode reward: 266.150, mean reward: 0.591 [-19.371, 100.000], mean action: 2.107 [0.000, 3.000], mean observation: 0.220 [-0.775, 1.387], loss: 15.023960, mae: 59.712429, mean_q: 77.095184
  926529/1100000: episode: 2170, duration: 1.764s, episode steps: 259, steps per second: 147, episode reward: 240.638, mean reward: 0.929 [-13.691, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.013 [-1.567, 1.409], loss: 8.784387, mae: 59.478733, mean_q: 77.190720
  926783/1100000: episode: 2171, duration: 1.731s, episode steps: 254, steps per second: 147, episode reward: 232.157, mean reward: 0.914 [-5.224, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: 0.061 [-0.676, 1.399], loss: 10.759474, mae: 59.830631, mean_q: 77.434135
  927123/1100000: episode: 2172, duration: 2.357s, episode steps: 340, steps per second: 144, episode reward: 249.338, mean reward: 0.733 [-17.369, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: -0.046 [-0.682, 1.505], loss: 9.273653, mae: 59.568565, mean_q: 77.454201
  927378/1100000: episode: 2173, duration: 1.714s, episode steps: 255, steps per second: 149, episode reward: -0.091, mean reward: -0.000 [-100.000, 27.738], mean action: 1.502 [0.000, 3.000], mean observation: -0.064 [-0.765, 1.490], loss: 7.667703, mae: 59.536453, mean_q: 77.027985
  927674/1100000: episode: 2174, duration: 2.039s, episode steps: 296, steps per second: 145, episode reward: 232.336, mean reward: 0.785 [-14.123, 100.000], mean action: 1.997 [0.000, 3.000], mean observation: 0.049 [-0.670, 1.455], loss: 12.173125, mae: 59.364735, mean_q: 76.526093
  928324/1100000: episode: 2175, duration: 4.935s, episode steps: 650, steps per second: 132, episode reward: 203.685, mean reward: 0.313 [-17.786, 100.000], mean action: 1.055 [0.000, 3.000], mean observation: -0.012 [-0.814, 1.458], loss: 8.613283, mae: 59.409950, mean_q: 76.920189
  928560/1100000: episode: 2176, duration: 1.637s, episode steps: 236, steps per second: 144, episode reward: 30.706, mean reward: 0.130 [-100.000, 11.724], mean action: 1.818 [0.000, 3.000], mean observation: -0.085 [-1.704, 1.386], loss: 18.414583, mae: 59.182030, mean_q: 76.768188
  929560/1100000: episode: 2177, duration: 7.770s, episode steps: 1000, steps per second: 129, episode reward: -25.579, mean reward: -0.026 [-19.385, 26.896], mean action: 1.984 [0.000, 3.000], mean observation: 0.109 [-0.762, 1.486], loss: 9.406194, mae: 58.793064, mean_q: 76.087502
  929815/1100000: episode: 2178, duration: 1.721s, episode steps: 255, steps per second: 148, episode reward: 262.134, mean reward: 1.028 [-9.694, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.096 [-0.816, 1.443], loss: 12.969723, mae: 58.183830, mean_q: 75.561333
  930487/1100000: episode: 2179, duration: 4.897s, episode steps: 672, steps per second: 137, episode reward: 232.512, mean reward: 0.346 [-19.900, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.013 [-0.724, 1.455], loss: 10.157451, mae: 57.692024, mean_q: 75.451996
  931074/1100000: episode: 2180, duration: 4.148s, episode steps: 587, steps per second: 142, episode reward: 200.896, mean reward: 0.342 [-18.018, 100.000], mean action: 2.395 [0.000, 3.000], mean observation: 0.162 [-0.688, 1.434], loss: 10.113122, mae: 58.050533, mean_q: 75.754608
  931611/1100000: episode: 2181, duration: 3.724s, episode steps: 537, steps per second: 144, episode reward: 231.157, mean reward: 0.430 [-17.595, 100.000], mean action: 0.739 [0.000, 3.000], mean observation: 0.102 [-0.823, 1.407], loss: 9.622424, mae: 58.471680, mean_q: 76.727425
  932066/1100000: episode: 2182, duration: 3.102s, episode steps: 455, steps per second: 147, episode reward: 245.866, mean reward: 0.540 [-14.071, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.172 [-0.665, 1.389], loss: 12.549345, mae: 58.014515, mean_q: 75.754997
  932617/1100000: episode: 2183, duration: 3.984s, episode steps: 551, steps per second: 138, episode reward: 219.940, mean reward: 0.399 [-13.776, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.110 [-0.600, 1.540], loss: 10.690657, mae: 57.900082, mean_q: 75.882462
  933467/1100000: episode: 2184, duration: 6.250s, episode steps: 850, steps per second: 136, episode reward: 13.348, mean reward: 0.016 [-21.231, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.035 [-0.909, 1.409], loss: 14.331567, mae: 58.262833, mean_q: 76.478287
  933958/1100000: episode: 2185, duration: 3.502s, episode steps: 491, steps per second: 140, episode reward: 183.317, mean reward: 0.373 [-21.042, 100.000], mean action: 1.788 [0.000, 3.000], mean observation: 0.109 [-0.647, 1.402], loss: 9.871561, mae: 58.388237, mean_q: 76.701263
  934552/1100000: episode: 2186, duration: 4.196s, episode steps: 594, steps per second: 142, episode reward: 184.635, mean reward: 0.311 [-21.067, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: -0.013 [-0.681, 1.411], loss: 14.053392, mae: 58.463856, mean_q: 76.851891
  935552/1100000: episode: 2187, duration: 8.248s, episode steps: 1000, steps per second: 121, episode reward: -66.298, mean reward: -0.066 [-14.586, 15.594], mean action: 1.708 [0.000, 3.000], mean observation: 0.025 [-0.632, 1.393], loss: 14.332203, mae: 57.666210, mean_q: 76.106781
  936053/1100000: episode: 2188, duration: 3.607s, episode steps: 501, steps per second: 139, episode reward: -139.874, mean reward: -0.279 [-100.000, 11.253], mean action: 1.756 [0.000, 3.000], mean observation: 0.012 [-0.854, 1.388], loss: 12.406715, mae: 57.498756, mean_q: 75.691833
  936469/1100000: episode: 2189, duration: 2.912s, episode steps: 416, steps per second: 143, episode reward: 194.508, mean reward: 0.468 [-17.823, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.182 [-0.783, 1.420], loss: 19.523273, mae: 56.897026, mean_q: 75.057785
  936975/1100000: episode: 2190, duration: 3.707s, episode steps: 506, steps per second: 136, episode reward: -296.903, mean reward: -0.587 [-100.000, 4.790], mean action: 1.804 [0.000, 3.000], mean observation: 0.108 [-1.001, 3.490], loss: 11.193071, mae: 57.300507, mean_q: 75.340767
  937267/1100000: episode: 2191, duration: 2.024s, episode steps: 292, steps per second: 144, episode reward: 247.978, mean reward: 0.849 [-18.317, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.141 [-0.915, 1.397], loss: 10.251156, mae: 57.446114, mean_q: 75.477768
  938027/1100000: episode: 2192, duration: 5.876s, episode steps: 760, steps per second: 129, episode reward: 143.842, mean reward: 0.189 [-19.137, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.081 [-0.814, 1.394], loss: 10.484863, mae: 57.190346, mean_q: 75.659126
  938436/1100000: episode: 2193, duration: 2.860s, episode steps: 409, steps per second: 143, episode reward: -274.684, mean reward: -0.672 [-100.000, 11.616], mean action: 1.870 [0.000, 3.000], mean observation: 0.166 [-2.058, 4.500], loss: 11.065971, mae: 57.343620, mean_q: 75.629593
  938580/1100000: episode: 2194, duration: 0.950s, episode steps: 144, steps per second: 152, episode reward: -26.103, mean reward: -0.181 [-100.000, 9.455], mean action: 1.375 [0.000, 3.000], mean observation: 0.074 [-1.133, 1.495], loss: 12.252239, mae: 57.796345, mean_q: 75.688286
  939036/1100000: episode: 2195, duration: 3.156s, episode steps: 456, steps per second: 144, episode reward: 291.010, mean reward: 0.638 [-16.959, 100.000], mean action: 1.967 [0.000, 3.000], mean observation: 0.189 [-1.006, 1.390], loss: 13.198720, mae: 57.299366, mean_q: 75.313644
  940036/1100000: episode: 2196, duration: 7.339s, episode steps: 1000, steps per second: 136, episode reward: 68.092, mean reward: 0.068 [-21.590, 22.111], mean action: 1.504 [0.000, 3.000], mean observation: 0.059 [-0.952, 1.404], loss: 12.082986, mae: 56.949024, mean_q: 75.087234
  940213/1100000: episode: 2197, duration: 1.193s, episode steps: 177, steps per second: 148, episode reward: -67.929, mean reward: -0.384 [-100.000, 17.161], mean action: 1.723 [0.000, 3.000], mean observation: -0.071 [-1.000, 1.387], loss: 9.600873, mae: 57.358063, mean_q: 75.806931
  940560/1100000: episode: 2198, duration: 2.437s, episode steps: 347, steps per second: 142, episode reward: -244.481, mean reward: -0.705 [-100.000, 25.055], mean action: 1.706 [0.000, 3.000], mean observation: -0.123 [-2.012, 1.393], loss: 14.053634, mae: 56.856308, mean_q: 75.313919
  940814/1100000: episode: 2199, duration: 1.743s, episode steps: 254, steps per second: 146, episode reward: 245.043, mean reward: 0.965 [-5.992, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.055 [-0.767, 1.403], loss: 6.997436, mae: 56.764809, mean_q: 74.814667
  941208/1100000: episode: 2200, duration: 2.729s, episode steps: 394, steps per second: 144, episode reward: 237.068, mean reward: 0.602 [-18.689, 100.000], mean action: 1.741 [0.000, 3.000], mean observation: 0.090 [-0.692, 1.391], loss: 7.812922, mae: 57.067844, mean_q: 75.207092
  941639/1100000: episode: 2201, duration: 3.118s, episode steps: 431, steps per second: 138, episode reward: 275.797, mean reward: 0.640 [-12.127, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.123 [-0.690, 1.388], loss: 10.467106, mae: 56.777569, mean_q: 74.339775
  941989/1100000: episode: 2202, duration: 2.429s, episode steps: 350, steps per second: 144, episode reward: 236.788, mean reward: 0.677 [-9.455, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.177 [-0.617, 1.394], loss: 10.302258, mae: 57.092461, mean_q: 75.684486
  942496/1100000: episode: 2203, duration: 3.621s, episode steps: 507, steps per second: 140, episode reward: 148.547, mean reward: 0.293 [-19.139, 100.000], mean action: 0.963 [0.000, 3.000], mean observation: 0.136 [-0.670, 1.400], loss: 10.795916, mae: 57.394855, mean_q: 75.957054
  943470/1100000: episode: 2204, duration: 7.753s, episode steps: 974, steps per second: 126, episode reward: 196.155, mean reward: 0.201 [-20.619, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.238 [-0.542, 1.491], loss: 22.667265, mae: 57.113209, mean_q: 75.172096
  944470/1100000: episode: 2205, duration: 7.712s, episode steps: 1000, steps per second: 130, episode reward: -76.622, mean reward: -0.077 [-4.772, 6.809], mean action: 1.694 [0.000, 3.000], mean observation: 0.014 [-0.847, 1.432], loss: 12.441832, mae: 56.634918, mean_q: 74.715538
  944863/1100000: episode: 2206, duration: 2.720s, episode steps: 393, steps per second: 144, episode reward: 214.439, mean reward: 0.546 [-18.974, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.169 [-0.728, 1.446], loss: 11.426153, mae: 56.668663, mean_q: 75.000961
  945268/1100000: episode: 2207, duration: 2.797s, episode steps: 405, steps per second: 145, episode reward: 260.175, mean reward: 0.642 [-18.633, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.057 [-0.971, 1.397], loss: 9.538159, mae: 56.392487, mean_q: 74.749084
  945788/1100000: episode: 2208, duration: 3.728s, episode steps: 520, steps per second: 140, episode reward: 168.701, mean reward: 0.324 [-20.910, 100.000], mean action: 2.140 [0.000, 3.000], mean observation: 0.179 [-0.800, 1.441], loss: 13.686071, mae: 56.365738, mean_q: 74.549072
  946112/1100000: episode: 2209, duration: 2.253s, episode steps: 324, steps per second: 144, episode reward: 211.824, mean reward: 0.654 [-9.372, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.122 [-0.633, 1.400], loss: 12.928693, mae: 55.617970, mean_q: 73.559715
  946748/1100000: episode: 2210, duration: 4.489s, episode steps: 636, steps per second: 142, episode reward: 202.308, mean reward: 0.318 [-19.705, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.224 [-0.806, 1.466], loss: 11.695657, mae: 56.035858, mean_q: 74.316528
  947136/1100000: episode: 2211, duration: 2.676s, episode steps: 388, steps per second: 145, episode reward: 290.722, mean reward: 0.749 [-19.798, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.113 [-0.807, 1.409], loss: 9.126611, mae: 55.980942, mean_q: 74.244934
  947446/1100000: episode: 2212, duration: 2.125s, episode steps: 310, steps per second: 146, episode reward: -70.411, mean reward: -0.227 [-100.000, 18.975], mean action: 1.945 [0.000, 3.000], mean observation: 0.031 [-0.958, 1.397], loss: 14.906600, mae: 56.523067, mean_q: 74.932060
  947887/1100000: episode: 2213, duration: 3.038s, episode steps: 441, steps per second: 145, episode reward: 212.770, mean reward: 0.482 [-19.683, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.204 [-0.731, 1.397], loss: 13.713659, mae: 56.580986, mean_q: 74.927620
  947985/1100000: episode: 2214, duration: 0.652s, episode steps: 98, steps per second: 150, episode reward: -40.154, mean reward: -0.410 [-100.000, 12.414], mean action: 1.449 [0.000, 3.000], mean observation: 0.099 [-1.613, 1.400], loss: 6.560512, mae: 57.287025, mean_q: 75.766701
  948194/1100000: episode: 2215, duration: 1.418s, episode steps: 209, steps per second: 147, episode reward: -96.783, mean reward: -0.463 [-100.000, 3.449], mean action: 1.938 [0.000, 3.000], mean observation: 0.077 [-1.001, 1.448], loss: 7.107366, mae: 56.460667, mean_q: 74.803574
  949194/1100000: episode: 2216, duration: 7.127s, episode steps: 1000, steps per second: 140, episode reward: -110.765, mean reward: -0.111 [-4.976, 4.225], mean action: 1.588 [0.000, 3.000], mean observation: 0.023 [-0.600, 1.414], loss: 14.254746, mae: 56.696968, mean_q: 75.178925
  949801/1100000: episode: 2217, duration: 4.366s, episode steps: 607, steps per second: 139, episode reward: 288.464, mean reward: 0.475 [-17.700, 100.000], mean action: 0.873 [0.000, 3.000], mean observation: 0.093 [-1.145, 1.389], loss: 18.183392, mae: 56.661160, mean_q: 75.268486
  950204/1100000: episode: 2218, duration: 2.872s, episode steps: 403, steps per second: 140, episode reward: -36.864, mean reward: -0.091 [-100.000, 24.811], mean action: 1.799 [0.000, 3.000], mean observation: 0.140 [-0.840, 1.410], loss: 10.592424, mae: 57.037609, mean_q: 75.889900
  950492/1100000: episode: 2219, duration: 1.960s, episode steps: 288, steps per second: 147, episode reward: 277.022, mean reward: 0.962 [-17.364, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.058 [-1.133, 1.388], loss: 10.014713, mae: 57.096523, mean_q: 76.275635
  950773/1100000: episode: 2220, duration: 1.931s, episode steps: 281, steps per second: 146, episode reward: 283.909, mean reward: 1.010 [-7.710, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.064 [-0.942, 1.494], loss: 14.192112, mae: 56.944931, mean_q: 76.072113
  951400/1100000: episode: 2221, duration: 4.471s, episode steps: 627, steps per second: 140, episode reward: -298.872, mean reward: -0.477 [-100.000, 20.694], mean action: 1.644 [0.000, 3.000], mean observation: -0.124 [-1.819, 1.388], loss: 10.506341, mae: 56.260571, mean_q: 74.658997
  951877/1100000: episode: 2222, duration: 3.395s, episode steps: 477, steps per second: 140, episode reward: 204.292, mean reward: 0.428 [-17.550, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: -0.012 [-0.600, 1.394], loss: 11.626926, mae: 55.958923, mean_q: 74.511879
  952451/1100000: episode: 2223, duration: 4.076s, episode steps: 574, steps per second: 141, episode reward: 131.022, mean reward: 0.228 [-20.714, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: -0.027 [-1.115, 1.411], loss: 8.985285, mae: 56.054787, mean_q: 74.529892
  952908/1100000: episode: 2224, duration: 3.127s, episode steps: 457, steps per second: 146, episode reward: 234.290, mean reward: 0.513 [-8.138, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.201 [-0.631, 1.415], loss: 9.609274, mae: 55.944855, mean_q: 74.545311
  953185/1100000: episode: 2225, duration: 1.873s, episode steps: 277, steps per second: 148, episode reward: 210.198, mean reward: 0.759 [-8.962, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.176 [-0.971, 1.405], loss: 10.028445, mae: 56.340168, mean_q: 75.104759
  953707/1100000: episode: 2226, duration: 3.731s, episode steps: 522, steps per second: 140, episode reward: 193.185, mean reward: 0.370 [-20.195, 100.000], mean action: 1.776 [0.000, 3.000], mean observation: 0.222 [-0.835, 1.432], loss: 14.583633, mae: 56.541412, mean_q: 75.367355
  954105/1100000: episode: 2227, duration: 2.728s, episode steps: 398, steps per second: 146, episode reward: 228.303, mean reward: 0.574 [-8.772, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.157 [-0.655, 1.398], loss: 35.847576, mae: 56.374245, mean_q: 75.413589
  954391/1100000: episode: 2228, duration: 2.006s, episode steps: 286, steps per second: 143, episode reward: -265.762, mean reward: -0.929 [-100.000, 6.182], mean action: 1.741 [0.000, 3.000], mean observation: 0.242 [-1.060, 3.021], loss: 8.423776, mae: 56.691925, mean_q: 75.877434
  954646/1100000: episode: 2229, duration: 1.747s, episode steps: 255, steps per second: 146, episode reward: 294.484, mean reward: 1.155 [-9.230, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.103 [-0.790, 1.488], loss: 8.829462, mae: 56.768314, mean_q: 75.886856
  955187/1100000: episode: 2230, duration: 3.837s, episode steps: 541, steps per second: 141, episode reward: 191.584, mean reward: 0.354 [-20.607, 100.000], mean action: 2.104 [0.000, 3.000], mean observation: 0.146 [-1.197, 1.401], loss: 11.775606, mae: 56.776089, mean_q: 75.606720
  955624/1100000: episode: 2231, duration: 3.133s, episode steps: 437, steps per second: 139, episode reward: -218.762, mean reward: -0.501 [-100.000, 25.243], mean action: 1.531 [0.000, 3.000], mean observation: 0.130 [-2.393, 1.390], loss: 7.115033, mae: 56.695213, mean_q: 75.476143
  955905/1100000: episode: 2232, duration: 1.902s, episode steps: 281, steps per second: 148, episode reward: 272.841, mean reward: 0.971 [-18.264, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.079 [-0.700, 1.411], loss: 8.915187, mae: 56.259201, mean_q: 74.594986
  956905/1100000: episode: 2233, duration: 7.306s, episode steps: 1000, steps per second: 137, episode reward: -82.084, mean reward: -0.082 [-4.379, 4.883], mean action: 1.502 [0.000, 3.000], mean observation: 0.041 [-0.600, 1.417], loss: 11.615152, mae: 56.594109, mean_q: 75.356735
  957240/1100000: episode: 2234, duration: 2.348s, episode steps: 335, steps per second: 143, episode reward: 248.973, mean reward: 0.743 [-9.634, 100.000], mean action: 0.913 [0.000, 3.000], mean observation: 0.059 [-0.758, 1.475], loss: 11.579512, mae: 56.331215, mean_q: 74.877228
  957493/1100000: episode: 2235, duration: 1.710s, episode steps: 253, steps per second: 148, episode reward: 225.300, mean reward: 0.891 [-20.021, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: -0.029 [-0.777, 1.393], loss: 6.911117, mae: 56.521770, mean_q: 75.132027
  958144/1100000: episode: 2236, duration: 4.593s, episode steps: 651, steps per second: 142, episode reward: 215.815, mean reward: 0.332 [-19.355, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.232 [-0.867, 1.405], loss: 11.891852, mae: 56.540726, mean_q: 75.073349
  958477/1100000: episode: 2237, duration: 2.265s, episode steps: 333, steps per second: 147, episode reward: 180.923, mean reward: 0.543 [-17.643, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: -0.068 [-0.746, 1.413], loss: 10.353733, mae: 56.562237, mean_q: 74.983528
  958951/1100000: episode: 2238, duration: 3.360s, episode steps: 474, steps per second: 141, episode reward: 273.111, mean reward: 0.576 [-19.885, 100.000], mean action: 0.802 [0.000, 3.000], mean observation: 0.130 [-0.788, 1.388], loss: 9.573272, mae: 56.492886, mean_q: 75.181473
  959359/1100000: episode: 2239, duration: 2.786s, episode steps: 408, steps per second: 146, episode reward: 249.688, mean reward: 0.612 [-18.282, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.157 [-0.908, 1.696], loss: 7.418968, mae: 55.920559, mean_q: 74.342148
  959610/1100000: episode: 2240, duration: 1.698s, episode steps: 251, steps per second: 148, episode reward: 50.796, mean reward: 0.202 [-100.000, 18.152], mean action: 1.466 [0.000, 3.000], mean observation: 0.140 [-1.392, 1.878], loss: 8.579205, mae: 55.509064, mean_q: 73.676651
  959985/1100000: episode: 2241, duration: 2.624s, episode steps: 375, steps per second: 143, episode reward: 269.118, mean reward: 0.718 [-17.872, 100.000], mean action: 0.896 [0.000, 3.000], mean observation: 0.123 [-0.715, 1.448], loss: 9.537741, mae: 55.667088, mean_q: 73.926872
  960086/1100000: episode: 2242, duration: 0.675s, episode steps: 101, steps per second: 150, episode reward: -181.960, mean reward: -1.802 [-100.000, 2.158], mean action: 1.455 [0.000, 3.000], mean observation: 0.265 [-0.568, 1.513], loss: 6.149127, mae: 55.425510, mean_q: 73.707512
  960373/1100000: episode: 2243, duration: 1.958s, episode steps: 287, steps per second: 147, episode reward: 235.793, mean reward: 0.822 [-13.835, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.026 [-0.647, 1.414], loss: 5.402073, mae: 55.959026, mean_q: 74.383255
  960621/1100000: episode: 2244, duration: 1.697s, episode steps: 248, steps per second: 146, episode reward: 205.966, mean reward: 0.831 [-13.754, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.147 [-0.960, 1.396], loss: 7.407814, mae: 55.716820, mean_q: 74.091301
  961003/1100000: episode: 2245, duration: 2.631s, episode steps: 382, steps per second: 145, episode reward: 259.182, mean reward: 0.678 [-17.707, 100.000], mean action: 0.822 [0.000, 3.000], mean observation: 0.095 [-0.823, 1.479], loss: 7.919561, mae: 55.700554, mean_q: 74.017540
  961134/1100000: episode: 2246, duration: 0.876s, episode steps: 131, steps per second: 150, episode reward: -206.508, mean reward: -1.576 [-100.000, 1.912], mean action: 1.733 [0.000, 3.000], mean observation: 0.293 [-0.452, 1.412], loss: 9.184265, mae: 56.282642, mean_q: 74.681816
  961311/1100000: episode: 2247, duration: 1.204s, episode steps: 177, steps per second: 147, episode reward: 289.369, mean reward: 1.635 [-9.653, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.052 [-0.786, 1.386], loss: 10.261125, mae: 55.148434, mean_q: 73.119583
  961445/1100000: episode: 2248, duration: 0.906s, episode steps: 134, steps per second: 148, episode reward: -166.296, mean reward: -1.241 [-100.000, 1.723], mean action: 1.672 [0.000, 3.000], mean observation: 0.301 [-0.359, 1.408], loss: 7.743738, mae: 55.834663, mean_q: 74.051819
  961909/1100000: episode: 2249, duration: 3.211s, episode steps: 464, steps per second: 145, episode reward: 267.873, mean reward: 0.577 [-17.895, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.049 [-0.610, 1.390], loss: 6.205845, mae: 55.682381, mean_q: 74.009735
  962209/1100000: episode: 2250, duration: 2.062s, episode steps: 300, steps per second: 146, episode reward: -242.854, mean reward: -0.810 [-100.000, 7.866], mean action: 1.807 [0.000, 3.000], mean observation: 0.239 [-1.950, 2.649], loss: 7.652136, mae: 55.670078, mean_q: 73.730667
  962540/1100000: episode: 2251, duration: 2.301s, episode steps: 331, steps per second: 144, episode reward: 289.587, mean reward: 0.875 [-17.998, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.115 [-0.606, 1.411], loss: 8.848023, mae: 55.962635, mean_q: 74.233376
  963397/1100000: episode: 2252, duration: 6.111s, episode steps: 857, steps per second: 140, episode reward: 208.666, mean reward: 0.243 [-18.982, 100.000], mean action: 0.610 [0.000, 3.000], mean observation: 0.228 [-0.608, 1.431], loss: 8.092720, mae: 55.918350, mean_q: 74.065620
  963695/1100000: episode: 2253, duration: 2.053s, episode steps: 298, steps per second: 145, episode reward: 219.969, mean reward: 0.738 [-10.128, 100.000], mean action: 1.685 [0.000, 3.000], mean observation: -0.030 [-0.780, 1.406], loss: 6.380321, mae: 55.852520, mean_q: 73.967331
  964141/1100000: episode: 2254, duration: 3.090s, episode steps: 446, steps per second: 144, episode reward: 169.334, mean reward: 0.380 [-18.486, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.041 [-0.882, 1.413], loss: 9.332248, mae: 56.051769, mean_q: 74.304756
  964860/1100000: episode: 2255, duration: 4.994s, episode steps: 719, steps per second: 144, episode reward: 163.463, mean reward: 0.227 [-19.086, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.025 [-0.718, 1.401], loss: 11.937872, mae: 55.779564, mean_q: 73.739449
  965251/1100000: episode: 2256, duration: 2.692s, episode steps: 391, steps per second: 145, episode reward: 184.519, mean reward: 0.472 [-19.547, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.173 [-0.735, 1.406], loss: 9.496621, mae: 55.595379, mean_q: 73.650253
  966251/1100000: episode: 2257, duration: 8.161s, episode steps: 1000, steps per second: 123, episode reward: -133.609, mean reward: -0.134 [-5.272, 6.235], mean action: 1.770 [0.000, 3.000], mean observation: 0.016 [-0.600, 1.431], loss: 6.594010, mae: 55.465118, mean_q: 73.604561
  966369/1100000: episode: 2258, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -195.286, mean reward: -1.655 [-100.000, 2.133], mean action: 2.000 [0.000, 3.000], mean observation: 0.073 [-1.004, 1.568], loss: 5.063043, mae: 55.506641, mean_q: 73.484985
  966849/1100000: episode: 2259, duration: 3.365s, episode steps: 480, steps per second: 143, episode reward: 175.435, mean reward: 0.365 [-10.965, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: -0.016 [-0.664, 1.458], loss: 8.429874, mae: 55.587803, mean_q: 73.611458
  967849/1100000: episode: 2260, duration: 7.421s, episode steps: 1000, steps per second: 135, episode reward: -49.768, mean reward: -0.050 [-5.605, 6.629], mean action: 1.798 [0.000, 3.000], mean observation: 0.093 [-0.894, 1.411], loss: 12.278500, mae: 55.621048, mean_q: 73.886414
  968496/1100000: episode: 2261, duration: 4.726s, episode steps: 647, steps per second: 137, episode reward: 110.538, mean reward: 0.171 [-12.282, 100.000], mean action: 1.835 [0.000, 3.000], mean observation: 0.031 [-0.600, 1.394], loss: 7.120581, mae: 55.701435, mean_q: 74.345932
  968851/1100000: episode: 2262, duration: 2.480s, episode steps: 355, steps per second: 143, episode reward: 272.117, mean reward: 0.767 [-10.773, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.065 [-0.704, 1.419], loss: 9.136043, mae: 56.092197, mean_q: 75.007584
  969015/1100000: episode: 2263, duration: 1.098s, episode steps: 164, steps per second: 149, episode reward: -202.025, mean reward: -1.232 [-100.000, 3.731], mean action: 1.689 [0.000, 3.000], mean observation: 0.070 [-1.000, 1.455], loss: 7.715507, mae: 55.965927, mean_q: 74.591484
  969202/1100000: episode: 2264, duration: 1.272s, episode steps: 187, steps per second: 147, episode reward: -42.987, mean reward: -0.230 [-100.000, 9.061], mean action: 1.765 [0.000, 3.000], mean observation: 0.125 [-0.634, 1.874], loss: 13.372640, mae: 56.285385, mean_q: 75.090981
  969327/1100000: episode: 2265, duration: 0.842s, episode steps: 125, steps per second: 148, episode reward: -103.733, mean reward: -0.830 [-100.000, 11.451], mean action: 1.640 [0.000, 3.000], mean observation: -0.084 [-1.462, 1.409], loss: 9.164990, mae: 56.000351, mean_q: 74.527336
  969824/1100000: episode: 2266, duration: 3.664s, episode steps: 497, steps per second: 136, episode reward: 221.132, mean reward: 0.445 [-17.562, 100.000], mean action: 2.443 [0.000, 3.000], mean observation: 0.189 [-0.551, 1.435], loss: 9.534161, mae: 56.008450, mean_q: 74.786385
  970774/1100000: episode: 2267, duration: 7.133s, episode steps: 950, steps per second: 133, episode reward: -288.462, mean reward: -0.304 [-100.000, 4.807], mean action: 1.731 [0.000, 3.000], mean observation: 0.024 [-1.249, 1.460], loss: 9.060424, mae: 55.174351, mean_q: 73.900017
  971774/1100000: episode: 2268, duration: 7.909s, episode steps: 1000, steps per second: 126, episode reward: -36.780, mean reward: -0.037 [-19.182, 13.467], mean action: 1.769 [0.000, 3.000], mean observation: 0.121 [-0.755, 1.425], loss: 8.102239, mae: 55.543552, mean_q: 74.303169
  972774/1100000: episode: 2269, duration: 7.251s, episode steps: 1000, steps per second: 138, episode reward: -165.782, mean reward: -0.166 [-4.767, 5.092], mean action: 1.906 [0.000, 3.000], mean observation: 0.117 [-0.813, 1.398], loss: 7.311641, mae: 56.259525, mean_q: 75.334442
  973163/1100000: episode: 2270, duration: 2.677s, episode steps: 389, steps per second: 145, episode reward: 283.263, mean reward: 0.728 [-17.378, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.121 [-0.751, 1.677], loss: 8.019666, mae: 56.562260, mean_q: 75.787407
  973374/1100000: episode: 2271, duration: 1.434s, episode steps: 211, steps per second: 147, episode reward: -138.288, mean reward: -0.655 [-100.000, 4.514], mean action: 1.848 [0.000, 3.000], mean observation: -0.109 [-1.118, 1.392], loss: 12.667993, mae: 56.276711, mean_q: 75.108772
  973523/1100000: episode: 2272, duration: 0.997s, episode steps: 149, steps per second: 149, episode reward: -69.081, mean reward: -0.464 [-100.000, 20.615], mean action: 1.396 [0.000, 3.000], mean observation: -0.163 [-1.374, 1.402], loss: 13.508920, mae: 57.267277, mean_q: 76.700401
  973724/1100000: episode: 2273, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: -90.047, mean reward: -0.448 [-100.000, 12.041], mean action: 1.886 [0.000, 3.000], mean observation: -0.014 [-0.803, 2.792], loss: 8.500101, mae: 56.438000, mean_q: 75.393234
  973908/1100000: episode: 2274, duration: 1.240s, episode steps: 184, steps per second: 148, episode reward: -10.295, mean reward: -0.056 [-100.000, 11.683], mean action: 1.652 [0.000, 3.000], mean observation: -0.161 [-0.908, 1.508], loss: 18.699335, mae: 56.694462, mean_q: 75.778275
  974093/1100000: episode: 2275, duration: 1.272s, episode steps: 185, steps per second: 145, episode reward: -111.447, mean reward: -0.602 [-100.000, 3.251], mean action: 1.308 [0.000, 3.000], mean observation: 0.273 [-0.350, 1.456], loss: 10.727392, mae: 56.339844, mean_q: 75.264160
  974329/1100000: episode: 2276, duration: 1.613s, episode steps: 236, steps per second: 146, episode reward: -32.132, mean reward: -0.136 [-100.000, 9.885], mean action: 1.839 [0.000, 3.000], mean observation: -0.056 [-1.431, 1.396], loss: 6.631389, mae: 56.055557, mean_q: 74.950974
  975004/1100000: episode: 2277, duration: 4.794s, episode steps: 675, steps per second: 141, episode reward: -158.894, mean reward: -0.235 [-100.000, 7.757], mean action: 1.658 [0.000, 3.000], mean observation: 0.130 [-1.406, 2.511], loss: 8.436059, mae: 56.228077, mean_q: 75.222168
  975331/1100000: episode: 2278, duration: 2.250s, episode steps: 327, steps per second: 145, episode reward: 295.782, mean reward: 0.905 [-17.365, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.107 [-0.859, 1.439], loss: 9.781477, mae: 57.308903, mean_q: 76.788422
  975484/1100000: episode: 2279, duration: 1.032s, episode steps: 153, steps per second: 148, episode reward: -174.437, mean reward: -1.140 [-100.000, 2.997], mean action: 1.961 [0.000, 3.000], mean observation: 0.089 [-1.003, 1.606], loss: 7.851824, mae: 56.272041, mean_q: 75.092743
  975955/1100000: episode: 2280, duration: 3.213s, episode steps: 471, steps per second: 147, episode reward: 229.108, mean reward: 0.486 [-9.484, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.022 [-0.600, 1.424], loss: 10.462811, mae: 56.750294, mean_q: 75.797600
  976222/1100000: episode: 2281, duration: 1.822s, episode steps: 267, steps per second: 147, episode reward: -98.811, mean reward: -0.370 [-100.000, 15.470], mean action: 1.566 [0.000, 3.000], mean observation: 0.082 [-0.842, 1.396], loss: 11.444049, mae: 56.094692, mean_q: 74.960091
  976601/1100000: episode: 2282, duration: 2.631s, episode steps: 379, steps per second: 144, episode reward: -79.395, mean reward: -0.209 [-100.000, 11.328], mean action: 1.686 [0.000, 3.000], mean observation: 0.012 [-1.054, 1.485], loss: 10.671511, mae: 56.430305, mean_q: 75.445770
  976754/1100000: episode: 2283, duration: 1.030s, episode steps: 153, steps per second: 148, episode reward: -190.274, mean reward: -1.244 [-100.000, 2.018], mean action: 1.765 [0.000, 3.000], mean observation: 0.118 [-1.004, 1.796], loss: 6.253891, mae: 56.214737, mean_q: 75.370491
  976970/1100000: episode: 2284, duration: 1.474s, episode steps: 216, steps per second: 147, episode reward: -217.460, mean reward: -1.007 [-100.000, 3.843], mean action: 1.866 [0.000, 3.000], mean observation: 0.103 [-1.003, 1.417], loss: 6.126697, mae: 55.943035, mean_q: 74.819832
  977137/1100000: episode: 2285, duration: 1.121s, episode steps: 167, steps per second: 149, episode reward: 25.383, mean reward: 0.152 [-100.000, 19.767], mean action: 1.533 [0.000, 3.000], mean observation: -0.106 [-0.783, 1.411], loss: 8.819992, mae: 56.261585, mean_q: 75.234093
  977335/1100000: episode: 2286, duration: 1.345s, episode steps: 198, steps per second: 147, episode reward: -5.557, mean reward: -0.028 [-100.000, 14.876], mean action: 1.924 [0.000, 3.000], mean observation: -0.023 [-1.535, 1.407], loss: 12.905950, mae: 56.089092, mean_q: 75.348053
  977772/1100000: episode: 2287, duration: 3.098s, episode steps: 437, steps per second: 141, episode reward: 162.746, mean reward: 0.372 [-18.707, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.066 [-0.764, 1.407], loss: 10.015409, mae: 55.612083, mean_q: 74.536652
  977977/1100000: episode: 2288, duration: 1.391s, episode steps: 205, steps per second: 147, episode reward: -264.223, mean reward: -1.289 [-100.000, 3.658], mean action: 1.902 [0.000, 3.000], mean observation: 0.130 [-1.004, 1.842], loss: 7.211019, mae: 56.155621, mean_q: 75.035660
  978133/1100000: episode: 2289, duration: 1.061s, episode steps: 156, steps per second: 147, episode reward: -198.912, mean reward: -1.275 [-100.000, 1.491], mean action: 1.994 [0.000, 3.000], mean observation: 0.119 [-1.002, 1.901], loss: 6.762790, mae: 56.551056, mean_q: 75.555756
  978581/1100000: episode: 2290, duration: 3.197s, episode steps: 448, steps per second: 140, episode reward: 206.018, mean reward: 0.460 [-19.804, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.179 [-1.358, 1.402], loss: 12.892057, mae: 55.593426, mean_q: 74.431152
  978712/1100000: episode: 2291, duration: 0.883s, episode steps: 131, steps per second: 148, episode reward: -179.245, mean reward: -1.368 [-100.000, 1.288], mean action: 1.878 [0.000, 3.000], mean observation: 0.098 [-1.006, 1.637], loss: 8.038960, mae: 55.917164, mean_q: 75.085892
  979400/1100000: episode: 2292, duration: 4.944s, episode steps: 688, steps per second: 139, episode reward: 197.216, mean reward: 0.287 [-19.878, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.025 [-0.884, 1.468], loss: 9.872635, mae: 55.328495, mean_q: 74.113823
  980006/1100000: episode: 2293, duration: 4.535s, episode steps: 606, steps per second: 134, episode reward: 271.480, mean reward: 0.448 [-10.138, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.132 [-0.616, 1.415], loss: 13.029274, mae: 55.284016, mean_q: 73.982697
  980314/1100000: episode: 2294, duration: 2.107s, episode steps: 308, steps per second: 146, episode reward: 262.603, mean reward: 0.853 [-8.993, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.163 [-0.550, 1.387], loss: 7.926328, mae: 54.955971, mean_q: 73.490181
  980774/1100000: episode: 2295, duration: 3.210s, episode steps: 460, steps per second: 143, episode reward: 259.251, mean reward: 0.564 [-19.746, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.023 [-1.515, 1.406], loss: 8.035894, mae: 55.262646, mean_q: 74.064201
  981459/1100000: episode: 2296, duration: 5.012s, episode steps: 685, steps per second: 137, episode reward: 241.769, mean reward: 0.353 [-17.774, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.184 [-0.810, 1.404], loss: 7.495673, mae: 55.146687, mean_q: 73.767807
  981907/1100000: episode: 2297, duration: 3.115s, episode steps: 448, steps per second: 144, episode reward: 199.301, mean reward: 0.445 [-17.613, 100.000], mean action: 2.252 [0.000, 3.000], mean observation: 0.076 [-0.737, 1.413], loss: 13.403804, mae: 54.808262, mean_q: 72.973831
  982265/1100000: episode: 2298, duration: 2.528s, episode steps: 358, steps per second: 142, episode reward: 263.312, mean reward: 0.736 [-17.351, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.091 [-1.080, 1.403], loss: 12.561201, mae: 54.976807, mean_q: 73.151360
  982582/1100000: episode: 2299, duration: 2.189s, episode steps: 317, steps per second: 145, episode reward: 247.951, mean reward: 0.782 [-17.613, 100.000], mean action: 2.038 [0.000, 3.000], mean observation: -0.012 [-1.069, 1.409], loss: 7.351542, mae: 54.543457, mean_q: 72.874092
  982843/1100000: episode: 2300, duration: 1.780s, episode steps: 261, steps per second: 147, episode reward: 265.137, mean reward: 1.016 [-10.426, 100.000], mean action: 1.686 [0.000, 3.000], mean observation: -0.048 [-0.756, 1.406], loss: 8.325023, mae: 54.439327, mean_q: 72.496666
  983166/1100000: episode: 2301, duration: 2.239s, episode steps: 323, steps per second: 144, episode reward: 268.533, mean reward: 0.831 [-10.189, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.080 [-0.769, 1.462], loss: 8.493649, mae: 55.252739, mean_q: 73.774353
  983759/1100000: episode: 2302, duration: 4.248s, episode steps: 593, steps per second: 140, episode reward: 221.794, mean reward: 0.374 [-20.739, 100.000], mean action: 2.067 [0.000, 3.000], mean observation: 0.053 [-0.600, 1.401], loss: 6.820236, mae: 54.717796, mean_q: 73.018349
  984197/1100000: episode: 2303, duration: 3.073s, episode steps: 438, steps per second: 143, episode reward: -203.428, mean reward: -0.464 [-100.000, 7.533], mean action: 1.737 [0.000, 3.000], mean observation: 0.163 [-0.677, 1.762], loss: 7.378463, mae: 54.352409, mean_q: 72.524872
  984521/1100000: episode: 2304, duration: 2.234s, episode steps: 324, steps per second: 145, episode reward: -209.842, mean reward: -0.648 [-100.000, 21.696], mean action: 1.812 [0.000, 3.000], mean observation: 0.143 [-0.564, 1.877], loss: 9.807299, mae: 54.504189, mean_q: 72.735771
  984979/1100000: episode: 2305, duration: 3.187s, episode steps: 458, steps per second: 144, episode reward: 279.333, mean reward: 0.610 [-6.452, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.114 [-1.009, 1.410], loss: 6.987946, mae: 54.558205, mean_q: 72.829239
  985217/1100000: episode: 2306, duration: 1.615s, episode steps: 238, steps per second: 147, episode reward: 206.997, mean reward: 0.870 [-3.785, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.160 [-0.614, 1.407], loss: 6.656451, mae: 53.982803, mean_q: 71.937149
  985462/1100000: episode: 2307, duration: 1.675s, episode steps: 245, steps per second: 146, episode reward: 229.564, mean reward: 0.937 [-3.943, 100.000], mean action: 1.910 [0.000, 3.000], mean observation: 0.157 [-0.699, 1.388], loss: 4.930959, mae: 54.632381, mean_q: 72.887711
  985740/1100000: episode: 2308, duration: 1.937s, episode steps: 278, steps per second: 144, episode reward: 280.547, mean reward: 1.009 [-9.595, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.051 [-1.338, 1.387], loss: 6.284477, mae: 54.086906, mean_q: 71.949707
  985973/1100000: episode: 2309, duration: 1.576s, episode steps: 233, steps per second: 148, episode reward: -52.933, mean reward: -0.227 [-100.000, 20.035], mean action: 1.489 [0.000, 3.000], mean observation: -0.012 [-1.917, 1.442], loss: 7.695669, mae: 53.902386, mean_q: 71.713737
  986156/1100000: episode: 2310, duration: 1.222s, episode steps: 183, steps per second: 150, episode reward: 9.697, mean reward: 0.053 [-100.000, 12.033], mean action: 1.623 [0.000, 3.000], mean observation: 0.054 [-0.825, 1.496], loss: 9.449677, mae: 54.190735, mean_q: 72.151077
  986740/1100000: episode: 2311, duration: 4.157s, episode steps: 584, steps per second: 140, episode reward: 197.425, mean reward: 0.338 [-17.762, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.003 [-0.802, 1.419], loss: 8.168653, mae: 53.841068, mean_q: 71.430634
  987434/1100000: episode: 2312, duration: 4.984s, episode steps: 694, steps per second: 139, episode reward: 187.805, mean reward: 0.271 [-18.101, 100.000], mean action: 1.530 [0.000, 3.000], mean observation: 0.134 [-0.629, 1.513], loss: 8.672996, mae: 53.756439, mean_q: 71.412506
  988434/1100000: episode: 2313, duration: 8.451s, episode steps: 1000, steps per second: 118, episode reward: 42.376, mean reward: 0.042 [-7.999, 16.958], mean action: 1.661 [0.000, 3.000], mean observation: -0.062 [-0.838, 1.450], loss: 7.300670, mae: 53.155769, mean_q: 70.393372
  988585/1100000: episode: 2314, duration: 1.020s, episode steps: 151, steps per second: 148, episode reward: -8.682, mean reward: -0.057 [-100.000, 13.606], mean action: 2.007 [0.000, 3.000], mean observation: 0.186 [-1.646, 1.386], loss: 6.482051, mae: 52.740593, mean_q: 70.113411
  988828/1100000: episode: 2315, duration: 1.656s, episode steps: 243, steps per second: 147, episode reward: -102.202, mean reward: -0.421 [-100.000, 11.996], mean action: 1.580 [0.000, 3.000], mean observation: 0.196 [-0.845, 1.391], loss: 8.868504, mae: 53.133087, mean_q: 70.296967
  989030/1100000: episode: 2316, duration: 1.364s, episode steps: 202, steps per second: 148, episode reward: -23.455, mean reward: -0.116 [-100.000, 10.292], mean action: 1.614 [0.000, 3.000], mean observation: 0.119 [-0.535, 1.404], loss: 6.135222, mae: 52.869797, mean_q: 70.443436
  989471/1100000: episode: 2317, duration: 3.140s, episode steps: 441, steps per second: 140, episode reward: 209.352, mean reward: 0.475 [-24.615, 100.000], mean action: 2.211 [0.000, 3.000], mean observation: 0.188 [-0.967, 1.462], loss: 8.023603, mae: 53.301323, mean_q: 70.794228
  989836/1100000: episode: 2318, duration: 2.536s, episode steps: 365, steps per second: 144, episode reward: 287.021, mean reward: 0.786 [-17.669, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.076 [-0.653, 1.407], loss: 6.987669, mae: 52.993027, mean_q: 70.503822
  990127/1100000: episode: 2319, duration: 1.965s, episode steps: 291, steps per second: 148, episode reward: 274.289, mean reward: 0.943 [-9.455, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.099 [-1.192, 1.458], loss: 8.225136, mae: 52.944664, mean_q: 70.348312
  990468/1100000: episode: 2320, duration: 2.340s, episode steps: 341, steps per second: 146, episode reward: -61.821, mean reward: -0.181 [-100.000, 18.791], mean action: 1.666 [0.000, 3.000], mean observation: -0.052 [-0.837, 1.421], loss: 7.949198, mae: 52.952175, mean_q: 70.556107
  990977/1100000: episode: 2321, duration: 3.775s, episode steps: 509, steps per second: 135, episode reward: 224.119, mean reward: 0.440 [-4.337, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: -0.073 [-1.389, 1.385], loss: 8.519315, mae: 52.916756, mean_q: 70.274200
  991405/1100000: episode: 2322, duration: 3.016s, episode steps: 428, steps per second: 142, episode reward: 224.098, mean reward: 0.524 [-19.189, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.116 [-0.892, 1.448], loss: 7.664273, mae: 53.358841, mean_q: 70.998833
  992405/1100000: episode: 2323, duration: 7.483s, episode steps: 1000, steps per second: 134, episode reward: -45.446, mean reward: -0.045 [-5.213, 5.770], mean action: 1.656 [0.000, 3.000], mean observation: -0.099 [-0.795, 1.416], loss: 8.169752, mae: 53.462078, mean_q: 71.034424
  992957/1100000: episode: 2324, duration: 3.929s, episode steps: 552, steps per second: 140, episode reward: 205.704, mean reward: 0.373 [-16.399, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.131 [-0.785, 1.401], loss: 9.793378, mae: 54.186359, mean_q: 72.140694
  993256/1100000: episode: 2325, duration: 2.042s, episode steps: 299, steps per second: 146, episode reward: 215.018, mean reward: 0.719 [-9.016, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: -0.035 [-0.609, 1.399], loss: 6.398620, mae: 53.779121, mean_q: 71.530327
  993642/1100000: episode: 2326, duration: 2.648s, episode steps: 386, steps per second: 146, episode reward: 231.514, mean reward: 0.600 [-9.861, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: -0.052 [-1.131, 1.517], loss: 8.485623, mae: 54.043930, mean_q: 72.092827
  994235/1100000: episode: 2327, duration: 4.094s, episode steps: 593, steps per second: 145, episode reward: 231.302, mean reward: 0.390 [-18.108, 100.000], mean action: 0.946 [0.000, 3.000], mean observation: 0.129 [-0.790, 1.405], loss: 9.055764, mae: 54.286362, mean_q: 72.370956
  994492/1100000: episode: 2328, duration: 1.742s, episode steps: 257, steps per second: 148, episode reward: 230.660, mean reward: 0.898 [-7.106, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.059 [-1.076, 1.409], loss: 12.646848, mae: 54.602680, mean_q: 72.822365
  994980/1100000: episode: 2329, duration: 3.407s, episode steps: 488, steps per second: 143, episode reward: 207.347, mean reward: 0.425 [-11.754, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: -0.005 [-0.811, 1.388], loss: 9.835461, mae: 54.888733, mean_q: 73.199417
  995426/1100000: episode: 2330, duration: 3.135s, episode steps: 446, steps per second: 142, episode reward: -622.348, mean reward: -1.395 [-100.000, 9.573], mean action: 1.744 [0.000, 3.000], mean observation: 0.154 [-1.541, 5.063], loss: 11.632292, mae: 55.084023, mean_q: 73.428360
  996158/1100000: episode: 2331, duration: 5.111s, episode steps: 732, steps per second: 143, episode reward: 194.010, mean reward: 0.265 [-20.422, 100.000], mean action: 0.645 [0.000, 3.000], mean observation: 0.042 [-0.838, 1.410], loss: 9.496285, mae: 54.898540, mean_q: 72.955116
  996581/1100000: episode: 2332, duration: 2.980s, episode steps: 423, steps per second: 142, episode reward: -32.294, mean reward: -0.076 [-100.000, 42.742], mean action: 1.626 [0.000, 3.000], mean observation: 0.073 [-0.818, 1.437], loss: 8.083458, mae: 55.055389, mean_q: 73.268867
  996853/1100000: episode: 2333, duration: 1.879s, episode steps: 272, steps per second: 145, episode reward: 284.411, mean reward: 1.046 [-9.064, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.065 [-0.694, 1.395], loss: 9.685216, mae: 55.913849, mean_q: 74.223419
  997230/1100000: episode: 2334, duration: 2.622s, episode steps: 377, steps per second: 144, episode reward: 209.239, mean reward: 0.555 [-11.188, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.087 [-0.831, 1.438], loss: 7.031306, mae: 55.760349, mean_q: 74.126579
  997798/1100000: episode: 2335, duration: 4.022s, episode steps: 568, steps per second: 141, episode reward: 216.754, mean reward: 0.382 [-17.631, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.164 [-0.834, 1.428], loss: 7.301559, mae: 55.601467, mean_q: 73.949318
  998142/1100000: episode: 2336, duration: 2.403s, episode steps: 344, steps per second: 143, episode reward: 192.316, mean reward: 0.559 [-10.912, 100.000], mean action: 1.596 [0.000, 3.000], mean observation: -0.046 [-0.663, 1.477], loss: 12.135643, mae: 55.679081, mean_q: 73.819763
  998582/1100000: episode: 2337, duration: 3.078s, episode steps: 440, steps per second: 143, episode reward: 197.382, mean reward: 0.449 [-13.654, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.136 [-0.612, 1.454], loss: 7.203805, mae: 56.127041, mean_q: 74.588989
  998955/1100000: episode: 2338, duration: 2.589s, episode steps: 373, steps per second: 144, episode reward: 184.531, mean reward: 0.495 [-15.995, 100.000], mean action: 1.501 [0.000, 3.000], mean observation: -0.010 [-0.740, 1.403], loss: 8.205089, mae: 56.248753, mean_q: 74.102386
  999607/1100000: episode: 2339, duration: 4.828s, episode steps: 652, steps per second: 135, episode reward: 240.064, mean reward: 0.368 [-20.123, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.049 [-0.600, 1.388], loss: 10.759574, mae: 56.260792, mean_q: 74.360321
 1000167/1100000: episode: 2340, duration: 4.037s, episode steps: 560, steps per second: 139, episode reward: 194.945, mean reward: 0.348 [-21.458, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: 0.166 [-0.607, 1.475], loss: 8.298105, mae: 56.261185, mean_q: 74.330894
 1000487/1100000: episode: 2341, duration: 2.194s, episode steps: 320, steps per second: 146, episode reward: 240.725, mean reward: 0.752 [-7.651, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.204 [-0.842, 1.386], loss: 7.776250, mae: 56.773609, mean_q: 74.763405
 1001092/1100000: episode: 2342, duration: 4.732s, episode steps: 605, steps per second: 128, episode reward: -16.521, mean reward: -0.027 [-100.000, 14.808], mean action: 1.701 [0.000, 3.000], mean observation: 0.023 [-1.501, 1.392], loss: 8.247235, mae: 57.204346, mean_q: 75.518715
 1001386/1100000: episode: 2343, duration: 2.018s, episode steps: 294, steps per second: 146, episode reward: 211.614, mean reward: 0.720 [-17.223, 100.000], mean action: 1.650 [0.000, 3.000], mean observation: 0.014 [-0.600, 1.414], loss: 11.073416, mae: 57.097046, mean_q: 75.716042
 1001865/1100000: episode: 2344, duration: 3.284s, episode steps: 479, steps per second: 146, episode reward: 226.550, mean reward: 0.473 [-21.261, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.217 [-0.929, 1.394], loss: 24.222681, mae: 56.769489, mean_q: 74.744293
 1002254/1100000: episode: 2345, duration: 2.729s, episode steps: 389, steps per second: 143, episode reward: 262.059, mean reward: 0.674 [-10.925, 100.000], mean action: 0.861 [0.000, 3.000], mean observation: 0.125 [-1.101, 1.465], loss: 8.525700, mae: 56.018700, mean_q: 74.040558
 1002901/1100000: episode: 2346, duration: 4.614s, episode steps: 647, steps per second: 140, episode reward: -151.355, mean reward: -0.234 [-100.000, 5.041], mean action: 1.700 [0.000, 3.000], mean observation: 0.109 [-1.002, 1.512], loss: 10.953880, mae: 56.207050, mean_q: 74.374413
 1003337/1100000: episode: 2347, duration: 3.046s, episode steps: 436, steps per second: 143, episode reward: 246.949, mean reward: 0.566 [-19.395, 100.000], mean action: 0.782 [0.000, 3.000], mean observation: 0.125 [-0.649, 1.397], loss: 8.903180, mae: 55.776981, mean_q: 73.969963
 1004108/1100000: episode: 2348, duration: 5.690s, episode steps: 771, steps per second: 136, episode reward: 121.993, mean reward: 0.158 [-18.705, 100.000], mean action: 2.075 [0.000, 3.000], mean observation: 0.093 [-1.071, 1.448], loss: 7.860446, mae: 55.646015, mean_q: 73.723900
 1005108/1100000: episode: 2349, duration: 10.229s, episode steps: 1000, steps per second: 98, episode reward: -141.890, mean reward: -0.142 [-5.234, 5.435], mean action: 1.652 [0.000, 3.000], mean observation: 0.147 [-0.809, 1.662], loss: 8.574307, mae: 55.879353, mean_q: 74.049812
 1006108/1100000: episode: 2350, duration: 7.776s, episode steps: 1000, steps per second: 129, episode reward: -158.764, mean reward: -0.159 [-5.291, 4.925], mean action: 1.949 [0.000, 3.000], mean observation: 0.184 [-0.757, 1.957], loss: 7.707350, mae: 56.448460, mean_q: 74.860268
 1006666/1100000: episode: 2351, duration: 3.975s, episode steps: 558, steps per second: 140, episode reward: 221.909, mean reward: 0.398 [-17.423, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.217 [-0.392, 1.414], loss: 7.289526, mae: 56.398567, mean_q: 74.867508
 1007666/1100000: episode: 2352, duration: 7.607s, episode steps: 1000, steps per second: 131, episode reward: -174.884, mean reward: -0.175 [-5.262, 5.364], mean action: 1.702 [0.000, 3.000], mean observation: 0.155 [-0.825, 1.876], loss: 8.462576, mae: 56.407063, mean_q: 74.698334
 1008348/1100000: episode: 2353, duration: 4.832s, episode steps: 682, steps per second: 141, episode reward: 202.412, mean reward: 0.297 [-20.005, 100.000], mean action: 1.979 [0.000, 3.000], mean observation: 0.157 [-1.296, 1.414], loss: 10.124392, mae: 56.344414, mean_q: 74.637108
 1008587/1100000: episode: 2354, duration: 1.604s, episode steps: 239, steps per second: 149, episode reward: 6.207, mean reward: 0.026 [-100.000, 13.089], mean action: 1.682 [0.000, 3.000], mean observation: 0.002 [-1.059, 1.484], loss: 8.843846, mae: 55.941711, mean_q: 73.929352
 1009017/1100000: episode: 2355, duration: 3.050s, episode steps: 430, steps per second: 141, episode reward: 284.832, mean reward: 0.662 [-15.184, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.073 [-0.816, 1.417], loss: 8.309066, mae: 56.396622, mean_q: 74.620453
 1009573/1100000: episode: 2356, duration: 3.909s, episode steps: 556, steps per second: 142, episode reward: 216.094, mean reward: 0.389 [-20.009, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: -0.018 [-1.249, 1.395], loss: 7.254924, mae: 56.478508, mean_q: 74.840965
 1010255/1100000: episode: 2357, duration: 5.080s, episode steps: 682, steps per second: 134, episode reward: -255.621, mean reward: -0.375 [-100.000, 27.066], mean action: 1.777 [0.000, 3.000], mean observation: 0.048 [-1.033, 2.585], loss: 8.415014, mae: 56.353302, mean_q: 74.575729
 1010776/1100000: episode: 2358, duration: 3.751s, episode steps: 521, steps per second: 139, episode reward: -256.932, mean reward: -0.493 [-100.000, 7.517], mean action: 1.841 [0.000, 3.000], mean observation: 0.063 [-1.924, 1.560], loss: 12.982677, mae: 56.655762, mean_q: 75.005577
 1011636/1100000: episode: 2359, duration: 6.607s, episode steps: 860, steps per second: 130, episode reward: 133.911, mean reward: 0.156 [-19.040, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: -0.020 [-1.310, 1.425], loss: 8.595371, mae: 56.102150, mean_q: 74.819633
 1011847/1100000: episode: 2360, duration: 1.424s, episode steps: 211, steps per second: 148, episode reward: -38.886, mean reward: -0.184 [-100.000, 10.913], mean action: 1.555 [0.000, 3.000], mean observation: 0.108 [-0.996, 1.406], loss: 13.226490, mae: 55.639477, mean_q: 73.948204
 1012386/1100000: episode: 2361, duration: 3.903s, episode steps: 539, steps per second: 138, episode reward: 228.696, mean reward: 0.424 [-19.952, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: -0.024 [-0.814, 1.408], loss: 9.371168, mae: 56.004539, mean_q: 74.751854
 1012555/1100000: episode: 2362, duration: 1.139s, episode steps: 169, steps per second: 148, episode reward: -185.744, mean reward: -1.099 [-100.000, 9.958], mean action: 1.497 [0.000, 3.000], mean observation: 0.072 [-2.473, 1.420], loss: 10.842153, mae: 55.629761, mean_q: 74.001556
 1012826/1100000: episode: 2363, duration: 1.819s, episode steps: 271, steps per second: 149, episode reward: 229.220, mean reward: 0.846 [-3.932, 100.000], mean action: 0.867 [0.000, 3.000], mean observation: 0.082 [-1.156, 1.403], loss: 9.297029, mae: 55.685928, mean_q: 74.245865
 1013076/1100000: episode: 2364, duration: 1.728s, episode steps: 250, steps per second: 145, episode reward: 11.319, mean reward: 0.045 [-100.000, 9.292], mean action: 1.680 [0.000, 3.000], mean observation: 0.005 [-0.600, 1.402], loss: 9.399943, mae: 55.662544, mean_q: 74.270935
 1013372/1100000: episode: 2365, duration: 2.119s, episode steps: 296, steps per second: 140, episode reward: 224.466, mean reward: 0.758 [-18.064, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: -0.026 [-0.655, 1.445], loss: 10.814279, mae: 55.904984, mean_q: 74.554031
 1013575/1100000: episode: 2366, duration: 1.380s, episode steps: 203, steps per second: 147, episode reward: 264.482, mean reward: 1.303 [-10.464, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.087 [-1.036, 1.391], loss: 13.689312, mae: 55.681103, mean_q: 74.081779
 1014085/1100000: episode: 2367, duration: 3.775s, episode steps: 510, steps per second: 135, episode reward: 191.183, mean reward: 0.375 [-15.194, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.125 [-0.771, 1.388], loss: 8.976341, mae: 56.589058, mean_q: 75.623596
 1014673/1100000: episode: 2368, duration: 4.527s, episode steps: 588, steps per second: 130, episode reward: 238.243, mean reward: 0.405 [-18.795, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.125 [-1.378, 1.389], loss: 9.890587, mae: 56.455784, mean_q: 74.903328
 1015068/1100000: episode: 2369, duration: 2.747s, episode steps: 395, steps per second: 144, episode reward: 210.208, mean reward: 0.532 [-21.766, 100.000], mean action: 1.625 [0.000, 3.000], mean observation: -0.046 [-0.600, 1.457], loss: 6.496758, mae: 56.317425, mean_q: 75.000328
 1015179/1100000: episode: 2370, duration: 0.737s, episode steps: 111, steps per second: 151, episode reward: 9.914, mean reward: 0.089 [-100.000, 19.739], mean action: 1.568 [0.000, 3.000], mean observation: 0.028 [-1.757, 1.402], loss: 9.894479, mae: 55.640026, mean_q: 74.516365
 1015800/1100000: episode: 2371, duration: 4.334s, episode steps: 621, steps per second: 143, episode reward: 193.363, mean reward: 0.311 [-18.269, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.211 [-0.689, 1.639], loss: 9.944812, mae: 56.313946, mean_q: 75.181396
 1016123/1100000: episode: 2372, duration: 2.230s, episode steps: 323, steps per second: 145, episode reward: 271.586, mean reward: 0.841 [-8.871, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.097 [-0.955, 1.389], loss: 6.809298, mae: 55.853413, mean_q: 74.375313
 1016749/1100000: episode: 2373, duration: 4.366s, episode steps: 626, steps per second: 143, episode reward: 243.162, mean reward: 0.388 [-21.520, 100.000], mean action: 0.850 [0.000, 3.000], mean observation: 0.006 [-0.913, 1.387], loss: 7.624374, mae: 56.349995, mean_q: 75.178680
 1017008/1100000: episode: 2374, duration: 1.744s, episode steps: 259, steps per second: 149, episode reward: 63.059, mean reward: 0.243 [-100.000, 12.926], mean action: 1.533 [0.000, 3.000], mean observation: -0.079 [-1.176, 1.420], loss: 6.204105, mae: 56.151375, mean_q: 75.014946
 1017563/1100000: episode: 2375, duration: 3.860s, episode steps: 555, steps per second: 144, episode reward: -22.695, mean reward: -0.041 [-100.000, 12.550], mean action: 1.672 [0.000, 3.000], mean observation: 0.113 [-0.714, 1.561], loss: 6.256590, mae: 56.162830, mean_q: 74.844391
 1018092/1100000: episode: 2376, duration: 3.794s, episode steps: 529, steps per second: 139, episode reward: 204.281, mean reward: 0.386 [-10.951, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.209 [-0.798, 1.386], loss: 12.562407, mae: 55.942276, mean_q: 74.531357
 1018743/1100000: episode: 2377, duration: 4.806s, episode steps: 651, steps per second: 135, episode reward: 178.543, mean reward: 0.274 [-19.570, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.190 [-0.666, 1.401], loss: 9.018416, mae: 55.611092, mean_q: 74.137703
 1019214/1100000: episode: 2378, duration: 3.322s, episode steps: 471, steps per second: 142, episode reward: 257.644, mean reward: 0.547 [-18.263, 100.000], mean action: 0.958 [0.000, 3.000], mean observation: 0.017 [-0.776, 1.393], loss: 8.469860, mae: 55.426655, mean_q: 73.768707
 1019436/1100000: episode: 2379, duration: 1.490s, episode steps: 222, steps per second: 149, episode reward: -209.933, mean reward: -0.946 [-100.000, 46.055], mean action: 1.514 [0.000, 3.000], mean observation: 0.067 [-1.928, 1.406], loss: 10.809062, mae: 55.225201, mean_q: 73.722191
 1019738/1100000: episode: 2380, duration: 2.070s, episode steps: 302, steps per second: 146, episode reward: -168.981, mean reward: -0.560 [-100.000, 16.333], mean action: 1.285 [0.000, 3.000], mean observation: 0.072 [-1.856, 1.474], loss: 5.885734, mae: 55.001099, mean_q: 73.170326
 1020131/1100000: episode: 2381, duration: 2.831s, episode steps: 393, steps per second: 139, episode reward: 235.226, mean reward: 0.599 [-19.608, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.164 [-0.937, 1.387], loss: 8.773816, mae: 55.090714, mean_q: 73.175003
 1020272/1100000: episode: 2382, duration: 0.929s, episode steps: 141, steps per second: 152, episode reward: 25.117, mean reward: 0.178 [-100.000, 20.393], mean action: 1.894 [0.000, 3.000], mean observation: 0.051 [-1.205, 1.460], loss: 6.753950, mae: 54.306343, mean_q: 71.883980
 1020374/1100000: episode: 2383, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: 37.094, mean reward: 0.364 [-100.000, 11.180], mean action: 1.892 [0.000, 3.000], mean observation: -0.009 [-1.024, 1.385], loss: 6.459045, mae: 54.806690, mean_q: 72.251945
 1020644/1100000: episode: 2384, duration: 1.855s, episode steps: 270, steps per second: 146, episode reward: 245.507, mean reward: 0.909 [-10.522, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: -0.017 [-0.752, 1.404], loss: 8.791475, mae: 54.562862, mean_q: 72.258118
 1020872/1100000: episode: 2385, duration: 1.550s, episode steps: 228, steps per second: 147, episode reward: 226.104, mean reward: 0.992 [-11.466, 100.000], mean action: 2.329 [0.000, 3.000], mean observation: 0.068 [-1.357, 1.403], loss: 5.525860, mae: 54.465046, mean_q: 72.284645
 1021520/1100000: episode: 2386, duration: 4.571s, episode steps: 648, steps per second: 142, episode reward: 254.044, mean reward: 0.392 [-17.811, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.051 [-0.634, 1.474], loss: 10.898724, mae: 54.345909, mean_q: 72.165253
 1021820/1100000: episode: 2387, duration: 2.158s, episode steps: 300, steps per second: 139, episode reward: 237.247, mean reward: 0.791 [-17.911, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.099 [-1.171, 1.406], loss: 9.485038, mae: 54.488167, mean_q: 71.966003
 1022495/1100000: episode: 2388, duration: 4.989s, episode steps: 675, steps per second: 135, episode reward: 190.002, mean reward: 0.281 [-20.551, 100.000], mean action: 2.092 [0.000, 3.000], mean observation: 0.102 [-0.930, 1.450], loss: 5.916339, mae: 54.238014, mean_q: 72.165741
 1022961/1100000: episode: 2389, duration: 3.246s, episode steps: 466, steps per second: 144, episode reward: 271.538, mean reward: 0.583 [-17.615, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.023 [-0.870, 1.599], loss: 8.361341, mae: 54.015018, mean_q: 71.335434
 1023488/1100000: episode: 2390, duration: 3.701s, episode steps: 527, steps per second: 142, episode reward: 274.474, mean reward: 0.521 [-18.754, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.105 [-1.372, 1.461], loss: 8.445454, mae: 54.138229, mean_q: 71.853622
 1023840/1100000: episode: 2391, duration: 2.481s, episode steps: 352, steps per second: 142, episode reward: 281.120, mean reward: 0.799 [-18.170, 100.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.132 [-1.309, 1.506], loss: 8.933558, mae: 54.165218, mean_q: 71.889198
 1024840/1100000: episode: 2392, duration: 7.203s, episode steps: 1000, steps per second: 139, episode reward: 147.351, mean reward: 0.147 [-17.668, 22.324], mean action: 1.489 [0.000, 3.000], mean observation: 0.236 [-1.132, 1.645], loss: 8.419145, mae: 53.914413, mean_q: 71.416252
 1025607/1100000: episode: 2393, duration: 5.682s, episode steps: 767, steps per second: 135, episode reward: -110.217, mean reward: -0.144 [-100.000, 21.452], mean action: 1.271 [0.000, 3.000], mean observation: 0.102 [-1.023, 1.630], loss: 8.022489, mae: 53.873878, mean_q: 71.371841
 1026106/1100000: episode: 2394, duration: 3.607s, episode steps: 499, steps per second: 138, episode reward: 197.906, mean reward: 0.397 [-10.372, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: 0.133 [-1.099, 1.389], loss: 8.282580, mae: 54.230515, mean_q: 71.852760
 1026751/1100000: episode: 2395, duration: 4.654s, episode steps: 645, steps per second: 139, episode reward: 237.114, mean reward: 0.368 [-12.276, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.173 [-0.783, 1.405], loss: 9.049148, mae: 53.948486, mean_q: 71.330040
 1027751/1100000: episode: 2396, duration: 8.048s, episode steps: 1000, steps per second: 124, episode reward: 54.189, mean reward: 0.054 [-18.133, 24.503], mean action: 1.100 [0.000, 3.000], mean observation: 0.221 [-0.977, 1.409], loss: 9.414619, mae: 53.697617, mean_q: 71.144745
 1028751/1100000: episode: 2397, duration: 7.806s, episode steps: 1000, steps per second: 128, episode reward: -79.148, mean reward: -0.079 [-13.941, 11.996], mean action: 2.023 [0.000, 3.000], mean observation: 0.132 [-0.791, 1.487], loss: 8.919719, mae: 53.798859, mean_q: 71.373123
 1029569/1100000: episode: 2398, duration: 6.256s, episode steps: 818, steps per second: 131, episode reward: 193.817, mean reward: 0.237 [-19.369, 100.000], mean action: 1.878 [0.000, 3.000], mean observation: 0.058 [-0.600, 1.407], loss: 8.340641, mae: 53.842419, mean_q: 71.547142
 1029972/1100000: episode: 2399, duration: 2.889s, episode steps: 403, steps per second: 140, episode reward: -131.748, mean reward: -0.327 [-100.000, 9.900], mean action: 1.732 [0.000, 3.000], mean observation: 0.142 [-1.422, 2.693], loss: 7.097761, mae: 54.092518, mean_q: 71.861229
 1030532/1100000: episode: 2400, duration: 4.040s, episode steps: 560, steps per second: 139, episode reward: 162.239, mean reward: 0.290 [-5.626, 100.000], mean action: 1.521 [0.000, 3.000], mean observation: 0.136 [-0.718, 1.401], loss: 7.379182, mae: 54.093731, mean_q: 71.807053
 1031532/1100000: episode: 2401, duration: 7.330s, episode steps: 1000, steps per second: 136, episode reward: 42.033, mean reward: 0.042 [-5.279, 11.335], mean action: 1.628 [0.000, 3.000], mean observation: 0.113 [-0.892, 1.523], loss: 10.107231, mae: 54.117775, mean_q: 71.597534
 1032105/1100000: episode: 2402, duration: 4.188s, episode steps: 573, steps per second: 137, episode reward: 198.743, mean reward: 0.347 [-18.945, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.119 [-0.644, 1.391], loss: 7.839725, mae: 53.643501, mean_q: 71.010071
 1033065/1100000: episode: 2403, duration: 7.697s, episode steps: 960, steps per second: 125, episode reward: 86.267, mean reward: 0.090 [-13.285, 100.000], mean action: 1.605 [0.000, 3.000], mean observation: 0.144 [-0.706, 1.473], loss: 9.329446, mae: 53.756424, mean_q: 71.088799
 1033438/1100000: episode: 2404, duration: 2.628s, episode steps: 373, steps per second: 142, episode reward: 263.317, mean reward: 0.706 [-20.554, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.109 [-1.265, 1.396], loss: 7.892544, mae: 53.193020, mean_q: 70.408981
 1033578/1100000: episode: 2405, duration: 0.944s, episode steps: 140, steps per second: 148, episode reward: -595.523, mean reward: -4.254 [-100.000, 5.414], mean action: 2.057 [0.000, 3.000], mean observation: -0.111 [-1.818, 4.161], loss: 10.054710, mae: 53.128483, mean_q: 70.019157
 1034394/1100000: episode: 2406, duration: 5.755s, episode steps: 816, steps per second: 142, episode reward: 214.905, mean reward: 0.263 [-19.511, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.149 [-0.982, 1.406], loss: 10.466012, mae: 53.400028, mean_q: 70.868698
 1034822/1100000: episode: 2407, duration: 3.122s, episode steps: 428, steps per second: 137, episode reward: 268.041, mean reward: 0.626 [-17.807, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.109 [-0.620, 1.385], loss: 8.894153, mae: 53.408806, mean_q: 70.886154
 1035434/1100000: episode: 2408, duration: 4.390s, episode steps: 612, steps per second: 139, episode reward: -162.201, mean reward: -0.265 [-100.000, 19.887], mean action: 1.549 [0.000, 3.000], mean observation: -0.012 [-1.181, 1.461], loss: 7.868504, mae: 52.779915, mean_q: 69.721725
 1035539/1100000: episode: 2409, duration: 0.704s, episode steps: 105, steps per second: 149, episode reward: -231.504, mean reward: -2.205 [-100.000, 2.517], mean action: 1.990 [0.000, 3.000], mean observation: 0.070 [-1.121, 1.587], loss: 5.430998, mae: 52.869560, mean_q: 69.377831
 1035869/1100000: episode: 2410, duration: 2.271s, episode steps: 330, steps per second: 145, episode reward: 250.843, mean reward: 0.760 [-19.493, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.121 [-0.784, 1.410], loss: 7.584486, mae: 53.190056, mean_q: 70.485786
 1036099/1100000: episode: 2411, duration: 1.562s, episode steps: 230, steps per second: 147, episode reward: 265.023, mean reward: 1.152 [-2.849, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.057 [-0.871, 1.393], loss: 9.435001, mae: 52.735249, mean_q: 69.987892
 1036499/1100000: episode: 2412, duration: 2.830s, episode steps: 400, steps per second: 141, episode reward: 272.290, mean reward: 0.681 [-10.659, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.118 [-0.543, 1.390], loss: 6.807626, mae: 52.787766, mean_q: 70.021858
 1036732/1100000: episode: 2413, duration: 1.575s, episode steps: 233, steps per second: 148, episode reward: -558.245, mean reward: -2.396 [-100.000, 5.342], mean action: 1.682 [0.000, 3.000], mean observation: -0.043 [-1.596, 3.206], loss: 9.634158, mae: 53.447735, mean_q: 71.026573
 1036893/1100000: episode: 2414, duration: 1.083s, episode steps: 161, steps per second: 149, episode reward: -573.316, mean reward: -3.561 [-100.000, 6.170], mean action: 1.851 [0.000, 3.000], mean observation: -0.054 [-2.864, 3.009], loss: 27.603468, mae: 52.975197, mean_q: 70.269279
 1037042/1100000: episode: 2415, duration: 1.006s, episode steps: 149, steps per second: 148, episode reward: -393.852, mean reward: -2.643 [-100.000, 4.643], mean action: 1.846 [0.000, 3.000], mean observation: -0.040 [-3.959, 2.217], loss: 5.954155, mae: 52.892696, mean_q: 70.056076
 1037440/1100000: episode: 2416, duration: 2.837s, episode steps: 398, steps per second: 140, episode reward: 288.346, mean reward: 0.724 [-18.690, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.061 [-0.751, 1.459], loss: 13.563437, mae: 53.394512, mean_q: 70.905289
 1037693/1100000: episode: 2417, duration: 1.724s, episode steps: 253, steps per second: 147, episode reward: -372.663, mean reward: -1.473 [-100.000, 5.602], mean action: 1.506 [0.000, 3.000], mean observation: -0.050 [-3.795, 1.709], loss: 11.938259, mae: 53.735943, mean_q: 71.265533
 1037816/1100000: episode: 2418, duration: 0.833s, episode steps: 123, steps per second: 148, episode reward: -18.001, mean reward: -0.146 [-100.000, 18.759], mean action: 1.732 [0.000, 3.000], mean observation: -0.056 [-1.616, 1.387], loss: 6.588682, mae: 53.235451, mean_q: 70.588356
 1037947/1100000: episode: 2419, duration: 0.878s, episode steps: 131, steps per second: 149, episode reward: 11.990, mean reward: 0.092 [-100.000, 26.410], mean action: 1.481 [0.000, 3.000], mean observation: 0.081 [-1.288, 1.402], loss: 17.343065, mae: 53.900345, mean_q: 71.570045
 1038188/1100000: episode: 2420, duration: 1.743s, episode steps: 241, steps per second: 138, episode reward: -648.387, mean reward: -2.690 [-100.000, 14.167], mean action: 1.639 [0.000, 3.000], mean observation: -0.026 [-4.571, 2.118], loss: 12.569322, mae: 53.746429, mean_q: 70.941231
 1039013/1100000: episode: 2421, duration: 5.803s, episode steps: 825, steps per second: 142, episode reward: 194.127, mean reward: 0.235 [-18.341, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.053 [-0.600, 1.445], loss: 8.445578, mae: 53.554413, mean_q: 70.869057
 1039129/1100000: episode: 2422, duration: 0.784s, episode steps: 116, steps per second: 148, episode reward: -25.658, mean reward: -0.221 [-100.000, 12.320], mean action: 2.103 [0.000, 3.000], mean observation: 0.015 [-0.740, 1.388], loss: 7.349820, mae: 53.812393, mean_q: 71.481117
 1039514/1100000: episode: 2423, duration: 2.631s, episode steps: 385, steps per second: 146, episode reward: 206.132, mean reward: 0.535 [-19.065, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.157 [-0.641, 1.468], loss: 9.894662, mae: 53.663818, mean_q: 70.926521
 1039789/1100000: episode: 2424, duration: 1.868s, episode steps: 275, steps per second: 147, episode reward: -18.735, mean reward: -0.068 [-100.000, 13.679], mean action: 1.589 [0.000, 3.000], mean observation: 0.146 [-0.970, 1.484], loss: 14.985986, mae: 53.391987, mean_q: 70.840622
 1040013/1100000: episode: 2425, duration: 1.523s, episode steps: 224, steps per second: 147, episode reward: 282.647, mean reward: 1.262 [-11.927, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.055 [-0.953, 1.387], loss: 23.465906, mae: 53.315269, mean_q: 70.565773
 1040396/1100000: episode: 2426, duration: 2.661s, episode steps: 383, steps per second: 144, episode reward: 246.639, mean reward: 0.644 [-17.298, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: -0.012 [-0.958, 1.527], loss: 8.216888, mae: 53.344429, mean_q: 70.813156
 1040897/1100000: episode: 2427, duration: 3.548s, episode steps: 501, steps per second: 141, episode reward: 216.905, mean reward: 0.433 [-20.373, 100.000], mean action: 1.731 [0.000, 3.000], mean observation: 0.195 [-1.104, 1.392], loss: 10.370123, mae: 53.546600, mean_q: 70.853653
 1041018/1100000: episode: 2428, duration: 0.807s, episode steps: 121, steps per second: 150, episode reward: -43.571, mean reward: -0.360 [-100.000, 10.080], mean action: 1.612 [0.000, 3.000], mean observation: -0.042 [-1.042, 1.406], loss: 16.025917, mae: 54.227253, mean_q: 71.325706
 1041279/1100000: episode: 2429, duration: 1.779s, episode steps: 261, steps per second: 147, episode reward: 275.081, mean reward: 1.054 [-8.598, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: 0.098 [-0.816, 1.507], loss: 8.685184, mae: 53.180119, mean_q: 70.196098
 1041725/1100000: episode: 2430, duration: 3.233s, episode steps: 446, steps per second: 138, episode reward: 202.963, mean reward: 0.455 [-10.525, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: -0.021 [-0.665, 1.390], loss: 10.181494, mae: 53.309055, mean_q: 70.285980
 1041962/1100000: episode: 2431, duration: 1.626s, episode steps: 237, steps per second: 146, episode reward: 178.351, mean reward: 0.753 [-9.727, 100.000], mean action: 1.992 [0.000, 3.000], mean observation: 0.051 [-0.942, 1.387], loss: 9.039008, mae: 53.608231, mean_q: 70.621269
 1042370/1100000: episode: 2432, duration: 2.809s, episode steps: 408, steps per second: 145, episode reward: 262.658, mean reward: 0.644 [-18.676, 100.000], mean action: 0.699 [0.000, 3.000], mean observation: 0.245 [-1.066, 1.523], loss: 14.708611, mae: 53.587097, mean_q: 70.754105
 1042483/1100000: episode: 2433, duration: 0.753s, episode steps: 113, steps per second: 150, episode reward: -0.895, mean reward: -0.008 [-100.000, 50.056], mean action: 2.000 [0.000, 3.000], mean observation: 0.038 [-1.047, 1.417], loss: 12.287262, mae: 52.402126, mean_q: 69.114021
 1042845/1100000: episode: 2434, duration: 2.542s, episode steps: 362, steps per second: 142, episode reward: 226.675, mean reward: 0.626 [-17.134, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: -0.054 [-0.899, 1.411], loss: 11.407747, mae: 53.882111, mean_q: 71.138268
 1043497/1100000: episode: 2435, duration: 4.814s, episode steps: 652, steps per second: 135, episode reward: 166.848, mean reward: 0.256 [-14.296, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: -0.046 [-0.600, 1.433], loss: 8.803165, mae: 53.434811, mean_q: 70.066299
 1044057/1100000: episode: 2436, duration: 3.942s, episode steps: 560, steps per second: 142, episode reward: 181.157, mean reward: 0.323 [-19.141, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.215 [-1.066, 1.408], loss: 10.142536, mae: 53.504486, mean_q: 70.489250
 1044399/1100000: episode: 2437, duration: 2.390s, episode steps: 342, steps per second: 143, episode reward: 182.194, mean reward: 0.533 [-9.561, 100.000], mean action: 1.944 [0.000, 3.000], mean observation: -0.003 [-1.042, 1.457], loss: 10.456202, mae: 54.079815, mean_q: 70.751152
 1044704/1100000: episode: 2438, duration: 2.121s, episode steps: 305, steps per second: 144, episode reward: 274.290, mean reward: 0.899 [-8.534, 100.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.102 [-0.720, 1.403], loss: 8.253625, mae: 53.916344, mean_q: 71.044060
 1045461/1100000: episode: 2439, duration: 5.470s, episode steps: 757, steps per second: 138, episode reward: 198.879, mean reward: 0.263 [-10.974, 100.000], mean action: 1.898 [0.000, 3.000], mean observation: 0.243 [-0.809, 1.405], loss: 10.266826, mae: 53.842140, mean_q: 70.651909
 1045855/1100000: episode: 2440, duration: 2.747s, episode steps: 394, steps per second: 143, episode reward: 276.769, mean reward: 0.702 [-19.287, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.121 [-1.166, 1.558], loss: 12.541069, mae: 53.262474, mean_q: 69.974106
 1046016/1100000: episode: 2441, duration: 1.090s, episode steps: 161, steps per second: 148, episode reward: -21.904, mean reward: -0.136 [-100.000, 9.147], mean action: 1.925 [0.000, 3.000], mean observation: -0.023 [-0.765, 1.392], loss: 8.024534, mae: 53.544037, mean_q: 70.303833
 1046248/1100000: episode: 2442, duration: 1.586s, episode steps: 232, steps per second: 146, episode reward: 244.914, mean reward: 1.056 [-9.168, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.146 [-0.914, 1.390], loss: 7.739379, mae: 53.034767, mean_q: 69.393433
 1046918/1100000: episode: 2443, duration: 4.942s, episode steps: 670, steps per second: 136, episode reward: 194.653, mean reward: 0.291 [-17.857, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.198 [-0.914, 1.393], loss: 9.971032, mae: 53.469414, mean_q: 70.144234
 1047384/1100000: episode: 2444, duration: 3.336s, episode steps: 466, steps per second: 140, episode reward: 191.150, mean reward: 0.410 [-15.959, 100.000], mean action: 2.260 [0.000, 3.000], mean observation: 0.194 [-0.872, 1.423], loss: 11.754372, mae: 53.297234, mean_q: 69.764259
 1047737/1100000: episode: 2445, duration: 2.450s, episode steps: 353, steps per second: 144, episode reward: 227.659, mean reward: 0.645 [-19.538, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.158 [-0.856, 1.386], loss: 7.498790, mae: 53.366711, mean_q: 69.799622
 1047853/1100000: episode: 2446, duration: 0.783s, episode steps: 116, steps per second: 148, episode reward: -127.640, mean reward: -1.100 [-100.000, 51.891], mean action: 1.759 [0.000, 3.000], mean observation: -0.152 [-1.677, 1.400], loss: 12.710553, mae: 52.654327, mean_q: 68.864105
 1048529/1100000: episode: 2447, duration: 4.655s, episode steps: 676, steps per second: 145, episode reward: 250.321, mean reward: 0.370 [-20.127, 100.000], mean action: 0.848 [0.000, 3.000], mean observation: 0.188 [-0.750, 1.413], loss: 10.911880, mae: 52.845184, mean_q: 69.001106
 1048922/1100000: episode: 2448, duration: 2.708s, episode steps: 393, steps per second: 145, episode reward: 297.954, mean reward: 0.758 [-15.393, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.055 [-0.926, 1.425], loss: 11.096387, mae: 52.844017, mean_q: 69.137642
 1049126/1100000: episode: 2449, duration: 1.403s, episode steps: 204, steps per second: 145, episode reward: 36.083, mean reward: 0.177 [-100.000, 13.521], mean action: 1.632 [0.000, 3.000], mean observation: 0.104 [-0.980, 1.468], loss: 7.298073, mae: 52.900879, mean_q: 69.577629
 1049803/1100000: episode: 2450, duration: 5.079s, episode steps: 677, steps per second: 133, episode reward: 210.584, mean reward: 0.311 [-17.234, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: -0.000 [-0.728, 1.463], loss: 11.949841, mae: 52.932201, mean_q: 69.880341
 1049969/1100000: episode: 2451, duration: 1.118s, episode steps: 166, steps per second: 148, episode reward: -33.302, mean reward: -0.201 [-100.000, 71.652], mean action: 1.922 [0.000, 3.000], mean observation: -0.059 [-0.802, 1.443], loss: 17.094454, mae: 53.275555, mean_q: 69.809914
 1050083/1100000: episode: 2452, duration: 0.776s, episode steps: 114, steps per second: 147, episode reward: -22.715, mean reward: -0.199 [-100.000, 17.660], mean action: 1.965 [0.000, 3.000], mean observation: 0.009 [-1.491, 1.395], loss: 6.656338, mae: 52.672535, mean_q: 69.776245
 1050366/1100000: episode: 2453, duration: 1.935s, episode steps: 283, steps per second: 146, episode reward: 251.767, mean reward: 0.890 [-2.908, 100.000], mean action: 2.127 [0.000, 3.000], mean observation: 0.070 [-0.673, 1.418], loss: 13.638873, mae: 53.162483, mean_q: 70.295113
 1050759/1100000: episode: 2454, duration: 2.778s, episode steps: 393, steps per second: 141, episode reward: 279.222, mean reward: 0.710 [-21.340, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.215 [-0.581, 1.451], loss: 9.997586, mae: 52.897877, mean_q: 69.819557
 1050926/1100000: episode: 2455, duration: 1.105s, episode steps: 167, steps per second: 151, episode reward: -26.648, mean reward: -0.160 [-100.000, 8.776], mean action: 1.587 [0.000, 3.000], mean observation: 0.048 [-2.496, 1.456], loss: 9.290687, mae: 52.890625, mean_q: 69.946800
 1051359/1100000: episode: 2456, duration: 3.065s, episode steps: 433, steps per second: 141, episode reward: 262.137, mean reward: 0.605 [-18.815, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: 0.126 [-0.769, 1.390], loss: 10.697888, mae: 53.110764, mean_q: 70.211967
 1051720/1100000: episode: 2457, duration: 2.496s, episode steps: 361, steps per second: 145, episode reward: 217.694, mean reward: 0.603 [-18.584, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.158 [-0.741, 1.406], loss: 9.713675, mae: 52.548607, mean_q: 69.387962
 1052083/1100000: episode: 2458, duration: 2.539s, episode steps: 363, steps per second: 143, episode reward: 273.472, mean reward: 0.753 [-11.167, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.114 [-0.670, 1.403], loss: 7.030848, mae: 52.922344, mean_q: 69.838280
 1052375/1100000: episode: 2459, duration: 1.996s, episode steps: 292, steps per second: 146, episode reward: -36.974, mean reward: -0.127 [-100.000, 13.573], mean action: 1.627 [0.000, 3.000], mean observation: -0.013 [-1.574, 1.473], loss: 8.810225, mae: 53.160095, mean_q: 70.017593
 1052772/1100000: episode: 2460, duration: 2.799s, episode steps: 397, steps per second: 142, episode reward: 278.791, mean reward: 0.702 [-17.937, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.108 [-0.989, 1.406], loss: 8.543938, mae: 53.089195, mean_q: 69.882370
 1053043/1100000: episode: 2461, duration: 1.845s, episode steps: 271, steps per second: 147, episode reward: 237.952, mean reward: 0.878 [-8.798, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.081 [-1.336, 1.401], loss: 7.974031, mae: 52.687946, mean_q: 69.572586
 1053449/1100000: episode: 2462, duration: 2.840s, episode steps: 406, steps per second: 143, episode reward: 203.960, mean reward: 0.502 [-18.876, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.064 [-1.150, 1.421], loss: 12.684358, mae: 52.432274, mean_q: 69.081589
 1054003/1100000: episode: 2463, duration: 3.965s, episode steps: 554, steps per second: 140, episode reward: -110.906, mean reward: -0.200 [-100.000, 11.280], mean action: 1.599 [0.000, 3.000], mean observation: 0.139 [-1.427, 1.401], loss: 10.998672, mae: 52.785538, mean_q: 69.452278
 1054290/1100000: episode: 2464, duration: 1.942s, episode steps: 287, steps per second: 148, episode reward: 238.416, mean reward: 0.831 [-18.720, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.253 [-1.089, 1.405], loss: 10.835997, mae: 51.937160, mean_q: 68.269043
 1054411/1100000: episode: 2465, duration: 0.823s, episode steps: 121, steps per second: 147, episode reward: 35.771, mean reward: 0.296 [-100.000, 15.617], mean action: 1.835 [0.000, 3.000], mean observation: 0.036 [-1.056, 1.392], loss: 7.498610, mae: 52.587997, mean_q: 69.533852
 1054651/1100000: episode: 2466, duration: 1.634s, episode steps: 240, steps per second: 147, episode reward: 28.658, mean reward: 0.119 [-100.000, 9.933], mean action: 1.933 [0.000, 3.000], mean observation: -0.064 [-0.829, 1.452], loss: 9.328059, mae: 53.196705, mean_q: 70.224007
 1054855/1100000: episode: 2467, duration: 1.376s, episode steps: 204, steps per second: 148, episode reward: -11.359, mean reward: -0.056 [-100.000, 8.897], mean action: 1.843 [0.000, 3.000], mean observation: -0.040 [-0.870, 1.385], loss: 11.945506, mae: 52.255123, mean_q: 69.122787
 1055372/1100000: episode: 2468, duration: 3.793s, episode steps: 517, steps per second: 136, episode reward: 232.808, mean reward: 0.450 [-18.042, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: -0.013 [-0.891, 1.427], loss: 10.130553, mae: 52.932819, mean_q: 69.545227
 1055444/1100000: episode: 2469, duration: 0.490s, episode steps: 72, steps per second: 147, episode reward: -52.621, mean reward: -0.731 [-100.000, 17.340], mean action: 1.792 [0.000, 3.000], mean observation: 0.175 [-1.241, 1.397], loss: 7.384308, mae: 52.474018, mean_q: 69.433884
 1055778/1100000: episode: 2470, duration: 2.333s, episode steps: 334, steps per second: 143, episode reward: 301.473, mean reward: 0.903 [-10.659, 100.000], mean action: 1.521 [0.000, 3.000], mean observation: 0.071 [-1.118, 1.388], loss: 6.049252, mae: 52.799473, mean_q: 69.276558
 1055918/1100000: episode: 2471, duration: 0.937s, episode steps: 140, steps per second: 149, episode reward: 2.610, mean reward: 0.019 [-100.000, 12.255], mean action: 1.564 [0.000, 3.000], mean observation: 0.114 [-2.299, 1.512], loss: 8.183210, mae: 52.695251, mean_q: 69.406548
 1056541/1100000: episode: 2472, duration: 4.342s, episode steps: 623, steps per second: 143, episode reward: 215.316, mean reward: 0.346 [-21.117, 100.000], mean action: 1.648 [0.000, 3.000], mean observation: 0.208 [-0.654, 1.394], loss: 7.349177, mae: 52.639816, mean_q: 69.500252
 1056926/1100000: episode: 2473, duration: 2.652s, episode steps: 385, steps per second: 145, episode reward: 215.380, mean reward: 0.559 [-17.544, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: -0.028 [-1.273, 1.442], loss: 7.596119, mae: 52.690285, mean_q: 68.942772
 1057676/1100000: episode: 2474, duration: 5.363s, episode steps: 750, steps per second: 140, episode reward: 131.612, mean reward: 0.175 [-12.936, 100.000], mean action: 1.832 [0.000, 3.000], mean observation: 0.150 [-0.992, 1.399], loss: 9.397759, mae: 52.884438, mean_q: 69.690781
 1057953/1100000: episode: 2475, duration: 1.877s, episode steps: 277, steps per second: 148, episode reward: 256.375, mean reward: 0.926 [-19.829, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: -0.046 [-1.296, 1.391], loss: 13.037313, mae: 52.786121, mean_q: 69.657898
 1058245/1100000: episode: 2476, duration: 2.007s, episode steps: 292, steps per second: 145, episode reward: 271.388, mean reward: 0.929 [-10.012, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.092 [-0.928, 1.471], loss: 8.423555, mae: 52.568394, mean_q: 69.937820
 1059149/1100000: episode: 2477, duration: 7.176s, episode steps: 904, steps per second: 126, episode reward: 184.346, mean reward: 0.204 [-10.348, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.110 [-0.711, 1.473], loss: 9.655684, mae: 52.956345, mean_q: 70.334885
 1059616/1100000: episode: 2478, duration: 3.302s, episode steps: 467, steps per second: 141, episode reward: 275.126, mean reward: 0.589 [-20.190, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.118 [-1.019, 1.512], loss: 7.803078, mae: 53.202137, mean_q: 70.447708
 1059702/1100000: episode: 2479, duration: 0.563s, episode steps: 86, steps per second: 153, episode reward: -47.316, mean reward: -0.550 [-100.000, 11.103], mean action: 0.802 [0.000, 3.000], mean observation: 0.126 [-3.166, 1.415], loss: 7.503278, mae: 54.706402, mean_q: 72.164497
 1059786/1100000: episode: 2480, duration: 0.561s, episode steps: 84, steps per second: 150, episode reward: 43.653, mean reward: 0.520 [-100.000, 12.887], mean action: 1.690 [0.000, 3.000], mean observation: 0.062 [-1.917, 1.386], loss: 8.905414, mae: 53.204620, mean_q: 71.044586
 1060017/1100000: episode: 2481, duration: 1.552s, episode steps: 231, steps per second: 149, episode reward: 273.540, mean reward: 1.184 [-5.607, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.063 [-0.657, 1.412], loss: 11.043323, mae: 54.276344, mean_q: 71.650787
 1060106/1100000: episode: 2482, duration: 0.596s, episode steps: 89, steps per second: 149, episode reward: -60.164, mean reward: -0.676 [-100.000, 17.771], mean action: 1.517 [0.000, 3.000], mean observation: 0.094 [-1.152, 1.674], loss: 5.573447, mae: 54.046597, mean_q: 71.181259
 1060197/1100000: episode: 2483, duration: 0.618s, episode steps: 91, steps per second: 147, episode reward: 28.163, mean reward: 0.309 [-100.000, 19.731], mean action: 1.956 [0.000, 3.000], mean observation: 0.030 [-1.637, 1.396], loss: 18.716904, mae: 53.438850, mean_q: 70.288399
 1060369/1100000: episode: 2484, duration: 1.161s, episode steps: 172, steps per second: 148, episode reward: -64.722, mean reward: -0.376 [-100.000, 12.940], mean action: 1.727 [0.000, 3.000], mean observation: -0.026 [-0.791, 1.400], loss: 9.696704, mae: 53.688828, mean_q: 71.113144
 1060786/1100000: episode: 2485, duration: 2.948s, episode steps: 417, steps per second: 141, episode reward: -225.439, mean reward: -0.541 [-100.000, 21.204], mean action: 1.573 [0.000, 3.000], mean observation: -0.036 [-1.454, 1.413], loss: 9.555165, mae: 53.576008, mean_q: 70.647179
 1060871/1100000: episode: 2486, duration: 0.569s, episode steps: 85, steps per second: 149, episode reward: -47.042, mean reward: -0.553 [-100.000, 8.603], mean action: 1.341 [0.000, 3.000], mean observation: 0.210 [-2.942, 1.419], loss: 11.632887, mae: 53.252739, mean_q: 70.282959
 1061345/1100000: episode: 2487, duration: 3.410s, episode steps: 474, steps per second: 139, episode reward: 178.951, mean reward: 0.378 [-17.214, 100.000], mean action: 2.306 [0.000, 3.000], mean observation: 0.096 [-0.724, 1.484], loss: 10.396435, mae: 53.266518, mean_q: 70.145210
 1062050/1100000: episode: 2488, duration: 5.036s, episode steps: 705, steps per second: 140, episode reward: 76.284, mean reward: 0.108 [-18.500, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.019 [-0.748, 1.488], loss: 12.878556, mae: 53.708527, mean_q: 70.536545
 1063039/1100000: episode: 2489, duration: 7.566s, episode steps: 989, steps per second: 131, episode reward: 211.508, mean reward: 0.214 [-19.766, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: -0.002 [-0.904, 1.413], loss: 11.578081, mae: 53.049023, mean_q: 69.928925
 1063412/1100000: episode: 2490, duration: 2.614s, episode steps: 373, steps per second: 143, episode reward: 253.415, mean reward: 0.679 [-10.114, 100.000], mean action: 2.086 [0.000, 3.000], mean observation: 0.143 [-0.780, 1.420], loss: 8.671063, mae: 52.615681, mean_q: 69.512459
 1063545/1100000: episode: 2491, duration: 0.893s, episode steps: 133, steps per second: 149, episode reward: -38.845, mean reward: -0.292 [-100.000, 16.171], mean action: 1.594 [0.000, 3.000], mean observation: 0.168 [-1.138, 1.414], loss: 10.135845, mae: 52.721634, mean_q: 69.963684
 1063836/1100000: episode: 2492, duration: 2.074s, episode steps: 291, steps per second: 140, episode reward: 265.093, mean reward: 0.911 [-17.802, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: -0.071 [-1.037, 1.385], loss: 7.298904, mae: 52.627106, mean_q: 69.571808
 1064159/1100000: episode: 2493, duration: 2.238s, episode steps: 323, steps per second: 144, episode reward: 245.609, mean reward: 0.760 [-18.830, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.103 [-0.833, 1.399], loss: 11.939466, mae: 52.653286, mean_q: 69.765846
 1064292/1100000: episode: 2494, duration: 0.886s, episode steps: 133, steps per second: 150, episode reward: 3.047, mean reward: 0.023 [-100.000, 19.483], mean action: 1.534 [0.000, 3.000], mean observation: 0.093 [-1.414, 1.508], loss: 14.804967, mae: 52.152058, mean_q: 69.206970
 1064540/1100000: episode: 2495, duration: 1.691s, episode steps: 248, steps per second: 147, episode reward: -53.214, mean reward: -0.215 [-100.000, 12.902], mean action: 1.746 [0.000, 3.000], mean observation: 0.011 [-0.600, 1.390], loss: 13.608077, mae: 52.507576, mean_q: 69.323738
 1065540/1100000: episode: 2496, duration: 8.056s, episode steps: 1000, steps per second: 124, episode reward: -25.526, mean reward: -0.026 [-3.831, 5.397], mean action: 1.436 [0.000, 3.000], mean observation: 0.097 [-0.715, 1.393], loss: 10.689533, mae: 52.374413, mean_q: 69.541008
 1066540/1100000: episode: 2497, duration: 7.837s, episode steps: 1000, steps per second: 128, episode reward: 10.939, mean reward: 0.011 [-4.426, 5.473], mean action: 1.470 [0.000, 3.000], mean observation: 0.105 [-0.819, 1.405], loss: 10.481929, mae: 51.360554, mean_q: 68.094284
 1066724/1100000: episode: 2498, duration: 1.226s, episode steps: 184, steps per second: 150, episode reward: -80.395, mean reward: -0.437 [-100.000, 12.239], mean action: 1.587 [0.000, 3.000], mean observation: 0.127 [-0.786, 1.417], loss: 7.135050, mae: 50.806820, mean_q: 67.174446
 1067025/1100000: episode: 2499, duration: 2.073s, episode steps: 301, steps per second: 145, episode reward: -208.891, mean reward: -0.694 [-100.000, 24.153], mean action: 1.482 [0.000, 3.000], mean observation: 0.139 [-0.679, 1.612], loss: 11.732490, mae: 50.303364, mean_q: 66.320770
 1067750/1100000: episode: 2500, duration: 5.297s, episode steps: 725, steps per second: 137, episode reward: 220.734, mean reward: 0.304 [-17.834, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.126 [-0.758, 1.416], loss: 10.748168, mae: 50.411213, mean_q: 66.593575
 1067917/1100000: episode: 2501, duration: 1.118s, episode steps: 167, steps per second: 149, episode reward: -53.831, mean reward: -0.322 [-100.000, 20.669], mean action: 1.665 [0.000, 3.000], mean observation: 0.056 [-0.600, 1.398], loss: 7.401945, mae: 50.441692, mean_q: 66.091911
 1068917/1100000: episode: 2502, duration: 7.295s, episode steps: 1000, steps per second: 137, episode reward: -27.684, mean reward: -0.028 [-5.400, 5.730], mean action: 1.523 [0.000, 3.000], mean observation: 0.084 [-0.778, 1.417], loss: 9.905121, mae: 49.559155, mean_q: 65.237801
 1069289/1100000: episode: 2503, duration: 2.552s, episode steps: 372, steps per second: 146, episode reward: 200.226, mean reward: 0.538 [-9.454, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: -0.023 [-1.097, 1.449], loss: 10.569651, mae: 48.841965, mean_q: 63.947498
 1069760/1100000: episode: 2504, duration: 3.401s, episode steps: 471, steps per second: 138, episode reward: 213.576, mean reward: 0.453 [-18.190, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: -0.008 [-0.600, 1.402], loss: 12.127275, mae: 48.452023, mean_q: 63.811092
 1070429/1100000: episode: 2505, duration: 5.129s, episode steps: 669, steps per second: 130, episode reward: 140.084, mean reward: 0.209 [-17.740, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.141 [-0.771, 1.402], loss: 11.094855, mae: 47.865440, mean_q: 63.123798
 1070903/1100000: episode: 2506, duration: 3.495s, episode steps: 474, steps per second: 136, episode reward: -144.124, mean reward: -0.304 [-100.000, 15.950], mean action: 1.795 [0.000, 3.000], mean observation: 0.055 [-1.568, 1.501], loss: 11.053159, mae: 47.087280, mean_q: 61.841843
 1071228/1100000: episode: 2507, duration: 2.244s, episode steps: 325, steps per second: 145, episode reward: 233.942, mean reward: 0.720 [-18.432, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: -0.033 [-0.743, 1.411], loss: 7.213601, mae: 46.808739, mean_q: 61.532536
 1071610/1100000: episode: 2508, duration: 2.678s, episode steps: 382, steps per second: 143, episode reward: -89.993, mean reward: -0.236 [-100.000, 16.556], mean action: 1.626 [0.000, 3.000], mean observation: 0.123 [-1.032, 1.399], loss: 8.130135, mae: 47.418583, mean_q: 61.671394
 1071904/1100000: episode: 2509, duration: 2.144s, episode steps: 294, steps per second: 137, episode reward: 270.807, mean reward: 0.921 [-20.455, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.070 [-0.670, 1.394], loss: 9.664615, mae: 47.282726, mean_q: 62.108856
 1072536/1100000: episode: 2510, duration: 4.462s, episode steps: 632, steps per second: 142, episode reward: 193.918, mean reward: 0.307 [-19.716, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.172 [-0.745, 1.406], loss: 10.186353, mae: 47.362713, mean_q: 62.003311
 1072850/1100000: episode: 2511, duration: 2.156s, episode steps: 314, steps per second: 146, episode reward: 245.438, mean reward: 0.782 [-20.262, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.081 [-0.815, 1.404], loss: 10.589820, mae: 46.946632, mean_q: 61.152519
 1073370/1100000: episode: 2512, duration: 3.624s, episode steps: 520, steps per second: 143, episode reward: 224.520, mean reward: 0.432 [-19.082, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: -0.019 [-0.600, 1.433], loss: 9.775251, mae: 47.151375, mean_q: 61.969982
 1073561/1100000: episode: 2513, duration: 1.290s, episode steps: 191, steps per second: 148, episode reward: -18.861, mean reward: -0.099 [-100.000, 23.626], mean action: 1.911 [0.000, 3.000], mean observation: 0.055 [-0.790, 1.402], loss: 10.451722, mae: 47.123150, mean_q: 62.190819
 1073892/1100000: episode: 2514, duration: 2.274s, episode steps: 331, steps per second: 146, episode reward: 206.886, mean reward: 0.625 [-10.673, 100.000], mean action: 1.798 [0.000, 3.000], mean observation: 0.040 [-0.749, 1.433], loss: 12.954739, mae: 47.098782, mean_q: 61.787975
 1074250/1100000: episode: 2515, duration: 2.505s, episode steps: 358, steps per second: 143, episode reward: 262.882, mean reward: 0.734 [-17.862, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: -0.024 [-1.095, 1.392], loss: 6.094750, mae: 47.183418, mean_q: 62.233906
 1074817/1100000: episode: 2516, duration: 4.044s, episode steps: 567, steps per second: 140, episode reward: 225.555, mean reward: 0.398 [-10.826, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: -0.017 [-0.600, 1.393], loss: 9.341778, mae: 47.134216, mean_q: 62.133804
 1075374/1100000: episode: 2517, duration: 3.928s, episode steps: 557, steps per second: 142, episode reward: 200.259, mean reward: 0.360 [-11.967, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: -0.006 [-0.748, 1.387], loss: 10.129888, mae: 47.122841, mean_q: 62.004150
 1075753/1100000: episode: 2518, duration: 2.582s, episode steps: 379, steps per second: 147, episode reward: 266.510, mean reward: 0.703 [-17.501, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.105 [-0.698, 1.401], loss: 10.104601, mae: 47.070858, mean_q: 61.968941
 1076219/1100000: episode: 2519, duration: 3.390s, episode steps: 466, steps per second: 137, episode reward: 238.877, mean reward: 0.513 [-18.762, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.099 [-0.938, 1.398], loss: 8.494949, mae: 47.301079, mean_q: 62.096825
 1077219/1100000: episode: 2520, duration: 7.519s, episode steps: 1000, steps per second: 133, episode reward: 17.955, mean reward: 0.018 [-24.075, 23.044], mean action: 1.630 [0.000, 3.000], mean observation: 0.130 [-0.722, 1.392], loss: 9.456226, mae: 46.888844, mean_q: 62.053421
 1077568/1100000: episode: 2521, duration: 2.448s, episode steps: 349, steps per second: 143, episode reward: 233.447, mean reward: 0.669 [-11.244, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: -0.041 [-0.758, 1.388], loss: 12.671458, mae: 46.655972, mean_q: 61.722198
 1078046/1100000: episode: 2522, duration: 3.308s, episode steps: 478, steps per second: 144, episode reward: 202.784, mean reward: 0.424 [-21.178, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.204 [-0.853, 1.424], loss: 8.955721, mae: 46.831451, mean_q: 62.302567
 1078426/1100000: episode: 2523, duration: 2.628s, episode steps: 380, steps per second: 145, episode reward: 262.531, mean reward: 0.691 [-14.473, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.034 [-0.891, 1.456], loss: 7.123479, mae: 47.100082, mean_q: 62.436779
 1078914/1100000: episode: 2524, duration: 3.791s, episode steps: 488, steps per second: 129, episode reward: 202.574, mean reward: 0.415 [-19.274, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.004 [-0.796, 1.403], loss: 11.028665, mae: 47.068501, mean_q: 62.455639
 1079598/1100000: episode: 2525, duration: 4.814s, episode steps: 684, steps per second: 142, episode reward: 201.601, mean reward: 0.295 [-20.359, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.272 [-1.451, 1.402], loss: 9.828720, mae: 47.411308, mean_q: 62.829918
 1080598/1100000: episode: 2526, duration: 7.389s, episode steps: 1000, steps per second: 135, episode reward: 64.785, mean reward: 0.065 [-21.492, 23.257], mean action: 1.310 [0.000, 3.000], mean observation: 0.236 [-0.703, 1.426], loss: 8.897799, mae: 47.361881, mean_q: 62.793331
 1080989/1100000: episode: 2527, duration: 2.785s, episode steps: 391, steps per second: 140, episode reward: 168.671, mean reward: 0.431 [-16.238, 100.000], mean action: 1.928 [0.000, 3.000], mean observation: 0.134 [-0.824, 1.409], loss: 12.098983, mae: 47.197876, mean_q: 62.329899
 1081545/1100000: episode: 2528, duration: 4.102s, episode steps: 556, steps per second: 136, episode reward: 159.325, mean reward: 0.287 [-20.103, 100.000], mean action: 2.270 [0.000, 3.000], mean observation: 0.163 [-0.863, 1.409], loss: 11.909956, mae: 46.977844, mean_q: 62.435364
 1081765/1100000: episode: 2529, duration: 1.498s, episode steps: 220, steps per second: 147, episode reward: 227.472, mean reward: 1.034 [-13.665, 100.000], mean action: 2.055 [0.000, 3.000], mean observation: 0.048 [-0.906, 1.392], loss: 10.101407, mae: 46.597401, mean_q: 62.026146
 1081910/1100000: episode: 2530, duration: 0.979s, episode steps: 145, steps per second: 148, episode reward: -33.171, mean reward: -0.229 [-100.000, 17.306], mean action: 2.000 [0.000, 3.000], mean observation: -0.022 [-1.769, 1.460], loss: 11.818399, mae: 46.823532, mean_q: 61.673935
 1082547/1100000: episode: 2531, duration: 4.399s, episode steps: 637, steps per second: 145, episode reward: 248.798, mean reward: 0.391 [-19.788, 100.000], mean action: 0.584 [0.000, 3.000], mean observation: 0.247 [-1.333, 1.441], loss: 11.088919, mae: 46.963581, mean_q: 62.683262
 1083480/1100000: episode: 2532, duration: 6.740s, episode steps: 933, steps per second: 138, episode reward: 204.812, mean reward: 0.220 [-20.282, 100.000], mean action: 2.054 [0.000, 3.000], mean observation: 0.076 [-0.735, 1.404], loss: 9.130721, mae: 46.979065, mean_q: 62.675488
 1083985/1100000: episode: 2533, duration: 3.568s, episode steps: 505, steps per second: 142, episode reward: 236.595, mean reward: 0.469 [-19.719, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: -0.009 [-0.808, 1.387], loss: 14.289881, mae: 47.343502, mean_q: 63.338879
 1084483/1100000: episode: 2534, duration: 3.466s, episode steps: 498, steps per second: 144, episode reward: 257.293, mean reward: 0.517 [-11.587, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.082 [-0.864, 1.502], loss: 8.619032, mae: 47.273315, mean_q: 63.414921
 1084911/1100000: episode: 2535, duration: 3.002s, episode steps: 428, steps per second: 143, episode reward: 256.768, mean reward: 0.600 [-11.537, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.181 [-0.717, 1.434], loss: 10.946125, mae: 47.245327, mean_q: 63.474174
 1085472/1100000: episode: 2536, duration: 3.918s, episode steps: 561, steps per second: 143, episode reward: 232.587, mean reward: 0.415 [-19.336, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.005 [-0.795, 1.391], loss: 11.935785, mae: 47.453545, mean_q: 63.617611
 1085761/1100000: episode: 2537, duration: 1.982s, episode steps: 289, steps per second: 146, episode reward: 222.539, mean reward: 0.770 [-17.960, 100.000], mean action: 2.083 [0.000, 3.000], mean observation: 0.058 [-0.740, 1.454], loss: 10.876676, mae: 47.296089, mean_q: 63.497932
 1085928/1100000: episode: 2538, duration: 1.114s, episode steps: 167, steps per second: 150, episode reward: -16.852, mean reward: -0.101 [-100.000, 13.568], mean action: 1.599 [0.000, 3.000], mean observation: 0.122 [-2.714, 1.510], loss: 9.741466, mae: 46.929283, mean_q: 63.063408
 1086035/1100000: episode: 2539, duration: 0.719s, episode steps: 107, steps per second: 149, episode reward: -55.740, mean reward: -0.521 [-100.000, 15.730], mean action: 1.916 [0.000, 3.000], mean observation: 0.122 [-0.946, 1.451], loss: 11.309872, mae: 46.795612, mean_q: 62.972485
 1086345/1100000: episode: 2540, duration: 2.157s, episode steps: 310, steps per second: 144, episode reward: -76.494, mean reward: -0.247 [-100.000, 21.662], mean action: 1.723 [0.000, 3.000], mean observation: -0.090 [-0.835, 1.410], loss: 11.395419, mae: 47.176456, mean_q: 63.191444
 1086946/1100000: episode: 2541, duration: 4.329s, episode steps: 601, steps per second: 139, episode reward: 249.202, mean reward: 0.415 [-19.557, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.213 [-0.959, 1.447], loss: 9.308903, mae: 47.231972, mean_q: 63.460464
 1087052/1100000: episode: 2542, duration: 0.713s, episode steps: 106, steps per second: 149, episode reward: -46.733, mean reward: -0.441 [-100.000, 10.873], mean action: 1.906 [0.000, 3.000], mean observation: 0.124 [-0.806, 1.398], loss: 10.718208, mae: 46.252583, mean_q: 62.267639
 1087192/1100000: episode: 2543, duration: 0.946s, episode steps: 140, steps per second: 148, episode reward: -142.410, mean reward: -1.017 [-100.000, 4.516], mean action: 1.850 [0.000, 3.000], mean observation: -0.022 [-1.822, 1.403], loss: 10.573125, mae: 46.884731, mean_q: 62.951633
 1087550/1100000: episode: 2544, duration: 2.494s, episode steps: 358, steps per second: 144, episode reward: 240.713, mean reward: 0.672 [-9.853, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.054 [-0.886, 1.400], loss: 10.724822, mae: 47.316994, mean_q: 63.362125
 1087707/1100000: episode: 2545, duration: 1.053s, episode steps: 157, steps per second: 149, episode reward: -120.895, mean reward: -0.770 [-100.000, 12.518], mean action: 1.471 [0.000, 3.000], mean observation: 0.020 [-0.765, 1.419], loss: 10.135698, mae: 47.773949, mean_q: 64.071297
 1087862/1100000: episode: 2546, duration: 1.045s, episode steps: 155, steps per second: 148, episode reward: 25.981, mean reward: 0.168 [-100.000, 18.805], mean action: 1.826 [0.000, 3.000], mean observation: 0.062 [-0.777, 1.408], loss: 25.813883, mae: 47.572609, mean_q: 63.742653
 1088141/1100000: episode: 2547, duration: 1.914s, episode steps: 279, steps per second: 146, episode reward: 254.088, mean reward: 0.911 [-3.531, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.085 [-1.062, 1.451], loss: 14.474611, mae: 47.351173, mean_q: 63.562431
 1088495/1100000: episode: 2548, duration: 2.433s, episode steps: 354, steps per second: 145, episode reward: 220.006, mean reward: 0.621 [-14.037, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.018 [-1.116, 1.478], loss: 10.273214, mae: 47.271770, mean_q: 63.303078
 1088617/1100000: episode: 2549, duration: 0.848s, episode steps: 122, steps per second: 144, episode reward: -2.996, mean reward: -0.025 [-100.000, 41.939], mean action: 1.713 [0.000, 3.000], mean observation: 0.086 [-0.818, 1.387], loss: 9.444537, mae: 46.400635, mean_q: 62.466347
 1088955/1100000: episode: 2550, duration: 2.460s, episode steps: 338, steps per second: 137, episode reward: 242.595, mean reward: 0.718 [-19.315, 100.000], mean action: 2.115 [0.000, 3.000], mean observation: 0.093 [-0.834, 1.402], loss: 10.113256, mae: 47.480839, mean_q: 63.668129
 1089311/1100000: episode: 2551, duration: 2.465s, episode steps: 356, steps per second: 144, episode reward: 234.205, mean reward: 0.658 [-17.278, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.100 [-0.849, 1.441], loss: 8.356375, mae: 47.040829, mean_q: 63.031075
 1089475/1100000: episode: 2552, duration: 1.103s, episode steps: 164, steps per second: 149, episode reward: -122.010, mean reward: -0.744 [-100.000, 7.424], mean action: 1.549 [0.000, 3.000], mean observation: 0.096 [-3.821, 1.448], loss: 21.611515, mae: 47.036781, mean_q: 63.176521
 1089759/1100000: episode: 2553, duration: 1.928s, episode steps: 284, steps per second: 147, episode reward: 276.437, mean reward: 0.973 [-19.897, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.109 [-1.066, 1.413], loss: 18.855658, mae: 47.358635, mean_q: 63.395851
 1090167/1100000: episode: 2554, duration: 2.888s, episode steps: 408, steps per second: 141, episode reward: 202.865, mean reward: 0.497 [-3.502, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: -0.064 [-0.655, 1.397], loss: 13.149309, mae: 47.102451, mean_q: 63.030224
 1090480/1100000: episode: 2555, duration: 2.134s, episode steps: 313, steps per second: 147, episode reward: -429.081, mean reward: -1.371 [-100.000, 17.373], mean action: 1.847 [0.000, 3.000], mean observation: -0.047 [-3.351, 2.161], loss: 10.649042, mae: 47.198318, mean_q: 63.299145
 1090650/1100000: episode: 2556, duration: 1.146s, episode steps: 170, steps per second: 148, episode reward: -1110.878, mean reward: -6.535 [-100.000, 4.543], mean action: 2.282 [0.000, 3.000], mean observation: -0.356 [-9.425, 2.361], loss: 11.330286, mae: 47.290882, mean_q: 63.288532
 1091018/1100000: episode: 2557, duration: 2.537s, episode steps: 368, steps per second: 145, episode reward: 243.017, mean reward: 0.660 [-16.659, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.217 [-1.070, 1.395], loss: 8.391351, mae: 47.165985, mean_q: 62.989281
 1091359/1100000: episode: 2558, duration: 2.372s, episode steps: 341, steps per second: 144, episode reward: -60.784, mean reward: -0.178 [-100.000, 12.850], mean action: 1.695 [0.000, 3.000], mean observation: 0.025 [-0.705, 1.444], loss: 10.098628, mae: 47.077301, mean_q: 63.105537
 1091553/1100000: episode: 2559, duration: 1.312s, episode steps: 194, steps per second: 148, episode reward: -57.311, mean reward: -0.295 [-100.000, 9.490], mean action: 1.722 [0.000, 3.000], mean observation: 0.037 [-1.154, 1.478], loss: 41.835602, mae: 47.637493, mean_q: 63.842644
 1092447/1100000: episode: 2560, duration: 6.907s, episode steps: 894, steps per second: 129, episode reward: 123.702, mean reward: 0.138 [-20.426, 100.000], mean action: 1.746 [0.000, 3.000], mean observation: 0.124 [-0.749, 1.423], loss: 9.296429, mae: 47.386036, mean_q: 63.297562
 1092550/1100000: episode: 2561, duration: 0.697s, episode steps: 103, steps per second: 148, episode reward: -102.793, mean reward: -0.998 [-100.000, 15.295], mean action: 1.757 [0.000, 3.000], mean observation: 0.138 [-1.179, 1.547], loss: 7.663971, mae: 47.281681, mean_q: 63.050129
 1093545/1100000: episode: 2562, duration: 7.437s, episode steps: 995, steps per second: 134, episode reward: 205.812, mean reward: 0.207 [-18.886, 100.000], mean action: 2.498 [0.000, 3.000], mean observation: 0.153 [-0.780, 1.422], loss: 8.663682, mae: 47.415867, mean_q: 63.166779
 1094293/1100000: episode: 2563, duration: 5.381s, episode steps: 748, steps per second: 139, episode reward: 207.853, mean reward: 0.278 [-18.829, 100.000], mean action: 0.753 [0.000, 3.000], mean observation: 0.116 [-1.585, 1.403], loss: 8.539474, mae: 47.351856, mean_q: 62.787651
 1095293/1100000: episode: 2564, duration: 7.713s, episode steps: 1000, steps per second: 130, episode reward: 107.452, mean reward: 0.107 [-21.051, 22.392], mean action: 1.017 [0.000, 3.000], mean observation: 0.233 [-0.692, 1.443], loss: 13.432118, mae: 47.468288, mean_q: 62.812378
 1096293/1100000: episode: 2565, duration: 7.535s, episode steps: 1000, steps per second: 133, episode reward: -103.081, mean reward: -0.103 [-4.766, 5.193], mean action: 1.740 [0.000, 3.000], mean observation: 0.002 [-0.714, 1.468], loss: 10.485975, mae: 47.618782, mean_q: 62.634880
 1096423/1100000: episode: 2566, duration: 0.872s, episode steps: 130, steps per second: 149, episode reward: -90.163, mean reward: -0.694 [-100.000, 8.563], mean action: 1.346 [0.000, 3.000], mean observation: -0.012 [-0.994, 1.445], loss: 10.294176, mae: 47.606655, mean_q: 62.955189
 1096843/1100000: episode: 2567, duration: 2.984s, episode steps: 420, steps per second: 141, episode reward: 203.153, mean reward: 0.484 [-9.877, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.039 [-1.008, 1.414], loss: 9.779790, mae: 48.624424, mean_q: 63.752457
 1097418/1100000: episode: 2568, duration: 4.190s, episode steps: 575, steps per second: 137, episode reward: 226.244, mean reward: 0.393 [-18.172, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.184 [-0.854, 1.392], loss: 10.031518, mae: 48.134632, mean_q: 63.504795
 1098041/1100000: episode: 2569, duration: 4.778s, episode steps: 623, steps per second: 130, episode reward: 199.018, mean reward: 0.319 [-18.531, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.164 [-0.791, 1.473], loss: 10.783954, mae: 48.451557, mean_q: 64.021217
 1098420/1100000: episode: 2570, duration: 2.637s, episode steps: 379, steps per second: 144, episode reward: 214.327, mean reward: 0.566 [-12.178, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.155 [-0.837, 1.388], loss: 9.124087, mae: 48.366207, mean_q: 63.677761
 1098727/1100000: episode: 2571, duration: 2.163s, episode steps: 307, steps per second: 142, episode reward: -39.499, mean reward: -0.129 [-100.000, 9.450], mean action: 1.850 [0.000, 3.000], mean observation: -0.046 [-0.811, 1.403], loss: 8.081100, mae: 48.556400, mean_q: 63.871513
 1099727/1100000: episode: 2572, duration: 7.297s, episode steps: 1000, steps per second: 137, episode reward: -25.894, mean reward: -0.026 [-12.735, 11.477], mean action: 1.762 [0.000, 3.000], mean observation: 0.096 [-0.608, 1.404], loss: 11.185101, mae: 48.214336, mean_q: 63.561382
done, took 7905.192 seconds
Testing for 1000 episodes ...
Episode 1: reward: 231.432, steps: 530
Episode 2: reward: 237.561, steps: 357
Episode 3: reward: 201.365, steps: 232
Episode 4: reward: 126.099, steps: 1000
Episode 5: reward: -603.246, steps: 88
Episode 6: reward: 247.281, steps: 426
Episode 7: reward: 210.218, steps: 413
Episode 8: reward: -28.697, steps: 324
Episode 9: reward: -207.229, steps: 1000
Episode 10: reward: 218.908, steps: 500
Episode 11: reward: -185.219, steps: 1000
Episode 12: reward: 188.732, steps: 222
Episode 13: reward: -34.688, steps: 162
Episode 14: reward: 266.768, steps: 429
Episode 15: reward: 237.162, steps: 391
Episode 16: reward: 301.329, steps: 277
Episode 17: reward: 180.517, steps: 673
Episode 18: reward: 242.113, steps: 433
Episode 19: reward: 157.405, steps: 743
Episode 20: reward: 91.687, steps: 859
Episode 21: reward: 224.391, steps: 320
Episode 22: reward: -250.973, steps: 246
Episode 23: reward: 243.219, steps: 351
Episode 24: reward: 240.002, steps: 388
Episode 25: reward: 231.427, steps: 347
Episode 26: reward: 145.649, steps: 1000
Episode 27: reward: 242.368, steps: 373
Episode 28: reward: 163.154, steps: 1000
Episode 29: reward: 210.679, steps: 326
Episode 30: reward: 206.770, steps: 324
Episode 31: reward: 236.132, steps: 367
Episode 32: reward: 193.357, steps: 293
Episode 33: reward: 248.800, steps: 474
Episode 34: reward: 249.332, steps: 354
Episode 35: reward: -217.001, steps: 1000
Episode 36: reward: 194.095, steps: 657
Episode 37: reward: 222.606, steps: 319
Episode 38: reward: 215.858, steps: 560
Episode 39: reward: 178.709, steps: 673
Episode 40: reward: -924.121, steps: 226
Episode 41: reward: 154.761, steps: 1000
Episode 42: reward: -161.123, steps: 1000
Episode 43: reward: -14.865, steps: 1000
Episode 44: reward: 250.030, steps: 389
Episode 45: reward: 205.753, steps: 520
Episode 46: reward: 269.377, steps: 347
Episode 47: reward: 87.922, steps: 1000
Episode 48: reward: 240.302, steps: 474
Episode 49: reward: 175.685, steps: 377
Episode 50: reward: 168.460, steps: 318
Episode 51: reward: 222.065, steps: 271
Episode 52: reward: 147.030, steps: 913
Episode 53: reward: 202.469, steps: 368
Episode 54: reward: 214.061, steps: 512
Episode 55: reward: 233.253, steps: 373
Episode 56: reward: 211.788, steps: 495
Episode 57: reward: 270.252, steps: 350
Episode 58: reward: 164.282, steps: 334
Episode 59: reward: 199.253, steps: 249
Episode 60: reward: 220.818, steps: 339
Episode 61: reward: 247.683, steps: 240
Episode 62: reward: 1.396, steps: 1000
Episode 63: reward: -169.613, steps: 1000
Episode 64: reward: 243.650, steps: 444
Episode 65: reward: -36.679, steps: 211
Episode 66: reward: -278.103, steps: 444
Episode 67: reward: -543.634, steps: 164
Episode 68: reward: 213.340, steps: 332
Episode 69: reward: -531.704, steps: 86
Episode 70: reward: 136.191, steps: 767
Episode 71: reward: 197.273, steps: 475
Episode 72: reward: 100.088, steps: 1000
Episode 73: reward: 213.001, steps: 349
Episode 74: reward: 277.142, steps: 258
Episode 75: reward: 233.543, steps: 372
Episode 76: reward: 189.733, steps: 401
Episode 77: reward: 229.419, steps: 445
Episode 78: reward: 201.836, steps: 238
Episode 79: reward: -228.716, steps: 219
Episode 80: reward: 255.311, steps: 212
Episode 81: reward: -578.116, steps: 152
Episode 82: reward: 193.042, steps: 234
Episode 83: reward: 195.991, steps: 320
Episode 84: reward: 238.859, steps: 264
Episode 85: reward: -1411.773, steps: 290
Episode 86: reward: -228.808, steps: 210
Episode 87: reward: 189.420, steps: 641
Episode 88: reward: -140.799, steps: 362
Episode 89: reward: 166.155, steps: 1000
Episode 90: reward: -731.001, steps: 108
Episode 91: reward: 269.416, steps: 285
Episode 92: reward: 253.561, steps: 428
Episode 93: reward: 193.118, steps: 409
Episode 94: reward: 242.033, steps: 382
Episode 95: reward: 194.566, steps: 623
Episode 96: reward: -498.605, steps: 255
Episode 97: reward: -148.400, steps: 1000
Episode 98: reward: 211.363, steps: 442
Episode 99: reward: 239.245, steps: 239
Episode 100: reward: 203.399, steps: 369
Episode 101: reward: 184.800, steps: 419
Episode 102: reward: -192.249, steps: 1000
Episode 103: reward: -33.478, steps: 1000
Episode 104: reward: 220.859, steps: 449
Episode 105: reward: 266.470, steps: 327
Episode 106: reward: -185.304, steps: 1000
Episode 107: reward: 88.882, steps: 1000
Episode 108: reward: 204.516, steps: 731
Episode 109: reward: 200.131, steps: 212
Episode 110: reward: -1856.420, steps: 405
Episode 111: reward: 263.437, steps: 314
Episode 112: reward: -140.767, steps: 1000
Episode 113: reward: 103.374, steps: 1000
Episode 114: reward: -166.868, steps: 370
Episode 115: reward: 101.210, steps: 648
Episode 116: reward: -198.453, steps: 1000
Episode 117: reward: 234.456, steps: 436
Episode 118: reward: 21.714, steps: 1000
Episode 119: reward: -209.918, steps: 1000
Episode 120: reward: 251.293, steps: 384
Episode 121: reward: 197.114, steps: 426
Episode 122: reward: 193.423, steps: 599
Episode 123: reward: 56.015, steps: 1000
Episode 124: reward: 226.065, steps: 289
Episode 125: reward: 27.031, steps: 313
Episode 126: reward: 168.207, steps: 572
Episode 127: reward: 212.547, steps: 387
Episode 128: reward: -525.879, steps: 258
Episode 129: reward: -160.719, steps: 1000
Episode 130: reward: 228.063, steps: 392
Episode 131: reward: 233.625, steps: 350
Episode 132: reward: 206.875, steps: 278
Episode 133: reward: 216.237, steps: 295
Episode 134: reward: 184.822, steps: 508
Episode 135: reward: -141.928, steps: 1000
Episode 136: reward: -152.026, steps: 1000
Episode 137: reward: 185.654, steps: 686
Episode 138: reward: 229.385, steps: 512
Episode 139: reward: 259.876, steps: 353
Episode 140: reward: 195.043, steps: 261
Episode 141: reward: 227.009, steps: 372
Episode 142: reward: -21.184, steps: 1000
Episode 143: reward: 193.058, steps: 198
Episode 144: reward: 239.182, steps: 360
Episode 145: reward: 229.601, steps: 332
Episode 146: reward: -513.456, steps: 182
Episode 147: reward: 201.509, steps: 442
Episode 148: reward: -1062.360, steps: 130
Episode 149: reward: -501.766, steps: 176
Episode 150: reward: 200.605, steps: 351
Episode 151: reward: 236.918, steps: 226
Episode 152: reward: 142.995, steps: 539
Episode 153: reward: -859.667, steps: 114
Episode 154: reward: 49.004, steps: 237
Episode 155: reward: 194.470, steps: 262
Episode 156: reward: 191.425, steps: 647
Episode 157: reward: 240.995, steps: 373
Episode 158: reward: 203.160, steps: 551
Episode 159: reward: 162.412, steps: 718
Episode 160: reward: 2.742, steps: 1000
Episode 161: reward: 247.868, steps: 361
Episode 162: reward: 149.430, steps: 575
Episode 163: reward: -160.406, steps: 1000
Episode 164: reward: -36.834, steps: 1000
Episode 165: reward: 235.373, steps: 418
Episode 166: reward: 169.162, steps: 1000
Episode 167: reward: 241.549, steps: 457
Episode 168: reward: -507.300, steps: 191
Episode 169: reward: 251.488, steps: 367
Episode 170: reward: 13.062, steps: 320
Episode 171: reward: 263.571, steps: 369
Episode 172: reward: 202.612, steps: 557
Episode 173: reward: 187.519, steps: 644
Episode 174: reward: 205.096, steps: 419
Episode 175: reward: 145.241, steps: 1000
Episode 176: reward: -190.863, steps: 1000
Episode 177: reward: -182.789, steps: 1000
Episode 178: reward: 268.713, steps: 362
Episode 179: reward: 241.347, steps: 245
Episode 180: reward: 227.726, steps: 407
Episode 181: reward: -540.088, steps: 193
Episode 182: reward: 204.315, steps: 490
Episode 183: reward: 215.581, steps: 260
Episode 184: reward: -218.150, steps: 1000
Episode 185: reward: 203.605, steps: 179
Episode 186: reward: 40.873, steps: 288
Episode 187: reward: 181.924, steps: 394
Episode 188: reward: 200.904, steps: 289
Episode 189: reward: 193.781, steps: 544
Episode 190: reward: 140.372, steps: 1000
Episode 191: reward: -631.456, steps: 101
Episode 192: reward: -136.773, steps: 1000
Episode 193: reward: 114.518, steps: 1000
Episode 194: reward: 169.848, steps: 1000
Episode 195: reward: 201.925, steps: 273
Episode 196: reward: 185.962, steps: 344
Episode 197: reward: -5.742, steps: 1000
Episode 198: reward: 232.655, steps: 350
Episode 199: reward: -209.694, steps: 1000
Episode 200: reward: 140.736, steps: 813
Episode 201: reward: -181.666, steps: 1000
Episode 202: reward: 22.147, steps: 1000
Episode 203: reward: 252.247, steps: 401
Episode 204: reward: 219.476, steps: 489
Episode 205: reward: 268.713, steps: 291
Episode 206: reward: 172.505, steps: 641
Episode 207: reward: 252.617, steps: 494
Episode 208: reward: 225.360, steps: 568
Episode 209: reward: 241.566, steps: 236
Episode 210: reward: 177.164, steps: 292
Episode 211: reward: 270.904, steps: 429
Episode 212: reward: 201.579, steps: 646
Episode 213: reward: -504.624, steps: 148
Episode 214: reward: 232.923, steps: 424
Episode 215: reward: 253.792, steps: 245
Episode 216: reward: -289.775, steps: 254
Episode 217: reward: 289.740, steps: 371
Episode 218: reward: -669.351, steps: 328
Episode 219: reward: -629.468, steps: 89
Episode 220: reward: -928.361, steps: 179
Episode 221: reward: 180.090, steps: 349
Episode 222: reward: 151.928, steps: 1000
Episode 223: reward: 229.440, steps: 345
Episode 224: reward: 150.792, steps: 489
Episode 225: reward: 187.414, steps: 402
Episode 226: reward: -276.274, steps: 315
Episode 227: reward: 241.918, steps: 266
Episode 228: reward: 219.390, steps: 504
Episode 229: reward: -432.220, steps: 142
Episode 230: reward: 203.671, steps: 369
Episode 231: reward: 216.345, steps: 408
Episode 232: reward: 171.711, steps: 664
Episode 233: reward: 265.707, steps: 343
Episode 234: reward: 233.318, steps: 523
Episode 235: reward: 211.299, steps: 225
Episode 236: reward: 190.975, steps: 631
Episode 237: reward: 200.108, steps: 670
Episode 238: reward: 260.216, steps: 355
Episode 239: reward: -161.432, steps: 1000
Episode 240: reward: -1.358, steps: 1000
Episode 241: reward: 171.054, steps: 316
Episode 242: reward: 233.944, steps: 394
Episode 243: reward: 242.406, steps: 314
Episode 244: reward: 266.391, steps: 371
Episode 245: reward: -11.833, steps: 1000
Episode 246: reward: 229.110, steps: 494
Episode 247: reward: 282.950, steps: 372
Episode 248: reward: -13.835, steps: 1000
Episode 249: reward: 204.913, steps: 534
Episode 250: reward: 256.576, steps: 224
Episode 251: reward: 242.501, steps: 384
Episode 252: reward: 227.674, steps: 255
Episode 253: reward: -170.855, steps: 1000
Episode 254: reward: 214.268, steps: 470
Episode 255: reward: 264.880, steps: 457
Episode 256: reward: -169.738, steps: 1000
Episode 257: reward: 231.309, steps: 411
Episode 258: reward: -154.599, steps: 1000
Episode 259: reward: 168.684, steps: 394
Episode 260: reward: 248.340, steps: 396
Episode 261: reward: 32.558, steps: 1000
Episode 262: reward: 257.132, steps: 309
Episode 263: reward: -25.711, steps: 524
Episode 264: reward: -621.127, steps: 98
Episode 265: reward: 184.101, steps: 315
Episode 266: reward: -186.373, steps: 1000
Episode 267: reward: -576.893, steps: 304
Episode 268: reward: 221.828, steps: 573
Episode 269: reward: 204.547, steps: 514
Episode 270: reward: -522.634, steps: 226
Episode 271: reward: 139.915, steps: 1000
Episode 272: reward: -206.696, steps: 1000
Episode 273: reward: 242.840, steps: 371
Episode 274: reward: 199.675, steps: 595
Episode 275: reward: 230.324, steps: 252
Episode 276: reward: 257.746, steps: 369
Episode 277: reward: 207.325, steps: 467
Episode 278: reward: -140.747, steps: 667
Episode 279: reward: 0.219, steps: 1000
Episode 280: reward: 232.960, steps: 337
Episode 281: reward: 183.777, steps: 470
Episode 282: reward: -497.626, steps: 404
Episode 283: reward: 182.856, steps: 632
Episode 284: reward: 223.468, steps: 624
Episode 285: reward: 181.471, steps: 239
Episode 286: reward: 211.527, steps: 437
Episode 287: reward: 208.940, steps: 340
Episode 288: reward: -182.316, steps: 1000
Episode 289: reward: 244.922, steps: 460
Episode 290: reward: 250.912, steps: 253
Episode 291: reward: 222.071, steps: 526
Episode 292: reward: 228.303, steps: 256
Episode 293: reward: 178.543, steps: 301
Episode 294: reward: 286.500, steps: 380
Episode 295: reward: 246.213, steps: 324
Episode 296: reward: -415.796, steps: 411
Episode 297: reward: 295.282, steps: 233
Episode 298: reward: -192.793, steps: 1000
Episode 299: reward: 187.167, steps: 568
Episode 300: reward: 3.024, steps: 1000
Episode 301: reward: 111.049, steps: 1000
Episode 302: reward: -816.657, steps: 337
Episode 303: reward: -150.857, steps: 1000
Episode 304: reward: -527.898, steps: 172
Episode 305: reward: 259.089, steps: 318
Episode 306: reward: 158.735, steps: 723
Episode 307: reward: 122.252, steps: 1000
Episode 308: reward: 239.771, steps: 338
Episode 309: reward: -181.718, steps: 1000
Episode 310: reward: 208.655, steps: 462
Episode 311: reward: 188.202, steps: 443
Episode 312: reward: 199.914, steps: 730
Episode 313: reward: 225.608, steps: 366
Episode 314: reward: 260.375, steps: 314
Episode 315: reward: 202.733, steps: 480
Episode 316: reward: -98.478, steps: 1000
Episode 317: reward: -63.616, steps: 156
Episode 318: reward: 216.089, steps: 289
Episode 319: reward: 219.986, steps: 579
Episode 320: reward: 286.960, steps: 330
Episode 321: reward: -185.071, steps: 1000
Episode 322: reward: 247.050, steps: 476
Episode 323: reward: -544.006, steps: 79
Episode 324: reward: 26.883, steps: 320
Episode 325: reward: 181.713, steps: 646
Episode 326: reward: 187.680, steps: 604
Episode 327: reward: 226.410, steps: 567
Episode 328: reward: 231.829, steps: 333
Episode 329: reward: 149.854, steps: 862
Episode 330: reward: 232.240, steps: 357
Episode 331: reward: -593.493, steps: 159
Episode 332: reward: -7.554, steps: 378
Episode 333: reward: -41.752, steps: 1000
Episode 334: reward: 231.104, steps: 456
Episode 335: reward: 5.161, steps: 1000
Episode 336: reward: -49.575, steps: 258
Episode 337: reward: -543.338, steps: 230
Episode 338: reward: 39.779, steps: 1000
Episode 339: reward: 133.060, steps: 509
Episode 340: reward: 160.306, steps: 697
Episode 341: reward: 260.025, steps: 424
Episode 342: reward: 216.775, steps: 554
Episode 343: reward: -39.500, steps: 1000
Episode 344: reward: -30.969, steps: 1000
Episode 345: reward: 179.477, steps: 455
Episode 346: reward: 93.202, steps: 903
Episode 347: reward: -74.698, steps: 1000
Episode 348: reward: 200.607, steps: 464
Episode 349: reward: 260.817, steps: 242
Episode 350: reward: 213.239, steps: 495
Episode 351: reward: 206.410, steps: 465
Episode 352: reward: 234.633, steps: 443
Episode 353: reward: 206.646, steps: 457
Episode 354: reward: 194.526, steps: 449
Episode 355: reward: 216.964, steps: 501
Episode 356: reward: 99.646, steps: 937
Episode 357: reward: 192.426, steps: 201
Episode 358: reward: 217.386, steps: 608
Episode 359: reward: -180.248, steps: 1000
Episode 360: reward: 13.778, steps: 196
Episode 361: reward: 137.156, steps: 1000
Episode 362: reward: 153.021, steps: 708
Episode 363: reward: -174.861, steps: 1000
Episode 364: reward: 207.853, steps: 779
Episode 365: reward: 192.202, steps: 750
Episode 366: reward: 212.590, steps: 282
Episode 367: reward: -233.223, steps: 1000
Episode 368: reward: 185.180, steps: 625
Episode 369: reward: 235.573, steps: 347
Episode 370: reward: 41.384, steps: 255
Episode 371: reward: -170.768, steps: 1000
Episode 372: reward: 154.473, steps: 321
Episode 373: reward: 242.148, steps: 453
Episode 374: reward: -198.929, steps: 1000
Episode 375: reward: 234.928, steps: 385
Episode 376: reward: 213.109, steps: 293
Episode 377: reward: 169.505, steps: 409
Episode 378: reward: 162.821, steps: 754
Episode 379: reward: -895.070, steps: 122
Episode 380: reward: 215.818, steps: 473
Episode 381: reward: 218.695, steps: 448
Episode 382: reward: 237.040, steps: 245
Episode 383: reward: 247.395, steps: 230
Episode 384: reward: 216.961, steps: 507
Episode 385: reward: 261.773, steps: 301
Episode 386: reward: -800.611, steps: 124
Episode 387: reward: 223.622, steps: 302
Episode 388: reward: 133.928, steps: 713
Episode 389: reward: 202.201, steps: 247
Episode 390: reward: 221.514, steps: 322
Episode 391: reward: 202.647, steps: 532
Episode 392: reward: -181.589, steps: 1000
Episode 393: reward: -553.557, steps: 162
Episode 394: reward: 214.999, steps: 278
Episode 395: reward: 220.952, steps: 186
Episode 396: reward: 220.127, steps: 266
Episode 397: reward: 203.848, steps: 279
Episode 398: reward: -223.208, steps: 1000
Episode 399: reward: 189.379, steps: 608
Episode 400: reward: -526.463, steps: 241
Episode 401: reward: 190.204, steps: 243
Episode 402: reward: -2.250, steps: 1000
Episode 403: reward: 144.983, steps: 703
Episode 404: reward: 194.703, steps: 216
Episode 405: reward: 304.973, steps: 239
Episode 406: reward: 197.091, steps: 455
Episode 407: reward: 235.231, steps: 283
Episode 408: reward: 161.934, steps: 596
Episode 409: reward: 224.004, steps: 298
Episode 410: reward: 261.395, steps: 433
Episode 411: reward: 198.362, steps: 618
Episode 412: reward: 149.740, steps: 339
Episode 413: reward: 188.125, steps: 329
Episode 414: reward: 279.596, steps: 240
Episode 415: reward: 217.489, steps: 384
Episode 416: reward: 169.924, steps: 699
Episode 417: reward: -196.835, steps: 1000
Episode 418: reward: 230.521, steps: 347
Episode 419: reward: 176.275, steps: 650
Episode 420: reward: -133.507, steps: 1000
Episode 421: reward: 219.233, steps: 286
Episode 422: reward: 104.132, steps: 1000
Episode 423: reward: 34.947, steps: 335
Episode 424: reward: 210.282, steps: 451
Episode 425: reward: 10.295, steps: 1000
Episode 426: reward: 149.223, steps: 615
Episode 427: reward: 59.828, steps: 1000
Episode 428: reward: 301.304, steps: 258
Episode 429: reward: -181.123, steps: 213
Episode 430: reward: 234.972, steps: 431
Episode 431: reward: 177.841, steps: 507
Episode 432: reward: 237.955, steps: 304
Episode 433: reward: 299.235, steps: 275
Episode 434: reward: 198.500, steps: 541
Episode 435: reward: -180.592, steps: 1000
Episode 436: reward: 187.845, steps: 451
Episode 437: reward: 198.623, steps: 598
Episode 438: reward: 221.247, steps: 422
Episode 439: reward: 196.711, steps: 688
Episode 440: reward: -211.097, steps: 1000
Episode 441: reward: 206.526, steps: 426
Episode 442: reward: -194.496, steps: 1000
Episode 443: reward: 203.761, steps: 534
Episode 444: reward: -940.637, steps: 182
Episode 445: reward: 168.782, steps: 282
Episode 446: reward: -24.021, steps: 1000
Episode 447: reward: 285.821, steps: 363
Episode 448: reward: 250.828, steps: 375
Episode 449: reward: -141.280, steps: 1000
Episode 450: reward: -169.232, steps: 1000
Episode 451: reward: -626.421, steps: 85
Episode 452: reward: -506.900, steps: 78
Episode 453: reward: 250.488, steps: 331
Episode 454: reward: 148.604, steps: 755
Episode 455: reward: 120.385, steps: 998
Episode 456: reward: 219.134, steps: 250
Episode 457: reward: 116.134, steps: 1000
Episode 458: reward: -613.140, steps: 89
Episode 459: reward: -140.187, steps: 1000
Episode 460: reward: 234.512, steps: 373
Episode 461: reward: 127.515, steps: 760
Episode 462: reward: -188.990, steps: 1000
Episode 463: reward: 177.589, steps: 531
Episode 464: reward: 265.904, steps: 369
Episode 465: reward: 173.209, steps: 512
Episode 466: reward: -2085.477, steps: 315
Episode 467: reward: 181.218, steps: 568
Episode 468: reward: 179.776, steps: 293
Episode 469: reward: 237.143, steps: 351
Episode 470: reward: -22.598, steps: 1000
Episode 471: reward: -1594.260, steps: 198
Episode 472: reward: -25.231, steps: 1000
Episode 473: reward: 201.936, steps: 628
Episode 474: reward: 250.643, steps: 316
Episode 475: reward: -179.916, steps: 1000
Episode 476: reward: 231.871, steps: 257
Episode 477: reward: 177.269, steps: 599
Episode 478: reward: 228.448, steps: 218
Episode 479: reward: 227.732, steps: 252
Episode 480: reward: 188.186, steps: 375
Episode 481: reward: 259.395, steps: 272
Episode 482: reward: 196.639, steps: 633
Episode 483: reward: -213.358, steps: 1000
Episode 484: reward: 262.688, steps: 338
Episode 485: reward: 180.659, steps: 528
Episode 486: reward: 152.827, steps: 684
Episode 487: reward: 188.210, steps: 451
Episode 488: reward: 252.567, steps: 351
Episode 489: reward: 187.999, steps: 606
Episode 490: reward: 185.717, steps: 673
Episode 491: reward: 220.196, steps: 268
Episode 492: reward: 214.572, steps: 583
Episode 493: reward: 213.196, steps: 470
Episode 494: reward: 227.845, steps: 464
Episode 495: reward: 219.350, steps: 205
Episode 496: reward: 198.448, steps: 511
Episode 497: reward: 3.175, steps: 1000
Episode 498: reward: 208.979, steps: 476
Episode 499: reward: 130.317, steps: 1000
Episode 500: reward: 103.902, steps: 976
Episode 501: reward: 145.129, steps: 877
Episode 502: reward: 197.827, steps: 452
Episode 503: reward: 289.342, steps: 244
Episode 504: reward: 207.025, steps: 507
Episode 505: reward: 251.451, steps: 349
Episode 506: reward: 215.627, steps: 397
Episode 507: reward: 230.981, steps: 231
Episode 508: reward: 210.562, steps: 336
Episode 509: reward: 120.204, steps: 368
Episode 510: reward: -57.215, steps: 1000
Episode 511: reward: -208.510, steps: 1000
Episode 512: reward: 97.891, steps: 987
Episode 513: reward: 206.449, steps: 385
Episode 514: reward: -117.740, steps: 377
Episode 515: reward: 295.205, steps: 356
Episode 516: reward: 218.860, steps: 345
Episode 517: reward: 195.705, steps: 500
Episode 518: reward: -36.169, steps: 1000
Episode 519: reward: -79.328, steps: 646
Episode 520: reward: 138.338, steps: 859
Episode 521: reward: 199.677, steps: 285
Episode 522: reward: 232.964, steps: 417
Episode 523: reward: 191.637, steps: 395
Episode 524: reward: 180.473, steps: 751
Episode 525: reward: 284.083, steps: 346
Episode 526: reward: -51.252, steps: 159
Episode 527: reward: -530.353, steps: 80
Episode 528: reward: 156.199, steps: 917
Episode 529: reward: 258.983, steps: 417
Episode 530: reward: 203.132, steps: 756
Episode 531: reward: 237.477, steps: 329
Episode 532: reward: 206.198, steps: 308
Episode 533: reward: 267.217, steps: 370
Episode 534: reward: -219.791, steps: 1000
Episode 535: reward: 228.089, steps: 391
Episode 536: reward: 197.696, steps: 681
Episode 537: reward: -167.659, steps: 1000
Episode 538: reward: 170.466, steps: 339
Episode 539: reward: 231.396, steps: 604
Episode 540: reward: -210.471, steps: 378
Episode 541: reward: 196.956, steps: 616
Episode 542: reward: 188.211, steps: 299
Episode 543: reward: 247.084, steps: 283
Episode 544: reward: 165.363, steps: 640
Episode 545: reward: 214.339, steps: 295
Episode 546: reward: 217.180, steps: 537
Episode 547: reward: 236.876, steps: 320
Episode 548: reward: 202.225, steps: 502
Episode 549: reward: 23.136, steps: 1000
Episode 550: reward: 188.985, steps: 500
Episode 551: reward: 177.812, steps: 299
Episode 552: reward: 192.640, steps: 499
Episode 553: reward: 286.104, steps: 405
Episode 554: reward: 194.759, steps: 582
Episode 555: reward: 162.397, steps: 841
Episode 556: reward: -199.108, steps: 1000
Episode 557: reward: 185.118, steps: 398
Episode 558: reward: -429.941, steps: 141
Episode 559: reward: 296.190, steps: 231
Episode 560: reward: 196.054, steps: 489
Episode 561: reward: 200.819, steps: 595
Episode 562: reward: 237.916, steps: 334
Episode 563: reward: 209.261, steps: 202
Episode 564: reward: 207.096, steps: 267
Episode 565: reward: -652.299, steps: 159
Episode 566: reward: -725.241, steps: 101
Episode 567: reward: -205.259, steps: 1000
Episode 568: reward: 238.197, steps: 412
Episode 569: reward: -28.490, steps: 1000
Episode 570: reward: 223.208, steps: 813
Episode 571: reward: 254.015, steps: 486
Episode 572: reward: -7.753, steps: 163
Episode 573: reward: 262.074, steps: 363
Episode 574: reward: 259.719, steps: 383
Episode 575: reward: 208.654, steps: 424
Episode 576: reward: 118.946, steps: 1000
Episode 577: reward: -222.707, steps: 304
Episode 578: reward: 235.507, steps: 434
Episode 579: reward: 219.260, steps: 574
Episode 580: reward: 196.238, steps: 540
Episode 581: reward: 131.341, steps: 1000
Episode 582: reward: 249.599, steps: 223
Episode 583: reward: 214.838, steps: 282
Episode 584: reward: 212.963, steps: 277
Episode 585: reward: 197.913, steps: 632
Episode 586: reward: 167.869, steps: 706
Episode 587: reward: -129.779, steps: 1000
Episode 588: reward: -578.145, steps: 457
Episode 589: reward: 239.326, steps: 313
Episode 590: reward: -690.922, steps: 221
Episode 591: reward: -747.751, steps: 114
Episode 592: reward: -724.449, steps: 253
Episode 593: reward: 260.232, steps: 245
Episode 594: reward: 225.869, steps: 557
Episode 595: reward: 185.910, steps: 430
Episode 596: reward: -58.258, steps: 1000
Episode 597: reward: 212.246, steps: 710
Episode 598: reward: -774.551, steps: 227
Episode 599: reward: 3.890, steps: 1000
Episode 600: reward: 168.954, steps: 191
Episode 601: reward: 191.108, steps: 468
Episode 602: reward: 212.700, steps: 401
Episode 603: reward: 250.969, steps: 354
Episode 604: reward: 36.395, steps: 235
Episode 605: reward: -182.574, steps: 1000
Episode 606: reward: -596.654, steps: 148
Episode 607: reward: -249.554, steps: 1000
Episode 608: reward: 236.010, steps: 406
Episode 609: reward: 11.998, steps: 189
Episode 610: reward: 251.622, steps: 306
Episode 611: reward: 185.245, steps: 512
Episode 612: reward: 37.393, steps: 256
Episode 613: reward: 197.644, steps: 373
Episode 614: reward: -198.727, steps: 1000
Episode 615: reward: 88.176, steps: 1000
Episode 616: reward: 238.570, steps: 324
Episode 617: reward: 115.741, steps: 987
Episode 618: reward: 14.259, steps: 1000
Episode 619: reward: -153.374, steps: 1000
Episode 620: reward: -203.083, steps: 1000
Episode 621: reward: 234.089, steps: 210
Episode 622: reward: 201.531, steps: 374
Episode 623: reward: 185.951, steps: 245
Episode 624: reward: -217.879, steps: 1000
Episode 625: reward: 168.920, steps: 726
Episode 626: reward: 271.771, steps: 327
Episode 627: reward: 193.987, steps: 335
Episode 628: reward: 236.966, steps: 382
Episode 629: reward: 222.445, steps: 484
Episode 630: reward: 41.249, steps: 1000
Episode 631: reward: 260.393, steps: 335
Episode 632: reward: 220.750, steps: 358
Episode 633: reward: -26.987, steps: 463
Episode 634: reward: 246.007, steps: 387
Episode 635: reward: -36.843, steps: 1000
Episode 636: reward: 193.538, steps: 567
Episode 637: reward: 212.360, steps: 497
Episode 638: reward: 227.234, steps: 426
Episode 639: reward: 226.716, steps: 439
Episode 640: reward: 208.859, steps: 295
Episode 641: reward: 182.014, steps: 404
Episode 642: reward: 108.797, steps: 1000
Episode 643: reward: 188.446, steps: 302
Episode 644: reward: 164.861, steps: 970
Episode 645: reward: 165.925, steps: 751
Episode 646: reward: 18.122, steps: 1000
Episode 647: reward: 108.404, steps: 768
Episode 648: reward: -139.632, steps: 511
Episode 649: reward: 217.837, steps: 267
Episode 650: reward: 204.539, steps: 453
Episode 651: reward: -45.274, steps: 1000
Episode 652: reward: -65.173, steps: 224
Episode 653: reward: 193.890, steps: 508
Episode 654: reward: 198.729, steps: 546
Episode 655: reward: -3.554, steps: 1000
Episode 656: reward: -826.094, steps: 184
Episode 657: reward: 203.296, steps: 288
Episode 658: reward: -13.518, steps: 1000
Episode 659: reward: 17.009, steps: 1000
Episode 660: reward: 214.872, steps: 469
Episode 661: reward: -198.138, steps: 1000
Episode 662: reward: -156.830, steps: 1000
Episode 663: reward: 274.582, steps: 363
Episode 664: reward: 208.177, steps: 302
Episode 665: reward: 194.838, steps: 368
Episode 666: reward: 167.028, steps: 789
Episode 667: reward: -170.246, steps: 1000
Episode 668: reward: 190.616, steps: 205
Episode 669: reward: 232.473, steps: 203
Episode 670: reward: 174.199, steps: 527
Episode 671: reward: -201.683, steps: 1000
Episode 672: reward: 223.260, steps: 483
Episode 673: reward: -178.783, steps: 1000
Episode 674: reward: 205.719, steps: 436
Episode 675: reward: 90.779, steps: 841
Episode 676: reward: -676.824, steps: 112
Episode 677: reward: 124.049, steps: 1000
Episode 678: reward: -168.960, steps: 1000
Episode 679: reward: 187.480, steps: 548
Episode 680: reward: 148.914, steps: 332
Episode 681: reward: 137.583, steps: 1000
Episode 682: reward: 181.267, steps: 299
Episode 683: reward: 235.108, steps: 336
Episode 684: reward: -160.522, steps: 1000
Episode 685: reward: 221.787, steps: 497
Episode 686: reward: 191.732, steps: 670
Episode 687: reward: 217.248, steps: 443
Episode 688: reward: -262.742, steps: 369
Episode 689: reward: 188.796, steps: 495
Episode 690: reward: 193.938, steps: 399
Episode 691: reward: 280.619, steps: 242
Traceback (most recent call last):
  File "/home/matheus/tcc_implementation/agents/dqn/keras_rl_train_dqn_agent.py", line 41, in <module>
    dqn.test(env, nb_episodes=1000, visualize=True)
  File "/home/matheus/tcc_implementation/keras-rl/rl/core.py", line 353, in test
    callbacks.on_action_end(action)
  File "/home/matheus/tcc_implementation/keras-rl/rl/callbacks.py", line 101, in on_action_end
    callback.on_action_end(action, logs=logs)
  File "/home/matheus/tcc_implementation/keras-rl/rl/callbacks.py", line 366, in on_action_end
    self.env.render(mode='human')
  File "/home/matheus/tcc_implementation/gym/gym/core.py", line 233, in render
    return self.env.render(mode, **kwargs)
  File "/home/matheus/tcc_implementation/gym/gym/envs/box2d/lunar_lander.py", line 365, in render
    return self.viewer.render(return_rgb_array = mode=='rgb_array')
  File "/home/matheus/tcc_implementation/gym/gym/envs/classic_control/rendering.py", line 92, in render
    self.window.clear()
  File "/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/pyglet/window/__init__.py", line 1228, in clear
    gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)
  File "/home/matheus/tcc_implementation/venv/lib/python3.7/site-packages/pyglet/gl/lib.py", line 85, in errcheck
    def errcheck(result, func, arguments):
KeyboardInterrupt

Process finished with exit code 1
